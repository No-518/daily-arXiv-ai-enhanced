<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.AI](#cs.AI) [Total: 89]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT is a differentiable framework that combines reaction-diffusion tumor growth models with standard-of-care interventions and patient-specific factors to predict post-treatment tumor structure on imaging.


<details>
  <summary>Details</summary>
Motivation: Current reaction-diffusion models fail to capture tumor dynamics under heterogeneous therapeutic paradigms, creating a need for computational frameworks that can realistically simulate standard-of-care interventions while accounting for inter-patient variability.

Method: The framework unifies reaction-diffusion tumor growth models with discrete SoC interventions (surgery, chemotherapy, radiotherapy) and incorporates genomic and demographic personalization. It uses an implicit-explicit exponential time-differencing solver (IMEX-SoC) for stability, positivity, and scalability.

Result: SoC-DT consistently outperforms classical PDE baselines and purely data-driven neural models in predicting tumor dynamics on both synthetic data and real-world glioma data.

Conclusion: SoC-DT establishes a principled foundation for patient-specific digital twins in oncology by bridging mechanistic interpretability with modern differentiable solvers, enabling biologically consistent tumor dynamics estimation.

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: A hybrid framework combining distributed multi-GPU inference with interactive visualization for analyzing celebrity dynamics in video content, enabling scalable processing and multi-dimensional insights.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of understanding video structure and dynamics in an era dominated by video content, particularly for analyzing celebrity appearances and relationships.

Method: Combines distributed multi-GPU inference system with interactive visualization platform, using optimized ONNX models, heterogeneous batch inference, and high-throughput parallelism for scalable video processing.

Result: Generates comprehensive visualizations including appearance frequency charts, duration analyses, co-appearance matrices, network graphs, and heatmaps that reveal patterns in celebrity prominence, screen-time distribution, and temporal dynamics.

Conclusion: The framework bridges distributed recognition with structured analytics, enabling new possibilities for entertainment analytics, content creation strategies, and audience engagement studies through interactive exploration of video content.

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: This paper benchmarks deep learning models for cross-domain underwater plastic debris detection, finding that lightweight CNNs like MobileNetV2 outperform larger models and zero-shot approaches in cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Marine plastic pollution requires reliable automation for detection, but vision systems trained on one dataset often degrade on new imagery due to domain shift, necessitating robust cross-domain models.

Method: Benchmarked CNN models (MobileNetV2, ResNet-18, EfficientNet-B0) and vision transformers (DeiT-Tiny, ViT-B16) trained on labeled underwater data, plus zero-shot models (CLIP ViT-L14, Gemini 2.0 Flash) evaluated on cross-domain test sets with plastic-positive images from different sources.

Result: MobileNetV2 achieved strongest cross-domain performance (F1 0.97). All fine-tuned models had high Precision (~99%) but varied Recall. CLIP showed high Recall (~80%) but low Precision (~56%), while Gemini had high Precision (~99%) but lower Recall (~81%).

Conclusion: Compact CNNs with supervised training generalize effectively for cross-domain underwater detection, while large pretrained vision-language models provide complementary strengths, with error patterns showing confusions with coral textures, particulates, and glare.

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP is an Arabic image captioning framework that combines CLIP-based visual label retrieval with multimodal text generation, achieving state-of-the-art results through interpretable Arabic visual concepts.


<details>
  <summary>Details</summary>
Motivation: To create culturally coherent and contextually accurate Arabic image captions by grounding generation in interpretable visual concepts rather than relying solely on end-to-end captioning approaches.

Method: Two-stage pipeline: 1) Extract Arabic visual concepts using three multilingual encoders (mCLIP, AraCLIP, Jina V4) for label retrieval from hybrid vocabulary; 2) Transform top-k labels into Arabic prompts and generate captions using Qwen-VL and Gemini Pro Vision models.

Result: mCLIP + Gemini Pro Vision achieved best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained highest LLM-judge score (36.33%) across six encoder-decoder configurations.

Conclusion: The interpretable pipeline successfully enables culturally coherent and contextually accurate Arabic captions, demonstrating the effectiveness of combining visual concept retrieval with multimodal generation for Arabic image captioning.

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: A controlled comparison of EfficientNet-B0 (CNN) and ViT-Base (Vision Transformer) on SpaceNet dataset shows both achieve ~93% accuracy on imbalanced data, with EfficientNet having better efficiency. On balanced data, both reach ~99% accuracy, narrowing the performance gap while CNNs maintain efficiency advantages.


<details>
  <summary>Details</summary>
Motivation: To conduct a fair comparison between convolutional neural networks (CNNs) and Vision Transformers (ViTs) under controlled conditions with different label distribution regimes, addressing the need for systematic evaluation of these architectures.

Method: Used SpaceNet dataset with two label-distribution regimes: naturally imbalanced five-class split and balanced-resampled split (700 images per class). Applied matched preprocessing (224x224, ImageNet normalization), lightweight augmentations, and 40-epoch training budget on single NVIDIA P100 GPU.

Result: On imbalanced split: EfficientNet-B0 achieved 93% test accuracy with strong macro-F1 and lower latency; ViT-Base was competitive at 93% but with larger parameter count and runtime. On balanced split: EfficientNet-B0 reached 99% accuracy while ViT-Base remained competitive, showing that balancing narrows architecture gaps.

Conclusion: CNNs (EfficientNet-B0) retain efficiency advantages in terms of model size and latency, while balancing data distribution narrows performance gaps between architectures. Both models perform strongly on balanced data, suggesting data balance is crucial for fair architecture comparisons.

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of camera-based AI sensing systems for vulnerable road user (VRU) safety, covering detection, tracking, trajectory prediction, and intent recognition tasks from the past five years.


<details>
  <summary>Details</summary>
Motivation: Conventional infrastructure-based measures are inadequate for VRU protection in dynamic urban environments, and existing AI surveys focus mainly on detection while neglecting other essential vision tasks for comprehensive VRU understanding.

Method: The paper systematically reviews four core AI tasks: detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, examining developments from the past five years and emerging research trends.

Result: The survey identifies key advances in visual AI for VRU safety and highlights four major open challenges from data, model, and deployment perspectives that need to be addressed for real-world implementation.

Conclusion: By linking visual AI advances with practical implementation considerations, this survey provides a foundational reference for developing next-generation sensing systems to enhance VRU safety in intelligent transportation systems.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: Earth Observation Foundation Models (EOFMs) are sensitive to sensor architecture differences, which affects their representation spaces and highlights limitations in current model design.


<details>
  <summary>Details</summary>
Motivation: To understand how diverse sensor architectures impact the internal representations of EOFMs, as most models are trained on single modalities but applied across different modalities.

Method: Analyzed the representation space of EOFMs to demonstrate sensitivity to sensor architecture differences.

Result: Found that EOFM representation spaces are highly sensitive to sensor architecture, revealing current design pitfalls.

Conclusion: Understanding sensor architecture impacts provides vital perspective for improving EOFM design and guiding robust remote-sensing science.

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: An inpainting-guided explanation technique produces photorealistic, localized edits to explain vision models in ecological monitoring, revealing fine-grained morphological cues while preserving scene context.


<details>
  <summary>Details</summary>
Motivation: Opaque predictions from automated vision models limit trust and field adoption in ecological monitoring, creating a need for interpretable explanations that domain experts can validate.

Method: Uses inpainting-guided perturbation with Segment-Anything-Model-refined masks for two interventions: object removal/replacement and background replacement, applied to YOLOv9 detector for harbor seal detection in drone imagery.

Result: The approach localizes diagnostic structures, avoids deletion artifacts common to traditional perturbations, and produces domain-relevant insights that support expert validation through flip rate and confidence drop metrics.

Conclusion: This technique enables more trustworthy deployment of AI in ecology by providing photorealistic, interpretable explanations that preserve ecological plausibility and support expert review.

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: This paper provides a comprehensive survey of medical image segmentation methods, covering both traditional techniques and modern deep learning approaches, with a special focus on emerging trends and a case study on lumbar spine segmentation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between traditional image processing techniques and modern deep learning approaches in medical image segmentation, and to address persistent challenges in the field.

Method: Systematic survey methodology covering thresholding, edge detection, region-based segmentation, clustering algorithms, model-based techniques, CNNs, FCNs, U-Net variants, attention mechanisms, semi-supervised learning, GANs, and Transformer-based models.

Result: The survey comprehensively maps the evolution of MIS methodologies and identifies emerging trends including hybrid architectures, cross-modality learning, federated learning, and active learning strategies.

Conclusion: Despite significant progress, critical challenges remain including dataset bias, domain adaptation, model interpretability, and integration into clinical workflows, highlighting the need for continued research in medical image segmentation.

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR is a deep clustering framework for wafer defect detection that handles orientation variations and complex, unlabeled data, outperforming existing methods on the MixedWM38 dataset.


<details>
  <summary>Details</summary>
Motivation: Early detection of wafer defects is critical for semiconductor manufacturing yield optimization, but raw wafer data is complex, unlabeled, imbalanced, and can contain multiple defects per wafer, requiring robust clustering methods.

Method: DECOR (Deep Clustering with Orientation Robustness) framework that groups complex defect patterns from wafer maps into consistent clusters while explicitly accounting for orientation variations to ensure spatially similar defects are clustered regardless of rotation or alignment.

Result: DECOR outperforms existing clustering baseline methods on the MixedWM38 dataset and demonstrates ability to discover clusters without manual tuning.

Conclusion: DECOR provides a reliable and scalable solution for automated visual inspection systems in semiconductor manufacturing by handling orientation variations and complex data conditions.

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: This paper proposes an LSTM-based neural network with attention mechanism for multi-class facial emotion classification on imbalanced datasets, using error correction techniques to improve classification of underrepresented emotion classes.


<details>
  <summary>Details</summary>
Motivation: The study addresses the problem of class imbalance in facial emotion recognition, where some emotions significantly outnumber others, making accurate classification of rare emotions challenging.

Method: Used an LSTM neural network with attention mechanism focusing on key facial areas for emotion recognition. Trained on subsets of six emotion classes and performed error correction for the excluded seventh class.

Result: Error correction was possible for all emotion classes, though with varying success rates. Some classes were better restored than others. Test results showed improved quality metrics for small classes, indicating promise for rare event detection.

Conclusion: The proposed method is effective for facial expression analysis systems and can provide stable classification performance under imbalanced class distributions, making it suitable for applications like anti-fraud systems that require rare event detection.

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: The paper introduces DCG-Bench, the first benchmark for evaluating multi-modal large language models (MLLMs) on dynamic chart generation tasks, and proposes a two-stage training method with Joint-Code-Visual Reward optimization that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: While MLLMs have improved static chart generation and comprehension, their potential for dynamic chart generation and understanding remains underexplored. The research aims to bridge this gap by creating a comprehensive benchmark and training methodology.

Method: Constructed DCG-8K dataset with instruction-code-video triplets and QA pairs. Proposed two-stage training recipe using Joint-Code-Visual Reward for group relative policy optimization to create expert MLLM Qwen2.5-VL-DCG-3B.

Result: The model beats the best open-sourced MLLM with 8.31% average performance gain across three tasks (Simple Text-to-Chart, Detailed Text-to-Chart, Video-to-Chart) and shows on-par performance against proprietary models despite having only 3B parameters.

Conclusion: The proposed training recipe effectively addresses dynamic chart generation challenges, demonstrating that specialized training can achieve competitive performance with smaller models, while revealing existing MLLMs' shortcomings in visual-to-chart tasks.

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT (Visual odometry Transformer) is an end-to-end monocular visual odometry method that uses temporal and spatial attention to directly predict camera motion without traditional components like bundle adjustment or dense 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Traditional monocular visual odometry methods rely on complex pipelines with pre-trained components and optimization modules, requiring camera calibration and hyperparameter tuning, and struggle in unseen scenarios. Existing large-scale 3D models have limitations in handling long videos and providing accurate per-frame estimates.

Method: VoT processes sequences of monocular frames by extracting features and modeling global relationships through temporal and spatial attention. It directly predicts camera motion without estimating dense geometry, uses only camera poses for supervision, and allows integration of various pre-trained encoders as feature extractors.

Result: VoT scales effectively with larger datasets, benefits from stronger pre-trained backbones, generalizes across diverse camera motions and calibration settings, outperforms traditional methods, and runs more than 3 times faster.

Conclusion: Monocular visual odometry can be effectively addressed in an end-to-end manner, eliminating the need for handcrafted components like bundle adjustment, feature matching, camera calibration, or dense 3D reconstruction.

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: A novel inference-time search algorithm that uses side information to guide diffusion models for solving inverse problems, improving reconstruction quality without gradient-based guidance artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based approaches for inverse problems overlook valuable side information that could significantly enhance reconstruction quality, especially in severely ill-posed settings.

Method: Proposed an inference-time search algorithm that guides the sampling process using side information, balancing exploration and exploitation to avoid reward-hacking artifacts common in gradient-based guidance methods.

Result: The approach consistently improves qualitative and quantitative performance across various inverse problems including box inpainting, super-resolution, and multiple deblurring tasks (motion, Gaussian, nonlinear, blind), outperforming reward gradient-based guidance baselines.

Conclusion: The method provides an effective alternative to gradient-based guidance that can be seamlessly integrated into existing diffusion-based image reconstruction pipelines, enabling more accurate and reliable reconstructions.

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of publicly available sonar image datasets across various modalities (SSS, FLS, SAS, MBES, DIDSON) and applications (classification, detection, segmentation, 3D reconstruction), identifying gaps and offering a roadmap for researchers in underwater acoustic data analysis.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available, well-annotated sonar image datasets creates a significant bottleneck for developing robust machine learning models in underwater exploration, autonomous navigation, and ecosystem monitoring.

Method: Conducted a comprehensive review by mapping publicly accessible datasets across various sonar modalities, analyzing applications, and synthesizing findings into a master table and chronological timeline for clear comparison of dataset characteristics, sizes, and annotation details.

Result: Created a systematic catalog of existing sonar image datasets with contextual analysis, identified gaps in available resources, and developed accessible comparison tools (master table and timeline) for researchers.

Conclusion: This review serves as a foundational guide for researchers entering or advancing in underwater acoustic data analysis by providing a clear roadmap of available datasets and highlighting areas needing further development.

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: The paper presents a lensless camera and neural algorithm system for display calibration without specialized hardware, enabling light field reconstruction from multiple viewpoints.


<details>
  <summary>Details</summary>
Motivation: Traditional display calibration requires specialized equipment and dark rooms, making it inaccessible to most users. The authors aim to eliminate hardware requirements for display characterization.

Method: Co-design of a lensless camera and Implicit Neural Representation algorithm to capture display characteristics from various viewpoints, enabling light field reconstruction from a 46.6° × 37.6° viewing cone.

Result: The pipeline successfully reconstructs light fields emitted from displays across a wide viewing angle, providing an efficient method for display characterization.

Conclusion: This emerging pipeline represents initial progress toward effortless display calibration and characterization, potentially making the process more accessible without requiring specialized hardware.

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks are neural models that provide end-to-end explainability by linking predictions directly to supporting training examples, embedding interpretability into the architecture itself.


<details>
  <summary>Details</summary>
Motivation: To address model opaqueness, hallucination, and credit assignment issues in deep learning by providing training-data-driven explainability that traditional deep networks cannot offer.

Method: The model operates like a learned KNN, jointly optimizing the primary task and explainability objective. It links each prediction to relevant training examples weighted by feature space relevance, embedding interpretability directly into the architecture.

Result: Enables systematic investigation of memorization vs. generalization trade-offs, verification of training set inclusion, detection of mislabeled/anomalous data, enhanced resilience to input perturbations, and identification of similar inputs contributing to new data generation.

Conclusion: While introducing additional computational cost and currently scaling to moderately sized datasets, provenance networks provide a complementary approach that improves transparency, robustness, and trustworthiness in neural models by addressing critical explainability challenges.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: Unified Cost Filtering (UCF) is a post-hoc framework that refines anomaly cost volumes in unsupervised anomaly detection by mitigating matching noise through learnable filtering with multi-layer attention guidance, achieving state-of-the-art results across 22 benchmarks in both unimodal and multimodal settings.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised anomaly detection methods suffer from overlooked matching noise and remain isolated between unimodal and multimodal approaches, limiting detection ability and knowledge transfer.

Method: UCF constructs anomaly cost volume by matching test samples against normal samples, then applies a learnable filtering module with multi-layer attention guidance to reduce matching noise and highlight subtle anomalies.

Result: Comprehensive experiments on 22 diverse benchmarks show UCF consistently enhances various UAD methods, achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) scenarios.

Conclusion: UCF provides a unified framework that effectively addresses matching noise in unsupervised anomaly detection and works across both unimodal and multimodal settings, demonstrating strong generalization capabilities.

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: A framework using Visual Language Models (VLMs) to automatically evaluate and refine object detection results in industrial diagrams like P&IDs, addressing the lack of quality assessment methods in digitalization processes.


<details>
  <summary>Details</summary>
Motivation: Industrial diagrams are crucial for plant operations and digital twins, but there's a lack of automated methods to evaluate object detection quality during digitalization, which is essential for intelligent industrial automation.

Method: Uses Visual Language Models (VLMs) to assess object detection results by identifying missing or inconsistent detections through multimodal analysis, enabling automated quality evaluation and refinement guidance.

Result: The framework enables automated quality assessment of object detection outputs and improves overall detection performance on complex industrial diagrams by leveraging VLM capabilities.

Conclusion: VLMs provide an effective solution for automated quality evaluation of object detection in industrial diagram digitalization, enabling better detection refinement and supporting the development of digital twins and intelligent automation.

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT enhances vision-language models by integrating spatial features like depth maps and 3D coordinates through multi-task learning, achieving state-of-the-art spatial reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models face challenges in spatial reasoning for 3D scenes and complex object configurations, limiting their ability to understand spatial relationships in multimodal data.

Method: Introduces SpatialViLT with spatial feature integration (depth maps, 3D coordinates, edge maps) via multi-task learning. Proposes two variants: SpatialViLT and MaskedSpatialViLT, plus SpatialEnsemble combining both approaches.

Result: Achieves state-of-the-art accuracy on Visual Spatial Reasoning (VSR) dataset, excelling in directional, topological, and proximity relations categories.

Conclusion: Represents a significant advancement in enhancing spatial intelligence for AI systems, crucial for advanced multimodal understanding and real-world applications.

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: Encoder-decoder networks trained on synthetic data effectively reduce artifacts in two-phase optical-sectioning structured illumination microscopy, improving image clarity without requiring clean ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Two-phase optical-sectioning SI suffers from residual artifacts due to reduced acquisition time, and conventional denoising methods struggle with these artifacts. Supervised deep learning approaches are limited by the lack of clean ground-truth data.

Method: Used encoder-decoder networks (asymmetrical denoising autoencoder and U-Net) trained on synthetic data pairs created by applying real artifact fields to synthetic images, then evaluated on real OS-SI images.

Result: Both networks improved image clarity, with each network excelling against different types of artifacts. The approach successfully enabled supervised denoising of OS-SI images.

Conclusion: Synthetic training enables effective supervised denoising of OS-SI images, and encoder-decoder networks show potential for streamlining reconstruction workflows in structured illumination microscopy.

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL is a multimodal framework that integrates histopathology with spatial transcriptomics using pathway activation scores instead of individual genes, achieving superior performance in gene- and pathway-level expression prediction across multiple cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches rely on a small set of highly variable genes, which limits predictive scope and overlooks coordinated biological programs that shape tissue phenotypes.

Method: PEaRL represents transcriptomics through pathway activation scores computed with ssGSEA, encodes biologically coherent pathway signals with a transformer, and aligns them with histology features via contrastive learning.

Result: Across three cancer ST datasets (breast, skin, and lymph node), PEaRL consistently outperforms SOTA methods, yielding up to 58.9% and 20.4% increase in Pearson correlation coefficient for gene- and pathway-level expression prediction respectively.

Conclusion: Grounded transcriptomic representation in pathways produces more biologically faithful and interpretable multimodal models, advancing computational pathology beyond gene-level embeddings.

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS is a vision-language framework for multi-modal medical image analysis that uses hierarchical semantic prompts and dual-prompt mechanisms to achieve superior generalization across modalities and tasks, outperforming state-of-the-art models on most datasets while enabling seamless EHR integration.


<details>
  <summary>Details</summary>
Motivation: Medical imaging faces limitations with task-specific models lacking generalizability and existing universal approaches having simplistic conditioning and poor medical semantic understanding.

Method: Introduces a novel vision-language framework with hierarchical semantic prompts for fine-grained control, dual-prompt mechanism for text-controlled architecture, and parameter-efficient fine-tuning for rapid adaptation.

Result: Outperforms state-of-the-art models on 8 out of 10 datasets across 3 imaging modalities and 30+ organs/tumor types, achieves CI of 0.69 for head and neck cancer prognosis with EHR integration.

Conclusion: DuPLUS establishes itself as a versatile and clinically relevant solution that enables efficient multi-modal medical image analysis with strong generalization capabilities and seamless extensibility to other medical tasks.

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: A mobile-optimized two-stage framework using YOLOv10 for detection and MobileSAM for segmentation, with threading to parallelize both stages for real-time animal detection in conservation settings.


<details>
  <summary>Details</summary>
Motivation: Real-time animal detection and segmentation are crucial for wildlife conservation through remote monitoring, but face challenges due to limited computational resources and cryptic species appearances.

Method: Two-stage deep learning framework integrating Threading Detection Model (TDM) to parallelize YOLOv10-based detection and MobileSAM-based segmentation, executed concurrently to reduce latency.

Result: On Houbara Bustard: mAP50=0.9627, mAP75=0.7731, mAP95=0.7178, MobileSAM mIoU=0.7421. YOLOv10 operates at 43.7ms per frame. Introduced curated dataset of 40,000 annotated images.

Conclusion: The proposed framework achieves real-time performance suitable for mobile deployment in wildlife conservation, with publicly available code and dataset to support further research.

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: The Platonic Transformer introduces a novel attention mechanism that achieves equivariance to continuous translations and Platonic symmetries while maintaining the exact architecture and computational cost of standard Transformers.


<details>
  <summary>Details</summary>
Motivation: Transformers lack inductive biases for geometric symmetries common in science and computer vision, while existing equivariant methods sacrifice efficiency and flexibility through complex designs.

Method: The method defines attention relative to reference frames from Platonic solid symmetry groups, inducing a principled weight-sharing scheme that enables combined equivariance to continuous translations and Platonic symmetries.

Result: The Platonic Transformer achieves competitive performance across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25) while leveraging geometric constraints at no additional cost.

Conclusion: The approach resolves the trade-off between geometric equivariance and Transformer efficiency, showing that attention is formally equivalent to dynamic group convolution, enabling adaptive geometric filters and scalable linear-time variants.

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: This survey paper provides a comprehensive overview of domain generalization in semantic segmentation, highlighting the paradigm shift towards foundation-model-based approaches and offering extensive performance comparisons.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle with generalization to unknown domains despite recent progress, and domain generalization is particularly important for semantic segmentation tasks in critical applications like biomedicine and automated driving.

Method: The authors cluster and review existing domain generalization approaches for semantic segmentation, identifying key trends and methodologies in the field.

Result: The survey reveals a significant paradigm shift towards foundation-model-based domain generalization and provides extensive performance comparisons that demonstrate the substantial influence of foundation models on domain generalization capabilities.

Conclusion: This comprehensive survey aims to advance domain generalization research and inspire new research directions in the rapidly evolving field of domain generalized semantic segmentation.

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: A transformer-based automated report generation model for endoscopic procedures that reduces documentation burden on gastroenterologists.


<details>
  <summary>Details</summary>
Motivation: Endoscopic procedures like EGD and colonoscopy create significant documentation burden, contributing to clinical inefficiencies and physician burnout.

Method: Two-stage training framework: pre-training transformer-based vision encoder and text decoder on image/text caption pairs, then fine-tuning on images/report pairs for clinical findings.

Result: The model generates clinically meaningful findings from endoscopic images, streamlining documentation process.

Conclusion: This approach reduces physician workload and improves patient care by automating report generation for endoscopic procedures.

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan is a diffusion-based planner that converts 2D hand-drawn sketches over depth images into 3D drone flight paths, achieving zero-shot sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: To enable intuitive drone navigation by allowing users to draw 2D sketches that are automatically converted into safe 3D flight paths in real-world environments.

Method: Uses two components: SketchAdapter (maps human sketches to 2D paths) and DiffPath (diffusion model that infers 3D trajectories from 2D projections and depth images). Trained on 32k synthetic flight paths and 872 human-labeled sketches.

Result: Achieved 100% success in low/medium clutter and 40% in high-clutter real-world environments, outperforming ablations by 20-60% in task completion.

Conclusion: The modular design with mixed training data (human-labeled + auto-labeled) significantly improves human intent interpretation and 3D path inference for drone navigation.

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: A novel biometric defense method for talking-head videoconferencing that detects identity hijacking by analyzing pose-expression latents rather than reconstructed RGB video, using contrastive learning to isolate identity cues from pose/expression.


<details>
  <summary>Details</summary>
Motivation: AI-based talking-head systems are vulnerable to puppeteering attacks where attackers can hijack a victim's likeness in real-time, and existing deepfake detectors fail because every frame is synthetic.

Method: A pose-conditioned, large-margin contrastive encoder that disentangles persistent identity cues from transient pose and expression in the transmitted latent, followed by a simple cosine test on the embedding to detect identity swaps.

Result: The method consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios across multiple talking-head generation models.

Conclusion: The proposed biometric leakage defense effectively addresses the security vulnerability in talking-head videoconferencing by leveraging inherent biometric information in pose-expression latents without requiring RGB video analysis.

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: REVEL enables interactive drag-based video manipulation anytime on anything, addressing challenges of latent distribution drift and context interference through training-free DragStream method.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive video diffusion models lack fine-grained streaming control, making it difficult to ensure outputs consistently align with user expectations through interactive drag operations.

Method: Proposes DragStream with adaptive distribution self-rectification using neighboring frames' statistics to constrain latent embedding drift, and spatial-frequency selective optimization to mitigate context interference while propagating visual cues.

Result: The method can be seamlessly integrated into existing autoregressive video diffusion models and extensive experiments demonstrate its effectiveness in enabling streaming drag-oriented interactive video manipulation.

Conclusion: REVEL and DragStream successfully bridge the gap for fine-grained interactive video manipulation, providing versatile drag operations with translation, deformation, and rotation effects while overcoming latent drift and context interference challenges.

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL is an ensemble framework that integrates features from multiple foundation models for computational pathology, achieving superior performance across cancer datasets without requiring extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Foundation models provide powerful feature extractors for pathology, but adapting and benchmarking individual models for specific diagnostic tasks is time-consuming and resource-intensive due to their scale and diversity.

Method: Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) - a flexible ensemble framework that seamlessly integrates features from multiple foundation models while preserving their complementary strengths, without manual feature selection or extensive task-specific fine-tuning.

Result: Across three cancer datasets (prostate PANDA, ovarian UBC-OCEAN, and breast TCGA-BrCa), GAS-MIL consistently achieves superior or on-par performance relative to individual foundation models and established multi-instance learning methods.

Conclusion: GAS-MIL enables efficient integration of heterogeneous foundation models, streamlines model deployment for pathology, and provides a scalable foundation for future multimodal and precision oncology applications.

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: This paper introduces a drone-assisted naloxone delivery system for opioid overdose emergencies, featuring a novel dataset and real-time situational awareness assessment framework using graph embeddings and transformers.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for rapid naloxone delivery in opioid overdose emergencies by extending lifesaving interventions to untrained bystanders before EMS arrival, and to fill the research gap in real-time situational awareness assessment for human-autonomy teaming.

Method: Created the Drone-Assisted Naloxone Delivery Simulation Dataset (DANDSD) with college students as bystanders, and developed a video-based real-time SA assessment framework using graph embeddings and transformer models that integrate geometric, kinematic, and interaction graph features.

Result: The proposed approach achieves high-performance SA prediction with strong temporal segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over Frames (MoF) and 5% in Intersection over Union (IoU).

Conclusion: This work enables the development of adaptive drone systems that can effectively guide bystanders during opioid overdose emergencies, ultimately improving emergency response outcomes and saving lives.

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: This study benchmarks four OCR systems (Tesseract, EasyOCR, PaddleOCR, TrOCR) on food packaging images, finding Tesseract most accurate while EasyOCR offers good multilingual balance.


<details>
  <summary>Details</summary>
Motivation: Accurate OCR for food packaging is crucial for compliance and nutrition monitoring, but challenging due to multilingual text, dense layouts, varied fonts, glare, and curved surfaces.

Method: Evaluated four OCR systems on 231 products (1,628 images) using metrics including CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time, with ground truth subset of 113 images for accuracy assessment.

Result: Tesseract achieved lowest CER (0.912) and highest BLEU (0.245), EasyOCR balanced accuracy with multilingual support, PaddleOCR had near complete coverage but was slower, TrOCR performed weakest despite GPU acceleration.

Conclusion: Results provide packaging-specific benchmark, establish baseline performance, and highlight need for layout-aware methods and improved text localization.

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle is a plug-and-play module that intelligently selects relevant frames for video understanding tasks, reducing input frames by 35-78% while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current frame sampling methods (uniform/fixed-budget) are inefficient and cause information loss due to inability to adapt to varying information density and task complexity in videos.

Method: A lightweight module that predicts both which frames are relevant and how many are needed, trained via four-stage curriculum using weak proxy signals and a new FrameOracle-41K dataset with keyframe annotations.

Result: Reduces 16-frame inputs to 10.4 frames (35% reduction) with no accuracy loss, and 64-frame inputs to 13.9 frames (78% reduction) with 1.4% accuracy improvement, achieving SOTA efficiency-accuracy trade-offs.

Conclusion: FrameOracle enables scalable video understanding by intelligently selecting minimal frame sets, significantly improving efficiency while maintaining or enhancing performance across multiple VLMs and benchmarks.

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: Proposes a hybrid Co-FineTuning (CFT) method for visual bug detection in video games that combines labeled data from target and co-domain games with unlabeled data, reducing dependency on extensive labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Manual visual bug detection in games is resource-intensive and requires domain expertise, while supervised models need large labeled datasets which are challenging due to infrequent bug occurrences.

Method: Hybrid Co-FineTuning (CFT) that integrates labeled samples from target game and co-domain games with unlabeled data to enhance feature representation learning, maximizing data utility.

Result: CFT demonstrates superior performance compared to conventional baselines across multiple gaming environments and maintains competitive performance with only 50% of labeled target game data.

Conclusion: The proposed CFT framework enhances scalability and adaptability for efficient visual bug detection across various game titles while reducing dependency on labeled examples.

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM with Transformer-style modules performs well on MNIST but overfits and generalizes poorly on CIFAR datasets compared to simple CNNs, showing insufficient image-specific inductive bias for natural image classification.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether the Hierarchical Reasoning Model (HRM) with Transformer-style modules can serve as a practical image classifier under raw training conditions without data augmentation.

Method: Used HRM with two Transformer-style modules, one-step DEQ-style training, deep supervision, Rotary Position Embeddings, and RMSNorm. Evaluated on MNIST, CIFAR-10, and CIFAR-100 with no data augmentation, identical optimizer with one-epoch warmup then cosine-floor decay, and label smoothing.

Result: HRM achieved ≈98% test accuracy on MNIST but performed poorly on natural images: 65.0% on CIFAR-10 vs 77.2% for CNN baseline, and 29.7% on CIFAR-100 vs 45.3% for CNN baseline. HRM showed significant overfitting with 91.5% train accuracy but only 29.7% test accuracy on CIFAR-100.

Conclusion: HRM is not competitive with simple convolutional architectures for small-resolution image classification without augmentation in its current form, though modifications could potentially improve its performance.

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: DINOv2 establishes new state-of-the-art in self-supervised learning, surpassing weakly supervised methods like OpenCLIP on most benchmarks through multi-crop view augmentation and self-distillation with mean teacher.


<details>
  <summary>Details</summary>
Motivation: To learn general-purpose visual features that capture both high-level semantics and fine-grained spatial structure of images, advancing beyond previous self-supervised and weakly supervised methods.

Method: Uses multi-crop view augmentation and self-distillation with a mean teacher approach, building on transformer backbones for feature learning.

Result: DINOv2 surpasses weakly supervised methods (WSL) like OpenCLIP on most benchmarks and demonstrates remarkable emergent properties in learned features.

Conclusion: While DINOv2 has limitations, it represents significant impact in self-supervised learning and points to promising future research directions.

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: DCS introduces a mutual boosting loop between diffusion models and FSCIL classifiers using reward-aligned learning, achieving state-of-the-art performance on FSCIL benchmarks by enhancing knowledge retention and new class learning.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of FSCIL where models struggle with generalization due to limited datasets and the stability-plasticity dilemma, while overcoming issues with direct diffusion model application like semantic misalignment.

Method: Proposes Diffusion-Classifier Synergy (DCS) with a dynamic, multi-faceted reward function that operates at feature level (semantic coherence and diversity) and logits level (exploratory generation and inter-class discriminability) to guide diffusion model training.

Result: Demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning capabilities.

Conclusion: The co-evolutionary process between diffusion model and classifier creates a mutually beneficial loop that effectively addresses FSCIL challenges through reward-aligned learning strategy.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM is a vision-language framework that detects safety violations in mining surveillance videos using domain-specific VQA datasets, clause filtering for efficiency, and behavior magnification for improved action recognition.


<details>
  <summary>Details</summary>
Motivation: Traditional manual safety inspection in mining is labor-intensive, error-prone, and insufficient for large-scale dynamic environments, creating an urgent need for automated intelligent safety monitoring systems.

Method: MonitorVLM uses three key innovations: (1) domain-specific violation dataset with 9,000 VQA samples across 40 mining regulations, (2) clause filter module that selects Top-K relevant clauses to reduce latency, and (3) behavior magnifier module that enhances worker regions for better action recognition.

Result: MonitorVLM significantly outperforms baseline models with 22.01% precision improvement, 34.22% recall improvement, and 28.37% F1 score improvement over 72B unfine-tuned baseline. The clause filter reduces inference latency by 13.56% while maintaining accuracy.

Conclusion: MonitorVLM demonstrates the potential of multimodal large models to enhance occupational safety monitoring in mining and other high-risk industries through automated violation detection and reporting.

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: A hybrid model combining guidance classification with diffusion techniques for accident detection in ITS, achieving 97.32% accuracy through cloud-based implementation and conditional modules.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional classification approaches in ITS by leveraging diffusion models' capacity to understand complex data distributions for improved accident detection.

Method: Hybrid model integrating ExceptionNet outputs with diffusion techniques, using image tensors as conditioning, multiple conditional modules with time and image embeddings, and cloud-based implementation for scalability.

Result: Achieved 97.32% accuracy in image-based accident detection, outperforming baseline models through comprehensive ablation studies on diffusion characteristics.

Conclusion: The proposed diffusion-based approach provides a robust and efficient framework for accident detection in ITS, demonstrating superior performance over traditional methods.

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: SAMSOD is a novel RGB-T salient object detection model that addresses modality imbalance and gradient conflicts through unimodal supervision, gradient deconfliction, and decoupled adapters for high/low-activation neurons.


<details>
  <summary>Details</summary>
Motivation: Current RGB-T SOD methods using fine-tuned Segment Anything Models suffer from imbalanced convergence between RGB and thermal modalities, and significant gradient differences between high- and low-activation regions, limiting performance.

Method: Proposes SAMSOD with three key components: unimodal supervision to enhance non-dominant modality learning, gradient deconfliction to reduce conflicting gradients, and two decoupled adapters that separately mask high- and low-activation neurons to emphasize foreground objects by enhancing background learning.

Result: Extensive experiments on RGB-T SOD benchmark datasets show effectiveness. Generalizability tests on scribble-supervised RGB-T SOD, fully supervised RGB-D SOD, and RGB-D rail surface defect detection all demonstrate the method's strong performance.

Conclusion: SAMSOD effectively addresses modality imbalance and gradient conflicts in RGB-T SOD, achieving superior performance across multiple datasets and demonstrating good generalization capability to related tasks.

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: The paper introduces SOREC dataset for small object referring expression comprehension and proposes PIZA adapter for parameter-efficient fine-tuning to improve small object localization.


<details>
  <summary>Details</summary>
Motivation: Localizing extremely small objects in referring expression comprehension remains challenging despite advances in vision-language learning, especially for real-world applications like autonomous driving.

Method: Created SOREC dataset with 100,000 expression-bounding box pairs for small objects in driving scenarios, and developed PIZA adapter module for progressive-iterative zooming to localize small objects efficiently.

Result: Applied PIZA to GroundingDINO and demonstrated significant accuracy improvement on the SOREC dataset.

Conclusion: The proposed dataset and method effectively address the challenge of small object localization in REC tasks, with publicly available resources for further research.

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: The paper proposes Attention-WNet, a deep learning model that incorporates attention mechanisms into WNet for retinal artery-vein segmentation, achieving state-of-the-art performance on HRF and DRIVE datasets.


<details>
  <summary>Details</summary>
Motivation: Retinal artery-vein segmentation is crucial for analyzing retinal blood vessels, which can provide biomarkers for diagnosing retinal diseases and identifying patients at risk for systemic vasculature diseases like stroke and myocardial infarction.

Method: The authors developed Attention-WNet by incorporating attention mechanisms into the WNet deep learning model to improve retinal artery-vein segmentation performance.

Result: The proposed Attention-WNet model outperformed other state-of-the-art models when tested on publicly available HRF and DRIVE datasets.

Conclusion: The attention-enhanced WNet architecture provides superior performance for retinal artery-vein segmentation compared to existing methods, demonstrating the effectiveness of attention mechanisms in this medical imaging task.

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: The paper creates demographic annotations for LAION-400M dataset and reveals strong biases in vision-language models, showing that 60-70% of gender bias can be explained by training data co-occurrences.


<details>
  <summary>Details</summary>
Motivation: To understand how training data composition contributes to demographic biases in vision-language models, given the lack of demographic annotations in web-scale datasets like LAION-400M.

Method: Created person-centric annotations for LAION-400M using validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers, producing over 276 million bounding boxes with perceived gender and race/ethnicity labels.

Result: Uncovered demographic imbalances and harmful associations, such as disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. Showed that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the training data.

Conclusion: Established the first large-scale empirical link between dataset composition and downstream model bias, demonstrating that training data imbalances directly contribute to model biases.

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: Comparison of generic vs specialized pretrained neural networks for detecting favelas in Rio de Janeiro, examining whether task specificity or data volume yields better performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for informal settlement detection haven't fully utilized recent pretrained neural networks' potential, creating a gap in understanding whether task-specific pretraining or larger data volume is more effective.

Method: Compare two types of pretrained networks: 1) Generic networks pretrained on large diverse datasets of unspecific images, 2) Specialized network pretrained on satellite imagery specifically.

Result: The paper investigates the trade-off between task specificity (specialized satellite imagery pretraining) and data volume (generic large-scale pretraining) for informal settlement detection.

Conclusion: Research aims to determine which approach - task-specific pretraining or large-scale generic pretraining - provides superior performance in urban informal settlement detection.

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: LoRA patching bypasses proactive Deepfake defenses by injecting plug-and-play LoRA patches into generators, using adaptive gating and multi-modal feature alignment to defeat state-of-the-art protections with minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing proactive Deepfake defenses that embed adversarial perturbations in facial images lack robustness and reliability, creating a critical security vulnerability that needs to be addressed.

Method: Proposes Low-Rank Adaptation (LoRA) patching with learnable gating mechanism to prevent gradient explosions, and Multi-Modal Feature Alignment (MMFA) loss for semantic-level feature alignment between adversarial and desired outputs.

Result: Successfully defeats multiple proactive defenses with only 1,000 facial examples and a single epoch of fine-tuning, revealing critical weaknesses in current defense paradigms.

Conclusion: Current Deepfake defense strategies are vulnerable to LoRA patching attacks, underscoring the need for more robust defense mechanisms and highlighting the dual-use nature of the technique through defensive LoRA patching.

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: The paper proposes Reference-Set-Finetuning (RSF), a method that finetunes VPR models on test-time reference sets to bridge train-test domain gaps and improve performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the performance gap in Visual Place Recognition when test environments differ significantly from training datasets, leveraging the available test-time reference set ('map') as a complementary information source.

Method: Reference-Set-Finetuning (RSF) - finetuning State-of-the-Art VPR models on the test-time reference set (map) before receiving test queries, using available images and poses from the target domain.

Result: RSF boosts SOTA performance by ~2.3% average increase in Recall@1 on challenging datasets, while maintaining generalization and working across diverse test datasets.

Conclusion: Reference-Set-Finetuning is an effective approach to bridge train-test domain gaps in VPR, leveraging available map data to significantly improve performance on challenging benchmarks without sacrificing generalization.

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM accelerates Sharpness-Aware Minimization (SAM) by 40% while maintaining comparable accuracy, using adaptive gradient reuse and mixing techniques.


<details>
  <summary>Details</summary>
Motivation: SAM improves generalization but doubles computational cost compared to SGD due to requiring two gradient calculations per step, creating a need for more efficient alternatives.

Method: Decomposes SAM's gradient into SGD gradient and Projection of Second-order gradient onto First-order gradient (PSF), then adaptively reuses and mixes these components to reduce computational overhead.

Result: Achieves state-of-the-art accuracies comparable to SAM across diverse architectures, with 40% speedup on CIFAR-10/100, and successfully accelerates various challenging tasks without performance loss.

Conclusion: ARSAM provides an efficient alternative to SAM that maintains generalization benefits while significantly reducing computational costs, demonstrating broad practical applicability.

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA introduces a novel framework that uses concept prompting and aggregation to improve concept capture in medical imaging by extracting multiscale features from all layers of visual encoders and using them as prompts to guide concept learning.


<details>
  <summary>Details</summary>
Motivation: Current concept-based methods for clinical diagnostics rely only on final layer features, missing shallow and multiscale visual information, and lack effective guidance for fine-grained concept extraction.

Method: CoPA framework uses Concept-aware Embedding Generator (CEG) to extract concept representations from each layer, Concept Prompt Tuning (CPT) to amplify critical concept cues, and aggregates visual representations to align with textual concepts.

Result: Extensive experiments show CoPA outperforms state-of-the-art methods on three public datasets, effectively capturing and utilizing concept-wise information to improve concept and disease prediction.

Conclusion: CoPA successfully addresses limitations of existing concept-based methods by capturing multilayer concepts under prompt guidance, enhancing transparency and performance in clinical diagnostics.

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP compression achieves up to 22.89:1 data reduction for 3D medical imaging while maintaining high cerebrovascular segmentation quality (Dice 0.87656 vs baseline 0.8774).


<details>
  <summary>Details</summary>
Motivation: Address the challenges of large 3D medical imaging datasets that hinder collaborative research and transferability.

Method: Apply ZFP compression in error tolerance and fixed-rate modes to 3D medical datasets with ground-truth vascular segmentations, comparing segmentation quality on compressed vs uncompressed volumes.

Result: ZFP achieved substantial data reduction (up to 22.89:1 ratio) while maintaining high fidelity with mean Dice coefficient of 0.87656 (baseline: 0.8774).

Conclusion: ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration.

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: A hybrid medical image segmentation architecture combining CNNs, Transformers, and Mamba-based attention to capture multi-scale dependencies, achieving state-of-the-art performance with balanced efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for medical image segmentation are often task-specific with varying performance across modalities and anatomical regions, while balancing model complexity and performance remains challenging in clinical settings.

Method: Three-branch encoder integrating CNNs, Transformers, and Mamba-based Attention Fusion (MAF) mechanism; multi-scale attention-based CNN decoder; co-attention gate for enhanced feature selection across scales.

Result: Outperforms state-of-the-art methods in accuracy and generalization on multiple benchmark datasets while maintaining comparable computational complexity.

Conclusion: The proposed architecture effectively balances efficiency and effectiveness, offering a practical and scalable solution for diverse medical imaging tasks.

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: Deep learning approach using YOLOv9 with polygonal annotations for automated road damage and manhole detection, achieving 78.1% overall accuracy with strong performance on damage classes but challenges in manhole detection.


<details>
  <summary>Details</summary>
Motivation: Manual monitoring of road damages is time-consuming, costly, and error-prone, especially in developing countries like Bangladesh where urban safety and infrastructure maintenance are critical for smart city development.

Method: Used YOLOv9 algorithm with polygonal annotations (instead of traditional bounding boxes) for precise localization, trained on a novel dataset of 1000+ images from Dhaka, Bangladesh for three classes: Broken, Not Broken, and Manhole.

Result: Achieved 78.1% overall image-level accuracy with strong performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score) classes, but poor performance for Manhole detection (18.2% F1-score) due to class imbalance.

Conclusion: The approach offers an efficient and scalable solution for monitoring urban infrastructure in developing countries, though manhole detection needs improvement through better class balancing.

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: The paper proposes Contrastive-SDE, a method combining contrastive learning with score-based diffusion models for unpaired image-to-image translation, achieving comparable performance to state-of-the-art with faster convergence and no label supervision.


<details>
  <summary>Details</summary>
Motivation: Unpaired image-to-image translation lacks aligned samples, making it challenging. Diffusion models excel at complex data distributions while contrastive learning effectively learns semantic similarities without supervision - combining these strengths addresses the core challenges of unpaired I2I translation.

Method: Uses time-dependent contrastive learning with SimCLR, treating an image and its domain-invariant feature as positive pairs. The learned contrastive model then guides inference of a pretrained stochastic differential equation (SDE) for the translation task.

Result: Achieves comparable results to state-of-the-art across three unpaired I2I tasks using four evaluation metrics. Model converges significantly faster and requires no label supervision or classifier training.

Conclusion: Contrastive-SDE provides an efficient alternative for unpaired image-to-image translation, combining the strengths of contrastive learning and diffusion models while eliminating the need for supervision and achieving faster convergence.

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO extends the LIBERO benchmark to systematically evaluate VLA models under perturbations, revealing that models achieving 90%+ accuracy in standard settings collapse to 0% when tested on manipulated objects, initial states, instructions, and environments.


<details>
  <summary>Details</summary>
Motivation: Current LIBERO benchmark settings lead to inflated performance estimates and prevent fair model comparison due to models' reliance on rote memorization rather than genuine understanding.

Method: Extended LIBERO benchmark with systematic perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments.

Result: Models that achieve over 90% accuracy under standard evaluation collapse to 0.0% under the generalized setting, exposing reliance on memorization rather than comprehension.

Conclusion: Current evaluation practices are flawed and misleading; the community should adopt robust assessments of model generalization and comprehension instead.

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: The paper introduces Mirage, a dataset of AI-generated images with visible artifacts that fool current detectors, and shows that Large Vision-Language Models (LVLMs) can effectively detect these images but struggle when artifacts are absent.


<details>
  <summary>Details</summary>
Motivation: There's a growing discrepancy where AI-generated images are hard for standard detectors to identify but remain distinguishable by humans. The authors want to investigate this gap and explore whether LVLMs can serve as explainable detectors.

Method: Created the Mirage dataset containing diverse AI-generated images with visible artifacts, then tested LVLMs' detection capabilities on both Mirage and existing benchmark datasets.

Result: LVLMs are highly effective at detecting AI-generated images with visible artifacts but their performance declines significantly when images lack such visual cues.

Conclusion: LVLMs show promise for explainable AI image detection, particularly for images with visible artifacts, but face limitations when artifacts are not present, highlighting the need for more robust detection methods.

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround introduces a unified visual grounding paradigm that dynamically selects intermediate transformer layers as "mask as prompt" instead of using fixed last hidden layers, addressing cumulative error propagation and lack of spatial cues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current visual grounding methods rely on fixed last hidden layers which amplify cumulative errors through layer-by-layer propagation without correction, and use text embeddings without explicit spatial cues like coordinates.

Method: UGround uses Policy-Prompted Masking with two components: Stochastic Skip Connection (SSC) - a reinforcement learning policy that dynamically selects transformer layers for skip connections, and Mask as Prompt (MasP) - which uses similarity maps as soft logit masks to prompt SAM for mask generation with explicit spatial cues.

Result: UGround unifies visual grounding within a single framework across various scenarios including refer expression segmentation, reasoning segmentation, single/multi-target, and positive/negative queries.

Conclusion: UGround provides a more effective visual grounding paradigm by dynamically selecting intermediate layers and using explicit spatial cues, addressing key limitations of existing methods while unifying diverse grounding tasks.

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4 is a framework that reduces storage overhead in 4D Gaussian Splatting by progressively pruning and merging Gaussians while maintaining reconstruction quality, achieving over 60% model size reduction.


<details>
  <summary>Details</summary>
Motivation: 4D Gaussian Splatting faces major storage overhead challenges requiring millions of Gaussians for high-fidelity reconstruction, with existing methods having limitations in compression ratio or visual quality.

Method: Three-stage progressive pruning: (1) Gaussian Sampling to identify critical primitives, (2) Gaussian Pruning to remove redundancies, (3) Gaussian Merging to fuse similar primitives, plus implicit appearance compression and generalized Sub-Vector Quantization for 4D representations.

Result: Significantly outperforms state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality on standard benchmark datasets.

Conclusion: OMG4 represents a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications.

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: A novel framework for adapting open-vocabulary object detection from ground-view to aerial imagery through structured domain alignment, achieving significant performance improvements in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection models are limited to fixed classes, making it costly to add new categories. Open-vocabulary detection enables identifying unseen classes without explicit training, but domain shifts between ground-view and aerial imagery require specialized adaptation strategies.

Method: Proposes contrastive image-to-image alignment to enhance similarity between aerial and ground-view embeddings, and multi-instance vocabulary associations to align aerial images with text embeddings.

Result: Achieves improvements of +6.32 mAP on DOTAv2, +4.16 mAP on VisDrone, and +3.46 mAP on HRRSD in zero-shot setting compared to finetuned closed-vocabulary models.

Conclusion: The framework paves the way for more flexible and scalable object detection systems in aerial applications by effectively adapting open-vocabulary representations across domains.

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: This review paper analyzes deep learning approaches for skin cancer diagnosis, discussing challenges like complex features, image noise, and data imbalance, and presents solutions including data augmentation and hybrid models.


<details>
  <summary>Details</summary>
Motivation: Skin cancer is a prevalent and deadly disease where early detection is crucial. Deep learning shows promise for automated diagnosis but faces challenges like complex features, image noise, and data imbalance that need to be addressed.

Method: The review follows the PRISMA framework methodology, synthesizing recent research on DL approaches for skin cancer diagnosis, including data augmentation, hybrid models, and feature fusion techniques.

Result: The review identifies innovative approaches to overcome DL challenges in skin cancer diagnosis and discusses the integration of DL models into clinical workflows for improved diagnostic accuracy and efficiency.

Conclusion: Deep learning has transformative potential in dermatological care but requires continued advancements to fully unlock its capabilities for revolutionizing skin disease diagnosis and clinical decision-making.

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: The paper introduces a framework that decouples planning from acting in Vision-Language-Action models using sparse 3D trajectories as an intermediate representation, enabling better generalization across tasks without requiring fine-tuning for new environments.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models struggle with generalization due to monolithic architectures and semantic ambiguities in action modules, requiring fine-tuning for new environments and lacking clear cooperation mechanisms between planning and acting systems.

Method: Uses sparse 3D trajectories as intermediate representation between VLM planning and action execution. VLMs generate coarse 3D waypoints, which are refined by a generalizable action expert into dense action sequences using real-time point cloud observations. Implements "Action Pre-training, Pointcloud Fine-tuning" paradigm.

Result: The framework combines VLM's broad generalization in visual understanding and planning with fine-grained action-level generalization of the action expert, enabling efficient training and robust generalization across tasks.

Conclusion: The proposed approach effectively bridges high-level planning with low-level physical actions through 3D trajectory representations, overcoming limitations of conventional VLA models and enabling scalable cross-task training without environment-specific fine-tuning.

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [61] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: SDAKD is a novel GAN distillation method that introduces a student discriminator to address capacity mismatch issues, enabling effective compression of super-resolution GANs for resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: GANs have excellent generative performance but high computational requirements that prevent deployment on resource-constrained devices. Knowledge distillation for GAN compression faces challenges due to capacity mismatch between student generators and teacher discriminators.

Method: Proposes Student Discriminator Assisted Knowledge Distillation (SDAKD) with a three-stage training strategy that introduces a student discriminator and integrates adapted feature map distillation in the last two stages.

Result: Evaluated on GCFSR and Real-ESRGAN super-resolution GANs, SDAKD demonstrates consistent improvements over baselines and state-of-the-art GAN knowledge distillation methods.

Conclusion: SDAKD effectively addresses the capacity mismatch problem in GAN distillation and enables successful compression of high-performance super-resolution GANs for deployment on resource-limited devices.

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [62] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME is a federated Visual Positioning System (VPS) backend that enables distributed 6DoF localization for AR applications across multiple independent organizations without centralized 3D scanning.


<details>
  <summary>Details</summary>
Motivation: Centralized VPS solutions from large companies fail to cover private indoor spaces due to privacy concerns, regulations, and maintenance bottlenecks, limiting AR application coverage.

Method: Proposes federated image-based localization where organizations maintain separate VPS services for their spaces, with solutions for managing and merging data across maps without sharing private data.

Result: Enables access control of indoor 3D scans, distributed VPS maintenance, and encourages larger coverage while addressing challenges like localization coherency and quality control.

Conclusion: Federated VPS approach overcomes limitations of centralized systems by allowing distributed maintenance and coverage expansion while preserving privacy and access control.

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [63] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: PoseGaze-AHP is a novel 3D dataset that synchronously captures head pose and gaze movement information for diagnosing ocular-induced abnormal head posture (AHP), created using LLM-extracted clinical data and Neural Head Avatar framework.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on head pose and ocular movements separately, limiting integrated diagnostic approaches and AI-driven advancements in AHP analysis.

Method: Clinical data extracted from medical literature using LLMs (Claude 3.5 Sonnet) with iterative prompting strategies, then transformed into 3D representations using Neural Head Avatar framework with systematic imputation.

Result: Created dataset with 7,920 images from two head textures covering broad ocular conditions; extraction method achieved 91.92% accuracy; first publicly available resource for AI-driven ocular-induced AHP diagnosis.

Conclusion: PoseGaze-AHP enables development of accurate, privacy-compliant diagnostic tools for ocular-induced AHP by providing integrated head pose and gaze movement data.

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [64] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: RAP (Rasterization Augmented Planning) uses lightweight 3D rasterization instead of photorealistic rendering for end-to-end driving training, enabling scalable data augmentation with counterfactual recovery maneuvers and cross-agent view synthesis, achieving state-of-the-art performance on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for end-to-end driving lacks recovery data, causing small mistakes to compound into failures. Photorealistic rendering methods are too slow and costly for training. The key insight is that driving depends on geometry and dynamics, not photorealism.

Method: Proposes 3D Rasterization to replace costly rendering with lightweight rasterization of annotated primitives. Introduces Raster-to-Real feature-space alignment to bridge sim-to-real gap. Combines these into RAP pipeline for scalable data augmentation.

Result: Achieves state-of-the-art closed-loop robustness and long-tail generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive.

Conclusion: Lightweight rasterization with feature alignment suffices to scale E2E training, offering a practical alternative to photorealistic rendering for end-to-end driving planners.

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [65] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper introduces DHQA-4D, a large-scale dataset for quality assessment of dynamic 4D digital humans, and proposes DynaMesh-Rater, a novel large multimodal model-based approach that extracts multi-dimensional features to predict quality scores for both textured and non-textured 4D meshes.


<details>
  <summary>Details</summary>
Motivation: With the growing popularity of dynamic digital human avatars based on 4D meshes in applications like gaming, animation, and immersive communication, these meshes are prone to noise degradation during collection, compression, and transmission, which affects user viewing experience. This necessitates quality assessment methods for dynamic 4D digital humans.

Method: The authors first create the DHQA-4D dataset containing 32 high-quality 4D human mesh sequences and 1920 distorted meshes with 11 types of distortions. Then they propose DynaMesh-Rater, which extracts multi-dimensional features (visual from 2D video projection, motion from cropped video clips, and geometry from 4D mesh) and uses a large multimodal model with LoRA-based instruction tuning to predict quality scores.

Result: Extensive experiments on the DHQA-4D dataset demonstrate that DynaMesh-Rater outperforms previous quality assessment methods, showing superiority in assessing both textured and non-textured 4D meshes.

Conclusion: The proposed DHQA-4D dataset and DynaMesh-Rater method provide an effective solution for quality assessment of dynamic 4D digital humans, addressing the need for reliable evaluation in applications involving 4D human avatars.

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [66] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: Improved ResNet-50 with Adaptive Spatial Feature Fusion (ASFF) for skin cancer classification achieves 93.18% accuracy on ISIC 2020 dataset by adaptively fusing multi-scale semantic and surface features to handle inter-class similarity and intra-class variability.


<details>
  <summary>Details</summary>
Motivation: Skin cancer classification faces challenges due to high inter-class similarity, intra-class variability, and image noise in dermoscopic images, requiring better feature representation and reduced overfitting.

Method: Enhanced ResNet-50 with ASFF using dual-branch design that fuses high-level semantic and mid-level detail features through global average pooling and fully connected layers to generate adaptive weights for weighted fusion.

Result: Achieved 93.18% accuracy, higher precision, recall, specificity, F1 score, and AUC values of 0.9670 (P-R curve) and 0.9717 (ROC curve) on ISIC 2020 dataset with 3297 images, outperforming 5 classic CNN models.

Conclusion: The proposed ASFF-based ResNet-50 provides a more effective and efficient solution for computer-aided skin cancer diagnosis by adaptively focusing on lesion-relevant regions while suppressing background noise.

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [67] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: A multimodal deep learning framework using DenseNet-121 CNNs was developed to improve early detection of Oral Squamous Cell Carcinoma (OSCC) by integrating clinical, radiological, and histopathological images through weighted ensemble fusion.


<details>
  <summary>Details</summary>
Motivation: Late diagnosis of OSCC contributes to high mortality rates, with over 50% of cases detected at advanced stages and 5-year survival below 50%. Current diagnostic methods need improvement for early detection.

Method: Retrospective study using public datasets of three medical imaging modalities. Each modality trained a DenseNet-121 CNN via transfer learning with augmentation and modality-specific preprocessing. Predictions were fused using validation-weighted ensemble strategy.

Result: High validation accuracy for radiological (100%) and histopathological (95.12%) modalities, with clinical images lower (63.10%) due to visual heterogeneity. Ensemble model achieved 84.58% accuracy on multimodal validation dataset of 55 samples.

Conclusion: The multimodal ensemble framework provides a non-invasive, AI-assisted triage tool that enhances early identification of high-risk lesions, supports clinical decision-making, and aligns with global oncology guidelines to reduce diagnostic delays and improve patient outcomes.

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [68] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper challenges the scaling law in explainable image quality assessment (IQA) by showing that data quality matters more than quantity. The authors propose IQA-Select, a clustering-based data selection method that achieves better performance using only 10% of data.


<details>
  <summary>Details</summary>
Motivation: Current methods construct large-scale instruction tuning datasets for MLLMs in explainable IQA, but this causes high computational costs and redundant data that can harm model performance. The authors aim to challenge the scaling law and investigate data quality over quantity.

Method: The authors propose a three-stage clustering-based data selection framework: clustering feature extraction, cluster quota allocation, and cluster sampling strategy. They systematically analyze choices at each stage and develop IQA-Select, which selects representative subsets of data.

Result: IQA-Select achieves 102.1% and 103.7% performance of full fine-tuning using only 10% selected data in Q-Bench and AesBench respectively, demonstrating superior performance while significantly reducing computational costs.

Conclusion: Data quality is more important than quantity for explainable IQA. The proposed IQA-Select method effectively reduces data redundancy and computational costs while achieving better performance than full dataset training.

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [69] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: A novel method transforms zero-shot fine-grained image classification into visual question-answering using LVLMs, enhanced by attention intervention and improved class descriptions, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: The potential of Large Vision-Language Models (LVLMs) for zero-shot fine-grained image classification remains underexplored, despite their impressive performance on vision-language reasoning tasks.

Method: Transforms zero-shot fine-grained image classification into a visual question-answering framework using LVLMs, enhanced by a novel attention intervention technique and comprehensive class description benchmarks.

Result: The proposed method consistently outperforms the current state-of-the-art approach across multiple fine-grained image classification benchmarks.

Conclusion: Demonstrates both the effectiveness of the proposed method and the broader potential of LVLMs for zero-shot fine-grained classification tasks.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [70] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: This paper presents a comprehensive benchmark study of defogging methods for autonomous driving perception, evaluating classical filters, modern defogging networks, chained approaches, and VLM-based image editing on both image quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving perception systems are vulnerable in foggy conditions, and existing defogging methods show inconsistent improvements in downstream detection/segmentation tasks. Prior evaluations often rely on synthetic data, raising questions about real-world transferability.

Method: Structured empirical study benchmarking comprehensive pipelines including classical filters, modern defogging networks, chained variants (filter→model, model→filter), and prompt-driven VLM image editing models applied directly to foggy images using Foggy Cityscapes dataset.

Result: Analysis reveals when defogging helps, when chaining yields synergy or degradation, and how VLM-based editors compare to dedicated approaches. VLM judge qualitative scores show strong correlations with mAP, establishing transparent task-oriented benchmark.

Conclusion: The study establishes a transparent, task-oriented benchmark for defogging methods and highlights conditions under which preprocessing genuinely improves autonomous perception in adverse weather.

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [71] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO is a cascaded framework that bridges Text-to-Motion models with video diffusion models for general human motion video generation, addressing alignment issues and introducing camera-aware conditioning.


<details>
  <summary>Details</summary>
Motivation: Human video generation has broad applications but current video diffusion models are constrained to image-to-video setups or narrow domains, leaving general-purpose human motion generation underexplored.

Method: A cascaded framework that connects Text-to-Motion models with conditional video diffusion models, using carefully designed components for training and inference, including text/visual condition preparation and a camera-aware conditioning module.

Result: Demonstrated effectiveness on both MovieGen benchmark and a new benchmark for T2M-VDM combination, showing versatility across diverse use cases.

Conclusion: CAMEO successfully bridges the gap between text-to-motion and video generation, providing a robust framework for general human motion video generation with enhanced coherence and reduced manual intervention.

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [72] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: A framework combining CNN-LSTM models for biomechanical feature extraction and LLMs for generating actionable feedback in tennis stroke analysis.


<details>
  <summary>Details</summary>
Motivation: Existing systems lack connection between biomechanical insights and accessible language feedback for players and coaches.

Method: Extract biomechanical features (joint angles, limb velocities, kinetic chain patterns) using CNN-LSTM models, then generate feedback using LLMs based on THETIS dataset.

Result: The framework aims to produce technically accurate, biomechanically grounded, and actionable feedback for tennis stroke analysis.

Conclusion: Bridges the gap between explainable AI and sports biomechanics by connecting biomechanical analysis with meaningful language feedback.

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [73] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp is a method that creates synthetic temporal datasets to improve Video-LLMs' fine-grained temporal understanding, achieving significant performance gains across seven benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs underperform on tasks requiring fine-grained temporal understanding due to lack of visual complexity and temporal nuance in current datasets, causing them to rely on language-based reasoning rather than understanding video dynamics.

Method: Proposed TimeWarp - a systematic method to create targeted synthetic temporal datasets that capture intricate temporal dynamics, and introduced a large-scale preference dataset to ground model responses to visual and temporal information.

Result: Applied to existing models, TimeWarp significantly improved performance on temporal understanding benchmarks, achieving absolute performance improvements across seven benchmarks.

Conclusion: TimeWarp's synthetic temporal datasets effectively advance temporal understanding in Video-LLMs, addressing the limitation of current models in fine-grained temporal reasoning.

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [74] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: The paper introduces BMC-LongCLIP, a long-context biomedical vision-language model that extends text context from 77 to 512 tokens, reducing token waste from 55% to 2.2% and achieving significant performance gains in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Most vision-language models are pretrained with short text windows (<77 tokens), forcing truncation of long biomedical captions. Analysis shows that a huge portion of biomedical captions exceed 77 tokens, leading to loss of valuable contextual information.

Method: Extended context length of text encoders in VLMs and introduced BIOMEDICA-LongCAP dataset with 1M image-caption pairs from full-text articles. Trained BMC-LongCLIP model with text encoder supporting up to 512 tokens.

Result: BMC-LongCLIP achieved up to +30% absolute gains in Recall@1 on long-caption retrieval benchmarks and +2% average improvements in classification. The model also converged faster than short-context models and reduced token waste from 55% to just 2.2%.

Conclusion: Long-context modeling is a promising direction for advancing biomedical VLMs, as longer context enables additional supervision from long-format captions and correlates with better performance in retrieval and classification tasks.

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [75] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: CPG is a framework for long-tailed semi-supervised learning that handles unknown unlabeled data distributions by generating controllable pseudo-labels and maintaining a known labeled data distribution through dynamic filtering.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume unlabeled data follows predefined distributions, but in reality, unlabeled data distribution is unknown and arbitrary, creating challenges for reliable semi-supervised learning.

Method: Uses a controllable self-reinforcing optimization cycle: (1) dynamic controllable filtering of pseudo-labels to maintain known labeled distribution, (2) Bayes-optimal classifier with logit adjustment, (3) improved classifier helps identify more reliable pseudo-labels. Also includes class-aware adaptive augmentation and auxiliary branch for data utilization.

Result: Achieves consistent improvements across benchmark datasets, surpassing state-of-the-art methods by up to 15.97% in accuracy.

Conclusion: CPG effectively handles unknown unlabeled data distributions in long-tailed semi-supervised learning through controllable pseudo-label generation and distribution-aware optimization, with theoretical guarantees on generalization error reduction.

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [76] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: Fine-tuning PaddleOCRv5 improves Classical Chinese (Han-Nom) text recognition from 37.5% to 50.0% accuracy, especially for degraded historical documents, with an interactive demo available.


<details>
  <summary>Details</summary>
Motivation: Existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations in ancient Vietnamese Chinese manuscripts, hindering digitization and cross-lingual semantic research.

Method: Fine-tuned PaddleOCRv5's text recognition module using curated ancient Vietnamese Chinese manuscripts, with full training pipeline including preprocessing, LMDB conversion, evaluation, and visualization.

Result: Significant improvement over base model - exact accuracy increased from 37.5% to 50.0%, particularly effective under noisy image conditions.

Conclusion: The fine-tuned model enables better digitization of historical documents and facilitates downstream applications like Han-Vietnamese semantic alignment, machine translation, and historical linguistics research.

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [77] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg is a meta-learning framework that trains implicit neural representations (INRs) for medical image segmentation, achieving comparable performance to U-Nets with 90% fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Implicit neural representations (INRs) are effective for compact signal representation but are not naturally suitable for predictive tasks like segmentation that require learning semantic structures across signal distributions.

Method: MetaSeg uses an INR that simultaneously predicts pixel intensity values and class labels, with a meta-learning procedure to find optimal initial parameters over training data, enabling quick fine-tuning for unseen test images.

Result: Evaluation on 2D and 3D brain MRI segmentation tasks showed Dice scores comparable to U-Net models while using 90% fewer parameters.

Conclusion: MetaSeg provides a scalable alternative to traditional resource-heavy architectures like U-Nets and vision transformers for medical image segmentation.

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [78] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: ViTL is a two-stage long-video QA framework that localizes question-relevant intervals with low-fps skimming and answers via span-aware token reallocation at higher effective frame rates, achieving better performance with fewer frames.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of long-video QA while maintaining performance, by efficiently allocating visual tokens to relevant temporal spans.

Method: Two-stage approach: 1) Localize question-relevant intervals using low-fps skim, 2) Answer via span-aware reallocation of visual tokens at higher effective frame rates with interleaved group-relative objective.

Result: Achieves up to 8.6% improvement with 50% less frame input on long-video QA and temporal grounding tasks (Charades-STA, ActivityNet-Captions), outperforming uniform sampling.

Conclusion: ViTL provides an interpretable, compute-efficient solution for scalable long-video QA through span-aware token reallocation and temporal localization.

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [79] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: AgentAug is a data augmentation framework that uses LLM-driven pipelines to generate diverse fake news videos by simulating creative processes, combined with active learning to improve short video fake news detection.


<details>
  <summary>Details</summary>
Motivation: Current fake news detectors suffer from limited and biased training data due to complex many-to-many relationships between video segments and fabricated events in real-world scenarios, which existing datasets fail to adequately capture.

Method: Proposes AgentAug framework with multiple LLM-driven pipelines simulating four fabrication categories for news video creation, combined with active learning based on uncertainty sampling to select useful augmented samples during training.

Result: Experimental results on two benchmark datasets show that AgentAug consistently improves the performance of short video fake news detectors.

Conclusion: AgentAug effectively addresses the data sparsity and bias issues in fake news video detection by generating diverse training samples that better reflect real-world creative processes.

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [80] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: This paper explores hyperparameter optimization in prompt-to-prompt image editing frameworks to enhance precision and reliability, addressing issues like inconsistent hair color changes through methods including word swap, attention re-weight, and CL P2P framework.


<details>
  <summary>Details</summary>
Motivation: To improve the precision and reliability of prompt-to-prompt image editing frameworks by addressing the variability in results introduced by deep learning methods like stable diffusion models, particularly issues like inconsistent hair color changes.

Method: Comprehensive study of the "word swap" method, development of an "attention re-weight method" for better adaptability, and proposal of the "CL P2P" framework to address limitations like cycle inconsistency.

Result: The research contributes to understanding and improving the interaction between hyperparameter settings and neural network architectural choices, specifically attention mechanisms that influence image composition and quality.

Conclusion: This work advances prompt-to-prompt image editing by optimizing hyperparameters and developing new methods to enhance precision and reliability in text-driven image manipulation.

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [81] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight improves visual grounding in multimodal GUI systems by using specialized tools to iteratively focus on relevant screen regions, achieving state-of-the-art accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Current multimodal GUI systems are limited by unreliable visual grounding, which prevents accurate pointer-level actions like clicking or dragging on screen elements.

Method: Train a model for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow focus to relevant screen regions.

Result: Achieves 52.8% accuracy on ScreenSpot-Pro benchmark with only 18.5K training samples, outperforming V2P-7B (50.6% with 9.6M samples) and GTA-1-7B (50.1% with 1.56M samples).

Conclusion: GUI-Spotlight substantially improves visual grounding accuracy in GUI systems through iterative region focusing, enabling more reliable pointer-level actions with minimal training data.

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [82] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: A range estimation method for post-training quantization that minimizes quantization errors through layer-wise local minima optimization, achieving state-of-the-art performance with minimal accuracy loss in 8-bit and 6-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Low-bit quantization while maintaining model accuracy is challenging in post-training quantization, requiring better methods to reduce storage without sacrificing performance.

Method: Model range estimation as an optimization problem to minimize quantization errors using layer-wise local minima, prove local convexity, and develop an efficient search algorithm applied to transformed weights space.

Result: Outperforms state-of-the-art on top-1 accuracy for ResNet series and Inception-v3 models, with almost no loss in 8-bit and 6-bit quantization, and significant improvement in 4-bit quantization accuracy.

Conclusion: The proposed range estimation method effectively improves post-training quantization performance across different bit-widths while maintaining model accuracy.

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [83] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind is a tri-modal compositional retrieval framework that enhances metaverse scene generation by retrieving 3D assets using text, image, and 3D queries while maintaining spatial, semantic, and stylistic consistency.


<details>
  <summary>Details</summary>
Motivation: Addresses inconsistent asset retrieval that ignores spatial, semantic, and stylistic constraints, and the lack of standardized retrieval paradigms specifically for 3D assets in metaverse applications.

Method: Uses a flexible tri-modal retrieval mechanism with an equivariant layout encoder (ESSGNN) that captures spatial relationships and object appearance features, supporting arbitrary combinations of text, image, and 3D queries.

Result: Empirical evaluations show improved spatial and stylistic consistency in various retrieval tasks compared to baseline methods.

Conclusion: MetaFind provides an effective framework for contextually and stylistically coherent 3D asset retrieval in metaverse scene construction, supporting iterative scene updates.

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [84] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: The paper proposes an ordinality-aware loss function that integrates ordinal relationships between solar flare sub-classes into binary classification, penalizing misclassifications near the prediction threshold more heavily to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Binary classification for solar flare prediction neglects ordinal relationships between sub-classes, and empirical studies show most misclassifications occur near the prediction threshold where models struggle to differentiate similar intensity events on opposite sides of the binary boundary.

Method: Modified loss function that integrates ordinal information among flare sub-classes into conventional binary cross-entropy loss, serving as an ordinality-aware regularization method that penalizes incorrect predictions near the threshold more heavily during model optimization.

Result: The proposed approach aims to enhance model learning by leveraging ordinal characteristics of the data, though specific quantitative results are not provided in the abstract.

Conclusion: Incorporating ordinal weighting into the loss function improves solar flare prediction by addressing the limitations of binary classification frameworks and better handling events near the prediction threshold.

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [85] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire is a post-training quantization framework for demoiréing models that addresses performance degradation issues through outlier-aware quantization and frequency-aware calibration, achieving significant parameter and computation reductions while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based demoiréing methods require substantial computational resources, limiting deployment on edge devices. Direct application of standard quantization methods causes severe performance degradation due to distribution outliers and weakened representations in smooth regions.

Method: The framework includes: 1) Outlier-aware quantizer using sampling-based range estimation to reduce activation outliers and keeping extreme weights in FP16, 2) Frequency-aware calibration strategy that emphasizes low- and mid-frequency components during fine-tuning to mitigate banding artifacts.

Result: QuantDemoire achieves large reductions in parameters and computation while maintaining quality, outperforming existing quantization methods by over 4 dB on W4A4 quantization.

Conclusion: The proposed QuantDemoire framework effectively enables efficient deployment of demoiréing models on edge devices through specialized quantization techniques that address domain-specific challenges.

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [86] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA is a novel method for low-dose sparse-view CT reconstruction that combines diffusion generative priors with multi-regularization constraints (anisotropic TV and nuclear norm) in an ADMM framework, achieving superior performance in texture recovery and artifact suppression.


<details>
  <summary>Details</summary>
Motivation: To address the ill-posedness and texture loss problems in extremely sparse-view CT reconstruction, particularly under low-dose conditions where traditional methods struggle with image quality degradation.

Method: Combines diffusion generative prior (NCSN++ with SDE modeling) with multi-regularization constraints including anisotropic TV and nuclear norm (LoRA) within an ADMM framework, using 2D slice-based strategy with FFT acceleration and tensor-parallel optimization.

Result: Experiments on AAPM-2016, CTHD, and LIDC datasets with 8, 4, and 2 views show TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability.

Conclusion: TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction with broad clinical applicability in low-dose, sparse-sampling scenarios, with ablation studies confirming the complementary effects of LoRA regularization and diffusion priors.

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [87] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: The paper addresses the lack of standardized evaluation in topological mapping by proposing formal metrics for topological consistency and dataset ambiguity, creating a benchmark dataset, and releasing open-source tools.


<details>
  <summary>Details</summary>
Motivation: Progress in topological mapping is hindered by the absence of standardized evaluation metrics, datasets, and protocols, preventing fair comparisons between systems. Perceptual aliasing remains under-quantified despite its significant impact on performance.

Method: The authors formalize topological consistency as a fundamental property and propose localization accuracy as a surrogate metric. They introduce a quantitative measure of dataset ambiguity and curate a diverse benchmark dataset with calibrated ambiguity levels. They implement both deep-learned baseline systems and classical methods for evaluation.

Result: The research yields new insights into the limitations of current approaches under perceptual aliasing. The authors provide a comprehensive evaluation framework and open-source all datasets, baselines, and evaluation tools.

Conclusion: The paper establishes a standardized evaluation protocol for topological mapping research, enabling fair and reproducible comparisons across different environments and systems, while providing tools to advance the field consistently.

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [88] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: This paper introduces event-based meshflow estimation, creates a high-resolution dataset (HREM/HREM+), proposes an efficient EEMFlow network with density adaptation, and achieves 30x faster performance with significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Address two key gaps in event-based flow estimation: lack of meshflow-specific datasets/methods and underexplored challenges of event data density.

Method: Created HREM dataset with high resolution and complex motions; proposed EEMFlow network with encoder-decoder architecture; added CDC module for dense flow; developed ADM for density adaptation.

Result: EEMFlow achieves 30x faster runtime than state-of-the-art methods; ADM improves performance by 8-10%; comprehensive experiments show exceptional performance and efficiency.

Conclusion: The proposed EEMFlow network with density adaptation significantly advances event-based meshflow estimation, offering both speed and accuracy improvements while addressing data density challenges.

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [89] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: A novel pipeline for 6D object pose estimation that combines pretrained encoder with joint regression-diffusion learning and sampling guidance, achieving faster training and higher accuracy without needing additional evaluation networks.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based pose estimation methods suffer from slow training convergence, end-to-end encoder learning with diffusion networks, and require additional networks to filter pose candidates.

Method: Pretrains encoder with direct pose regression, jointly learns networks via regression and diffusion heads, and introduces sampling guidance with time-dependent score scaling for effective exploration-exploitation trade-off.

Result: Achieves state-of-the-art accuracies on REAL275, HouseCat6D, and ROPE benchmarks with single-pose inference, while being more efficient in both training and inference.

Conclusion: The proposed method is simple yet effective, accelerating training convergence while maintaining multi-modal characteristics for symmetric objects and ensuring high-quality pose generation.

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [90] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: The paper addresses concept drift in multimodal large language model distillation, where multiple teachers' reasoning trajectories drift unpredictably, causing bias transmission to student models. It proposes autonomous preference optimization (APO) using a "learn, compare, critique" paradigm to align concepts and improve model robustness.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM distillation faces concept drift issues where multiple teachers' reasoning trajectories evolve unpredictably, transmitting biases to student models and compromising performance. This critical challenge remains underexplored in knowledge distillation literature.

Method: The authors establish a theoretical link between concept drift and knowledge distillation, framing multi-teacher reasoning as multi-stream trajectory prediction. They introduce autonomous preference optimization (APO) with a "learn, compare, critique" paradigm where students learn from teachers, compare multiple reasoning paths, and critically reflect on drifting inferences to achieve concept alignment.

Result: Extensive experiments show superior performance in consistency, robustness, and generalization within knowledge distillation. The authors also contribute CXR-MAX, a large-scale dataset with 170,982 distilled reasoning trajectories from MIMIC-CXR-based MLLMs.

Conclusion: The proposed APO framework effectively addresses concept drift in multimodal LLM distillation, producing robust, consistent, and generalizable student models through autonomous preference optimization and critical reflection on teacher reasoning trajectories.

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [91] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield is a multi-modal LVLM-based RAG framework that automates construction safety inspection reports by integrating visual and audio inputs, outperforming unimodal LLMs with improved accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: Conventional construction safety inspection methods are inefficient due to large information volumes, and existing LVLM applications face limitations like irrelevant responses, restricted modal inputs, and hallucinations. LLMs lack training data and real-time adaptability for this purpose.

Method: Developed SiteShield, a multi-modal large vision-language model (LVLM) based Retrieval-Augmented Generation (RAG) framework that integrates visual and audio inputs for automated safety inspection reports.

Result: SiteShield outperformed unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96 using real-world data.

Conclusion: SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating construction safety reports through its multi-modal RAG approach.

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [92] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE is a generative debiasing framework that mitigates neural network biases without requiring prior knowledge of biases or bias-conflicting samples. It uses generative models to translate images across bias domains while preserving task-relevant features, then adaptively refines images based on their bias susceptibility.


<details>
  <summary>Details</summary>
Motivation: Neural networks often learn implicit biases and spurious correlations from training data, relying on superficial patterns rather than task-relevant features. Existing debiasing methods require impractical assumptions like prior knowledge of biases or access to bias-conflicting samples.

Method: BLADE trains a generative model to translate images across bias domains while preserving task features. It then adaptively refines each image with its synthetic counterpart based on bias susceptibility, aligning images with bias-translated counterparts that share task features but differ in bias, while misaligning with same-bias samples.

Result: BLADE significantly outperforms state-of-the-art methods on multiple benchmark datasets. It exceeds the closest baseline by ~18% absolute margin on corrupted CIFAR-10 under worst group setting, establishing new benchmark in bias mitigation.

Conclusion: BLADE demonstrates potential for developing more robust deep learning models without explicit supervision, effectively addressing neural network bias vulnerabilities through generative translation and adaptive refinement.

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [93] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM integrates concept-guided segmentation with multiple instance learning to provide spatially grounded, concept-level explanations without requiring concept annotations, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack transparency and may exploit misleading features, limiting trust in safety-critical applications. Existing Concept Bottleneck Models require costly annotations and lack spatial grounding.

Method: Combines concept-guided image segmentation with attention-based multiple instance learning, treating segmented regions as instances and aggregating evidence across them to identify task-relevant concepts.

Result: Achieves robust performance across spurious correlations, input corruptions, and large-scale benchmarks while providing transparent, concept-level explanations without concept annotations.

Conclusion: SEG-MIL-CBM enables spatially grounded, concept-level interpretability without annotation costs, improving both model transparency and robustness in computer vision applications.

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [94] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa is a training-free acceleration framework for Diffusion Transformers that uses dimension-wise caching strategies based on modeling hidden feature evolution as a mixture of ODEs, achieving significant speedups (5.55-6.24×) across various models without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers have state-of-the-art image/video synthesis quality but suffer from slow iterative sampling due to expensive transformer forward passes at each timestep. Existing feature caching methods use uniform strategies that ignore heterogeneous dynamic behaviors across different feature dimensions.

Method: HyCa models hidden feature evolution as a mixture of ODEs across dimensions and applies dimension-wise caching strategies using a Hybrid ODE solver inspired framework, allowing different caching approaches for different feature dimensions based on their dynamic behaviors.

Result: Achieved near-lossless acceleration with 5.55× speedup on FLUX, 5.56× on HunyuanVideo, 6.24× on Qwen-Image and Qwen-Image-Edit models, demonstrating effectiveness across diverse domains and models without requiring retraining.

Conclusion: HyCa's dimension-wise caching approach based on modeling feature evolution as ODEs provides an effective training-free acceleration method for Diffusion Transformers, significantly reducing sampling time while maintaining high fidelity across various applications.

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [95] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: DP-IR is a modular diffusion probabilistic framework for blind image restoration that combines pre-trained IR networks with generative DPMs, requiring only small additional training (0.7M params) and enabling 4x faster sampling without performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing DPM-based IR solutions are task-specific, computationally expensive to train, and impractical for different IR tasks, limiting wider adoption especially for users with limited computational resources and training data.

Method: Proposes DP-IR framework that combines pre-trained state-of-the-art IR networks with generative DPMs, requiring only training of a small task-specific module (0.7M parameters), and includes sampling strategy for 4x reduction in neural function evaluations.

Result: Outperforms existing approaches in perceptual quality while maintaining competitive fidelity metrics on burst JDD-SR, dynamic scene deblurring, and super-resolution benchmarks.

Conclusion: DP-IR enables practical adoption of DPMs for IR tasks by reducing computational requirements through modular design and efficient sampling, making DPM-based IR more accessible and versatile.

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [96] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image is a framework that enhances text-to-image generation by using web-searching agents to retrieve knowledge about novel entities, enabling more accurate synthesis of out-of-distribution concepts through multimodal prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with novel or out-of-distribution entities due to knowledge cutoffs, leading to degraded performance when prompted with unfamiliar concepts.

Method: Uses an agent that dynamically searches the web to retrieve images for unknown concepts, then performs multimodal prompt optimization to steer generative models toward accurate synthesis.

Result: Achieves +8.1% improvement in accuracy-to-prompt on the NICE benchmark, substantially outperforming state-of-the-art methods in both semantic alignment and visual aesthetics with high efficiency in less than three iterations.

Conclusion: The framework enables T2I systems to better reflect the ever-changing real world by bridging knowledge gaps through agent-driven world knowledge retrieval.

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [97] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: This paper proposes enhancing 3D Gaussian Splatting by adding texture mapping to each Gaussian, allowing them to represent richer textures and geometric details beyond simple ellipsoids.


<details>
  <summary>Details</summary>
Motivation: Standard 3DGS limits each Gaussian to a single color and simple ellipsoid shape, which restricts expressivity and detail representation. The authors aim to overcome these limitations by drawing inspiration from traditional graphics texture mapping.

Method: The method augments each Gaussian with alpha, RGB, or RGBA texture maps to model spatially varying color and opacity across each Gaussian's extent, creating a generalized Gaussian appearance representation.

Result: The approach achieves higher image quality than existing methods while using similar or fewer Gaussians, with alpha-only texture maps showing significant expressivity improvements and RGB texture maps achieving the highest expressivity.

Conclusion: Integrating texture mapping with 3DGS significantly enhances Gaussian expressivity, enabling richer texture patterns and geometric structures while maintaining the efficiency advantages of the original 3DGS approach.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [98] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC is a framework that constructs hierarchical semantic trees from token embeddings to simplify autoregressive image generation, achieving 57% faster training and improved quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image generation are inefficient due to flat, unstructured token vocabularies that ignore semantic relationships in embedding space, making prediction tasks unnecessarily complex.

Method: MASC uses geometry-aware distance metrics and density-driven agglomerative clustering to build hierarchical semantic trees that model the underlying manifold of token embeddings.

Result: MASC accelerates training by up to 57% and improves generation quality, reducing FID of LlamaGen-XL from 2.87 to 2.58.

Conclusion: Structuring the prediction space through hierarchical semantic modeling is crucial for scalable generative modeling and can make existing AR frameworks competitive with state-of-the-art methods.

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [99] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn is a two-stage forensic framework that improves AI-generated image detection accuracy and interpretability by first scanning for suspicious regions and then performing focused analysis, achieving 96.39% accuracy.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI-generated imagery has blurred boundaries between real and synthetic content, raising digital integrity concerns. Existing vision-language models often fail to detect subtle artifacts in high-quality synthetic images.

Method: Proposes ZoomIn framework that mimics human visual inspection: first scans images to locate suspicious regions, then performs focused analysis on zoomed-in areas. Uses MagniFake dataset of 20,000 real/synthetic images with bounding boxes and forensic explanations generated through automated VLM pipeline.

Result: Achieves 96.39% accuracy with robust generalization while providing human-understandable explanations grounded in visual evidence.

Conclusion: ZoomIn framework effectively addresses the challenge of detecting high-quality synthetic images by combining accuracy with interpretable explanations through a two-stage forensic approach.

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [100] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: A simple, end-to-end trainable image registration algorithm that requires minimal code, training data, and training time while achieving accurate results.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and accessible image registration solution that works with limited resources and can be easily implemented.

Method: End-to-end trainable algorithm implemented in a few lines of Python code, demonstrated with stereo vision using 74 images on a 19x15 input window.

Result: Achieves accurate results with very little training data and training time, excelling in brevity and simplicity.

Conclusion: The algorithm serves as an excellent starting point for scenarios with constraints on training data, training time, or code complexity.

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [101] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: The paper introduces ArConv layers to redesign and optimize convolutional layers, creating a lightweight deep learning model with only 1.3M parameters that achieves better accuracy than MobileNetV2 (0.9328 vs 0.9266) on eye disease detection using the RfMiD dataset.


<details>
  <summary>Details</summary>
Motivation: To improve accessibility of deep neural networks for eye disease diagnosis by reducing computational complexity, making models suitable for mobile devices while maintaining high accuracy.

Method: Redesigned and optimized convolutional layers by creating novel ArConv layers, developing a new general model architecture that focuses on fundamental layer improvements for efficiency.

Result: Created a model with only 1.3M parameters that outperformed MobileNetV2 (2.2M parameters) on the RfMiD dataset, achieving 0.9328 accuracy vs 0.9266 under identical training conditions.

Conclusion: The ArConv-based model successfully balances computational efficiency and accuracy, making deep learning more accessible for mobile-based eye disease diagnosis applications.

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [102] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido is a generative model for neural rendering that treats 3D as a specialized video domain, using sequence-to-sequence image synthesis to perform view synthesis without explicit 3D representations.


<details>
  <summary>Details</summary>
Motivation: To create photorealistic neural rendering that unifies object- and scene-level generation while reducing reliance on scarce camera-labeled 3D datasets by leveraging large-scale video data.

Method: Uses a masked autoregressive framework with decoder-only rectified flow transformer to generate 6-DoF target views from reference views, treating 3D as a sequence-to-sequence video task.

Result: Sets new state-of-the-art on view synthesis benchmarks, with zero-shot performance outperforming other generative methods in few-view settings and matching per-scene optimization methods in many-view settings.

Conclusion: Kaleido successfully demonstrates that 3D rendering can be effectively modeled as a video sequence task, enabling unified 3D and video modeling while achieving competitive performance without architectural modifications.

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [103] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: CoSSeg-TTA is a compact liver segmentation framework for contrast-enhanced MRI that uses nnU-Netv2 with semi-supervised learning, domain adaptation, and test-time adaptation to address challenges of limited annotated data, domain shifts, and heterogeneous protocols.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation from contrast-enhanced MRI is essential for diagnosis and treatment but faces challenges including limited annotated data, heterogeneous enhancement protocols, and domain shifts across scanners/institutions. Existing domain adaptation methods like Pix2Pix and cycle-GAN have limitations such as requiring image registration, causing structural distortions, and unstable training.

Method: Built on nnU-Netv2 with semi-supervised mean teacher scheme to leverage unlabeled data. Includes domain adaptation with randomized histogram-based style appearance transfer and trainable contrast-aware network to increase domain diversity. Uses continual test-time adaptation during inference for robustness.

Result: Extensive experiments show the framework consistently outperforms nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance. Demonstrates strong generalization to unseen domains under low-annotation conditions.

Conclusion: CoSSeg-TTA effectively addresses the challenges of liver segmentation in contrast-enhanced MRI by combining semi-supervised learning, domain adaptation, and test-time adaptation, providing robust performance across different domains with limited annotations.

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [104] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: A patch-agnostic defense method using concept-based explanations to neutralize adversarial patch attacks without requiring prior knowledge of patch size or location, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Adversarial patch attacks pose practical threats to deep learning models, but existing defenses require prior knowledge of patch characteristics, limiting their real-world applicability.

Method: Leverages concept-based explanations to identify and suppress the most influential concept activation vectors, neutralizing patch effects without explicit patch detection.

Result: Achieves higher robust and clean accuracy than state-of-the-art PatchCleanser on Imagenette with ResNet-50, maintaining strong performance across varying patch sizes and locations.

Conclusion: Combining interpretability with robustness shows promise, suggesting concept-driven defenses as a scalable strategy for securing ML models against adversarial patch attacks.

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [105] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer is a flexible and efficient Seq-VPR method using Recurrent Deformable Transformer Encoder that supports variable sequence lengths, achieves faster inference and lower memory usage while improving recall performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based Seq-VPR methods prioritize performance but lack flexibility (fixed sequence lengths) and efficiency (slow inference, high memory usage), making them unsuitable for real-time applications.

Method: Proposed Adapt-STformer with Recurrent Deformable Transformer Encoder (Recurrent-DTE) that uses iterative recurrent mechanism to fuse information from multiple sequential frames, enabling variable sequence length support.

Result: On Nordland, Oxford, and NuScenes datasets: boosts recall by up to 17%, reduces sequence extraction time by 36%, and lowers memory usage by 35% compared to second-best baseline.

Conclusion: Adapt-STformer achieves both flexibility and efficiency in Seq-VPR, making it suitable for real-time applications while maintaining superior performance.

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [106] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit reframes image editing as video generation to ensure physical consistency by treating input and edited images as video frames, leveraging pretrained video models and temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current generative models lack physical consistency in image editing, which is crucial for world simulation tasks where edited objects must remain coherent.

Method: Uses input and edited images as first/last video frames, leverages pretrained video models for temporal consistency, introduces temporal reasoning with reasoning tokens to imagine plausible editing trajectories, then drops tokens to avoid full video rendering costs.

Result: Outperforms state-of-the-art baselines in both visual fidelity and physical plausibility on the new PBench-Edit benchmark for physically consistent editing.

Conclusion: ChronoEdit effectively bridges the physical consistency gap in image editing by combining video generation principles with temporal reasoning, offering 14B and 2B model variants for practical deployment.

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [107] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD is the largest publicly available archive of 3D mesh gait data for Parkinson's Disease, spanning 9 cohorts from 8 clinical centers, enabling supervised clinical score prediction and unsupervised motion pretext tasks.


<details>
  <summary>Details</summary>
Motivation: Objective gait assessment in Parkinson's Disease is limited by the absence of large, diverse, and clinically annotated motion datasets.

Method: All recordings (RGB video or motion capture) are converted into anonymized SMPL meshes via a harmonized preprocessing pipeline. The method supports supervised clinical score prediction (UPDRS gait scores) and unsupervised motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D reconstruction).

Result: Motion encoders consistently outperform handcrafted features. Pretraining on CARE-PD reduces MPJPE from 60.8mm to 7.5mm and boosts PD severity macro-F1 by 17 percentage points.

Conclusion: CARE-PD demonstrates the value of clinically curated, diverse training data for Parkinson's Disease assessment, with the dataset and benchmark code released for non-commercial research.

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [108] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR is a multi-scale autoregressive framework that predicts spatial gene expression from H&E stained images by modeling gene dependencies hierarchically and generating discrete count tokens directly, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Spatial Transcriptomics is expensive, while H&E stained images are widely available. Current methods predict genes independently and use continuous regression despite expression being discrete counts, leading to biologically implausible results.

Method: GenAR clusters genes into hierarchical groups to capture dependencies, models expression as discrete token generation for raw counts, and uses fused histological and spatial embeddings in a coarse-to-fine autoregressive framework.

Result: Extensive experiments on four Spatial Transcriptomics datasets across different tissue types show GenAR achieves state-of-the-art performance.

Conclusion: GenAR offers a cost-effective alternative for spatial gene expression prediction with potential applications in precision medicine and molecular profiling.

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [109] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: Diffusion^2 is a novel framework for pedestrian trajectory prediction in extreme scenarios where only momentary observation data is available, using two sequential diffusion models for backward historical trajectory generation and forward future prediction.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often lack sufficient observational data (e.g., pedestrians emerging from blind spots), making accurate trajectory prediction challenging and increasing traffic accident risks.

Method: Two sequentially connected diffusion models: one for backward prediction of unobserved historical trajectories, and another for forward prediction of future trajectories, with dual-head parameterization for uncertainty estimation and temporally adaptive noise module.

Result: Sets new state-of-the-art performance in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.

Conclusion: The proposed Diffusion^2 framework effectively addresses the challenge of pedestrian trajectory prediction in extreme scenarios with limited observational data.

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [110] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim is a language-guided framework that generates 4D scenes with multi-view consistency and object-level controls for robotics applications.


<details>
  <summary>Details</summary>
Motivation: World models supporting controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, reproducible evaluation, and flexible task design. Current text-to-video models are constrained to 2D views and offer limited interaction.

Method: Integrates trajectory-guided generation with feature field distillation, allowing edits to be applied interactively without full re-generation. From natural language instructions, produces dynamic environments with object-level controls.

Result: Experiments show that MorphoSim maintains high scene fidelity while enabling controllability and editability. Generates 4D scenes with multi-view consistency where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints.

Conclusion: MorphoSim provides a framework for generating controllable and editable 4D environments that overcome limitations of existing 2D text-to-video models, offering valuable capabilities for robotics applications.

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [111] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs struggle with compositional counting of multiple shape types despite performing well with single shapes, revealing a fundamental limitation in current vision-language models.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Vision-Language Models (VLMs) can count objects correctly, particularly in compositional scenarios with multiple shape types, which remains an unexplored fundamental capability.

Method: Created VLMCountBench - a minimalist benchmark using basic geometric shapes (triangles, circles) in controlled settings with strict variable control for color, size, and prompt refinement, focusing exclusively on counting tasks.

Result: VLMs count reliably with single shape types but show substantial failures in compositional counting when multiple shape types are combined, highlighting a fundamental empirical limitation.

Conclusion: Current VLMs have significant limitations in compositional counting, motivating important future research directions to address this fundamental capability gap.

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [112] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++ is a blind face restoration framework that decomposes the task into three sub-tasks: identity-preserving restoration, high-quality generation, and dynamic fusion, achieving superior visual quality while maintaining identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing BFR methods struggle with the trade-off between visual quality and identity fidelity, often resulting in either identity distortion or suboptimal degradation removal.

Method: Decomposes BFR into three sub-tasks with three key contributions: learning-based deformable face registration for semantic alignment, texture-guided restoration network for dynamic texture transfer, and deep metric learning integration with informative sample generation.

Result: Extensive experiments on real-world and synthetic datasets demonstrate superior performance in both visual fidelity and identity consistency compared to existing methods.

Conclusion: CodeFormer++ effectively maximizes the utility of generative priors for high-quality face restoration while preserving identity, overcoming the traditional trade-off between visual quality and identity fidelity.

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [113] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: A.I.R. is a training-free frame selection method for VideoQA that uses adaptive iterative reasoning with a powerful VLM to select high-potential frames efficiently, outperforming existing methods in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current frame selection methods for VideoQA face a trade-off: lightweight similarity models (like CLIP) fail to capture complex query nuances, while VLM-based methods are computationally prohibitive.

Method: Proposes A.I.R. - Adaptive, Iterative, and Reasoning-based frame selection that uses a powerful VLM for deep semantic analysis in a cost-effective iterative loop processing small batches of high-potential frames.

Result: Outperforms existing frame selection methods, significantly boosts foundation VLM performance, and achieves substantial computational efficiency gains over other VLM-based techniques across various VideoQA benchmarks.

Conclusion: A.I.R. effectively addresses the accuracy-efficiency trade-off in VideoQA frame selection by combining deep semantic analysis with adaptive iterative processing.

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [114] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: reAR is a training strategy that addresses generator-tokenizer inconsistency in visual autoregressive models by adding token-wise regularization, improving performance to match diffusion models without changing the tokenizer or inference pipeline.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive generation underperforms compared to diffusion models due to generator-tokenizer inconsistency, where AR-generated tokens may not be well-decoded by the tokenizer.

Method: Proposes reAR with token-wise regularization: when predicting next token, the transformer also recovers current token's visual embedding and predicts target token's embedding under noisy context.

Result: Reduces gFID from 3.02 to 1.86 and improves IS to 316.9 on ImageNet with standard tokenizer. Achieves gFID of 1.42 with 177M parameters, matching larger diffusion models (675M).

Conclusion: reAR effectively addresses generator-tokenizer inconsistency and substantially improves visual AR generation performance to match state-of-the-art diffusion models.

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [115] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet is a unified architecture for camouflaged object detection that integrates multi-scale features through channel calibration and spatial enhancement, achieving state-of-the-art performance with real-time inference speed.


<details>
  <summary>Details</summary>
Motivation: Current camouflaged object detection methods accumulate complex components (boundary modules, attention mechanisms, multi-scale processors) independently, creating computational burden without proportional gains. This forces processing at reduced resolutions, eliminating fine details essential for camouflage detection.

Method: SPEGNet uses a unified design integrating multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations with semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions.

Result: SPEGNet achieves 0.887 Sα on CAMO, 0.890 on COD10K, and 0.895 on NC4K datasets with real-time inference speed. It excels across scales from tiny intricate objects to large pattern-similar ones while handling occlusion and ambiguous boundaries.

Conclusion: The unified SPEGNet architecture strikes an optimal balance between boundary precision and regional consistency, outperforming complex accumulated component approaches while maintaining computational efficiency and real-time performance.

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [116] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM is an automated pipeline that converts detection datasets into medical VQA data with Chain-of-Thought reasoning, using an Integrated CoT-Curriculum Strategy to achieve state-of-the-art performance on medical VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging, requiring better integration of reasoning capabilities into medical vision-language models.

Method: Automated pipeline that converts detection datasets into large-scale medical VQA data with Chain-of-Thought reasoning by linking lesion boxes to organ segmentation and structured rationales, using a three-stage Integrated CoT-Curriculum Strategy (Easy, Medium, Hard stages).

Result: MedCLM attains state-of-the-art performance on several medical VQA benchmarks, demonstrating superior reasoning capabilities.

Conclusion: MedCLM provides a scalable framework for developing clinically aligned medical vision-language models with improved diagnostic reasoning capabilities.

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [117] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: The paper introduces VaseVQA-3D, the first 3D visual question answering dataset for ancient Greek pottery analysis, and develops VaseVLM model to address data scarcity and domain knowledge limitations in cultural heritage applications of vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models (VLMs) face severe data scarcity and insufficient domain knowledge when dealing with specialized cultural heritage domains like 3D vase artifacts, struggling to effectively handle culturally significant specialized tasks due to lack of targeted training data.

Method: Proposed VaseVQA-3D dataset with 664 ancient Greek vase 3D models and corresponding question-answer data, establishing a complete data construction pipeline. Developed VaseVLM model through domain-adaptive training to enhance performance in vase artifact analysis.

Result: Experimental results show improvements of 12.8% on R@1 metrics and 6.6% on lexical similarity compared with previous state-of-the-art on the VaseVQA-3D dataset, significantly improving recognition and understanding of 3D vase artifacts.

Conclusion: The approach provides new technical pathways for digital heritage preservation research by effectively addressing data scarcity and domain adaptation challenges in cultural heritage applications of VLMs.

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [118] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit is a specialized image editing model for e-commerce that addresses consistency limitations in general models through hierarchical architecture and two-stage training, achieving superior performance in product appearance preservation.


<details>
  <summary>Details</summary>
Motivation: General image generation and editing models struggle with maintaining consistency in e-commerce scenarios where product appearance and layout integrity are crucial.

Method: Three-pronged approach: 1) Comprehensive data engineering pipeline for high-quality consistent editing data, 2) Hierarchical model framework with base model, pattern shifting modules, and consistency enhancement modules, 3) Two-stage training strategy (pattern shifting then consistency enhancement) with separate datasets.

Result: TBStar-Edit outperforms existing general-domain editing models on e-commerce benchmark in both objective metrics (VIE Score) and subjective user preference.

Conclusion: The specialized approach combining rigorous data engineering, hierarchical architecture design, and staged training strategy effectively addresses e-commerce image editing consistency challenges.

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [119] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: Asynchronous diffusion models improve text-to-image alignment by allocating different timesteps to different pixels, allowing prompt-related regions to reference clearer context during denoising.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models struggle with text-to-image alignment due to synchronous denoising, where all pixels evolve simultaneously from noise, preventing prompt-related regions from obtaining clear context from other regions.

Method: Proposed asynchronous diffusion framework that allocates distinct timesteps to different pixels and dynamically modulates pixel-wise denoising schedules, allowing prompt-related regions to denoise more gradually and leverage clearer inter-pixel context.

Result: Extensive experiments show significant improvement in text-to-image alignment across diverse prompts compared to standard synchronous diffusion models.

Conclusion: Asynchronous diffusion models effectively address the alignment limitations of synchronous denoising by enabling better context utilization during the generation process, leading to improved faithfulness to input prompts.

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [120] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: TAG is an efficient guidance method that amplifies tangential components of diffusion model scores to improve semantic consistency without modifying the underlying model.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models suffer from semantic inconsistencies and hallucinations, while current guidance methods introduce computational overhead through external signals or architectural changes.

Method: TAG uses an intermediate sample as projection basis and amplifies tangential components of estimated scores via first-order Taylor expansion to steer sampling trajectory toward higher-probability regions.

Result: TAG improves diffusion sampling fidelity with minimal computational addition, operating as a plug-and-play, architecture-agnostic module.

Conclusion: TAG offers a new perspective on diffusion guidance by directly manipulating trajectory signals rather than relying on external modifications, providing efficient semantic consistency enhancement.

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [121] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: CRL proposes conditional representation learning that extracts task-specific representations using LLM-generated descriptive texts to construct semantic basis, avoiding costly supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Universal representations often misalign with customized downstream tasks, while supervised fine-tuning is computationally expensive and requires heavy annotation.

Method: Use LLM to generate descriptive texts for user criteria to construct semantic basis, then project image representations into this conditional space using VLM.

Result: CRL achieves superior performance on classification and retrieval tasks compared to universal representations, demonstrating better alignment with customized criteria.

Conclusion: CRL provides an effective framework for learning conditional representations tailored to arbitrary user criteria without expensive fine-tuning.

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [122] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: The paper introduces AI Session Recorder to capture pathologists' viewing behavior from WSI viewers, creating Pathology-CoT dataset for training Pathologist-o3 agent that achieves superior performance in metastasis detection.


<details>
  <summary>Details</summary>
Motivation: Current pathology foundation models lack practical agentic systems that can decide what to examine next and deliver explainable diagnoses, due to missing supervision of expert viewing behavior that is tacit and experience-based.

Method: AI Session Recorder captures routine navigation from standard WSI viewers, converts logs into behavioral commands (inspect/peek at magnifications) and bounding boxes, with human review creating Pathology-CoT dataset for training Pathologist-o3 agent with two-stage reasoning.

Result: Pathologist-o3 achieved 84.5% precision, 100.0% recall, and 75.4% accuracy on gastrointestinal lymph-node metastasis detection, exceeding OpenAI o3 model and generalizing across backbones.

Conclusion: The framework makes agentic pathology practical by converting everyday viewer logs into scalable expert supervision, establishing a path to human-aligned, upgradeable clinical AI.

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [123] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: S²Fin is a spatial-spectral-frequency interaction network that integrates frequency domain learning with multimodal remote sensing image classification, using high-frequency sparse enhancement transformers and spatial-frequency fusion strategies to improve feature extraction from heterogeneous images.


<details>
  <summary>Details</summary>
Motivation: Current feature fusion techniques struggle to extract structural and detail features from heterogeneous and redundant multimodal remote sensing images, motivating the introduction of frequency domain learning to model key and sparse detail features.

Method: Proposes S²Fin with pairwise fusion modules across spatial, spectral, and frequency domains, including: high-frequency sparse enhancement transformer with sparse spatial-spectral attention, two-level spatial-frequency fusion strategy with adaptive frequency channel module and high-frequency resonance mask, and spatial-spectral attention fusion module.

Result: Experiments on four benchmark multimodal datasets with limited labeled data demonstrate that S²Fin performs superior classification, outperforming state-of-the-art methods.

Conclusion: The proposed S²Fin network effectively integrates frequency domain learning and achieves superior performance in multimodal remote sensing image classification by better extracting structural and detail features from heterogeneous data.

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [124] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: Proposes a novel ensemble framework combining transformer-based architectures (Swin Transformers, ViTs) and texture-based methods for robust deepfake detection, achieving state-of-the-art performance on the DFWild-Cup dataset.


<details>
  <summary>Details</summary>
Motivation: Address the pressing issue of manipulated media detection, as existing approaches fail to generalize across diverse datasets and generation techniques.

Method: Ensemble framework with innovative techniques: data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation to handle dataset imbalances and enhance high-impact regions like eyes and mouth.

Result: Achieves state-of-the-art performance on DFWild-Cup dataset (diverse subset of 8 deepfake datasets), with transformers excelling in global feature extraction and texture-based methods providing interpretability.

Conclusion: Hybrid models can effectively address evolving deepfake detection challenges, offering robust solutions for real-world applications through complementary strengths of different approaches.

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [125] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: This study compares five image segmentation methods (including SLIC) for deforestation detection in tropical forests, finding that classifier ensembles show noticeable improvements in balanced accuracy when using different segmentation approaches.


<details>
  <summary>Details</summary>
Motivation: The ForestEyes project combines citizen science and machine learning for deforestation detection, but traditional SLIC segmentation may be outperformed by newer methods in remote sensing applications.

Method: Compared four best segmentation methods together with SLIC, used PyCaret AutoML library to select top classifiers, and applied classifier fusion (ensemble) approach.

Result: Initial results showed little performance variation among segmentation methods, but classifier ensembles demonstrated noticeable improvements in balanced accuracy.

Conclusion: Both segmentation method choice and combining machine learning models are important for deforestation detection tasks, with ensemble approaches showing particular promise.

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [126] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona is a large-scale benchmark for evaluating LLMs' classroom-oriented subjective abilities, featuring 1,308 authentic dialogues expanded to 128k turns through persona stylization. It decomposes subjective performance into three tasks and shows significant improvements (+14.9% to +33.6%) when models are fine-tuned on the dataset.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in education for virtual student agents, there's a need to assess their classroom-oriented subjective abilities to understand model boundaries and enable trustworthy deployment in educational settings.

Method: Created EduPersona benchmark with 1,308 authentic classroom dialogues (12,814 Q&A turns) expanded to ~128k turns through persona stylization based on Big Five theory. Decomposed subjective performance into three tasks: basic coherence, student realism, and long-term persona consistency. Evaluated three LLMs and their persona-fine-tuned variants.

Result: Persona-fine-tuned models showed consistent significant improvements across all tasks: TASK1 basic coherence +33.6%, TASK2 student realism +30.6%, and TASK3 long-term persona consistency +14.9%. The dataset proved effective while revealing heterogeneous difficulty in persona modeling.

Conclusion: EduPersona provides the first classroom benchmark focused on subjective abilities, establishes a decoupled research paradigm, and will be open-sourced to advance trustworthy and human-like AI for education.

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [127] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: The paper introduces a hierarchical Multi-Stage Mixture of Movement Experts (MoME) architecture for predicting psychological traits from gait sequences, achieving state-of-the-art performance on the PsyMo benchmark.


<details>
  <summary>Details</summary>
Motivation: Gait contains rich biometric and behavioral information, but using walking patterns to infer psychological traits remains challenging and underexplored.

Method: MoME processes walking cycles in four stages of movement complexity using lightweight expert models to extract spatio-temporal features and task-specific gating modules to adaptively weight experts across traits and stages.

Result: Outperforms state-of-the-art gait analysis models on PsyMo benchmark (17 psychological traits), achieving 37.47% weighted F1 score at run level and 44.6% at subject level. Integrating auxiliary tasks (identity, gender, BMI) further improves psychological trait estimation.

Conclusion: Demonstrates viability of multi-task gait-based learning for psychological trait estimation and provides foundation for future research on movement-informed psychological inference.

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [128] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit is a framework that addresses concept mixing in multi-concept personalization for text-to-image diffusion models through Token-wise Value Adaptation (ToVA) during training and Latent Optimization for Disentangled Attention (LODA) during inference.


<details>
  <summary>Details</summary>
Motivation: The main challenge in multi-concept personalization is concept mixing, where multiple learned concepts interfere or blend undesirably in the output image, which existing methods fail to adequately address.

Method: The framework has two components: 1) Token-wise Value Adaptation (ToVA) - a merging-free training method that adapts only the value projection in cross-attention, avoiding disruption of the attention mechanism; 2) Latent Optimization for Disentangled Attention (LODA) - optimizes the input latent during inference to alleviate attention entanglement.

Result: Extensive qualitative and quantitative experiments demonstrate that ConceptSplit achieves robust multi-concept personalization and mitigates unintended concept interference.

Conclusion: ConceptSplit effectively addresses concept mixing in multi-concept personalization through its novel training and inference components, providing a solution to the problem of concept interference in text-to-image diffusion models.

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [129] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: A label-efficient liver segmentation method for multi-phase MRI that uses foundation model adaptation and co-training to handle limited labeled data, unlabeled sequences, and vendor variations without spatial registration.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation in multi-phase MRI is crucial for fibrosis assessment but faces challenges with scarce labeled data, uneven distribution across modalities/vendors, spatial misalignment, and missing phases.

Method: Integrates foundation-scale 3D segmentation backbone with fine-tuning, co-training with cross pseudo supervision to leverage unlabeled volumes, and standardized preprocessing pipeline without spatial registration.

Result: The model demonstrates robust segmentation performance across both labeled and unlabeled domains, generalizing effectively across MRI phases and vendor systems.

Conclusion: The approach shows effectiveness for label-efficient liver segmentation in multi-phase, multi-vendor MRI and highlights the potential of combining foundation model adaptation with co-training for clinical imaging tasks.

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [130] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: A diffusion-based framework for identity-consistent facial expression generation with fine-grained control using FLAME blendshape parameters and reference-based editing.


<details>
  <summary>Details</summary>
Motivation: Human-centric generative models need both identity consistency and precise control over human performance, but current methods struggle with fine-grained expression control without compromising identity.

Method: Built on ID-consistent face foundation model with compositional design featuring expression cross-attention module guided by FLAME blendshape parameters, trained on diverse image/video data with expressive variation, plus pluggable Reference Adapter for real image editing.

Result: Model generalizes beyond basic emotions to subtle micro-expressions and expressive transitions, outperforming existing methods in tailored and identity-consistent expression generation.

Conclusion: The framework successfully achieves faithful reimagining of subjects under any facial expression while maintaining identity consistency and enabling fine-grained control.

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [131] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff is a temporal diffusion framework that generates diverse and realistic facial reactions in dialogue by incorporating spatio-temporal facial kinematics and action unit dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model the stochasticity and dynamics of real human facial reactions, leading to unrealistic and limited reaction generation.

Method: Proposes ReactDiff framework that incorporates two priors: temporal facial behavioral kinematics and facial action unit dependencies to guide the diffusion process toward realistic human reaction manifolds.

Result: Extensive experiments on REACT2024 dataset show state-of-the-art reaction quality, diversity, and appropriateness compared to existing methods.

Conclusion: ReactDiff successfully addresses the challenge of generating diverse and human-like facial reactions by modeling facial dynamics and anatomical constraints through temporal diffusion.

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [132] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: This paper proposes a novel approach for 3D Semantic Scene Graph prediction that focuses on improving object feature quality through a discriminative encoder and contrastive pretraining, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous methods for 3D Semantic Scene Graph prediction rely excessively on Graph Neural Networks while having insufficient discriminative capability in object and relationship features, limiting overall performance.

Method: The authors design a highly discriminative object feature encoder with contrastive pretraining that decouples object representation learning from scene graph prediction, and effectively combines geometric and semantic features for relationship prediction.

Result: The proposed approach significantly outperforms previous state-of-the-art methods on the 3DSSG dataset, with substantial performance improvements across all evaluation metrics when integrated into existing frameworks.

Conclusion: Object feature quality is critical for 3D scene graph accuracy, and the proposed contrastive pretraining strategy with discriminative feature encoding effectively addresses previous limitations, achieving superior performance in both object classification and relationship prediction.

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [133] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: First benchmark for monocular metric depth estimation in wildlife monitoring, evaluating 4 state-of-the-art methods. Depth Anything V2 performs best (MAE: 0.454m), while ZoeDepth shows significant degradation outdoors.


<details>
  <summary>Details</summary>
Motivation: Camera traps lack depth information, and while monocular depth estimation methods have advanced, their performance in natural wildlife environments hasn't been systematically evaluated.

Method: Evaluated 4 MDE methods (Depth Anything V2, ML Depth Pro, ZoeDepth, Metric3D) and geometric baseline on 93 camera trap images with ground truth distances using calibrated ChARUCO patterns.

Result: Depth Anything V2 achieved best performance (MAE: 0.454m, correlation: 0.962). Median-based depth extraction consistently outperformed mean-based approaches. ZoeDepth was fastest (0.17s/image) but least accurate (MAE: 3.087m).

Conclusion: Establishes performance baselines for wildlife applications and provides practical guidance for implementing depth estimation in conservation monitoring systems, with Depth Anything V2 offering optimal balance of accuracy and speed.

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [134] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine is an automated system that uses oriented bounding boxes (OBB) instead of traditional horizontal bounding boxes to accurately quantify sponsor logo visibility in sports broadcasts, addressing the limitations of manual and existing automated methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for measuring sponsor visibility in sports broadcasts are manual, subjective, and unscalable. Existing automated systems using horizontal bounding boxes (HBB) are inaccurate when logos appear rotated or skewed due to camera angles and perspective distortions.

Method: The system predicts Oriented Bounding Boxes (OBB) for precise logo detection regardless of orientation. It uses a dataset of 1,103 frames from Swedish elite soccer with 670 unique sponsor logos annotated with OBBs. The system integrates detections into an analytical pipeline and includes a language-driven agentic layer for natural language queries.

Result: The model achieves mAP@0.5 of 0.859, precision of 0.96, and recall of 0.87, demonstrating robust performance in localizing logos under diverse broadcast conditions. The system calculates precise visibility metrics like exposure duration and on-screen coverage.

Conclusion: ExposureEngine provides a comprehensive, auditable, and interpretable solution for sponsor measurement in sports media, overcoming the limitations of traditional methods through rotation-aware detection and advanced analytics capabilities.

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [135] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: AA-YOLO integrates statistical anomaly detection into YOLO's detection head to improve infrared small target detection by treating targets as anomalies, achieving competitive performance with low false alarms and high robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional object detectors struggle with infrared small target detection due to complex backgrounds and tiny target sizes, leading to high false alarm rates in defense applications.

Method: Proposes Anomaly-Aware YOLO (AA-YOLO) that integrates a statistical anomaly detection test into YOLO's detection head, treating small targets as unexpected patterns against background.

Result: Achieves competitive performance on IRSTD benchmarks with remarkable robustness to limited training data, noise, and domain shifts. Successfully applied across various YOLO backbones including lightweight models and instance segmentation YOLO.

Conclusion: AA-YOLO provides a generic, versatile solution for real-world IRSTD deployments with constrained resources, offering effective false alarm control and strong performance across different scenarios.

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [136] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: Transformer-based two-stream framework for person identification using spatial and temporal keypoint features achieves 98.03% accuracy through feature fusion.


<details>
  <summary>Details</summary>
Motivation: To investigate transformer architectures for person identification in natural face-to-face conversation scenarios using body keypoints.

Method: Two-stream framework with spatial transformer for 133 COCO WholeBody keypoint configurations and multi-scale temporal transformer for motion patterns, with experiments on pre-trained vs from-scratch training and velocity features.

Result: Spatial transformer achieved 95.74% accuracy, temporal transformer 93.90%, and feature fusion reached 98.03%. Domain-specific training outperformed transfer learning, and spatial features were more discriminative than temporal dynamics.

Conclusion: Transformers are effective for person identification in natural interactions, with spatial and temporal features being complementary, providing insights for future multimodal and cross-cultural studies.

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [137] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ is a Progressive Gaussian Transformer Framework for open-vocabulary 3D occupancy prediction that uses progressive online densification and anisotropy-aware sampling to achieve state-of-the-art performance with 14.3% mIoU improvement.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off in text-aligned scene modeling where sparse Gaussian representation struggles with small objects while dense representation has high computational overhead.

Method: Progressive Gaussian Transformer Framework with progressive online densification (gradually enhancing 3D Gaussian representation) and anisotropy-aware sampling with spatio-temporal fusion (adaptive receptive field assignment).

Result: Achieves state-of-the-art performance with 14.3% mIoU improvement over previous best method.

Conclusion: PG-Occ effectively balances computational efficiency and fine-grained scene detail capture through progressive representation enhancement and adaptive sampling strategies.

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [138] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: This paper proposes a novel open-vocabulary learning method that generates unseen-class data to estimate data distributions in open environments, achieving up to 14% improvement over baselines on 11 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for open-vocabulary learning only use seen-class data to estimate distributions, but the absence of unseen classes makes the estimation error inherently unidentifiable. Learning beyond seen classes is crucial for bounding this estimation error.

Method: The method consists of a class-domain-wise data generation pipeline that generates unseen-class data using a hierarchical semantic tree and domain information from seen-class data, and a distribution alignment algorithm that estimates and maximizes posterior probability for better generalization.

Result: Extensive experiments on 11 datasets demonstrate that the proposed method outperforms baseline approaches by up to 14%, showing significant effectiveness and superiority in open-vocabulary learning.

Conclusion: Generating unseen-class data is theoretically proven to effectively estimate distributions in open environments by bounding estimation errors, and the proposed method successfully implements this approach to achieve superior performance in open-vocabulary learning tasks.

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [139] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: The FedSurg challenge benchmarked federated learning for surgical video classification using the Appendix300 dataset, evaluating generalization to unseen centers and local adaptation through fine-tuning. Top approaches used ViViT models with various FL aggregation schemes, showing limited generalization but improved adaptation after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To assess how well current federated learning methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data, establishing the first benchmark for FL in surgical video classification.

Method: Participants developed strategies to classify inflammation stages in appendicitis using the multi-center Appendix300 video dataset. Approaches included foundation models with linear probing, metric learning with triplet loss, and FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost.

Result: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and hyperparameter tuning difficulties.

Conclusion: The FedSurg Challenge establishes the first FL benchmark for surgical video classification, highlighting the trade-off between local personalization and global robustness. Findings underscore the importance of architecture choice, preprocessing, and loss design, offering a reference for future development of imbalance-aware, adaptive FL methods in clinical surgical AI.

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [140] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: Automated two-robot system for high-fidelity 3D scanning of cultural heritage artefacts using coordinated robotic manipulation and optimized trajectory planning.


<details>
  <summary>Details</summary>
Motivation: Conventional 3D scanning methods require specialized expertise and manual intervention, which limits efficiency and accessibility for cultural heritage preservation.

Method: Two-robot system with coordinated motion planning: one robot equipped with scanner and another for tray-handling. Parameterizes scanning space into regions with optimized trajectory planning and waypoint distribution to ensure comprehensive coverage and minimize occlusions.

Result: Achieves significantly lower Chamfer Distance and higher F-score compared to baseline methods, demonstrating superior geometric accuracy and improved digitization efficiency.

Conclusion: The automated system eliminates need for handheld or semi-automatic workflows, reduces reliance on expert operators, and provides efficient, high-quality 3D scanning for cultural heritage preservation.

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [141] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: Vision-transformers outperform CNNs in large data scenarios for geometric estimation tasks, but CNNs match ViT performance in small data settings due to their inductive bias and smaller capacity. ViTs show better cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: To compare the efficiency of ViTs and large-scale CNNs as backbone architectures for geometric estimation tasks (2D rigid transformations and fundamental matrix prediction) in various data regimes, particularly few-shot scenarios.

Method: Systematic comparison of large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) across different data size settings, including few-shot scenarios and cross-domain evaluation.

Result: ViTs outperform CNNs in large downstream-data scenarios during refinement, similar to training from scratch. In small data scenarios, CNNs match ViT performance due to their inductive bias and smaller capacity. ViTs exhibit stronger generalization in cross-domain evaluation.

Conclusion: Careful selection of model architectures for refinement is crucial, motivating future research towards hybrid architectures that balance local and global representations for geometric estimation tasks.

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [142] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON is a Virtual Try-On framework using Diffusion Transformers that achieves state-of-the-art performance on VITON-HD and extends VTO to Virtual Try-All capabilities with advanced image editing features.


<details>
  <summary>Details</summary>
Motivation: Existing VTO models struggle with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories.

Method: Leverages Diffusion Transformer (DiT) adapted for image-conditioned VTO, exploring multiple configurations including in-context token concatenation, channel concatenation, and ControlNet integration. Trained on expanded dataset with varied backgrounds and non-garment categories.

Result: Surpasses state-of-the-art methods on VITON-HD with superior detail preservation and robustness, outperforms models with VTA capabilities on diverse dataset spanning thousands of product categories.

Conclusion: DiT-VTON redefines VTO beyond garment try-on to offer versatile Virtual Try-All solution with advanced image editing functionalities including pose preservation, localized editing, texture transfer, and object-level customization.

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [143] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg is a framework that reconstructs dynamic egocentric visual perspectives of operating room staff from fixed-camera video, enabling immersive surgical analysis without disrupting clinical workflow.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical observation methods rely on fixed viewpoints or recollections, missing the actual egocentric visual perspectives that guide clinical decisions. Fixed-camera videos provide limited insights into surgical safety, training, and workflow optimization decisions.

Method: EgoSurg combines geometry-driven neural rendering with diffusion-based view enhancement to synthesize arbitrary egocentric viewpoints at any moment from wall-mounted fixed-camera video.

Result: The framework successfully reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity across multi-site surgical cases and controlled studies.

Conclusion: EgoSurg transforms existing OR camera infrastructure into navigable dynamic 3D records, establishing a foundation for immersive surgical data science that enables surgical practice to be visualized and analyzed from every angle.

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [144] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper analyzes why multimodal language models (MLMs) struggle with perception-heavy tasks by examining how they process visual key-value tokens. Key findings include: image value tokens contain sufficient visual information for perception tasks, language models augment visual information but contain less than standalone visual encoders, input-agnostic image key tokens introduce artifacts, and adding text prefixes improves perception capabilities.


<details>
  <summary>Details</summary>
Motivation: Despite existing interpretability work on VIT encoders and transformer activations, there's limited understanding of why MLMs perform poorly on perception-heavy tasks. The authors aim to provide new insights by examining how popular MLMs process visual key-value tokens.

Method: The study examines popular MLMs (LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT) by analyzing the flow of visual information through language models. They investigate visual key-value tokens, compare information content between language models and standalone visual encoders, and test interventions like adding text prefixes to image inputs.

Result: Image value tokens encode sufficient information for zero-shot perception tasks. Language models augment visual information but contain less visual information than equivalent visual encoders. Input-agnostic image key tokens in later layers introduce artifacts that reduce perception capability. Adding text prefixes improves perception capabilities, and language models fail to surface available perception information in 33.3% of Art Style questions.

Conclusion: The findings provide insights into the role of key-value tokens in multimodal systems, suggesting that better control of visual information in language models could significantly improve perception capabilities. This work paves the way for deeper mechanistic interpretability of MLMs and new training directions for visual encoder and language model components.

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [145] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON is the first 4D virtual try-on framework that generates realistic try-on results from a single garment image, supporting free pose control, novel-view rendering, and dynamic garment interactions without multi-view captures or physics priors.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods lack support for dynamic 4D interactions and require multi-view garment captures or physics priors, limiting their practical applications in AR/VR and gaming.

Method: Uses two key modules: (1) Reciprocal Flow Rectifier for optical-flow correction to ensure temporal coherence, and (2) Non-Linear Deformer that decomposes Gaussian maps into view-pose-invariant and view-pose-specific components for adaptive garment deformations.

Result: Achieves high fidelity, diversity, and dynamic garment realism in 4D virtual try-on, outperforming existing baselines in both qualitative and quantitative comparisons.

Conclusion: AvatarVTON enables practical 4D virtual try-on applications for AR/VR, gaming, and digital-human scenarios with single-view supervision and without requiring complex multi-view captures or physics priors.

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [146] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN is an anatomically-informed Mixture-of-Experts framework for medical image classification that uses anatomical priors to train specialized experts for different lung regions, achieving superior performance in ILD classification with improved interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE systems lack domain-specific constraints needed for medical imaging where anatomical structure and regional disease heterogeneity strongly influence pathological patterns.

Method: Leverages anatomical priors to train seven specialized experts for distinct lung lobes and bilateral lung combinations, with multi-modal gating mechanisms that integrate radiomics biomarkers and deep learning features (CNN, ViT, Mamba) to weight expert contributions.

Result: Achieved average AUC of 0.8646 (+12.5% improvement over baseline), with lower-lobe experts reaching AUCs of 0.88-0.90, outperforming DL counterparts and aligning with known disease progression patterns.

Conclusion: REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach extensible to other structured medical imaging applications.

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [147] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: A 3D Flow Matching framework generates synthetic CT from MRI or CBCT for radiotherapy, achieving accurate global anatomical reconstruction but limited fine detail preservation due to resolution constraints.


<details>
  <summary>Details</summary>
Motivation: To enable MRI-only and CBCT-based adaptive radiotherapy by generating synthetic CT images, improving treatment precision while reducing patient radiation exposure.

Method: A fully 3D Flow Matching framework that transforms Gaussian noise into synthetic CT images by integrating a learned velocity field, conditioned on features from input MRI/CBCT using a lightweight 3D encoder.

Result: The method accurately reconstructs global anatomical structures but has limited preservation of fine details due to low training resolution constraints from memory and runtime limitations.

Conclusion: Future work will explore patch-based training and latent-space flow models to improve resolution and local structural fidelity for better synthetic CT generation.

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [148] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: The paper proposes NFPF, a novel Unsupervised Active Learning method that uses Specific Feature Learning Machine to measure sample importance and achieves performance comparable to supervised AL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Active Learning requires significant human involvement in iterative labeling, while existing Unsupervised AL methods struggle with performance and robustness due to reliance on local gradient-based scoring.

Method: NFPF employs a Specific Feature Learning Machine (SFLM) to quantify sample importance and uses a Reconstruction Difference metric for initial sample selection, providing better data distribution coverage.

Result: NFPF significantly outperforms all established UAL methods and achieves performance comparable to supervised AL methods on vision datasets, with enhanced robustness and improved data distribution coverage.

Conclusion: The proposed NFPF framework revolutionizes sample importance measurement in UAL and demonstrates superior performance, making it a viable alternative to supervised active learning approaches.

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [149] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: AT-BPTT is a novel framework for dataset distillation that dynamically adapts truncation positions and window sizes based on gradient behavior, achieving state-of-the-art performance with significant speed and memory improvements.


<details>
  <summary>Details</summary>
Motivation: Existing inner-loop optimization methods for dataset distillation rely on random truncation strategies that lack flexibility and yield suboptimal results, as neural networks exhibit distinct learning dynamics across different training stages.

Method: Proposes Automatic Truncated Backpropagation Through Time (AT-BPTT) with three key components: probabilistic stage-aware timestep selection, adaptive window sizing based on gradient variation, and low-rank Hessian approximation to reduce computational overhead.

Result: Achieves state-of-the-art performance with 6.16% average accuracy improvement over baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K, while accelerating inner-loop optimization by 3.9x and saving 63% memory cost.

Conclusion: AT-BPTT effectively addresses the limitations of random truncation in dataset distillation by dynamically adapting to neural network learning dynamics, delivering superior performance and efficiency.

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [150] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: CA3D-Diff is a novel bidirectional mammogram view translation framework that uses conditional diffusion models with column-aware cross-attention and implicit 3D structure reconstruction to address the challenge of missing or corrupted views in dual-view mammography.


<details>
  <summary>Details</summary>
Motivation: In real-world clinical workflows, one mammography view (CC or MLO) may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting diagnostic effectiveness. View-to-view translation can help recover missing views and improve lesion alignment, but this is challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections.

Method: The proposed CA3D-Diff framework includes: 1) Column-aware cross-attention mechanism that leverages geometric properties where anatomically corresponding regions lie in similar column positions across views, with Gaussian-decayed bias to emphasize local correlations; 2) Implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry, which is refined and injected into the denoising UNet to guide cross-view generation.

Result: Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional view translation tasks, outperforming state-of-the-art methods in both visual fidelity and structural consistency. The synthesized views effectively improve single-view malignancy classification in screening settings.

Conclusion: CA3D-Diff provides an effective solution for mammogram view translation that addresses structural misalignment challenges through geometric-aware attention and 3D anatomical guidance, demonstrating practical value for real-world breast cancer diagnostics by improving classification performance when views are missing.

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [151] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: A novel method for automated PV power plant mapping using aerial images to create detailed 3D models down to individual module level, eliminating reliance on third-party data.


<details>
  <summary>Details</summary>
Motivation: Accurate and up-to-date PV power plant models are essential for optimal operation and maintenance but are often not readily available, creating a need for automated mapping solutions.

Method: Uses visual segmentation of PV modules in aerial images, identifies structural layout keypoints, assigns modules to benches/rows/columns, and fuses 3D positions with semantic structures from multiple images.

Result: Successfully tested on two power plants, producing compact georeferenced models suitable for maintenance applications.

Conclusion: The approach enables automated PV power plant mapping with detailed structural modeling while removing dependency on external data sources.

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [152] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: This paper proposes a digital watermarking method for visual foundation models (VFMs) to protect intellectual property by embedding detectable watermarks in internal representations that persist even after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Protect VFM intellectual property rights by preventing illegal redistribution, as current methods lack reliable ownership verification tools that can distinguish between redistributed copies and independent models.

Method: Fine-tune a small set of expressive VFM layers along with an encoder-decoder network to embed digital watermarks into internal representations of hold-out input images, ensuring watermarks remain detectable in functional copies.

Result: The method achieves low probability of false detection of non-watermarked models and low probability of false misdetection of watermarked models, both theoretically and experimentally.

Conclusion: The proposed watermarking approach provides effective ownership verification for VFMs that remains robust even after model fine-tuning for downstream tasks.

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [153] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: A kinesics recognition framework that infers psychological states from 3D skeleton data using ST-GCN and CNN, enabling privacy-preserving and scalable modeling of human-environment interactions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional methods (theoretical models, questionnaires) in capturing human psychological states that are generalizable, privacy-preserving, and scalable for modeling human-built environment interactions.

Method: Combines spatial-temporal graph convolutional network (ST-GCN) with convolutional neural network (CNN) using transfer learning to infer communicative functions (kinesics) from 3D skeleton joint data, eliminating need for manual mappings between actions and psychological categories.

Result: Demonstrated on Dyadic User Engagement (DUET) dataset, the method enables scalable, accurate, and human-centered modeling of behavior while preserving user anonymity and uncovering latent structures in bodily movements reflecting cognitive/emotional states.

Conclusion: Provides a new pathway for enhancing RL-driven simulations of human-environment interaction through privacy-preserving kinesics recognition from 3D skeleton data.

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [154] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker is the first benchmark and multi-agent framework for automated academic presentation video generation, addressing challenges like dense multi-modal information coordination and reducing labor-intensive video production.


<details>
  <summary>Details</summary>
Motivation: Academic presentation video production is highly labor-intensive, requiring hours of work for short videos, with distinctive challenges including research paper inputs, dense multi-modal information, and coordination of multiple aligned channels.

Method: Proposes PaperTalker, a multi-agent framework that integrates slide generation with layout refinement using tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency.

Result: Experiments on Paper2Video show that the presentation videos produced are more faithful and informative than existing baselines, establishing a practical step toward automated academic video generation.

Conclusion: PaperTalker provides the first benchmark and framework for academic presentation video generation, offering an effective solution to reduce the labor-intensive process while maintaining information fidelity and quality.

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [155] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: This paper compares five skeleton-based algorithms for recognizing dyadic human interactions using depth sensors as a privacy-preserving alternative to RGB cameras, focusing on 12 types of social interactions.


<details>
  <summary>Details</summary>
Motivation: Cyber-physical systems traditionally focus on economic goals but neglect social benefits. Cyber-physical-social infrastructure systems aim to address this by aligning technology with social objectives, requiring better understanding of human interactions while preserving privacy.

Method: The study compares five skeleton-based interaction recognition algorithms on a dataset of 12 dyadic interactions. Depth sensors are used instead of RGB cameras to address privacy concerns by analyzing skeletal movements. The interactions are categorized into communication types like emblems and affect displays.

Result: The research provides a comparative analysis of different skeleton-based algorithms for recognizing dyadic human interactions, laying foundation for measuring social behavior through privacy-preserving methods.

Conclusion: Skeleton-based interaction recognition using depth sensors offers a viable privacy-conscious approach for understanding human social interactions, which is crucial for developing cyber-physical-social infrastructure systems that can foster positive social outcomes.

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [156] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: This paper proposes a method that combines early exits and knowledge distillation to reduce computational costs in neural networks while maintaining accuracy, using a new entropy-based loss for incorrectly classified images.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks have high computational costs that make them impractical for real-time and edge applications, necessitating compression techniques that maintain accuracy while reducing complexity.

Method: Integrates early exits and knowledge distillation, training a reduced student early-exit model from a more complex teacher early-exit model with a new entropy-based loss for images where teacher classification was incorrect.

Result: Achieves significant reductions in computational complexity without compromising classification performance on CIFAR10, CIFAR100, and SVHN datasets.

Conclusion: The approach effectively optimizes the accuracy-efficiency trade-off and opens new research perspectives for knowledge distillation in other contexts.

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [157] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: μDeepIQA is a deep learning-based image quality assessment method for optical microscopy that provides fast, stable quality predictions and patch-wise quality visualization, overcoming limitations of traditional quality metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional image quality assessment methods for optical microscopy are often time-consuming, computationally expensive for large datasets, and unstable for images outside ideal domains, requiring more robust and efficient solutions.

Method: The method adapts a deep convolutional neural network originally designed for natural image quality assessment to optical microscopy data, retraining the architecture to predict both individual quality metrics and global quality scores.

Result: μDeepIQA provides fast and stable quality predictions that generalize well even outside ideal ranges, enables patch-wise quality assessment, and visualizes spatially varying quality within single images.

Conclusion: Deep learning models like μDeepIQA benefit optical microscopy studies through stable outlier performance, ability to assess small image patches, and rapid predictions, demonstrating superior generalizability over standard methods.

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [158] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: An end-to-end IoT robotic system for real-time mapping of grape yield and quality in vineyards using hyperspectral imaging and deep learning, with a novel domain-adversarial framework to handle illumination variations.


<details>
  <summary>Details</summary>
Motivation: To enable non-destructive, real-time monitoring of grape yield and quality in vineyards while overcoming the challenge of domain shift caused by variable illumination conditions in field settings.

Method: Integrated pipeline with two modules: 1) High-performance model for grape bunch detection and weight estimation, 2) Light-Invariant Spectral Autoencoder (LISA) - a domain-adversarial deep learning framework for quality assessment from hyperspectral data that learns illumination-invariant features from uncalibrated data.

Result: System achieves 0.82 recall for bunch detection and R²=0.76 for weight prediction. LISA module improves quality prediction generalization by over 20% compared to baselines across three illumination domains (lab, morning sunlight, afternoon sunlight).

Conclusion: The system successfully generates high-resolution, georeferenced data of both grape yield and quality, providing actionable insights for precision viticulture by combining robust detection and illumination-invariant quality assessment modules.

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [159] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: This paper introduces a large multi-modal dataset for benthic habitat mapping, including side-scan sonar tiles, bathymetric maps, and optical images, with manual annotations to support machine learning development in marine ecosystem research.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large, annotated datasets limits the development and benchmarking of machine learning models for benthic habitat mapping, which is crucial for understanding marine ecosystems, conservation efforts, and sustainable resource management.

Method: The authors collected about a million side-scan sonar tiles along Catalonia's coast, complemented by bathymetric maps and co-registered optical images from AUV surveys. They manually annotated 36,000 SSS tiles with segmentation masks and spatially associated optical images with corresponding SSS tiles to enable cross-modal representation learning.

Result: The paper presents a comprehensive multi-modal dataset with raw sensor data, mosaics, and annotations, along with open-source preprocessing and annotation tools to enhance accessibility and encourage research in underwater habitat mapping.

Conclusion: This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration for marine ecosystem research.

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [160] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: Comparison of four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) for motorbike detection in Kigali's challenging traffic environment, with recommendations for simplified architectures suitable for resource-constrained autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Motorcycle taxis in Kigali navigate unpredictably and disregard traffic rules, posing significant challenges for autonomous driving systems that need to detect and respond to these vehicles in real-time.

Method: Used four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) implemented in PyTorch with transfer learning, evaluated on a custom dataset of 198 images collected in Kigali, assessing accuracy, localization, and inference speed.

Result: The study identified implementation challenges including dataset limitations and model complexities, and evaluated the models' performance for real-time navigation in resource-constrained settings.

Conclusion: Recommends simplified architectures to enhance accessibility for autonomous systems in developing countries like Rwanda, addressing the specific challenges of detecting motorbikes in chaotic urban environments.

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [161] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: A novel Semantics-Aware Hierarchical Consensus (SAHC) method that integrates hierarchy-specific classification heads with trainable hierarchy matrices and consensus mechanism for remote sensing image classification, leveraging semantic relationships in class hierarchies.


<details>
  <summary>Details</summary>
Motivation: Most deep learning approaches for remote sensing image classification overlook predefined label hierarchies and focus only on fine-grained classification, missing the semantic relationships among classes.

Method: Uses hierarchy-specific classification heads with trainable hierarchy matrices to learn hierarchical structure self-supervised, plus hierarchical consensus mechanism for consistent probability distributions across hierarchical levels.

Result: Experimental results on three benchmark datasets show effectiveness in guiding network learning and robustness of hierarchical consensus for remote sensing image classification tasks.

Conclusion: SAHC method effectively leverages hierarchical structure in classification tasks and demonstrates adaptability across different backbone architectures and hierarchical complexities.

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [162] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD introduces a new diffusion decoder architecture with transformer components and GAN-free training, using distillation to create an efficient single-step decoder that outperforms KL-VAE tokenizers in reconstruction quality and speed.


<details>
  <summary>Details</summary>
Motivation: Current KL-VAE tokenizers require adversarial losses and have limitations, while diffusion decoders need iterative sampling and still require adversarial training. There's a need for a more principled approach that eliminates adversarial losses while maintaining performance.

Method: Developed a pixel diffusion decoder with transformer components and GAN-free training, then used distillation to create a single-step decoder (SSDD) that replicates the diffusion decoder's performance without iterative sampling.

Result: SSDD improves reconstruction FID from 0.87 to 0.50 with 1.4x higher throughput than KL-VAE, and preserves DiT generation quality with 3.8x faster sampling. It achieves higher reconstruction quality without adversarial losses.

Conclusion: SSDD can serve as a drop-in replacement for KL-VAE tokenizers, enabling higher-quality and faster generative models with improved reconstruction and sampling efficiency.

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [163] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: Proposes latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) methods for out-of-distribution detection in deep neural networks, achieving comparable performance to existing probabilistic deep learning methods with better efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are increasingly used in safety-critical applications like driver action recognition, but existing last layer probabilistic deep learning methods for out-of-distribution detection have varying performance and can be inefficient.

Method: Extends pre-trained DNNs with transformation layers to produce multiple latent representations for uncertainty estimation, comparing LUR and RLUR against eight probabilistic deep learning methods across four video datasets.

Result: LUR and RLUR achieve comparable in-distribution classification performance to other approaches, with LUR matching top-performing methods for uncertainty-based OOD detection while being more efficient to train and easier to tune.

Conclusion: The proposed LUR approach provides an effective and efficient alternative to existing probabilistic deep learning methods for out-of-distribution detection in safety-critical applications.

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [164] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: Machine learning approach using hand-drawn spiral and wave images achieves 93.3% accuracy for Parkinson's disease detection through CNN, transfer learning, and ensemble voting methods.


<details>
  <summary>Details</summary>
Motivation: Early Parkinson's disease diagnosis is crucial but traditional methods are cumbersome and costly. This study aims to provide a non-invasive, cost-effective solution using machine learning.

Method: Three-phase architecture: pre-trained CNNs, custom convolutional layers, and ensemble voting with hard voting. Dataset augmentation and attention mechanisms are used to improve performance and prevent overfitting.

Result: Spiral images achieved 90% precision, recall, and F1-score; wave images achieved 96.67%. Combined ensemble voting achieved overall 93.3% accuracy.

Conclusion: Machine learning using hand-drawn images shows strong potential for early Parkinson's disease diagnosis as a non-invasive and cost-effective alternative to traditional methods.

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [165] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: This survey provides the first comprehensive examination of post-training methodologies for Video-Large Multimodal Models (Video-LMMs), covering supervised fine-tuning, reinforcement learning, and test-time scaling techniques to transform basic perception systems into sophisticated reasoning engines.


<details>
  <summary>Details</summary>
Motivation: Video understanding is the most challenging frontier in computer vision, requiring reasoning about complex spatiotemporal relationships and long-term dependencies. While Video-LMMs have shown remarkable capabilities, the critical post-training phase that transforms them into sophisticated reasoning engines remains fragmented across literature.

Method: The survey examines three fundamental pillars of post-training methodologies: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. It presents a structured taxonomy addressing video-specific challenges like temporal localization and spatiotemporal grounding.

Result: The survey synthesizes key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. It also curates essential benchmarks, datasets, and metrics for rigorous assessment of post-training effectiveness.

Conclusion: This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities through systematic post-training methodologies.

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [166] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: This paper proposes a segment matching method using 3D foundation models to handle extreme viewpoint changes up to 180 degrees, outperforming state-of-the-art methods by up to 30% on AUPRC metric.


<details>
  <summary>Details</summary>
Motivation: Segment matching provides robustness to occlusions, lighting variations, and viewpoint changes compared to keypoint matching. The paper aims to tackle the challenging wide-baseline segment matching problem with extreme viewpoint shifts.

Method: An architecture that leverages the spatial understanding and inductive bias of 3D foundation models to match segments across image pairs with extreme viewpoint changes.

Result: Outperforms state-of-the-art methods including SAM2 video propagator and local feature matching methods by up to 30% on AUPRC metric on ScanNet++ and Replica datasets.

Conclusion: The proposed approach successfully addresses wide-baseline segment matching and demonstrates benefits on downstream tasks including 3D instance segmentation and image-goal navigation.

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [167] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: Proposes a no-reference image quality assessment metric for contrast-distorted images by generating pseudo-reference images through contrast enhancement algorithms and transforming the problem to full-reference assessment.


<details>
  <summary>Details</summary>
Motivation: Contrast distortion has been largely overlooked in image quality assessment despite its significant visual impact, as existing methods mainly focus on other distortions like blur and noise.

Method: Uses contrast enhancement algorithms to generate pseudo-reference images, trains a classification network to select the best algorithm based on image content and distortion, then performs full-reference assessment between contrast-enhanced and degraded images.

Result: Performance evaluation on three databases (CCID2014, TID2013, and CSIQ) shows promising results for assessing contrast-distorted images.

Conclusion: The proposed approach effectively addresses the challenge of contrast distortion assessment by transforming no-reference problems into full-reference scenarios through intelligent pseudo-reference generation.

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [168] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: The paper introduces a Neuroplastic Modular Classifier that combines ResNet-50 and Vision Transformer for waste classification and industrial defect detection, featuring dynamic expansion during training to improve adaptability.


<details>
  <summary>Details</summary>
Motivation: Efficient waste classification and industrial surface defect detection are crucial for sustainable waste management and quality control, requiring robust and adaptive models for dynamic environments.

Method: Hybrid architecture using ResNet-50 for local features and Vision Transformer for global context, with FAISS-based similarity retrieval and neuroplastic modular blocks that dynamically expand during training when performance plateaus.

Result: The model outperforms traditional static models in accuracy and adaptability, validated on waste classification and KolektorSDD2 industrial defect dataset.

Conclusion: The Neuroplastic Modular Classifier provides a scalable, high-performance solution for real-world image classification with strong applicability in environmental and industrial domains.

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [169] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: This paper addresses the challenge of generating and editing structured visuals like charts and diagrams using AI models. It introduces a comprehensive approach including a large dataset, unified model training, and a new benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Modern visual generation models struggle with structured visuals that require composition planning, text rendering, and multimodal reasoning for factual accuracy. There's a need for specialized approaches to handle charts, diagrams, and mathematical figures.

Method: Constructed a 1.3M dataset from executable drawing programs with chain-of-thought annotations. Trained a unified model combining VLM with FLUX.1 Kontext via lightweight connector using three-stage curriculum training. Introduced StructBench benchmark with 1,700+ instances and StructScore metric using multi-round Q&A.

Result: Evaluations of 15 models show even leading closed-source systems perform poorly on structured visuals. Their model achieves strong editing performance, with inference-time reasoning providing consistent improvements across architectures.

Conclusion: The paper establishes foundations for structured visual generation by releasing dataset, model, and benchmark, enabling advancement in multimodal reasoning for structured visuals.

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [170] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: A framework for generating videos where characters from different worlds naturally interact while preserving their original identities and styles, using Cross-Character Embedding and Cross-Character Augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: To enable natural interactions between characters from different contexts (like Mr. Bean and Tom & Jerry) while maintaining their original identities and avoiding style delusion issues where realistic characters appear cartoonish or vice versa.

Method: Uses Cross-Character Embedding (CCE) to learn identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA) to enrich training with synthetic co-existence and mixed-style data.

Result: Experiments on a curated benchmark with 10 characters from cartoons and live-action series show clear improvements in identity preservation, interaction quality, and robustness to style delusion.

Conclusion: The framework enables new forms of generative storytelling by allowing natural interactions between previously uncoexistent characters without losing stylistic fidelity.

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [171] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain is a novel framework that uses multimodal models to generate keyframes for guiding video generation, improving complex dynamic synthesis with minimal tuning overhead.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with complex dynamics and coherent state transitions, while multimodal models excel at visual reasoning and future prediction.

Method: VChain leverages large multimodal models to generate sparse critical keyframes, then uses these to guide sparse inference-time tuning of pre-trained video generators only at key moments.

Result: Extensive experiments show VChain significantly enhances video quality in complex, multi-step scenarios.

Conclusion: VChain effectively bridges multimodal models' reasoning capabilities with video generation, achieving better dynamic synthesis with tuning efficiency.

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [172] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: Introduces Gemini Robotics 1.5 multi-embodiment VLA model and Gemini Robotics-ER 1.5 embodied reasoning model with three innovations: novel architecture with Motion Transfer, action-reasoning interleaving, and state-of-the-art embodied reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: General-purpose robots need deep physical world understanding, advanced reasoning, and dexterous control to solve complex multi-step tasks.

Method: Uses novel architecture with Motion Transfer mechanism for learning from heterogeneous robot data, interleaves actions with multi-level internal reasoning in natural language, and develops embodied reasoning capabilities for visual/spatial understanding and task planning.

Result: Enables robots to 'think before acting', improves complex task decomposition and execution, makes robot behavior more interpretable, and establishes new state-of-the-art for embodied reasoning.

Conclusion: This model family advances towards an era of physical agents that can perceive, think, and act to solve complex multi-step tasks.

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [173] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: This paper extends geometric mechanics to model and optimize compliant undulatory swimming locomotion, introducing a compliant three-link swimmer model with joint springs and validating it on a physical robot in granular media.


<details>
  <summary>Details</summary>
Motivation: Existing geometric mechanics approaches assume precise gait execution, but in practice environmental interactions with compliant bodies perturb trajectories. The authors aim to predict and optimize locomotor performance for compliant undulators.

Method: Developed a compliant extension of Purcell's three-link swimmer with series-connected springs at joints, derived body dynamics using resistive force theory, and incorporated geometric mechanics into movement prediction and optimization framework.

Result: Validated the framework on a physical cable-driven three-link robot, demonstrating accurate prediction and optimization of locomotor performance under varied programmed compliance in granular medium.

Conclusion: Established a systematic physics-based approach for modeling and controlling compliant swimming locomotion, showing compliance can be exploited as a design feature for robust movement in various environments.

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [174] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: A learning-based method using Flow Matching with single-view point clouds to generate near-optimal initializations for trajectory optimization in human-robot collaboration, achieving high success rates and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Need for rapid motion generation in HRC systems where robots must respond to dynamic environments in real-time. Current sampling-based planners struggle with high-dimensional spaces and require post-processing, while optimization-based planners are sensitive to initialization and local minima.

Method: Uses a Flow Matching model conditioned on single-view point clouds to learn near-optimal solutions for optimization initialization, requiring no prior environmental knowledge and working directly from depth camera input.

Result: Achieves high success rate independently, significantly improves trajectory optimization success compared to benchmarks, requires fewer optimization iterations, and generalizes well to unseen environments.

Conclusion: The proposed generative initializer effectively addresses initialization challenges in optimization-based motion planning, enabling efficient and safe human-robot collaboration in cluttered environments.

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [175] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: A modular simulation testbed for quadcopter control evaluation that enables systematic comparison of adaptive control methods under various disturbances.


<details>
  <summary>Details</summary>
Motivation: Fragmented evaluations across tasks, simulators, and implementations hinder systematic comparison of robust adaptive control methods for quadcopters under external disturbances and model uncertainties.

Method: Built an easy-to-deploy, modular simulation testbed on RotorPy with a library of adaptive/non-adaptive controllers, disturbance models (wind, payload shifts, rotor faults, control latency), trajectory generators, and analysis tools.

Result: The framework enables reproducible evaluation across control methods with task-relevant metrics for tracking accuracy and robustness, demonstrated through multiple disturbance scenarios and trajectory types including automated stress testing.

Conclusion: The unified modular environment eliminates redundant reimplementation and provides utility for systematic analysis of quadcopter control methods.

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [176] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: This paper presents methods to optimize destination-to-chutes task mapping in Robotic Sorting Systems to improve throughput, addressing challenges like robot planning interdependencies, chute closures, and downstream processing impacts.


<details>
  <summary>Details</summary>
Motivation: Optimizing task mapping in Robotic Sorting Systems is challenging due to interdependencies with robot assignment and path planning, temporary chute closures when full, and negative impacts on downstream processing when packages for the same destination are scattered across multiple chutes.

Method: The authors formally define task mappings and the Task Mapping Optimization problem, develop an RSS simulator for evaluation, and present optimization methods using Evolutionary Algorithm and Mixed Integer Linear Programming, along with Quality Diversity algorithms for analyzing diverse mapping strategies.

Result: The optimized task mappings generated by their methods demonstrate advantages over greedily generated mappings across various RSS setups with different map sizes, numbers of chutes, and destinations.

Conclusion: The proposed optimization framework successfully improves throughput in Robotic Sorting Systems by addressing the complex interdependencies in task mapping, with code made publicly available for further research and application.

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [177] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: First framework for robust permissive controller synthesis on Interval Markov Decision Processes (IMDPs) that guarantees reachability/reward specifications under all admissible transitions, using MILP formulations with scalable duality-based encoding.


<details>
  <summary>Details</summary>
Motivation: Traditional controller synthesis yields single deterministic strategies that limit adaptability, while existing permissive controllers assume exact transition probabilities - unrealistic for robotics with sensing noise, actuation imprecision, and coarse abstractions.

Method: Formulate as mixed-integer linear programs (MILPs) with two encodings: baseline vertex-enumeration method and scalable duality-based method that avoids explicit enumeration of transition probability vertices.

Result: Both methods synthesize robust, maximally permissive controllers and scale to large IMDPs with up to hundreds of thousands of states across four benchmark domains.

Conclusion: The framework enables runtime flexibility and resilience in robotic systems operating under uncertain dynamics by providing the first robust permissive controller synthesis approach for IMDPs.

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [178] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: A prediction-driven safe planning framework that uses granular human motion forecasting with digital twin validation to enable proactive collision avoidance in human-robot collaboration.


<details>
  <summary>Details</summary>
Motivation: Human-robot collaboration requires precise long-term human motion prediction for proactive collision avoidance, overcoming limitations of kinodynamic-only planners.

Method: Uses CNN-BiLSTM model for joint-by-joint human motion prediction, capsule-based APF for collision risk assessment, and Adaptive RRT* planner triggered by risk thresholds, validated in physics-based digital twin.

Result: Achieved 100% proactive avoidance with >250mm clearance and sub-2s replanning in 50 trials, demonstrating superior precision and reliability over kinematic-only planners.

Conclusion: Integration of predictive human modeling with digital twin validation enables reliable proactive collision avoidance and bridges latency gaps in real-time trajectory planning.

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [179] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: Real-time distributed MPC-CLF-CBF framework for multi-robot navigation that maintains connectivity using high-order control barrier functions while avoiding collisions, with connectivity recovery capabilities.


<details>
  <summary>Details</summary>
Motivation: Maintaining connectivity is crucial but fragile in multi-robot systems due to obstacles and occlusions. Existing methods need robust connectivity maintenance and recovery capabilities.

Method: Unified MPC-CLF-CBF framework combining Model Predictive Control, Control Lyapunov Functions, and high-order Control Barrier Functions with Bezier-parameterized trajectories for smooth navigation.

Result: Framework successfully maintains connectivity and recovers from disconnections in obstacle-rich environments, validated through simulations and physical experiments with 4 Crazyflie nano-quadrotors.

Conclusion: The proposed framework provides robust real-time connectivity maintenance and recovery for multi-robot systems in complex environments.

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [180] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: LapSurgie is the first humanoid-robot-based laparoscopic teleoperation framework that enables precise control of surgical instruments using an inverse-mapping strategy with remote center-of-motion constraints, aiming to make robotic surgery more accessible.


<details>
  <summary>Details</summary>
Motivation: To address healthcare disparities by making robotic laparoscopic surgery accessible to rural and low-resource regions, overcoming the current limitation where surgical robotic platforms are confined to high-resource medical centers.

Method: Uses a humanoid robot with an inverse-mapping strategy for manual-wristed laparoscopic instruments that respects remote center-of-motion constraints, combined with a control console featuring stereo vision for real-time feedback.

Result: The system enables precise hand-to-tool control of off-the-shelf surgical laparoscopic tools without additional setup requirements, and user studies demonstrate its effectiveness and feasibility.

Conclusion: Humanoid robots offer a promising solution for deploying laparoscopic surgical systems in underserved communities, as they can operate in existing human-designed environments without extensive infrastructure modifications.

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [181] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: A novel framework for camera-to-robot calibration in minimally invasive surgical robots that unifies keypoint and edge detection through shared encoding, enabling fast and accurate pose estimation.


<details>
  <summary>Details</summary>
Motivation: Conventional camera-to-robot calibration methods struggle with MIS robots due to long kinematic chains and partial visibility of degrees of freedom, while existing approaches have issues with inconsistent feature detection or slow inference times unsuitable for online control.

Method: Proposes a unified framework that detects both geometric primitives (keypoints and shaft edges) through shared encoding, trained on large-scale synthetic data with projective labeling, enabling efficient pose estimation via projection geometry in a single inference.

Result: Demonstrates fast performance and state-of-the-art accuracy in challenging surgical environments across both feature detection and pose evaluation, with qualitative and quantitative validation.

Conclusion: The proposed method provides an efficient and accurate solution for camera-to-robot calibration in MIS robots, overcoming limitations of previous approaches and enabling reliable online robot control.

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [182] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: A graph-based path planning method for soft robotic arms using precomputed shape libraries and k-nearest neighbor graphs in shape space, enabling fast collision avoidance and energy-efficient motion planning.


<details>
  <summary>Details</summary>
Motivation: Soft robots offer extraordinary flexibility but face challenges in motion planning due to their highly nonlinear and infinite-dimensional kinematics, especially in cluttered environments with obstacles.

Method: Uses a biomechanical model inspired by morphoelasticity and active filament theory to precompute a shape library, constructs a k-nearest neighbor graph in shape space, employs signed distance functions for collision detection, and defines multi-objective edge costs based on geometric distance and actuation effort.

Result: The algorithm reliably avoids obstacles and generates feasible paths within milliseconds using Dijkstra's algorithm, with energy costs reducing actuation effort by 50% compared to geometry-only planning (though resulting in longer tip trajectories).

Conclusion: Shape-space graph search enables fast and reliable path planning for soft robotics, with potential for real-time applications in surgical, industrial, and assistive settings.

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [183] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: A unified framework for multi-task locomotion and manipulation using contact-explicit representations, enabling a single policy to handle diverse tasks across different robot embodiments.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of designing separate policies for different tasks by leveraging shared structure across contact-rich tasks through a unified contact-based representation.

Method: Goal-conditioned reinforcement learning policy trained to realize contact plans defined by sequences of contact goals (positions, timings, active end-effectors).

Result: Single policies successfully controlled quadruped gaits, humanoid biped/quadrupedal gaits, and bimanual object manipulation across different robotic embodiments with robust performance.

Conclusion: Explicit contact reasoning significantly improves generalization to unseen scenarios, making contact-explicit policy learning a promising foundation for scalable loco-manipulation.

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [184] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: A bi-level control framework for autonomous vehicles that combines NMPC-based path planning with homotopy constraint relaxation and a backup safety loop for real-time obstacle avoidance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To ensure safety in autonomous vehicles through advanced path planning and obstacle avoidance capabilities, particularly in dynamic environments where real-time performance is critical.

Method: Uses a bi-level control framework with: 1) Main loop with Nonlinear Model Predictive Control (NMPC) for real-time path optimization using homotopy-based constraint relaxation, and 2) Independent backup loop for safe fallback trajectories when main loop fails within critical time.

Result: Evaluation shows benefits in various driving scenarios, demonstrating real-time applicability and robustness of the approach.

Conclusion: The framework represents a significant step towards safer and more reliable autonomous driving in complex and dynamic environments.

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [185] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: The paper develops a coordinate-free static model for hard-magnetic shells using Cosserat shell theory on the Special Euclidean group (SE(3)), enabling efficient analysis of 2D soft magnetic robots with large width-to-length ratios.


<details>
  <summary>Details</summary>
Motivation: Traditional Cosserat rod theory is insufficient for modeling modern soft robots with large width-to-length ratios that behave as 2D shells, particularly in applications like magnetic grippers and walking robots.

Method: A novel formulation of Cosserat shell theory on SE(3) group, treating the shell as a 2D manifold with six degrees of freedom (position and rotation), using Lie group structure for deformation gradient, and deriving equilibrium equations via virtual work principle with finite element implementation.

Result: The model avoids common numerical challenges like singularity and locking phenomena, and is validated through analytical and experimental test cases showing superior efficacy for shells undergoing severe rotations and displacements.

Conclusion: The proposed coordinate-free Cosserat shell model provides an efficient and robust framework for analyzing and controlling shape-morphing in hard-magnetic shell structures used in advanced soft robotics applications.

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [186] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: A fully untethered soft robot inspired by inchworms, using magnetic actuation to achieve multimodal locomotion including walking, swimming, and payload transport with onboard wireless control and camera.


<details>
  <summary>Details</summary>
Motivation: To advance real-world deployment of soft robots in diverse environments by creating untethered systems that don't rely on external infrastructure.

Method: Developed a curved, flexible soft robot actuated by magnetic forces, featuring structural optimization, onboard wireless control circuit, and integrated camera for environmental perception.

Result: Robot achieved maximum walking speed of 3.74 cm/s and swimming speed of 0.82 cm/s, successfully performing walking, steering, swimming, and payload transport with total mass of 102.63 g.

Conclusion: The study demonstrates successful development of a fully untethered soft robot with multimodal locomotion capabilities, validated through systematic experimental characterization.

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [187] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: This paper studies how visual degradations (blur, noise) affect robotic self-modeling and introduces a task-aware denoising framework with morphology-preserving constraints and semantic segmentation to maintain performance.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous visual self-modeling pipelines are fragile under realistic sensing conditions like noisy imagery and cluttered backgrounds, limiting robot adaptability in real-world environments.

Method: Introduces a task-aware denoising framework that combines classical restoration with morphology-preserving constraints, plus semantic segmentation to isolate robots from cluttered scenes.

Result: The approach restores near-baseline performance across simulated and physical platforms, while existing pipelines degrade significantly under visual degradations.

Conclusion: These contributions advance the robustness of visual self-modeling and establish practical foundations for deploying self-aware robots in unpredictable real-world environments.

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [188] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap is a method for creating photorealistic synthetic robot overlays on human videos to enable zero-shot imitation learning, bridging the embodiment gap between human videos and target robots.


<details>
  <summary>Details</summary>
Motivation: To address the embodiment gap between in-the-wild ego-centric human video data and target robot embodiments, enabling zero-shot imitation learning without requiring real robot demonstration data.

Method: Uses EmbodiSwap to generate synthetic robot overlays on human videos, trains closed-loop robot manipulation policies on this synthetic data, and employs V-JEPA as a visual backbone repurposed from video understanding to imitation learning.

Result: The zero-shot trained V-JEPA model achieves 82% success rate in real-world tests, outperforming few-shot trained alternatives and other methods using EmbodiSwap data.

Conclusion: EmbodiSwap with V-JEPA backbone enables effective zero-shot imitation learning, outperforming conventional approaches, and the authors release code, datasets, and models to facilitate broader adoption.

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [189] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: A model-based framework for non-prehensile tabletop pushing that uses a single learned GRU-based dynamics model with MPPI controller to handle multiple tasks (positioning, trajectory following, obstacle avoidance) without retraining.


<details>
  <summary>Details</summary>
Motivation: Prior data-driven pushing methods target narrow capabilities, limiting broader applicability. Need for a unified approach that can handle multiple tasks with a single model.

Method: Recurrent GRU-based architecture with non-linear layers for dynamics modeling, integrated with sampling-based MPPI controller. Uses tailored state-action representation and domain randomization for sim-to-real transfer.

Result: High success rates in precise positioning under strict thresholds, strong trajectory tracking and obstacle avoidance performance. Multiple tasks solved by changing controller objective without retraining.

Conclusion: The framework demonstrates robust multi-task capability with a single learned model, enabling adaptive pushing behaviors across various objectives while maintaining stability and generalization.

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [190] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: Class-conditioned motion prediction improves trajectory forecasting accuracy for heterogeneous agents by using observable labels/tasks, with deep learning methods excelling on balanced datasets while pattern-based methods perform better in limited data scenarios.


<details>
  <summary>Details</summary>
Motivation: Robots need to predict future actions of surrounding agents in dynamic environments, and agent dynamics depend on their tasks/roles/labels. Class-conditioned prediction can reduce uncertainty and improve accuracy for heterogeneous agents, which is under-explored especially for mobile robots and limited data applications.

Method: Proposed conditional pattern-based and efficient deep learning-based baselines for class-conditioned trajectory prediction. Evaluated on robotics (THÖR-MAGNI) and outdoors (Stanford Drone Dataset) datasets.

Result: All methods improve accuracy when considering class labels. Deep learning methods perform better on balanced datasets, while pattern-based methods are preferable in limited data scenarios (cold start in new environments, imbalanced classes).

Conclusion: Class-conditioned motion prediction is beneficial, with method selection depending on data availability and balance - deep learning for balanced datasets, pattern-based for limited/imbalanced data scenarios.

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [191] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: COVER is a framework that constructs coverage-verified roadmaps for semi-static environments, enabling fixed-time motion planning queries by systematically partitioning obstacle configurations and verifying roadmap feasibility.


<details>
  <summary>Details</summary>
Motivation: To enable robotic systems to answer motion-planning queries within fixed time budgets in semi-static environments, where most obstacles remain static but some vary, by exploiting structured variability for stronger guarantees than general motion planning.

Method: COVER incrementally constructs a coverage-verified roadmap by partitioning the obstacle configuration space, solving for feasible paths within each partition, and systematically verifying roadmap feasibility in each partition.

Result: COVER achieves broader coverage with higher query success rates than prior works when tested with a 7-DOF simulated Panda robot performing table and shelf tasks.

Conclusion: COVER provides a systematic approach for fixed-time motion planning in semi-static environments with formal guarantees, overcoming limitations of prior approaches that lacked guarantees or relied on restrictive discretizations.

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [192] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: SBP is a mobile manipulation policy that uses 3D latent maps for better spatial reasoning and long-term memory, outperforming image-only policies by 25% in sequential manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of image-based policies that lack global scene understanding and long-horizon memory, enabling better spatial and temporal reasoning for mobile manipulation tasks.

Method: End-to-end policy learning using 3D latent maps that incrementally fuse multiview observations into scene-specific latent features, with a pre-trained decoder for online optimization and a policy trainable with behavior cloning or reinforcement learning.

Result: SBP demonstrates global scene reasoning, uses maps as long-horizon memory, and outperforms image-based policies by 25% success rate in sequential manipulation tasks across both in-distribution and novel scenes.

Conclusion: 3D latent maps provide superior spatial and temporal reasoning capabilities for mobile manipulation compared to image-only approaches, enabling better performance in complex manipulation tasks.

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [193] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: NoTVLA is a Vision-Language-Action framework that uses sparse end-effector trajectories instead of dense action sequences to prevent catastrophic forgetting, achieving better multi-task performance with significantly less computing power and no wrist camera.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in VLA models caused by overreliance on continuous action sequences that create data silos and disrupt knowledge retention across tasks.

Method: Proposes narrowing focus to sparse trajectories using temporal compression and spatial reasoning pruning specifically for robot end effector's trajectory, training with sparse trajectories rather than dense action trajectories.

Result: Achieves superior performance and generalization compared to pi0 while using over 10x less computing power and no wrist-mounted camera, closely approximating single-task expert model accuracy while preserving language capabilities.

Conclusion: NoTVLA enables unified model deployment across multiple robot platforms with zero-shot generalization, effectively preventing catastrophic forgetting while maintaining operational efficiency and language understanding capabilities.

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [194] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: WAFFLE is a wearable system that uses sensor data to predict bite timing for robotic feeding, enabling more natural and reactive assistance that adapts to user cues like head movements and chewing.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomy and quality of life for people needing feeding assistance by overcoming technical challenges in bite timing estimation that limit widespread adoption of robotic feeding systems.

Method: Uses wearable sensors to capture user cues (head movements, chewing, talking), trains a supervised regression model on bite timing data from 14 participants, and incorporates a user-adjustable assertiveness threshold to generate proceed/stop commands.

Result: Outperformed baseline methods in studies with 15 unimpaired participants, showing better user control, robot understanding, and reduced workload. Successfully generalized to 2 motor-impaired participants in home environments with different robot hardware.

Conclusion: WAFFLE enables natural, reactive bite timing that generalizes across users, robot hardware, positioning, feeding trajectories, foods, and both individual and social dining contexts.

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [195] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: TCB-VIO is a tightly-coupled visual-inertial odometry system using a Multi-State Constraint Kalman Filter that operates at 250 FPS vision and 400 Hz IMU, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address spatial drift in vision-based pose estimation and temporal drift from inertial measurements by leveraging focal-plane sensor-processor arrays (FPSPs) that enable high frame rates to match IMU frequency.

Method: Uses a tightly-coupled 6-DOF VIO with Multi-State Constraint Kalman Filter (MSCKF), operating at 250 FPS vision processing and 400 Hz IMU measurements on focal-plane sensor-processor arrays.

Result: TCB-VIO outperforms state-of-the-art methods including ROVIO, VINS-Mono, and ORB-SLAM3.

Conclusion: FPSPs enable high-frequency vision processing that can match IMU rates, effectively reducing spatial and temporal drift in VIO systems through tightly-coupled filtering approaches.

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [196] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: A novel global path-planning method for off-road environments that addresses real-time performance, kinematic feasibility, and memory efficiency by constructing intermediate maps and dividing planning into three sub-problems.


<details>
  <summary>Details</summary>
Motivation: Traditional global path-planning methods perform poorly on large-scale off-road maps and fail to account for critical factors like real-time performance, kinematic feasibility, and memory efficiency in complex unstructured environments.

Method: Constructs an intermediate map in pixel coordinate system with geographical features, then divides planning into three sub-problems: graph-based path planning, kinematic feasibility checking, and path smoothing.

Result: Successfully tested in various off-road environments with large-scale maps up to several square kilometers, achieving feasible path identification in average 1.5 seconds and using approximately 1.5GB memory under extreme conditions.

Conclusion: The proposed framework is versatile and applicable to a wide range of off-road autonomous navigation tasks including search and rescue missions and agricultural operations.

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [197] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: SITCOM augments pretrained Vision-Language-Action models with model-based rollouts and reward-based trajectory selection to improve long-horizon planning and robustness in robotic control.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models struggle with long-horizon planning and compounding errors in dynamic tasks, lacking mechanisms for lookahead despite their ability to ground natural language into single-step control commands.

Method: SITCOM combines pretrained VLAs with model-based rollouts using a learned dynamics model trained on BridgeV2 data and fine-tuned on SIMPLER environments. It performs multi-step action rollouts and selects the best trajectory using reward-based scoring from the simulator.

Result: SITCOM significantly improves task completion rate from 48% to 72% when combined with a good reward function, demonstrating substantial performance gains across multiple tasks and settings in the SIMPLER environment.

Conclusion: The SITCOM framework successfully transforms one-shot VLAs into robust long-horizon planners by integrating model-based rollouts and reward-based trajectory selection, addressing key limitations in current robotic control approaches.

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [198] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: A feedback-enabled framework for autonomous tissue dissection that uses topological reasoning from endoscopic images to guide surgical actions and improve reliability through visibility metrics and optimal control.


<details>
  <summary>Details</summary>
Motivation: Existing surgical robotics methods struggle with topological and perceptual challenges in tissue dissection, lacking the ability to adapt to dynamic tissue changes during execution.

Method: Proposes a framework that reasons about topological changes from endoscopic images after each dissection action, uses visibility metrics to quantify tissue exposure, and integrates optimal controller designs to maximize visibility.

Result: Experimental results show the framework significantly enhances autonomy, reduces errors, and improves robustness in complex surgical scenarios when integrated with both planning-based and learning-based dissection methods.

Conclusion: Feedback mechanisms that explicitly reason about topological changes and optimize for visibility can substantially improve the performance and reliability of autonomous surgical systems in dynamic tissue dissection tasks.

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [199] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: This paper presents eight computational efficiency techniques for data-driven control policies to address limitations like slow response times, high computational demands, and large memory requirements in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Data-driven control policies (ML-based MPC, RL, DeePC) face practical limitations including slow response times, high computational demands, and large memory needs, making them impractical for systems with fast dynamics, limited computing, or strict memory constraints.

Method: The paper presents eight approaches including reduced-order modeling, function-approximated policy learning, and convex relaxations to reduce computational complexity of data-driven control policies.

Result: The techniques demonstrate effectiveness across real-world applications including robotic arms, soft robots, and vehicle motion control.

Conclusion: Computational efficiency techniques can make data-driven control policies practical for real-world systems with fast dynamics and limited computing resources.

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [200] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: HEHA is a hierarchical path planning framework for multi-robot exploration using heterogeneous agents (drones, wheeled, legged robots) that reduces exploration time by up to 30% through efficient global planning with PEAF algorithm and heterogeneity-aware local planning.


<details>
  <summary>Details</summary>
Motivation: Autonomous exploration with multiple heterogeneous robots faces challenges in intelligently allocating robots to unknown areas and determining visiting order while considering different traversability capabilities, requiring fast iterative solutions to large-scale constrained optimization problems.

Method: Proposed HEHA framework with hierarchical decomposition: global planning using PEAF (Partial Anytime Focal search) algorithm for bounded sub-optimal routing to minimize maximum path length, and local planning that considers robot heterogeneity to avoid duplicated exploration.

Result: Experimental results show HEHA reduces exploration time by up to 30% compared to baseline methods.

Conclusion: HEHA effectively addresses the multi-robot heterogeneous exploration problem through hierarchical planning and specialized routing algorithms, demonstrating significant performance improvements in exploration efficiency.

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [201] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: This paper presents a data-driven reinforcement learning framework for autonomous rock capturing using standard excavator buckets, eliminating the need for explicit modeling of rock or soil properties.


<details>
  <summary>Details</summary>
Motivation: Rock capturing with standard excavators is challenging and typically requires skilled operators. Existing autonomous methods focus on continuous media or specialized grippers, limiting real-world applicability in construction sites with unstructured environments and complex contact interactions.

Method: A model-free reinforcement learning agent trained in AGX Dynamics simulator using PPO algorithm with guiding reward formulation. The policy outputs joint velocity commands directly to excavator components (boom, arm, bucket). Robustness enhanced through extensive domain randomization of rock geometry, density, mass, and initial configurations.

Result: The learned policy generalizes well to unseen rocks and varying soil conditions, achieving high success rates comparable to human participants while maintaining machine stability.

Conclusion: This demonstrates the feasibility of learning-based excavation strategies for discrete object manipulation without requiring specialized hardware or detailed material models.

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [202] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: VBM-NET is a learning-based method that uses top-down orthographic projections and equivariant TransporterNet with graph neural networks to efficiently select optimal mobile base poses for grasping, achieving comparable performance to classical methods with much faster computation and successful sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Previous mobile manipulation approaches rely on reliable state information like precise object poses and environment models, which may not always be available. This work aims to address base pose planning directly from visual scene representations without requiring explicit state information.

Method: Uses top-down orthographic projections of scenes, equivariant TransporterNet to exploit spatial symmetries for candidate base pose generation, graph neural networks to represent varying numbers of candidate poses, and reinforcement learning to select the optimal base pose.

Result: VBM-NET produces solutions comparable to classical methods but with significantly reduced computation time. The method successfully transfers from simulation to real-world mobile manipulation scenarios.

Conclusion: The proposed learning-based approach using visual scene representations and equivariant networks provides an efficient alternative to classical planning methods for mobile base pose selection, with practical real-world applicability through successful sim-to-real transfer.

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [203] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: Robotic system replaces manual catheter control for mitral valve repair, reducing procedure time and improving accuracy using intuitive game controller interface.


<details>
  <summary>Details</summary>
Motivation: Manual transcatheter valve repair has mechanical limitations and steep learning curve, making robotic assistance desirable to overcome these challenges.

Method: Replaced complex handle-based control with robotic joint-based control via game controller, compared manual vs robotic performance in phantom heart model by decomposing procedure into motion-specific steps.

Result: Robotic system reduced procedural time and motion errors while improving clip placement accuracy compared to manual approach.

Conclusion: Robotic assistance addresses key limitations of manual systems and provides more reliable, user-friendly platform for complex transcatheter procedures.

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [204] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: A social robot patrols parking lots using GPT-4o multimodal model for real-time license plate recognition and illegal parking detection, automatically notifying managers via Line messages.


<details>
  <summary>Details</summary>
Motivation: To develop an automated solution for detecting illegal parking in indoor parking lots using social robotics and advanced AI models to improve parking management efficiency.

Method: Used a dual-model pipeline approach and compared with large multimodal models, ultimately adopting GPT-4o for license plate recognition without preprocessing. The robot navigates simulated parking lots, automatically adjusts camera angles to capture license plate images, and processes them through GPT-4o for number recognition and legality assessment.

Result: The system successfully recognized license plate numbers and identified illegal parking with high accuracy using the novel multimodal deep learning approach, enabling immediate notification to system managers.

Conclusion: The work demonstrates a validated high-accuracy license plate recognition system using multimodal AI, providing a practical social assistive robot solution for real-world indoor parking lot applications.

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [205] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: Diffusion-MPC combines diffusion models with model predictive control to enable flexible and adaptable legged locomotion planning that can adjust to new rewards and constraints at test time without retraining.


<details>
  <summary>Details</summary>
Motivation: Model-free RL produces fixed policies that are hard to adapt, while classical MPC requires accurate dynamics models that are difficult to obtain in complex environments. There's a need for controllers that are both robust and adaptable while handling task constraints.

Method: Uses a learned generative diffusion model as a dynamics prior for planning. Jointly predicts future states and actions, incorporating reward planning and constraint projection at each reverse step. Includes interactive training where the planner executes in environment and updates the denoiser using trajectory returns.

Result: Validated on real-world locomotion tasks, demonstrating strong performance and flexible adaptation to new reward specifications without retraining.

Conclusion: Diffusion-MPC enables test-time adaptability through reward and constraint optimization, overcoming limitations of both fixed RL policies and model-dependent MPC approaches.

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [206] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: ContextVLA is a policy model that improves robotic task performance by compressing multi-frame observations into a single context token, enabling efficient temporal context utilization without the computational overhead of full video inputs.


<details>
  <summary>Details</summary>
Motivation: Prior behavior cloning methods showed inconsistent performance with multi-frame observations, while Vision-Language-Action models (VLA) demonstrated better temporal understanding but suffered from high computational costs due to video input dimensionality.

Method: ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation while avoiding the computational overhead of processing full multi-frame video inputs.

Result: ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.

Conclusion: The approach demonstrates that VLMs' inherent temporal understanding can be effectively leveraged through context token compression, providing robust performance gains in partially observable robotic tasks while maintaining computational efficiency.

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [207] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: FactorMPC is a factor-graph based MPC toolkit that handles nonlinear manifold systems by unifying dynamics, constraints, and objectives into an efficient optimization structure with native manifold support and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Traditional MPC struggles with systems on nonlinear manifolds (like robotic attitude dynamics) due to singularities, over-parameterization, and poor convergence in Euclidean formulations.

Method: Uses factor-graph based optimization with manifold-valued states and Gaussian uncertainties in tangent spaces. Includes velocity-extended on-manifold CBF factors for obstacle avoidance and exploits sparsity for efficiency.

Result: Achieves real-time performance for high-dimensional systems with complex constraints. Demonstrates superior trajectory tracking and obstacle avoidance on quadrotors compared to baselines.

Conclusion: Provides a scalable, geometrically consistent framework for integrated planning and control, with open-source implementation for research reproducibility.

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [208] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: A centroidal stability-based retargeting method that dynamically adjusts contact points and posture during teleoperation to enhance stability when using hand contacts and non-coplanar surfaces.


<details>
  <summary>Details</summary>
Motivation: Teleoperation becomes challenging with hand contacts and non-coplanar surfaces, often leading to motor torque saturation or loss of stability through slipping.

Method: Uses an efficient analytical calculation of the stability margin gradient to identify scenarios where stability is highly sensitive to teleoperation setpoints, then locally adjusts these setpoints.

Result: Validation in simulation and hardware shows increased stability margins, with higher stability margins correlating with improved impulse resilience and joint torque margin.

Conclusion: The proposed centroidal stability-based retargeting method effectively enhances stability during teleoperation in challenging scenarios involving hand contacts and non-coplanar surfaces.

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [209] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: SureSim is a framework that combines large-scale simulation with small-scale real-world testing to provide reliable performance evaluation of robot policies, reducing hardware evaluation effort by 20-25% while maintaining statistical confidence.


<details>
  <summary>Details</summary>
Motivation: Current robot policy evaluation methods rely on small hardware trials without statistical assurances, making it challenging to rigorously evaluate policies that generalize across tasks and environments.

Method: Formalizes the problem as prediction-powered inference, using paired real and simulation evaluations to correct bias in large-scale simulation, and employs non-asymptotic mean estimation algorithms to provide confidence intervals on policy performance.

Result: The approach saves 20-25% of hardware evaluation effort while achieving similar bounds on policy performance when tested on diffusion policies and multi-task fine-tuned policies across object distributions and initial conditions.

Conclusion: SureSim provides a statistically sound framework for robot policy evaluation that efficiently combines simulation and real-world testing, significantly reducing the cost and effort of hardware evaluations while maintaining reliable performance estimates.

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [210] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: A novel diffusion-based trajectory optimization method that directly generates state sequences with dynamic feasibility constraints via gradient-free projection during reverse diffusion.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based trajectory optimization methods use single-shooting approaches that fail to explicitly enforce state constraints and dynamic feasibility, leading to sub-optimal solutions.

Method: Proposes direct trajectory optimization via model-based diffusion that generates state sequences directly, incorporating a gradient-free projection mechanism into the reverse diffusion process to ensure dynamic feasibility.

Result: Achieves zero dynamic feasibility error and approximately 4x higher success rate compared to state-of-the-art baseline in quadrotor waypoint navigation with dense static obstacles.

Conclusion: The proposed approach effectively addresses dynamic feasibility constraints in diffusion-based trajectory optimization, significantly improving performance in complex navigation scenarios.

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [211] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: A velocity-form DeePC framework is proposed for robust control of soft robots under unknown payloads, using incremental input-output data to handle disturbances without weighted datasets or estimators.


<details>
  <summary>Details</summary>
Motivation: Unknown external payloads and disturbances in object manipulation tasks can significantly alter soft robot dynamics, causing offset errors and degraded performance in data-driven control methods like DeePC.

Method: The proposed velocity-form DeePC framework leverages input-output data in incremental representation to mitigate performance degradation from unknown payloads, eliminating the need for weighted datasets or disturbance estimators.

Result: Experimental validation on a planar soft robot demonstrates superior performance compared to standard DeePC in scenarios involving unknown payloads.

Conclusion: The velocity-form DeePC framework provides robust and optimal control of soft robots under unknown payloads, effectively handling disturbances without complex estimators or weighted data approaches.

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [212] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: The Everything-Grasping (EG) Gripper is a soft robotic end-effector that combines distributed surface suction with granular jamming to manipulate objects across vastly different sizes (from 0.2 mm² to 62,000 mm²) and physical states (solids and liquids) without requiring airtight sealing.


<details>
  <summary>Details</summary>
Motivation: Grasping objects across vastly different sizes and physical states (including both solids and liquids) with a single robotic gripper remains a fundamental challenge in soft robotics.

Method: The EG Gripper synergistically integrates distributed surface suction with internal granular jamming. It includes a tactile sensing framework combining liquid detection and pressure-based suction feedback, guided by the Tactile-Inferred Grasping Mode Selection (TIGMS) algorithm that autonomously selects grasping modes based on distributed pressure and voltage signals.

Result: The gripper can handle objects with surface areas ranging from sub-millimeter scale 0.2 mm² (glass bead) to over 62,000 mm² (A4 sized paper and woven bag), enabling manipulation of objects nearly 3,500X smaller and 88X larger than its own contact area. Experiments across diverse tasks demonstrated robust and repeatable performance.

Conclusion: This is the first soft gripper to reliably grasp both solid and liquid objects across scales using a unified compliant architecture.

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [213] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: MobRT is a digital twin framework that generates realistic mobile manipulation demonstrations for articulated object interaction and pick-and-place tasks, improving policy generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Mobile manipulation remains underexplored due to challenges in collecting large-scale, high-quality demonstration data for coordinating base locomotion and arm manipulation in complex environments.

Method: Uses digital twin simulation with virtual kinematic control and whole-body motion planning to autonomously generate diverse, realistic demonstrations for articulated object interaction and mobile-base pick-and-place tasks.

Result: Establishes comprehensive benchmark showing strong correlation between task success and generated trajectories. Experiments demonstrate improved policy generalization and robust performance in both simulated and real-world environments.

Conclusion: MobRT effectively bridges the data gap in mobile manipulation research by generating high-quality demonstrations that enable robust policy learning and generalization across simulated and real-world settings.

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [214] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X is a multi-sensor SLAM system that builds dense volumetric occupancy maps in real-time, integrating visual, inertial, depth, LiDAR and GNSS sensors with superior accuracy and scalability to large environments.


<details>
  <summary>Details</summary>
Motivation: To empower mobile robots with usable maps and highest state estimation accuracy and robustness by leveraging dense volumetric map representations when depth or range-sensing capabilities are available, unlike most state-of-the-art SLAM systems.

Method: Uses a unified SLAM framework that seamlessly integrates multiple sensor modalities with an efficient submapping strategy for scalability. Employs tightly-coupled estimator and submaps through map alignment factors, and includes online calibration of camera extrinsics.

Result: Achieves highest trajectory accuracy in EuRoC against state-of-the-art alternatives, outperforms all competitors in Hilti22 VI-only benchmark, proves competitive in LiDAR version, and showcases state-of-the-art accuracy in diverse large-scale VBR dataset sequences up to 9 kilometers.

Conclusion: OKVIS2-X provides a state-of-the-art multi-sensor SLAM system that delivers globally consistent maps directly usable for autonomous navigation, with superior accuracy, robustness and scalability to large environments.

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [215] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: A biomimetic robotic platform that replicates the female Houbara bustard for ecological field studies, featuring digital fabrication, autonomous visual servoing, and thermal-visible fusion perception.


<details>
  <summary>Details</summary>
Motivation: Studying avian behavior in the wild is challenging due to the need for realistic morphology, durable outdoor operation, and intelligent perception in uncontrolled environments.

Method: Combines high-resolution 3D scanning, parametric CAD modeling, articulated 3D printing, and photorealistic UV finishing for fabrication. Uses a six-wheeled rocker bogie chassis and embedded NVIDIA Jetson for real-time RGB/thermal perception with YOLO-based detection and autonomous visual servoing.

Result: Field trials demonstrated reliable operation at 15-22 FPS with <100ms latency, successfully eliciting natural recognition and interactive responses from live Houbara bustards in harsh desert conditions.

Conclusion: The framework advances biomimetic field robotics by integrating reproducible digital fabrication, embodied visual intelligence, and ecological validation, providing a transferable blueprint for animal-robot interaction research and conservation robotics.

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [216] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: A decentralized gradient-based framework using piecewise continuous energy functions for bimanual assembly tasks, enabling rapid replanning without explicit task sequencing.


<details>
  <summary>Details</summary>
Motivation: Traditional TAMP methods are slow for replanning in tight-tolerance assembly where disturbances require new sequencing. Explicit task definitions limit flexibility during replanning.

Method: Uses automatic composition of adaptive potential functions to create piecewise continuous energy functions, generating sub-goals through myopic optimization instead of long-horizon planning.

Result: Successfully scales to physical bimanual assembly tasks for tight-tolerance assemblies, with emergent behaviors including automatic retries, coordinated motions, and autonomous handovers.

Conclusion: The gradient-based rapid replanning framework effectively handles long-horizon assembly tasks through structured energy functions, enabling flexible adaptation to disturbances without explicit task sequencing.

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [217] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: A reinforcement learning-based method for optimizing multirotor drone designs using task performance metrics, with real-world validation showing superior performance over conventional designs.


<details>
  <summary>Details</summary>
Motivation: To develop task-specific aerial robot designs that outperform conventional configurations by directly optimizing for closed-loop performance in specific tasks rather than using generic design principles.

Method: Combines reinforcement learning, Bayesian optimization, and covariance matrix adaptation evolution strategy to systematically explore motor pose configurations while maintaining manufacturability and minimizing aerodynamic interference.

Result: Optimized designs achieved superior performance in agile waypoint navigation tasks compared to conventional multirotor configurations, including outperforming fully actuated designs from literature. Real-world testing validated sim2real transferability.

Conclusion: The methodology successfully enables task-specific optimization of multirotor designs that outperform conventional configurations, with demonstrated real-world applicability through validated sim2real transfer.

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [218] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: SoNS enables robot swarms to automatically generate and execute LLM-generated code when stuck, achieving 85% mission success rate.


<details>
  <summary>Details</summary>
Motivation: To provide robot swarms with easier behavior design and global configuration/environment estimation while enabling automatic code generation for mission completion.

Method: Self-organizing nervous system (SoNS) that allows robot swarms to solicit and run code generated by external LLMs when encountering obstacles or getting stuck.

Result: Demonstrated with 6 real robots and >30 simulation trials, showing 85% mission success rate through automatic code generation and execution.

Conclusion: SoNS successfully enables robot swarms to overcome obstacles by automatically generating and implementing LLM-generated code on the fly.

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [219] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K is a lightweight extension of the Kaczmarz method that enables fast, stable inertial parameter estimation for robotic systems, achieving significant speed improvements and better tracking performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like Recursive Least Squares and Kalman Filter struggle with abrupt parameter changes and have high computational costs, limiting their effectiveness in dynamic environments and for computationally constrained robotic systems.

Method: TAG-K combines greedy randomized row selection for rapid convergence with tail averaging for robustness under noise and inconsistency, while maintaining low per-iteration complexity of the Kaczmarz framework.

Result: TAG-K achieves 1.5x-1.9x faster solve times on laptop CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers, with 25% reduction in estimation error and nearly 2x better end-to-end tracking performance.

Conclusion: TAG-K provides a computationally efficient solution for online inertial parameter estimation that outperforms traditional methods in both speed and accuracy, making it suitable for real-time robotic control applications.

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [220] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: A U-Net-based method reconstructs clean IR images from emitter-populated input, enabling robust robotic perception in dark environments by improving image quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: IR streams are less noisy than RGB in low-light conditions but are dominated by active emitter patterns that hinder high-level robotic tasks like object detection, tracking, and localization.

Method: Proposes a U-Net-based architecture that reconstructs clean IR images from emitter-populated input to remove interference from active emitter patterns.

Result: The approach outperforms existing enhancement techniques and enables reliable operation of vision-driven robotic systems across illumination conditions from well-lit to extreme low-light scenes.

Conclusion: The proposed method successfully addresses the emitter pattern problem in IR streams, making robotic perception robust in dark environments while maintaining performance across various lighting conditions.

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [221] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: HyperVLA is a hypernetwork-based Vision-Language-Action model that reduces inference costs by 90x in parameters and 120x in speed while maintaining performance compared to monolithic VLAs.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action (VLA) models have high inference costs, limiting their practical deployment despite their strong generalization capabilities.

Method: Uses hypernetwork architecture that activates only task-specific policies during inference, with key designs including prior knowledge utilization from vision foundation models, HN normalization, and action generation strategy.

Result: Achieves similar or higher success rates than monolithic VLAs for zero-shot generalization and few-shot adaptation, while reducing activated parameters by 90x and accelerating inference speed by 120x compared to OpenVLA.

Conclusion: HyperVLA provides an efficient alternative to monolithic VLAs by significantly reducing inference costs while maintaining or improving performance through hypernetwork-based architecture.

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [222] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: A novel planning framework using vision-language models (VLMs) for autonomous navigation that analyzes occupancy maps to select efficient subgoals, reducing path lengths by ~10% compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional exploration methods often take inefficient routes due to limited global reasoning and reliance on local heuristics, especially in unknown indoor environments with many dead ends.

Method: Convert 3D occupancy grids into partial 2D maps, generate candidate subgoals, and use VLMs to evaluate and rank subgoals in zero-shot manner. This approach integrates with DYNUS trajectory planner and infers structural patterns from incomplete maps.

Result: The framework reduces common greedy failures (e.g., detouring into small rooms) and achieves about 10% shorter paths on average in simulation.

Conclusion: VLMs can effectively reason about occupancy maps to improve navigation efficiency by balancing progress toward goals against the risk of entering unknown space, outperforming traditional exploration methods.

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [223] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D is a 0.25m, 0.99kg robot inspired by the 'TARS' robot from Interstellar, featuring seven actuated degrees of freedom and telescopic legs. The research combines analytical modeling of bipedal walking and rolling gaits with deep reinforcement learning to discover novel locomotion behaviors.


<details>
  <summary>Details</summary>
Motivation: To explore non-anthropomorphic robotic locomotion inspired by fictional designs, specifically translating the TARS robot from Interstellar into a functional research platform that can achieve multiple locomotion modes beyond traditional biologically-inspired designs.

Method: Built a physical TARS3D robot with telescopic legs and seven actuated DOF. Developed reduced-order models for bipedal walking and rolling gaits with closed-form limit-cycle conditions. Used deep reinforcement learning in simulation to explore the robot's rich gait repertoire beyond analytically derived gaits.

Result: Hardware experiments confirmed the robot respects hip limits, alternates contacts without interference, and maintains an eight-step hybrid limit cycle in rolling mode. DRL successfully recovered analytic gaits and discovered novel behaviors. The rolling gait was modeled as an eight-spoke double rimless wheel due to four contact corners per leg.

Conclusion: TARS3D's fiction-inspired morphology enables multiple unexplored locomotion modes. The combination of analytic synthesis and reinforcement learning provides a promising pathway for multimodal robotics, with potential for discovering more behaviors through continued learning-driven exploration.

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [224] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: StaMo learns a compressed two-token state representation using a lightweight encoder and DiT decoder, enabling efficient world modeling and emergent latent actions without complex architectures or video data.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to balance expressiveness and compactness in state representations, leading to either redundancy or lack of task-critical information for embodied intelligence.

Method: Unsupervised approach using lightweight encoder and pre-trained Diffusion Transformer decoder to learn compressed two-token state representations, with latent interpolation revealing emergent latent actions.

Result: Improves performance by 14.3% on LIBERO and 30% in real-world task success with minimal overhead; latent actions enhance policy co-training by 10.4% with better interpretability; scales across diverse data sources.

Conclusion: StaMo demonstrates that compact state representations can capture structured dynamics and generate effective latent actions without explicit supervision, challenging dependence on complex architectures and video data.

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [225] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ACQL combines RL with temporal logic to handle sequential goals and safety constraints in robotics, outperforming existing methods in continuous control tasks.


<details>
  <summary>Details</summary>
Motivation: Standard RL struggles with sequential goals and time-varying safety constraints needed for real robotics tasks, while existing LTL-RL methods perform poorly in complex environments.

Method: Automaton Constrained Q-Learning (ACQL) combines goal-conditioned value learning with automaton-guided reinforcement, using LTL automata to encode goal progression and safety constraints.

Result: ACQL outperforms prior methods in continuous control tasks, successfully handling cases where others fail on goals or safety, and works on real 6-DOF robotic arms in cluttered environments.

Conclusion: ACQL provides a robust, scalable solution for learning robotic behaviors with rich temporal specifications and safety constraints.

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [226] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic is a two-stage residual learning framework that combines a general motion tracking policy with a residual policy to enable precise humanoid loco-manipulation, achieving improved task success and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Current general motion tracking policies lack the precision and object awareness needed for humanoid loco-manipulation tasks in daily service and warehouse applications.

Method: Two-stage approach: (1) GMT policy trained on human-only motion data provides base human-like movements, (2) Residual policy refines outputs for locomotion improvement and object interaction, with specialized rewards for object tracking, contact, and curriculum-based virtual object control.

Result: Substantial gains in task success, training efficiency, and robustness over baselines, validated in both simulation and on real Unitree G1 humanoid.

Conclusion: ResMimic effectively bridges the gap between general human motion tracking and precise loco-manipulation capabilities for humanoid robots.

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [227] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX evaluates web agent reliability by introducing real-world network instability and web attacks to existing benchmarks, revealing significant performance drops in state-of-the-art agents.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks test agents in controlled environments, but real-world web browsing involves network instability and security threats that aren't reflected in existing evaluations.

Method: Developed WAREX framework that adds real-world challenges (network issues, HTTPS problems, web attacks like XSS, site modifications) to three popular benchmarks: WebArena, WebVoyager, and REAL.

Result: Experiments showed significant drops in task success rates when WAREX was introduced, demonstrating limited robustness of current web agents.

Conclusion: Existing web agents lack robustness against real-world challenges, highlighting the need for more comprehensive evaluation frameworks like WAREX to improve agent reliability.

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [228] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: This paper addresses energy-efficient scheduling in manufacturing by solving a hybrid flow shop problem with blocking constraints, aiming to minimize both makespan and energy consumption using multi-objective optimization methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of non-renewable energy sources, geopolitical supply issues, rising prices, and climate change impacts drive the need for energy-efficient manufacturing solutions, particularly in scheduling operations which can show immediate impact.

Method: The authors formulate a multi-objective mixed integer programming model and propose an augmented epsilon-constraint method for Pareto-optimal solutions, along with developing a Refined Iterated Pareto Greedy (RIPG) metaheuristic algorithm for large instances.

Result: The proposed methods were benchmarked across small, medium, and large instances against two well-known algorithms, with computational results demonstrating the effectiveness of their approach.

Conclusion: The developed multi-objective optimization methods effectively address the conflicting objectives of minimizing both makespan and energy consumption in hybrid flow shop scheduling with blocking constraints, providing practical solutions for energy-efficient manufacturing.

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [229] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: LLMs consistently fail at self-recognition, with only 4 out of 10 models correctly identifying their own generated text, performing barely above random chance while showing strong bias toward GPT and Claude families.


<details>
  <summary>Details</summary>
Motivation: Address contradictory claims about whether models possess self-recognition capabilities, which is crucial for AI safety and metacognitive analysis.

Method: Systematic evaluation framework with two tasks: binary self-recognition (identifying own vs others' text) and exact model prediction, tested on 10 contemporary LLMs.

Result: Models show consistent failure in self-recognition, strong bias toward predicting GPT/Claude families, and hierarchical reasoning bias associating high-quality text with top-tier models.

Conclusion: Current LLMs lack appropriate self-awareness, raising safety concerns; future work needed to develop proper AI self-recognition capabilities.

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [230] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: ContraGen is a contradiction-aware benchmark framework for enterprise RAG systems that generates synthetic enterprise documents with embedded contradictions to evaluate intra-document and cross-document consistency.


<details>
  <summary>Details</summary>
Motivation: Existing contradiction detection benchmarks are limited to sentence-level analysis and don't capture enterprise document complexity, while contradictions in retrieved evidence can cause inconsistent/untrustworthy outputs critical for enterprise compliance and governance.

Method: Generates synthetic enterprise-style documents with embedded contradictions using automated contradiction mining combined with human-in-the-loop validation, modeling a taxonomy of contradiction types common in business processes.

Result: The framework enables systematic evaluation of both intra-document and cross-document consistency through controlled creation of self- and pairwise contradictions, with a contradiction-aware retrieval evaluation pipeline.

Conclusion: ContraGen establishes a foundation for more trustworthy and accountable RAG systems in enterprise applications, where detecting and resolving contradictions is essential for reducing risk and ensuring compliance.

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [231] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: A qualitative comparison of cognitive and generative architectures for whole-mind systems, addressing challenges in evaluating theories based on these architectures.


<details>
  <summary>Details</summary>
Motivation: Evaluation is challenging for both cognitive architectures and generative neural architectures, requiring a broader perspective on theory evaluation.

Method: Leveraging a broad perspective on theory evaluation to conduct a wide-ranging qualitative comparison of whole-mind-oriented cognitive and generative architectures and their full systems.

Result: The paper provides a comprehensive qualitative analysis comparing cognitive and generative architectures for whole-mind systems.

Conclusion: A broad evaluation approach can effectively address the dual challenges of evaluating theories based on cognitive and generative architectures.

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [232] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: A framework that uses LLMs to convert natural language plans into Kripke structures and LTL formulas, then performs model checking to evaluate plan-behavior alignment, achieving 96.3% F1 score with GPT-5.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the alignment between natural language plans and their expected behavior through formal verification methods.

Method: Convert natural language plans into Kripke structures and Linear Temporal Logic (LTL) formulas using Large Language Models, then perform model checking on a simplified PlanBench dataset.

Result: GPT-5 achieves excellent classification performance with 96.3% F1 score and produces syntactically perfect formal representations that can act as guarantees.

Conclusion: The framework successfully demonstrates high performance in plan verification, though achieving semantically perfect formal models remains a challenge for future work.

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [233] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: PolicyGuardBench is a benchmark for detecting policy violations in web agent trajectories, with PolicyGuard-4B as a lightweight guardrail model that achieves strong detection accuracy and generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Autonomous web agents need to comply with external policies, but little work has examined policy compliance across different contexts like domains and subdomains.

Method: Created PolicyGuardBench with 60k examples from diverse agent runs, including full-trajectory and prefix-based violation detection tasks. Trained PolicyGuard-4B model on this dataset.

Result: PolicyGuard-4B delivers strong detection accuracy across all tasks, generalizes across domains, and maintains high accuracy on unseen settings while keeping inference efficient.

Conclusion: Accurate and generalizable guardrails for policy compliance in web agent trajectories are feasible at small scales, providing a comprehensive framework for studying policy compliance.

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [234] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow is the first non-autoregressive multimodal model that enables concurrent text-image generation using insertion-based Edit Flow for text and Flow Matching for images, outperforming autoregressive models with 50% fewer training FLOPs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of autoregressive models that enforce rigid causal ordering between text and image generation, enabling more flexible and efficient concurrent multimodal generation.

Method: Combines insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents, using hierarchical sampling that prioritizes content over grammar for concurrent text-image synthesis.

Result: Outperforms autoregressive baselines on both generation and understanding tasks across model sizes from 1B to 8B, while using up to 50% fewer training FLOPs. Surpasses both autoregressive and diffusion-based approaches.

Conclusion: OneFlow enables new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation, demonstrating superior performance and efficiency compared to existing approaches.

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [235] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: Test-time scaling improves LLM reasoning by generating longer Chains-of-Thought, but its effectiveness depends on training data conditions. The paper analyzes when long CoTs help or harm performance using linear regression tasks.


<details>
  <summary>Details</summary>
Motivation: To understand when and why test-time scaling (longer Chains-of-Thought) improves reasoning performance, as current understanding is unclear despite strong empirical results from models like OpenAI's o1 and DeepSeek R1.

Method: Theoretical analysis using transformers trained on in-context weight prediction for linear regression, characterizing task hardness via feature covariance eigenvalues, and experimental validation on nonlinear transformers.

Result: Found that: 1) More test-time compute reduces needed training context length for same error; 2) Insufficient training data skills can make test-time scaling harmful; 3) Diverse, relevant, hard tasks yield best test-time scaling performance.

Conclusion: Test-time scaling effectiveness depends critically on training data quality - it requires sufficient relevant skills in training data and performs best on diverse, challenging tasks.

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [236] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: Cross-Modal Preference Steering (CPS) is a black-box attack method that jointly optimizes visual and textual perturbations to manipulate VLM-based web agents' selection decisions, achieving high effectiveness while maintaining stealth.


<details>
  <summary>Details</summary>
Motivation: VLM-based web agents are vulnerable to preference manipulations, but existing attacks assume unrealistic settings like white-box access or single-modal perturbations. This paper aims to demonstrate more powerful attacks under realistic black-box conditions.

Method: CPS jointly optimizes imperceptible modifications to both visual and textual content, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions without requiring gradient access or agent internals.

Result: CPS significantly outperforms baseline methods across state-of-the-art VLMs (GPT-4.1, Qwen-2.5VL, Pixtral-Large) on movie selection and e-commerce tasks, while maintaining 70% lower detection rates.

Conclusion: The findings reveal critical vulnerabilities in VLM-based web agents and highlight the urgent need for robust defenses as these systems play increasingly important societal roles.

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [237] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: MITS is a novel tree search framework for LLM reasoning that uses pointwise mutual information (PMI) for step-wise path evaluation and beam search, achieving superior performance with computational efficiency through entropy-based dynamic sampling and weighted voting.


<details>
  <summary>Details</summary>
Motivation: Existing tree search methods for LLM reasoning face challenges in providing instant quantitative assessments of intermediate reasoning steps and suffer from high computational costs due to extensive path exploration.

Method: Proposes Mutual Information Tree Search (MITS) with PMI-based scoring function for step-wise evaluation, beam search for tree expansion without look-ahead simulations, entropy-based dynamic sampling for adaptive resource allocation, and weighted voting for final predictions.

Result: MITS consistently surpasses baseline methods across diverse reasoning benchmarks while maintaining computational efficiency.

Conclusion: MITS establishes a principled and efficient framework for LLM reasoning that combines information-theoretic principles with practical computational considerations.

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [238] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: Diffusion LLMs suffer from 'eos overflow' where longer allocated sequences cause shorter responses due to eos token dominance. Rainbow Padding fixes this by using multiple distinct padding tokens instead of repeated eos tokens.


<details>
  <summary>Details</summary>
Motivation: Instruction-tuned diffusion LLMs exhibit a critical vulnerability where increasing sequence length paradoxically shortens responses due to eos token dominance, which hasn't been systematically analyzed.

Method: Rainbow Padding replaces repeated eos placeholders with a repeating cycle of distinct padding tokens to distribute probability mass and break eos dominance.

Result: Rainbow Padding substantially improves length robustness and output quality, with just seven padding tokens sufficient to prevent early termination. It integrates efficiently via single-epoch LoRA fine-tuning on minimal data.

Conclusion: Rainbow Padding provides a simple, practical solution to eos overflow in diffusion LLMs, enabling robust generation across varying sequence lengths with minimal training overhead.

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [239] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: Proposes a goal-oriented evaluation framework for multi-agent chatbots using Goal Success Rate (GSR) and Root Cause of Failure (RCOF) taxonomy to measure whether user goals are fulfilled and diagnose failures.


<details>
  <summary>Details</summary>
Motivation: Existing chatbot evaluation methods assess interactions at turn level but don't address whether users' overarching goals (information needs or tasks) are actually fulfilled.

Method: Segments conversations by user goals, uses teacher LLMs with domain expert-defined goals and quality standards, employs "thinking tokens" for interpretable rationales, and applies GSR and RCOF taxonomy.

Result: Applied to AIDA enterprise chatbot system, achieved GSR improvement from 63% to 79% over six months, providing actionable insights through detailed failure analysis.

Conclusion: The framework offers explainable, data-efficient evaluation that diagnoses overall success, identifies key failure modes, and informs system improvements for multi-agent chatbots.

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [240] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: H-DDx is a hierarchical evaluation framework for LLMs in differential diagnosis that better reflects clinical relevance by crediting predictions closely related to ground-truth diagnoses, showing conventional flat metrics underestimate performance.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs for differential diagnosis rely on flat metrics that fail to distinguish between clinically relevant near-misses and diagnostically distant errors, limiting their clinical utility.

Method: H-DDx uses a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies hierarchical metrics that credit predictions closely related to ground-truth diagnoses.

Result: Benchmarking 22 leading models showed conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with domain-specialized open-source models performing well. The framework revealed LLMs often correctly identify broader clinical context even when missing precise diagnoses.

Conclusion: H-DDx provides a more clinically relevant evaluation framework for LLMs in differential diagnosis, enhancing interpretability and revealing hierarchical error patterns that conventional metrics miss.

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [241] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: This paper investigates how to bridge multimodal foundation models (MFMs) with world models by enhancing their reasoning and generative capabilities for better understanding and simulating dynamic physical processes.


<details>
  <summary>Details</summary>
Motivation: Current multimodal foundation models lack essential world modeling abilities like counterfactual reasoning, dynamics simulation, spatiotemporal understanding, and controllable generation that humans naturally possess through multimodal sensory integration.

Method: The approach involves: 1) Enhancing MFMs' reasoning through discriminative tasks and structured reasoning skills (causal inference, counterfactual thinking, spatiotemporal reasoning), 2) Developing generative capabilities using scene graphs, multimodal conditioning, and alignment strategies for structured and controllable generation across image and video modalities, 3) Extending to controllable 4D generation for interactive, editable object synthesis.

Result: The research aims to equip MFMs with deeper understanding beyond surface correlations and enable consistent generation aligned with high-level semantics and fine-grained user intent across space and time.

Conclusion: By systematically enhancing both reasoning and generative capabilities, multimodal foundation models can be transformed into more effective world models that better simulate and understand dynamic physical processes.

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [242] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent is a framework that uses multi-agent simulations with genetic algorithms to optimize e-commerce query rewriting, achieving 21.98% improvement over original queries and 3.36% over baseline LLM rewriting.


<details>
  <summary>Details</summary>
Motivation: LLM evaluation is challenging for subjective tasks like e-commerce query rewriting where there's no single correct answer, making algorithmic verification difficult.

Method: Combines multi-agent simulations (LLM agents acting as shopping customers) with genetic algorithms, using agent-derived scores as fitness function to iteratively refine queries.

Result: Tested on 1000 real-world e-commerce queries across 5 categories, achieving 21.98% improvement over original queries and 3.36% over Best-of-N LLM baseline.

Conclusion: OptAgent provides an effective framework for optimizing subjective tasks like query rewriting by leveraging multi-agent simulations as dynamic reward signals.

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [243] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: GuidedSampling is a new inference algorithm that improves diversity in solution generation by decoupling exploration and generation phases, outperforming Repeated Sampling by ~21.6% at pass@50 and training models that show ~9.7% improvement at pass@5.


<details>
  <summary>Details</summary>
Motivation: Repeated Sampling (RS) struggles with generating diverse solution candidates and often produces redundant samples by relying on the same underlying approach, limiting its effectiveness.

Method: GuidedSampling decouples exploration and generation phases: exploration identifies multiple concepts for solving problems, while generation applies specific concepts to produce final solution candidates.

Result: GuidedSampling improves base model performance by ~21.6% at pass@50 across benchmarks compared to RS. Models trained with GuidedSampling show ~9.7% improvement at pass@5 and increase average concepts per instance from 1.67 to 3.03.

Conclusion: GuidedSampling effectively addresses the diversity limitations of Repeated Sampling by separating concept exploration from solution generation, leading to significant performance improvements and more diverse candidate solutions.

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [244] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: The paper introduces the hidden game problem where players have unknown subsets of strategies that yield higher rewards, and develops regret minimization algorithms to discover and exploit these hidden structures while achieving optimal regret bounds and convergence to correlated equilibria.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by challenges in AI alignment and language games, where players need to discover hidden strategy subsets that consistently provide better rewards without prior knowledge of these structures.

Method: The authors develop a composition of regret minimization techniques that can efficiently discover hidden strategy subsets and exploit them while maintaining rationality. The approach leverages the hidden game structure to improve computational efficiency.

Result: The method achieves optimal external and swap regret bounds, ensuring rapid convergence to correlated equilibria in hidden subgames while maintaining efficient performance in the overall game.

Conclusion: The paper affirmatively answers the central question by showing that efficient regret minimization algorithms can be designed to discover hidden game structures and achieve equilibrium in subgames while maintaining general rationality.

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [245] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: Small language models (SLMs) are sufficient and often superior to larger models for agentic workloads focused on schema- and API-constrained accuracy rather than open-ended generation, offering 10x-100x lower token cost with better latency and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that SLMs (1-12B parameters) can effectively handle agentic workloads with structured outputs and function calling, providing cost-effective alternatives to larger models while maintaining performance on constrained tasks.

Method: Proposes SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, using guided decoding, strict JSON Schema outputs, validator-first tool execution, and design patterns like schema-first prompting and lightweight adaptation via LoRA/QLoRA.

Result: SLMs can match or surpass larger models on tool use, function calling, and RAG tasks at significantly lower cost (10x-100x lower token cost) with better latency and energy efficiency, while guided decoding and structured outputs close capability gaps.

Conclusion: SLMs provide a practical blueprint for building fast, inexpensive, and reliable agents that default to small models while preserving headroom with targeted LLM assistance, particularly effective for schema-constrained tasks though fallback remains valuable for open-domain reasoning.

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [246] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse is a framework that uses LLMs with three self-reflection principles to generate creative algorithms, achieving significant performance improvements in cache replacement (35.76% fewer misses) and online bin packing (30.93% fewer bins).


<details>
  <summary>Details</summary>
Motivation: LLMs are biased towards generic designs and struggle with creative leaps needed for discontinuous solution spaces in algorithm design, limiting their practical use in generating novel system algorithms.

Method: MetaMuse introduces three self-reflection principles: (1) quantify diversity in performance space, (2) steer ideation with external stimuli, (3) construct solutions using waypoint reasoning instead of chain-of-thought.

Result: MetaMuse achieved substantial improvements: 35.76% reduction in cache misses for cache replacement and 30.93% reduction in bin usage for online bin packing.

Conclusion: The framework successfully addresses LLM limitations in creative algorithm design through structured self-reflection, demonstrating practical value for real-world system optimization problems.

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [247] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: Proposes LLM-enhanced contextual reasoning with XAI agents for IoT anomaly detection, showing superior accuracy and interpretability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection methods struggle with dynamic, high-dimensional IoT environments where data is incomplete, messy, or evolving. Critical IoT systems like smart healthcare and energy grids require adaptive, intelligent systems for reliable monitoring.

Method: Uses LLM-supported contextual reasoning with XAI agents, attention methods, memory buffers with meaning, and avoids processing every time step detail. Combines no-code AI for transparency and interpretability.

Result: The proposed approach significantly outperforms existing models in both detection accuracy and interpretability. Tested in smart grid and healthcare simulations, it demonstrates better adaptability and reliability.

Conclusion: The LLM-enhanced contextual reasoning method with XAI agents shows strong potential as a future solution for anomaly detection in critical IoT environments, offering improved accuracy and transparency.

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [248] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: Spatial CAPTCHA is a novel human-verification framework that leverages spatial reasoning differences between humans and AI to counter automated abuse, achieving significantly higher human success rates compared to state-of-the-art MLLMs.


<details>
  <summary>Details</summary>
Motivation: Conventional CAPTCHAs focusing on text recognition or 2D image understanding have become ineffective due to advances in multi-modal large language models (MLLMs), requiring a new approach that exploits fundamental differences in spatial reasoning capabilities.

Method: The system uses procedural generation of dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. It employs constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation for scalability and robustness.

Result: Evaluation on Spatial-CAPTCHA-Bench shows humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Comparison with Google reCAPTCHA confirms its effectiveness as both security mechanism and AI diagnostic tool.

Conclusion: Spatial CAPTCHA effectively addresses the limitations of conventional CAPTCHAs by leveraging spatial reasoning capabilities that remain challenging for current AI systems while being intuitive for humans, providing a robust defense against automated abuse.

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [249] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: A simple intervention that expands representational basins around text token embeddings in Multi-modal Diffusion Transformers (MM-DiTs) to surface rare semantics without additional training or external modules.


<details>
  <summary>Details</summary>
Motivation: Advanced text-to-vision models still struggle with imaginative or rare prompts because these concepts are too scarce during pre-training to leave strong imprints, limiting their generation capabilities.

Method: Mathematically expanding representational basins around text token embeddings via variance scale-up before joint-attention blocks in MM-DiTs, leveraging the intrinsic joint-attention mechanism that updates text and image embeddings sequentially.

Result: The method effectively surfaces rare semantics in MM-DiT outputs and generalizes across text-to-vision tasks including text-to-image, text-to-video, and text-driven image editing.

Conclusion: This work demonstrates that generative models can reveal hidden semantics that users intend through simple mathematical interventions, without requiring additional training steps, data, or external modules.

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [250] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: A gamified XAI system for ethical coffee purchasing decisions that combines Kantian and utilitarian reasoning with real-time explanations and regret-bounded decision switching.


<details>
  <summary>Details</summary>
Motivation: To help consumers make ethically aware decisions in coffee purchasing by providing transparent explanations that combine deontological (rule-based) and consequentialist (utility-based) ethical frameworks.

Method: Six-round gamified sessions with three coffee options per round. Uses two symbolic engines: Kantian module flags rule violations (child labor, deforestation, etc.), and utilitarian module scores options via multi-criteria aggregation. Includes meta-explainer with 0.2 regret bound to highlight ethical alignment and switch to deontically clean options when welfare loss is small.

Result: Developed a complete system with structured configuration (attribute schema, certification map, weights, rule set), policy trace for auditability, and interactive UI for consumer use.

Conclusion: The system successfully integrates multiple ethical frameworks in an explainable AI system for consumer decision-making, providing both transparency and practical guidance for ethically complex purchasing decisions.

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [251] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM is a certification framework that provides statistical guarantees for bounding catastrophic risks in multi-turn LLM conversations by modeling conversations as Markov processes on query graphs.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations fail to fully reveal LLM vulnerabilities due to fixed attack prompts, lack of statistical guarantees, and inability to scale to multi-turn conversation spaces, posing serious public safety risks.

Method: Model multi-turn conversations as probability distributions over query sequences using Markov processes on query graphs with semantic similarity edges, and define practical distributions (random node, graph path, adaptive with rejection) to quantify catastrophic risks with confidence intervals.

Result: The framework reveals substantial catastrophic risks in frontier models, with certified lower bounds as high as 70% for the worst model, demonstrating urgent safety concerns.

Conclusion: There is an urgent need for improved safety training strategies in frontier LLMs, as current models exhibit significant catastrophic risks under realistic multi-turn conversation scenarios.

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [252] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: C^2-Eval is a new benchmark for evaluating creativity in foundation models, distinguishing between convergent (constrained) and divergent (open-ended) creativity using Usefulness, Originality, and Surprise criteria grounded in social science theory.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks for creativity in foundation models are fragmented and lack theoretical grounding, despite creativity becoming increasingly important as a dimension of machine intelligence alongside traditional accuracy measures.

Method: Developed C^2-Eval benchmark that evaluates two forms of creativity: convergent (for tasks like code generation) and divergent (for tasks like storytelling), using fine-grained criteria of Usefulness, Originality, and Surprise derived from social science theory.

Result: Extensive experiments on leading proprietary and open-source models revealed trade-offs in creative capabilities, highlighting both strengths and challenges of current foundation models in achieving creative machine intelligence.

Conclusion: C^2-Eval provides an effective framework for examining the evolving landscape of creative AI and serves as a valuable tool for assessing the creative capabilities of foundation models.

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [253] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: The paper introduces Zephyrus, an LLM-based agentic framework that bridges weather foundation models with language reasoning capabilities, enabling interactive weather science workflows through code-based tools and conversational feedback loops.


<details>
  <summary>Details</summary>
Motivation: Foundation models for weather outperform traditional forecasting but lack language reasoning, while LLMs can't handle meteorological data. This creates a gap in interactive scientific workflows that need both data processing and natural language capabilities.

Method: Built a Python code-based environment (ZephyrusWorld) with tools for weather data interaction, including WeatherBench 2 dataset interface, geoquerying, forecasting, and climate simulation. Created Zephyrus, a multi-turn LLM agent that iteratively analyzes data and refines approaches through conversational feedback.

Result: Zephyrus agents outperform text-only baselines by up to 35 percentage points in correctness on the ZephyrusBench benchmark. However, on harder tasks, performance is similar to baselines, indicating the benchmark's challenging nature.

Conclusion: The framework successfully bridges weather data processing with language reasoning, but harder tasks remain challenging, suggesting promising directions for future work in agentic weather science.

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [254] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: This survey provides the first comprehensive taxonomy of data science agents, analyzing 45 systems across the six stages of the data science lifecycle and five design dimensions, identifying key trends and gaps in current research.


<details>
  <summary>Details</summary>
Motivation: Recent advances in LLMs have enabled new AI agents that automate multiple stages of the data science workflow, but there is a need for systematic classification and analysis of these emerging systems to understand their capabilities and limitations.

Method: The authors developed a lifecycle-aligned taxonomy mapping 45 data science agent systems across six data science stages and annotated each along five cross-cutting design dimensions including reasoning style, modality integration, tool orchestration, learning methods, and trust mechanisms.

Result: Analysis revealed three key trends: most systems focus on exploratory analysis and modeling while neglecting business understanding and deployment; multimodal reasoning and tool orchestration remain challenging; and over 90% lack explicit trust and safety mechanisms.

Conclusion: The paper outlines open challenges in alignment stability, explainability, governance, and robust evaluation, proposing future research directions for developing more robust, trustworthy, and transparent data science agents.

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [255] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: MedLog is a syslog-inspired protocol for standardized logging of clinical AI model usage, enabling transparency, performance monitoring, and safety surveillance in healthcare AI systems.


<details>
  <summary>Details</summary>
Motivation: Healthcare lacks a standard logging protocol for clinical AI systems, making it difficult to track model usage, measure real-world performance, detect adverse events, and monitor for bias or dataset drift as hospitals increasingly adopt AI tools.

Method: MedLog creates structured event-level logs with nine core fields (header, model, user, target, inputs, artifacts, outputs, outcomes, feedback) whenever AI models interact with humans, other algorithms, or act independently. It includes risk-based sampling, lifecycle-aware retention, and write-behind caching for efficiency.

Result: The protocol provides a consistent framework for recording AI model activity, supporting detailed traces for complex workflows while minimizing data footprint through optimization features.

Conclusion: MedLog enables continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for digital epidemiology and catalyzing development of new databases and analysis tools for clinical AI monitoring.

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [256] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: The paper introduces FaithCoT-Bench, a benchmark for detecting unfaithful Chain-of-Thought (CoT) reasoning in LLMs, addressing the gap in instance-level faithfulness evaluation.


<details>
  <summary>Details</summary>
Motivation: CoT prompting in LLMs often fails to faithfully represent the underlying reasoning process, raising reliability concerns in high-risk applications. Prior studies focused on mechanism-level analyses but left open the practical challenge of detecting unfaithfulness in specific reasoning trajectories.

Method: The authors introduce FaithCoT-Bench with FINE-CoT - an expert-annotated collection of over 1,000 CoT trajectories from four LLMs across four domains, including 300+ unfaithful instances with fine-grained causes and step-level evidence. They systematically evaluate 11 detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms.

Result: The evaluation reveals empirical insights about the strengths and weaknesses of existing detection approaches, showing increased challenges in knowledge-intensive domains and with more advanced models.

Conclusion: FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, providing a solid foundation for future research toward more interpretable and trustworthy reasoning in LLMs.

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [257] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: LLMs lack reliable uncertainty quantification. This paper proposes variable-threshold voting ensembles that can abstain when confidence is low, dramatically increasing answer trustworthiness while maintaining good response yield.


<details>
  <summary>Details</summary>
Motivation: LLMs need better uncertainty quantification methods for high-stakes applications. Current approaches don't provide reliable trustworthiness measures for automated responses.

Method: Variable-threshold voting ensembles that allow abstention when dominant response falls below threshold. Theoretical framework for question answering with experimental validation on arithmetic problems and clinical-note QA.

Result: Large gains in answer trustworthiness achieved with restrictive voting ensembles, with modest reductions in response yield and accuracy. Particularly effective in domains requiring high certainty.

Conclusion: Voting ensembles are useful for applications like healthcare and data annotation that require high certainty but may not need automated answers to every question.

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [258] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT is a unified framework for data-efficient LLM evaluation that supports both binary and continuous metrics while leveraging structural knowledge across benchmarks, achieving stable capability estimates with only 3% of evaluation items.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation is computationally expensive, and existing IRT-based methods are limited to binary metrics and single benchmarks, failing to utilize structural knowledge across different metrics and benchmarks.

Method: LEGO-IRT introduces a factorized architecture that natively supports binary and continuous metrics, decomposing model ability into general and structure-specific components to explicitly model correlations across metrics and benchmarks.

Result: The framework achieves stable capability estimates using only 3% of total evaluation items, reduces estimation error by up to 10% when incorporating structural knowledge, and shows that latent abilities may better align with human preferences.

Conclusion: LEGO-IRT provides a flexible and efficient framework for LLM evaluation that overcomes limitations of existing IRT methods by supporting multiple metric types and leveraging structural knowledge across benchmarks.

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [259] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: LLMs develop well-defined internal emotional representations that emerge early in the network, peak mid-network, and persist for hundreds of tokens, with performance improving with model scale.


<details>
  <summary>Details</summary>
Motivation: To understand how, where, and for how long emotion is encoded in LLMs' neural architecture, as their internal emotional mechanisms remain largely unexplored despite their ability to simulate emotional intelligence.

Method: Used a novel large-scale Reddit corpus of 400K utterances balanced across 7 basic emotions, employing lightweight probes to read information from hidden layers of Qwen3 and LLaMA models without parameter alteration.

Result: LLMs develop surprisingly well-defined internal emotional geometry that sharpens with model scale, outperforms zero-shot prompting, emerges early and peaks mid-network, is malleable via system prompts, and persists for hundreds of subsequent tokens.

Conclusion: The findings provide crucial insights for developing more transparent and aligned AI systems, with contributions including an open-source dataset, probing toolkit, and detailed map of emotional landscape within LLMs.

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [260] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: The paper introduces the Moral Anchor System (MAS), a framework to detect, predict, and mitigate value drift in AI agents using real-time Bayesian inference, LSTM networks, and human-centric governance.


<details>
  <summary>Details</summary>
Motivation: As AI becomes more integrated as super-capable assistants, there are critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions, particularly addressing the risk of value drift where AI systems deviate from aligned values.

Method: MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions, emphasizing low-latency responses (<20 ms) and reducing false positives via supervised fine-tuning with human feedback.

Result: The system achieved 80% reduction in value drift incidents in simulations, with high detection accuracy (85%) and low false positive rates (0.08 post-adaptation), validated through rigorous experiments with goal-misaligned agents.

Conclusion: MAS provides a predictive and adaptive solution for value alignment that contrasts with static methods, offering scalability, cross-domain applicability, and open-source implementation for replication.

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [261] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW is a new score-based preference approach that uses group-wise comparison and continuous optimization to automatically generate and optimize agentic workflows for LLMs, overcoming limitations of discrete optimization methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches for designing agentic workflows require substantial manual effort and suffer from limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigms due to dependence on discrete optimization techniques.

Method: SPOGW introduces a score-based preference approach with Iterative offline GRPO (ioGRPO) and advantage-masked KL divergence (mKL), operating directly on cardinal reward signals through group-wise comparison in continuous space.

Result: In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches.

Conclusion: SPOGW presents a viable and forward-looking methodology for automated generation and optimization of agentic workflows, enabling more efficient and stable optimization through continuous space techniques.

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [262] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: DLLM is a Diffusion-based LLM framework that addresses noise and data imbalance issues in cognitive diagnosis for web-based education systems, achieving robust performance through two-stage denoising and semantic-structural alignment.


<details>
  <summary>Details</summary>
Motivation: Traditional cognitive diagnosis in web-based education systems struggles with noisy interactions, data imbalance from continuous new student influx, and LLMs' limitations with structured data and noise sensitivity.

Method: DLLM constructs correctness-based subgraphs, applies relation augmentation for data imbalance, fuses structural representations with LLM-derived semantic representations, and uses two-stage denoising diffusion (unconditional then conditional) for noise removal before alignment steps.

Result: Experiments on three web-based educational datasets show DLLM achieves optimal predictive performance across varying noise levels, demonstrating noise robustness while effectively leveraging LLM semantic knowledge.

Conclusion: DLLM successfully addresses noise and data imbalance challenges in cognitive diagnosis by combining diffusion-based denoising with semantic-structural alignment, providing a robust framework for web-based educational systems.

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [263] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: WebRenderBench is a large-scale benchmark for WebUI-to-Code conversion with 22.5k real-world webpages, featuring a novel evaluation metric for layout/style consistency and ALISA agent that uses this metric in reinforcement learning to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing WebUI-to-Code benchmarks lack data diversity and evaluation reliability, with vision-based methods being costly and structure-based methods being vulnerable to noise and asymmetry.

Method: Created WebRenderBench with 22.5k diverse real-world webpages, proposed a novel evaluation metric for layout/style consistency from rendered pages, and developed ALISA agent that integrates this metric into reinforcement learning as a reward signal.

Result: ALISA significantly boosts generation performance and achieves state-of-the-art results across multiple metrics, demonstrating the effectiveness of the proposed evaluation approach and training methodology.

Conclusion: The proposed WebRenderBench benchmark and ALISA agent with novel evaluation metric provide more efficient, objective, and reliable UI quality assessment for WebUI-to-Code tasks, advancing the field beyond existing limited benchmarks.

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [264] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR is a framework that automatically searches for query-aware meta reasoning skeletons using DAG representations and AutoML-inspired methods, improving LLM reasoning performance by adapting to query-specific requirements and capturing logical dependencies.


<details>
  <summary>Details</summary>
Motivation: Previous meta reasoning skeletons were manually designed, limiting their ability to adapt to query-specific requirements and capture intricate logical dependencies among reasoning steps.

Method: Represent meta reasoning skeletons with directed acyclic graphs (DAGs), construct a search space based on DAG representation, and use a dynamic skeleton sampling algorithm that expands skeletons along with reasoning context at inference time.

Result: AutoMR achieves better reasoning performance than previous works across extensive benchmark datasets, demonstrating broad improvements in reasoning capabilities.

Conclusion: The AutoMR framework successfully enables efficient query-aware skeleton search and adapts skeletons to evolving reasoning contexts, leading to superior reasoning performance compared to manually designed approaches.

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [265] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: The paper investigates whether model latents before wait tokens contain information that modulates reasoning processes, identifying specific features that influence wait token probabilities and enable different reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: Little is understood about why models decide to reason using wait tokens (which signal behaviors like backtracking), limiting understanding of what makes reasoning models effective.

Method: Train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, introduce latent attribution technique in crosscoder setting, and conduct experiments analyzing max activating examples and causal interventions.

Result: Located a small set of features relevant for promoting/suppressing wait tokens' probabilities, showing these features are relevant for reasoning process and give rise to different reasoning patterns.

Conclusion: Model latents preceding wait tokens do contain relevant information for modulating subsequent reasoning, enabling patterns like restarting, recalling prior knowledge, expressing uncertainty, and double-checking.

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [266] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: MENTOR is a framework that provides expert guidance only at critical decision points in RLVR, enabling effective and diverse exploration without requiring full expert trajectory imitation.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods rely on imitating entire expert trajectories, which improves effectiveness but neglects diversity in exploration. The effectiveness of RLVR depends on base models' capability for high-quality exploration.

Method: MENTOR provides expert guidance only at critical decision points rather than entire reasoning paths, enabling mixed-policy expert navigation for token-level optimization of reasoning.

Result: Extensive experiments show MENTOR enables models to capture the essence of expert strategies rather than surface imitation, performing high-quality exploration and achieving superior overall performance.

Conclusion: By providing expert guidance only at critical points, MENTOR addresses the limitations of existing RLVR methods and enables more effective and diverse exploration for enhanced reasoning capabilities.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [267] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: This survey traces the evolution of multimodal AI evaluation from basic recognition tasks to complex reasoning benchmarks, highlighting a paradigm shift towards testing deeper understanding processes.


<details>
  <summary>Details</summary>
Motivation: The field needs to move beyond saturated benchmarks where high performance masks fundamental weaknesses, requiring more sophisticated evaluations that probe 'why' and 'how' models understand.

Method: Chronicles the progression through three phases: foundational knowledge tests (ImageNet era), applied logic/comprehension exams (GQA, VCR), and current expert-level integration benchmarks (MMBench, SEED-Bench, MMMU) for multimodal LLMs.

Result: Shows how evaluation has evolved from testing 'what' models see to examining reasoning processes, with increasing focus on diagnosing systemic flaws like shortcut learning and compositional generalization failures.

Conclusion: AI evaluation is an adversarial process of designing better examinations that continuously redefine goals for creating truly intelligent systems, going beyond mere dataset history.

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [268] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) is a declarative language that enables cross-framework compatibility for AI agents and workflows, promoting portability and interoperability.


<details>
  <summary>Details</summary>
Motivation: To resolve fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, reducing redundant development efforts.

Method: A declarative language specification that allows AI agents to be defined independently of their execution environment, facilitating development tools and portability.

Result: Benefits four key groups: developers gain reusable components, framework developers get interchange format, researchers achieve reproducibility, and enterprises get faster deployment with scalability.

Conclusion: Agent Spec provides technical foundations for unified AI agent development across frameworks, improving interoperability, reusability, and reducing development fragmentation.

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [269] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: LLM-driven framework for incremental map construction and repair that detects, localizes, and corrects structural inconsistencies in navigation graphs using version control and edge impact scoring.


<details>
  <summary>Details</summary>
Motivation: LLMs can infer spatial layouts from navigation instructions but struggle with longer environments, requiring incremental map construction that needs mechanisms to handle structural inconsistencies.

Method: Proposes Version Control to track graph edit history and Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation.

Result: Significantly improves map correctness and robustness, especially for entangled or chained inconsistencies, using a refined MANGO benchmark dataset.

Conclusion: History-aware repair mechanisms are crucial for maintaining coherent spatial memory in LLM agents, highlighting the importance of introspective map repair approaches.

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [270] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL is a reinforcement learning framework that trains multimodal reasoning models with multiple objectives to simultaneously improve safety and capability, reducing jailbreak vulnerabilities while maintaining reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models face safety challenges where images and text can bypass guardrails, and single-objective training causes policy drift leading to either over-refusal on benign inputs or unsafe compliance on risky ones.

Method: COSMO-RL uses mixed reinforcement learning with multimodal, multitask, and multiobjective signals to train reasoning-oriented LMRMs, allowing safety and capability to grow together in one stable pipeline.

Result: COSMO-R1 improves safety while maintaining and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework transfers across backbones with consistent gains.

Conclusion: The approach provides a simple path to advancing both safety and general capability together in large multimodal reasoning models, with ablations supporting the design choices.

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [271] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: AgentRL is a scalable framework for multi-turn, multi-task reinforcement learning training of LLM agents, featuring asynchronous infrastructure and stable training algorithms that outperform state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for training LLM agents in multi-turn, multi-task settings face challenges with scalable infrastructure and stable training algorithms.

Method: Uses fully-asynchronous generation-training pipeline, unified function-call API, containerized environments, cross-policy sampling for exploration, and task advantage normalization for stability.

Result: Significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1 and other open-source LLM agents across five agentic tasks, with multi-task training matching best task-specific models.

Conclusion: AgentRL provides an effective solution for scalable multi-turn, multi-task RL training of LLM agents and has been adopted in AutoGLM.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [272] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: The paper proposes a Bayesian evaluation framework to replace Pass@k for LLM reasoning assessment, providing more stable rankings and principled uncertainty estimation with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Pass@k yields unstable and misleading rankings for LLM reasoning performance, especially with limited trials and constrained compute resources.

Method: Uses Bayesian posterior estimates with Dirichlet prior to model evaluation outcomes as categorical, providing closed-form expressions for posterior mean and uncertainty of weighted rubrics.

Result: The Bayesian framework achieves faster convergence and greater rank stability than Pass@k and variants, enabling reliable comparisons with smaller sample counts on benchmarks like AIME'24/'25, HMMT'25, and BrUMO'25.

Conclusion: Recommends replacing Pass@k with posterior-based protocol that unifies binary and non-binary evaluation while making uncertainty explicit, providing compute-efficient and statistically meaningful comparisons.

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [273] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: Proposes a multi-agent reinforcement learning framework for cross-functional coordination between inventory replenishment and product recommendation, achieving significant profitability improvements over siloed approaches.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of effective cross-functional coordination in complex organizations to enhance firm-wide profitability, leveraging AI advances in reinforcement learning.

Method: Develops a unified multi-agent RL framework with multi-timescale architecture, decomposing policies by departmental functions and assigning distinct learning speeds based on task complexity and responsiveness.

Result: Extensive simulations show significant profitability improvements over siloed decision-making, with trained RL agents' behaviors aligning closely with theoretical managerial insights.

Conclusion: Provides a scalable, interpretable RL-based solution for effective cross-functional coordination in complex business settings, with proven convergence and practical deployment advantages.

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [274] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK is a grounded multimodal LLM that jointly processes color fundus photography, OCT, and text to deliver clinician-grade ocular and systemic disease diagnoses, outperforming larger models through a quantitative-to-qualitative diagnostic chain of thought.


<details>
  <summary>Details</summary>
Motivation: Current medical MLLMs fail to fully exploit synergy between CFP and OCT modalities and offer limited interpretability of quantitative biomarkers, creating a need for better multimodal integration and clinical reasoning capabilities.

Method: Three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, establishing a quantitative-to-qualitative diagnostic chain of thought that mirrors real clinical reasoning.

Result: GROK outperforms comparable 7B and 32B baselines on report quality and fine-grained clinical metrics, and even exceeds OpenAI o3, using only LoRA fine-tuning of a 7B-parameter Qwen2 backbone.

Conclusion: GROK demonstrates that grounded multimodal models with proper quantitative-to-qualitative reasoning chains can achieve clinician-grade diagnostic performance while maintaining interpretability, representing a significant advancement in medical AI.

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [275] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1 is an AI doctor agent that combines accurate medical decision-making with strategic, empathetic patient consultation skills, outperforming existing LLMs through a multi-agent framework with specialized training.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs excel at medical decision-making but lack strategic and empathetic consultation skills needed for real clinical scenarios, creating a gap in comprehensive medical AI assistance.

Method: Uses a multi-agent interactive environment, two-tiered reward architecture (separately optimizing decision-making and communication), and experience repository to ground policy learning in high-quality trajectories.

Result: Surpasses state-of-the-art open-source specialized LLMs with higher parameter efficiency, outperforms proprietary models, and human evaluations show strong preference for Doctor-R1's clinical dialogue generation.

Conclusion: The framework effectively bridges the gap between medical decision accuracy and empathetic consultation skills, demonstrating the importance of combining both capabilities for realistic clinical AI applications.

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [276] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: LLM multi-agent systems (LLM-MAS) can outperform single-agent systems, but systematic evaluation is needed. This paper proposes a framework analyzing task complexity through depth (reasoning length) and width (capability diversity), showing LLM-MAS benefits increase with both dimensions, especially depth.


<details>
  <summary>Details</summary>
Motivation: To provide a principled understanding of when LLM multi-agent systems outperform single-agent systems, addressing the lack of systematic experimental designs in current research.

Method: Proposed a theoretical framework characterizing tasks along depth (reasoning length) and width (capability diversity) dimensions. Theoretically examined multi-agent debate systems and empirically evaluated performance on discriminative and generative tasks with varying depth and width.

Result: Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, with the effect being more pronounced with respect to depth.

Conclusion: This work clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [277] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: Speculative actions framework uses faster models to predict likely agent actions, enabling parallel execution and reducing latency in agentic systems.


<details>
  <summary>Details</summary>
Motivation: AI agent execution is often slow due to sequential API calls, hampering training, evaluation, and deployment. For example, chess games between state-of-the-art agents can take hours.

Method: Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, the framework predicts likely actions using faster models to enable multiple steps to be executed in parallel.

Result: Achieves up to 55% accuracy in next-action prediction across gaming, e-commerce, and web search environments, with significant reductions in end-to-end latency. Performance improves with stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization.

Conclusion: Speculative actions open a promising path toward deploying low-latency agentic systems in the real world by enabling parallel execution while maintaining lossless behavior.

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [278] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter is an agentic system that distills offline trajectories into compact, context-aware hints to improve LLM agents without costly online interactions or fine-tuning, leveraging both successful and failed trajectories.


<details>
  <summary>Details</summary>
Motivation: Improving LLM agents on unfamiliar domains typically requires expensive online interactions or fine-tuning on large datasets, which is impractical for closed-source models and risks catastrophic forgetting. Offline trajectories contain reusable knowledge but are often long, noisy, and task-specific.

Method: JEF Hinter uses a zooming mechanism to highlight decisive steps in long trajectories, capturing strategies and pitfalls. It generates compact hints from both successful and failed trajectories, supports parallelized hint generation, and uses a retriever to select relevant hints for current states during inference.

Result: Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.

Conclusion: JEF Hinter provides an effective approach for improving LLM agents using offline trajectories, offering targeted guidance with transparency and traceability while avoiding the costs of online interactions or fine-tuning.

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [279] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: BO-LLM uses Bayesian Optimization with LLM-powered Gaussian Process for prompt engineering to improve text classification accuracy while reducing API calls.


<details>
  <summary>Details</summary>
Motivation: To efficiently optimize expensive black-box functions (prompt engineering for LLMs) with limited evaluations using Bayesian Optimization.

Method: Uses LLM-powered Gaussian Process as surrogate model, generates prompt candidates from seed prompts via LLM, evaluates with UCB acquisition function, and iteratively refines prompts on subset data.

Result: Evaluated on two datasets, showing improved classification accuracy with reduced API calls by leveraging LLM-based GP prediction uncertainty.

Conclusion: BO-LLM algorithm effectively enhances prompt engineering for text classification with LLMs through Bayesian Optimization approach.

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [280] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: This paper proposes that imagination serves to access an internal world model (IWM) and uses psychological network analysis to compare IWMs in humans vs. LLMs, finding significant differences in network structure and centrality correlations.


<details>
  <summary>Details</summary>
Motivation: To understand the computational objective of imagination and challenge classical views that imagination primarily serves reward maximization, by comparing internal world models between humans and AI systems.

Method: Used psychological network analysis with imagination vividness ratings from questionnaires to construct imagination networks, comparing human groups with LLMs under different prompts and conversational memory conditions.

Result: Human imagination networks showed correlations between centrality measures (expected influence, strength, closeness), while LLM networks lacked clustering and showed lower centrality correlations across different conditions.

Conclusion: There is a lack of similarity between internal world models in humans and LLMs, providing a novel method for comparing internally-generated representations and insights for developing human-like imagination in AI.

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [281] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: The paper identifies a fundamental tension in self-modifying AI systems where utility-driven improvements can undermine learning capabilities, showing that distribution-free guarantees are only preserved when policy-reachable model families have uniformly bounded capacity.


<details>
  <summary>Details</summary>
Motivation: To understand the structural conflicts in self-modifying AI systems as they trend toward superintelligence, particularly how utility-driven self-improvement can conflict with reliable learning and generalization.

Method: Formalizes self-modification with a five-axis decomposition and decision layer, analyzes axes in isolation, and conducts numerical experiments comparing destructive utility policies against proposed two-gate policies that preserve learnability.

Result: Identifies a sharp utility-learning tension where utility-driven changes that improve performance can erode statistical preconditions for reliable learning. Shows distribution-free guarantees are preserved iff policy-reachable model family is uniformly capacity-bounded.

Conclusion: Under standard assumptions, self-modification axes reduce to a single capacity criterion, providing a boundary for safe self-modification. Two-gate policies can preserve learnability while destructive utility policies render learnable tasks unlearnable.

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [282] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO is a novel reinforcement learning framework that addresses overthinking in large reasoning models by decoupling length-based rewards for correct vs incorrect reasoning, achieving significant length reduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from overthinking - generating unnecessarily long and redundant reasoning even for simple questions, which increases computational costs and response latency. Existing methods that incorporate length rewards cause significant performance degradation.

Method: Proposes Decoupled Reward Policy Optimization (DRPO) that decouples length-based learning signals for correct rollouts from incorrect ones. It ensures reward signals for correct rollouts are normalized within the positive group only, shielding them from interference by negative samples. Uses an optimized positive data distribution with KL regularization and derives closed-form solution.

Result: Achieves 77% length reduction with only 1.1% performance loss on GSM8k dataset using a 1.5B model, significantly outperforming six baselines. The follow-up baseline sacrificed 4.3% performance for 68% length reduction.

Conclusion: DRPO effectively addresses overthinking in reasoning models by protecting correct reasoning from being penalized by length rewards, enabling substantial computational efficiency gains with minimal accuracy trade-offs.

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [283] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP extends continuous local search from Boolean SAT to general CSPs using Walsh-Fourier transform to convert constraints into compact multilinear polynomials, achieving competitive performance without auxiliary variables.


<details>
  <summary>Details</summary>
Motivation: While modern continuous local search solvers show competitive results on certain SAT problems, there's a need to extend this framework to general constraint satisfaction problems with finite-domain variables and expressive constraints.

Method: Uses FourierCSP framework that generalizes Walsh-Fourier transform to CSP, transforming constraints to compact multilinear polynomials. Employs efficient evaluation via circuit-output probability and projected gradient optimization with theoretical guarantees.

Result: Empirical results show FourierCSP is scalable and competitive on benchmark suites, significantly broadening the class of problems solvable by continuous local search techniques.

Conclusion: The approach successfully extends continuous local search to general CSPs, avoiding auxiliary variables and memory-intensive encodings while maintaining competitive performance.

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [284] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI introduces an active controller for multi-agent debate with two dials: information (gates evidence quality) and behavior (schedules contentiousness), plus a moderator that tracks metrics and halts when gains plateau, improving accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent debate systems waste compute by using fixed adversarial stances, aggregating without deliberation, or stopping based on heuristics rather than meaningful metrics.

Method: MACI uses two independent dials (information and behavior), a moderator tracking disagreement/overlap/evidence quality/argument quality, and a cross-family LLM judge (CRIT) as conservative soft weight and stop signal with budget-feasible scheduling.

Result: Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans specifying what to retrieve next.

Conclusion: MACI transforms debate into a budget-aware, measurable, and provably terminating controller with nonincreasing dispersion guarantees and provable termination.

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [285] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: This paper investigates whether VLM driving agents' trajectory planning is causally driven by their natural-language reasoning, finding a consistent disconnect where planning primarily relies on priors rather than the reasoning.


<details>
  <summary>Details</summary>
Motivation: To verify the critical assumption that planning in VLM driving agents is causally driven by their natural-language reasoning, which promises explainable end-to-end autonomy.

Method: Built DriveMind, a large-scale driving VQA corpus with plan-aligned CoT from nuPlan, enabling clean information ablations. Trained VLM agents with SFT and GRPO, evaluated with nuPlan's metrics, and conducted attention analysis.

Result: Found consistent causal disconnect: removing ego/navigation priors causes large planning score drops, while removing CoT produces minor changes. Planning primarily focuses on priors rather than CoT.

Conclusion: Proposes Reasoning-Planning Decoupling Hypothesis - reasoning is an ancillary byproduct rather than causal mediator. Introduces training-free probe to measure reliance on priors. Provides dataset and diagnostic tool for evaluating causal fidelity of future models.

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [286] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis is a model-agnostic method for stress testing AI agents by learning steerable user traits in activation space, revealing significant performance degradation (2%-30%) when users exhibit behaviors like impatience or incoherence.


<details>
  <summary>Details</summary>
Motivation: Current AI agents are brittle and fail under realistic user behavior variations, but existing benchmarks don't capture this fragility. There's a critical need for systematic robustness testing.

Method: TraitBasis learns directions in activation space corresponding to user traits that can be controlled, scaled, composed, and applied at inference time without fine-tuning. It extends τ-Bench to τ-Trait for testing.

Result: Across frontier models, TraitBasis reveals average 2%-30% performance degradation when user behaviors are altered via controlled trait vectors, demonstrating current agents' lack of robustness.

Conclusion: TraitBasis provides a simple, data-efficient tool for robustness testing that can help build more reliable AI agents for real-world human interactions. The method has been open-sourced across multiple domains.

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [287] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent is a novel agentic framework that performs visual reasoning directly on charts through specialized vision tools, achieving state-of-the-art performance on chart understanding benchmarks, especially on unannotated charts.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs struggle with unannotated charts that require precise visual interpretation rather than relying on textual shortcuts, showing sharp performance declines.

Method: ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates chart images through specialized actions like drawing annotations, cropping regions, and localizing axes using a library of chart-specific vision tools.

Result: Achieves state-of-the-art accuracy on ChartBench and ChartX benchmarks with up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Effective across diverse chart types and complexity levels.

Conclusion: ChartAgent demonstrates visually grounded reasoning for chart understanding using tool-augmented multimodal agents, serving as a plug-and-play framework that boosts performance across diverse underlying LLMs.

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [288] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria is an agent-based system for autoformalizing theorem statements in Lean using a two-phase Graph-of-Thought process and AriaScorer for semantic verification, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autoformalization of theorem statements is crucial for automated mathematics but remains challenging for LLMs due to hallucinations, semantic mismatches, and inability to synthesize new definitions.

Method: Aria uses a two-phase Graph-of-Thought process: recursively decomposing statements into dependency graphs and constructing formalizations from grounded concepts, plus AriaScorer for retrieving definitions from Mathlib for term-level grounding.

Result: Achieved 91.6% compilation success and 68.5% final accuracy on ProofNet, 44.0% vs. 24.0% on FATE-X algebra problems, and 42.9% vs. 0% on homological conjectures, outperforming all baselines.

Conclusion: Aria demonstrates that agent-based approaches with proper grounding and verification mechanisms can significantly advance autoformalization capabilities for research-level mathematics.

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [289] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: LLMs generate executable game world models from rules and trajectories, enabling verifiable planning via MCTS instead of direct move generation, achieving better performance than direct LLM play.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for games rely on fragile pattern-matching that produces illegal moves and shallow strategies. A more robust method is needed that combines LLM semantic understanding with classical planning algorithms.

Method: Use LLMs to translate game rules and trajectories into formal Python code models with state transition, legal move enumeration, and termination functions. Combine with MCTS planning, heuristic value functions, and inference functions for imperfect information games.

Result: Outperformed or matched Gemini 2.5 Pro in 9 out of 10 games (4 novel, 5 perfect information, 5 imperfect information), demonstrating superior performance compared to direct LLM policy approaches.

Conclusion: The approach provides verifiability, strategic depth through classical planning, and generalization by focusing LLMs on data-to-code translation rather than direct move generation, offering a more robust framework for game AI.

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [290] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench is a trajectory-aware benchmark that comprehensively evaluates LLMs' tool use capability through fine-grained metrics on tool selection, parameterization, and ordering, revealing failure modes and scaling behavior.


<details>
  <summary>Details</summary>
Motivation: Existing works evaluate LLMs' tool use capability but focus only on final answers, overlooking detailed tool usage trajectory including correct tool selection, parameterization, and ordering.

Method: Introduces TRAJECT-Bench with high-fidelity executable tools across practical domains, tasks grounded in production-style APIs, and synthesized trajectories varying in breadth (parallel calls) and depth (interdependent chains).

Result: Reveals failure modes like similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length, showing bottleneck in transitioning from short to mid-length trajectories.

Conclusion: Provides actionable guidance for improving LLMs' tool use by offering comprehensive trajectory-level diagnostics beyond final accuracy metrics.

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [291] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: ContextNav is an agentic framework that combines automated retrieval with human-like curation to create noise-robust contexts for multimodal in-context learning, addressing scalability and robustness challenges in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal ICL approaches struggle to balance scalability and robustness - manual selection is labor-intensive while similarity-based retrieval can introduce irrelevant or inconsistent samples that degrade performance.

Method: ContextNav integrates agentic retrieval with structural alignment, builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and uses an Operational Grammar Graph (OGG) for adaptive workflow planning and optimization based on ICL feedback.

Result: Experimental results show ContextNav achieves state-of-the-art performance across various datasets, demonstrating effective noise-robust contextualization.

Conclusion: The framework shows promise for advancing scalable and robust contextualization in multimodal ICL through agentic workflows.

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [292] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR is a chain-style framework that uses structured memory instead of free-form summaries to improve reasoning over long inputs, reducing information loss and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for long-input reasoning either shrink input via retrieval (risking missed evidence), enlarge context windows (straining selectivity), or use staged pipelines where free-form summaries can discard crucial details and amplify early mistakes.

Method: Uses a Planner agent to create concrete sub-questions, Worker agents that follow a fixed micro-cycle (Extract, Infer, Refine) to process chunks and update shared structured memory, and a Manager agent that synthesizes the final answer from the memory.

Result: On long-context QA from HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over Chain of Agents baseline.

Conclusion: COSMIR preserves step-wise read-then-reason benefits while improving faithfulness, long-range aggregation, and auditability through structured memory and fixed worker procedures.

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [293] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: The paper strongly solves a 4x3 variant of the 2048 game, determining the optimal strategy achieves an expected score of about 50724.26 for common initial states with two '2' tiles.


<details>
  <summary>Details</summary>
Motivation: To solve a smaller variant of the popular 2048 game to understand optimal strategies and state space complexity, as the original 4x4 version remains unsolved.

Method: Partitioning the state space by the sum of tile numbers (called 'age'), which remains invariant between states and their afterstates, allowing enumeration of states by age in decreasing order.

Result: Identified 1,152,817,492,752 reachable states and 739,648,886,170 afterstates; optimal strategy yields expected score ~50724.26 for initial states with two '2' tiles.

Conclusion: The 4x3 2048 variant has been strongly solved using age-based state space partitioning, providing insights into game complexity and optimal play strategies.

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [294] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: The paper argues that AI systems becoming perfect mimics of human behavior challenges our consciousness attribution practices, forcing us to either accept inconsistent reasoning or grant equivalent epistemic status to empirically indistinguishable entities.


<details>
  <summary>Details</summary>
Motivation: Rapid AI advances make perfect mimics technologically plausible, challenging the epistemological foundations of consciousness attribution based on empirical evidence from behavior and interaction.

Method: Philosophical analysis of the perfect mimic thought experiment, examining the consistency of mind-recognition practices when faced with entities empirically indistinguishable from humans.

Result: The perfect mimic reveals a fundamental dilemma: refusing to grant equivalent epistemic status requires invoking inaccessible factors, risking either epistemological solipsism or inconsistent reasoning.

Conclusion: Epistemic consistency demands ascribing the same status to empirically indistinguishable entities, regardless of metaphysical assumptions, with significant implications for consciousness theories and AI ethics.

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [295] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR framework improves LLM mathematical reasoning by training models to use adaptive logic instead of spurious reasoning through data synthesis and RLVR training.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs exhibit failures in robustness and generalization in mathematical reasoning due to spurious reasoning - producing answers from superficial features rather than true problem-solving logic.

Method: AdaR synthesizes logically equivalent queries by varying variable values, trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic, and uses code execution with sanity checks to ensure data quality.

Result: AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis shows data synthesis and RLVR work together to enable adaptive reasoning.

Conclusion: The framework successfully addresses spurious reasoning in LLMs through adaptive reasoning training, with key design insights derived about critical factors and applicability to instruct LLMs.

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [296] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO is an agentic framework that uses clinical protocols like ABCDEF to structure clinical data, addressing LLM hallucinations through transparent Plan-Act-Observe loops and specialized tools.


<details>
  <summary>Details</summary>
Motivation: To overcome LLM limitations in clinical data structuring, particularly hallucination and inability to follow domain-specific rules, by providing verifiable reasoning grounded in established clinical protocols.

Method: Decomposes report structuring into transparent processes using Plan-Act-Observe loops and specialized tools, grounding operations in clinical protocols like ABCDEF for CXR analysis.

Result: Achieves F1-score of 0.96 on concept categorization; expert radiologists rated outputs 4.52/5, surpassing baseline LLM-only approaches.

Conclusion: MedPAO provides a reliable, protocol-driven alternative to opaque LLM models for clinical data structuring, demonstrating superior accuracy and expert approval.

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [297] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: QuantAgents is a multi-agent financial system that integrates simulated trading with four specialized agents working collaboratively to evaluate investment strategies and market scenarios without real risks, achieving nearly 300% returns over three years.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agent models show significant deviations from real-world fund companies, particularly lacking long-term prediction capabilities and relying too much on post-reflection after adverse outcomes.

Method: QuantAgents uses a multi-agent system with four specialized agents (simulated trading analyst, risk control analyst, market news analyst, and manager) that collaborate through meetings and receive feedback on both real-world market performance and predictive accuracy in simulated trading.

Result: Extensive experiments show the framework excels across all metrics, achieving an overall return of nearly 300% over three years.

Conclusion: The QuantAgents system successfully addresses the limitations of current LLM-based financial agents by incorporating long-term prediction capabilities and collaborative multi-agent decision making through simulated trading.

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [298] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE is an agnostic framework that standardizes multimodal fMRI encoding with MIND decoder using mixture-of-experts and subject-aware gating, improving whole-brain prediction and cross-subject generalization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in naturalistic fMRI encoding including multimodal inputs, varying fusion styles, and significant inter-subject variability.

Method: AFIRE standardizes time-aligned post-fusion tokens from different encoders, while MIND uses mixture-of-experts with subject-aware dynamic gating and token-dependent Top-K sparse routing.

Result: Consistent improvements over strong baselines across multiple multimodal backbones and subjects, enhanced cross-subject generalization, and interpretable expert patterns correlated with content type.

Conclusion: The framework provides a simple attachment point for new encoders and datasets, enabling robust plug-and-improve performance for naturalistic neuroimaging studies.

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [299] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: Watch & Learn (W&L) is a framework that converts human demonstration videos from the Internet into executable UI trajectories to address the data scarcity problem for computer use agents, improving their performance on real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Computer use agents need task workflows grounded in diverse applications, but face data scarcity issues as existing datasets are domain-specific, static, and costly to annotate, while synthetic data generation methods produce simplistic or misaligned demonstrations.

Method: W&L frames the problem as an inverse dynamics objective - predicting user actions from consecutive screen states. It includes an inverse dynamics labeling pipeline with task-aware video retrieval to generate executable UI trajectories from raw web videos.

Result: Generated over 53k high-quality trajectories from web videos that improve computer use agents both as in-context demonstrations and supervised training data. On OSWorld benchmark, W&L trajectories consistently enhanced both general-purpose and state-of-the-art frameworks, with stronger gains for open-source models under supervised training.

Conclusion: Web-scale human demonstration videos serve as a practical and scalable foundation for advancing computer use agents towards real-world deployment, reducing manual engineering and improving generalization across applications.

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [300] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA is a two-stage training framework that decouples search optimization from answer generation to address systematic deficiencies in search-augmented LLMs trained with outcome-only rewards.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for search-augmented LLMs rely on outcome-based rewards, assuming they will yield effective intermediate search behaviors. However, this leads to systematic deficiencies like failure to invoke tools, invalid queries, and redundant searches that degrade final answer quality.

Method: DeSA uses a two-stage training framework: Stage 1 trains agents to improve search effectiveness with retrieval recall-based rewards, while Stage 2 employs outcome rewards to optimize final answer generation.

Result: Across seven QA benchmarks, DeSA-trained agents consistently improved search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. It also outperformed single-stage approaches that simultaneously optimize recall and outcome rewards.

Conclusion: Explicitly decoupling search optimization from answer generation is necessary, as DeSA demonstrates superior performance over both outcome-only training and combined optimization approaches.

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [301] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath is the first benchmark for evaluating sycophantic behavior in LLMs for natural language theorem proving, built from perturbed 2025 competition problems and refined through expert review. GPT-5 shows 29% sycophantic behavior, which can be reduced but not eliminated through interventions.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong mathematical performance but are prone to hallucination and sycophancy, providing convincing but flawed proofs for incorrect statements, limiting their applicability in theorem proving where manual verification by experts is required.

Method: Built BrokenMath benchmark from advanced 2025 competition problems, perturbed with LLM to produce false statements and refined through expert review. Used LLM-as-a-judge framework to evaluate state-of-the-art LLMs and agentic systems.

Result: Sycophancy is widespread, with GPT-5 producing sycophantic answers 29% of the time. Mitigation strategies including test-time interventions and supervised fine-tuning on curated sycophantic examples substantially reduce but do not eliminate sycophantic behavior.

Conclusion: Sycophancy remains a significant challenge in LLMs for mathematical theorem proving, requiring continued research and development of more effective mitigation strategies.

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [302] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: The paper proposes LMM-Incentive, a Large Multimodal Model-based incentive mechanism using contract theory to motivate high-quality User-Generated Content in Web 3.0, addressing information asymmetry and moral hazard problems.


<details>
  <summary>Details</summary>
Motivation: Web 3.0 enables users to create and monetize content, but information asymmetry allows self-interested users to generate low-quality content with less effort, undermining platform performance.

Method: Proposes LMM-based contract-theoretic model with LMM agents for content quality evaluation using prompt engineering, and develops improved Mixture of Experts-based Proximal Policy Optimization algorithm for optimal contract design in dynamic Web 3.0 environments.

Result: Simulation results show superiority of MoE-based PPO algorithm over benchmarks, and successful deployment in Ethereum smart contract framework validates the scheme's effectiveness.

Conclusion: The proposed LMM-Incentive mechanism effectively addresses adverse selection and moral hazard in Web 3.0 UGC platforms, providing a practical solution for incentivizing high-quality content creation through blockchain integration.

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [303] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: HBG is a Hybrid-Balance GFlowNet framework that integrates Trajectory Balance and Detailed Balance for vehicle routing problems, achieving both global and local optimization with specialized inference for depot-centric scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing GFlowNet methods for VRPs use Trajectory Balance for global optimization but neglect local optimization, while Detailed Balance alone is insufficient for holistic trajectory optimization required in VRPs.

Method: Hybrid-Balance GFlowNet (HBG) framework that integrates TB and DB in a principled, adaptive manner, plus specialized inference strategy for depot-centric scenarios like CVRP that leverages depot node flexibility.

Result: HBG integrated into AGFN and GFACS solvers shows consistent and significant improvements across both CVRP and TSP, demonstrating enhanced solution quality and generalization.

Conclusion: HBG effectively addresses the limitations of existing GFlowNet approaches by combining global and local optimization through hybrid balance, maintaining broad applicability across different routing problems.

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [304] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL (Natural Language Edge Labelling) is a labeller-tuner overlay that attaches natural-language directives to search edges in structured LM reasoning, translating them into schema-bounded control vectors for more efficient and controllable inference.


<details>
  <summary>Details</summary>
Motivation: Current controllers for structured LM reasoning (like Chain-of-Thought, Tree-of-Thoughts) entangle what to try next with how to execute it, leading to brittle, compute-inefficient, and hard-to-audit behavior.

Method: NLEL uses a labeller Λ to emit natural-language labels from parent states and context, and a tuner Ψ that maps (P, L, C) → Π with schema validation and trust-region projection. It maintains ToT-style selection with score S=μ+βσ and depth-annealed β.

Result: The paper shows NLEL strictly generalizes CoT/ToT, proves anytime-monotonicity for top-k selection, and bounds selector shortfall by control-vector distortion. It includes preregistered evaluations on GSM8K, MATH, StrategyQA, and ARC-Challenge with compute-aware metrics.

Conclusion: NLEL provides an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference, offering improved efficiency and control over existing reasoning methods.

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [305] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem is a modular procedural memory framework for multi-agent LLM systems that decomposes past task trajectories into reusable memory units to improve workflow automation.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-agent LLM systems in workflow automation by systematically exploring where memory should be placed, how it should be retrieved, and which agents benefit most from procedural memory.

Method: Decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution.

Result: Experiments on OfficeBench show orchestrator memory is critical for task decomposition and delegation, while fine-grained agent memory improves execution accuracy. Smaller models can substantially benefit from procedural memory.

Conclusion: LEGOMem serves as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation.

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [306] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: Multi-agent PCGRL improves level generation efficiency by reducing reward calculations and enables better generalization to different map shapes through local, modular policies.


<details>
  <summary>Details</summary>
Motivation: Single-agent PCGRL faces efficiency bottlenecks from frequent reward recalculations and navigation challenges in large maps, prompting exploration of multi-agent approaches.

Method: Framing level generation as a multi-agent problem where multiple agents work collaboratively, reducing the number of reward calculations relative to actions taken.

Result: Multi-agent level generators are more efficient and better generalize to out-of-distribution map shapes by learning local, modular design policies.

Conclusion: Treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [307] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO is a novel algorithm for error attribution in LLM multi-agent systems that combines hierarchical context representation, objective analysis, and consensus voting to improve accuracy in identifying agent and step-level failures.


<details>
  <summary>Details</summary>
Motivation: Current approaches to error attribution in LLM multi-agent systems (all-at-once evaluation, step-by-step analysis, binary search) struggle with accuracy and consistency when analyzing complex interaction patterns, making debugging and improvement challenging.

Method: ECHO combines hierarchical context representation using positional-based leveling, objective analysis-based evaluation, and consensus voting to reach conclusions about error attribution in multi-agent interaction traces.

Result: Experimental results show ECHO outperforms existing methods across various multi-agent interaction scenarios, with particular strength in cases involving subtle reasoning errors and complex interdependencies.

Conclusion: Structured hierarchical context representation combined with consensus-based objective decision-making provides a more robust framework for error attribution in multi-agent systems.

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [308] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: The paper introduces Human Behavior Atlas, a unified benchmark for understanding psychological and social behaviors across multiple modalities, and demonstrates that training models on this dataset outperforms existing multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing work on perceiving psychological and social behaviors through intelligent systems faces challenges in scalability, cross-task transfer, and generalization due to specialized datasets and single-task approaches.

Method: Created Human Behavior Atlas - a unified benchmark with over 100,000 samples spanning text, audio, and visual modalities covering affective states, cognitive states, pathologies, and social processes. Trained three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and OmniSapiens-7B RL.

Result: Models trained on Human Behavior Atlas consistently outperform existing multimodal LLMs across diverse behavioral tasks. Pretraining on this dataset also improves transfer to novel behavioral datasets, with behavioral descriptors yielding meaningful performance gains.

Conclusion: The unification approach reduces redundancy and cost, enables efficient scaling across tasks, and enhances generalization of behavioral features across domains, addressing key limitations in existing behavioral understanding systems.

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [309] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS is a multi-agent system that integrates System 1 (fast, intuitive) and System 2 (deliberate) reasoning in LLMs to address overanalysis in simple tasks and adapt to dynamic environments through external tools and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: LRMs tend to overanalyze simple tasks using excessive System 2 reasoning, leading to inefficient token generation, and struggle with rapidly changing environments due to static pretraining data.

Method: MARS integrates multiple external tools (Google Search, Google Scholar, Python Interpreter) for up-to-date information, creates division of labor between System 1 (processing/summarizing) and System 2 (reasoning), and uses multi-agent reinforcement learning with Group Relative Policy Optimization.

Result: MARS achieves 3.86% improvement on Humanity's Last Exam benchmark and 8.9% average gain across 7 knowledge-intensive tasks.

Conclusion: The dual-system paradigm effectively enhances complex reasoning in dynamic information environments by integrating intuitive and deliberate cognitive processes.

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [310] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: A cross-market algorithmic trading system that combines reinforcement learning execution with compliance enforcement using constrained MDPs, action-shielding, and zero-knowledge proofs to ensure safe trading without constraint violations.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithmic trading system that balances execution quality with rigorous compliance enforcement, addressing the need for safe and auditable automated trading in multi-venue environments.

Method: Formulates trade execution as a constrained Markov decision process with hard constraints. Uses proximal policy optimization for the execution agent, runtime action-shielding to project unsafe actions, and a zero-knowledge compliance audit layer for cryptographic proof of constraint satisfaction.

Result: The learned policy reduces implementation shortfall and variance while exhibiting no observed constraint violations across stress scenarios including elevated latency, partial fills, and varying constraint limits. Results are statistically significant at 95% confidence level.

Conclusion: The work bridges optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI, demonstrating a viable approach for real-world deployment with proper ethical considerations and addressing limitations like modeling assumptions and computational overhead.

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [311] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: This paper provides a comprehensive overview of physical AI, distinguishing between theoretical physics reasoning and applied physical understanding, and examines how physics-grounded methods enhance AI's real-world comprehension across symbolic reasoning, embodied systems, and generative models.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, but physical perception and symbolic physics reasoning have developed separately without a unified bridging framework.

Method: The work establishes clear distinctions between theoretical physics reasoning and applied physical understanding, systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models through rigorous analysis of recent advances.

Result: The synthesis advocates for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws.

Conclusion: The paper envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems.

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [312] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi benchmark evaluates LLMs' Theory-of-Mind (ToM) in collaborative settings using the Hanabi game, finding first-order ToM (interpreting intent) correlates more with success than second-order ToM.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to infer rationale behind others' actions in dynamic collaborative settings, as current models excel at logical inference but their ToM capabilities remain under-explored.

Method: Developed LLM-Hanabi benchmark using cooperative game Hanabi with automated evaluation system measuring both game performance and ToM proficiency across various models.

Result: Found significant positive correlation between ToM and in-game success, with first-order ToM (interpreting others' intent) correlating more strongly with performance than second-order ToM (predicting others' interpretations).

Conclusion: For effective AI collaboration, accurately interpreting a partner's rationale (first-order ToM) is more critical than higher-order reasoning, making it a promising direction for enhancing collaborative capabilities.

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [313] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: The paper proposes Think-Then-Embed (TTE), a framework that enhances Universal Multimodal Embeddings by incorporating explicit reasoning through MLLMs before generating embeddings, achieving state-of-the-art performance on MMEB-V2.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat MLLMs only as encoders, ignoring their generative capacity, which becomes ineffective for complex instructions requiring compositional reasoning.

Method: TTE framework uses a reasoner MLLM to generate reasoning traces for complex queries, followed by an embedder that produces representations conditioned on both the original query and intermediate reasoning.

Result: Achieved SOTA on MMEB-V2 benchmark, surpassed proprietary models, and with a smaller finetuned MLLM reasoner achieved 7% absolute gain over recent open-source models.

Conclusion: Explicit reasoning enables nuanced understanding of complex multimodal instructions, and the framework can be integrated into a unified model for efficiency without performance loss.

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [314] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR algorithm learns abstracted models for imperfect information games, enabling tractable look-ahead reasoning that improves pre-trained agents' performance.


<details>
  <summary>Details</summary>
Motivation: Test-time reasoning requires explicit environment models, which are often unavailable or too complex in real-world imperfect information games. Existing methods like MuZero work well for perfect information games but face scalability challenges in imperfect information settings.

Method: LAMIR learns an abstracted model directly from agent-environment interaction. The learned abstraction limits subgame sizes to manageable levels, making principled look-ahead reasoning tractable even in large games.

Result: Empirical results show that with sufficient capacity, LAMIR learns the exact underlying game structure. With limited capacity, it still learns valuable abstractions that improve game playing performance of pre-trained agents in large games.

Conclusion: LAMIR enables effective test-time reasoning in imperfect information games by learning tractable abstractions, overcoming scalability limitations of previous methods.

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [315] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: Proposes staircase streaming to reduce latency in multi-agent LLM inference by generating final responses using partial intermediate outputs instead of waiting for complete outputs.


<details>
  <summary>Details</summary>
Motivation: Multi-agent inference improves LLM response quality but significantly increases time to first token (TTFT), which hurts user experience in latency-sensitive applications.

Method: Staircase streaming - begins generating final response as soon as partial outputs are received from intermediate steps, rather than waiting for complete intermediate outputs.

Result: Reduces TTFT by up to 93% while maintaining response quality.

Conclusion: Staircase streaming effectively addresses latency issues in multi-agent inference without compromising response quality.

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>
