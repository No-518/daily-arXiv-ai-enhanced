{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "RoboOmni is a framework for robotic manipulation that uses multimodal LLMs to infer user intentions from contextual cues like speech, sounds, and visual information rather than explicit commands.", "motivation": "Current VLA models rely on explicit instructions, but real-world human-robot collaboration requires proactive intention inference from natural interactions like dialogue and environmental cues.", "method": "Proposes RoboOmni framework with Perceiver-Thinker-Talker-Executor architecture using end-to-end omni-modal LLMs. Fuses auditory and visual signals spatiotemporally for intention recognition and supports direct speech interaction. Built OmniAction dataset with 140k episodes for training.", "result": "RoboOmni outperforms text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance in both simulation and real-world experiments.", "conclusion": "The work demonstrates the effectiveness of cross-modal contextual instructions for enabling robots to proactively infer human intentions and collaborate more naturally."}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "Social robots can effectively support college students during self-study by providing goal reminders and emotional support, improving focus, productivity, and engagement compared to just physical presence.", "motivation": "To explore how social robots can assist students in self-studying contexts, which has not been extensively investigated despite extensive research on social robots in learning tasks.", "method": "Exploratory Wizard-of-Oz study comparing robotic support behaviors (goal reminders and emotional support) against a control condition with only physical presence, measuring students' perceived focus, productivity, and engagement.", "result": "Participants in goal reminder and emotional support conditions reported greater ease of use, with goal reminder condition showing higher willingness for future use. Satisfaction correlated with perceiving robot as social other, which predicted goal achievement.", "conclusion": "Socially assistive robots have potential to support self-study through both functional (goal reminders) and emotional engagement, with perception of robot as social other being key to effectiveness."}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "A recovery-aware visual-inertial navigation system for low-cost wheeled quadrupeds that combines depth camera perception with deep reinforcement learning for robust locomotion and autonomous fall recovery across diverse terrains.", "motivation": "Many state-of-the-art wheeled-legged robots rely on costly actuators and sensors, and lack integrated fall-recovery capabilities, especially for wheeled-legged morphologies. This work aims to lower deployment barriers for autonomous navigation in budget-constrained platforms.", "method": "Leverages vision-based perception from a depth camera and deep reinforcement learning policies for robust locomotion and autonomous recovery from falls across diverse terrains.", "result": "Simulation experiments show agile mobility with low-torque actuators over irregular terrain and reliable recovery from external perturbations and self-induced failures. Also demonstrates goal-directed navigation in structured indoor spaces with low-cost perception.", "conclusion": "This approach successfully lowers the barrier to deploying autonomous navigation and robust locomotion policies in budget-constrained robotic platforms by combining cost-effective hardware with intelligent recovery capabilities."}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "Proposes an adaptive keyframe selection method for 3D scene reconstruction that combines error-based selection using photometric/SSIM errors with momentum-based threshold adjustment, outperforming traditional static selection methods.", "motivation": "Addresses the data bottleneck in real-time perception for dynamic environments by dynamically curating the most informative frames, enabling high-quality 3D world representations from compressed data streams for scalable robot learning.", "method": "Integrates two modules: error-based selection using photometric and SSIM errors, and momentum-based update that dynamically adjusts keyframe selection thresholds based on scene motion dynamics.", "result": "Significant improvements over traditional static keyframe selection strategies, with consistent performance gains across Spann3r and CUT3R 3D reconstruction networks, validated through extensive ablation studies.", "conclusion": "Represents a meaningful advancement toward adaptive perception systems that can dynamically respond to complex and evolving visual scenes in dynamic environments."}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS is a generalist game agent using unified keyboard-mouse action space that achieves state-of-the-art performance across various game domains through large-scale pre-training with 500B tokens.", "motivation": "To create a generalist game agent that can operate across heterogeneous domains (OS, web, simulation games) using human-aligned native inputs rather than API- or GUI-based approaches, enabling large-scale continual pre-training.", "method": "Uses unified keyboard-mouse action space, decaying continual loss to reduce causal confusion, and Sparse-Thinking strategy for balancing reasoning depth and inference cost. Pre-trained on 500B tokens with diverse trajectories and multimodal data.", "result": "Achieves 2x success rate over previous SOTA on Minecraft tasks, close to human generality in unseen web 3D games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling confirms sustained improvements with cross-game and multimodal data.", "conclusion": "Simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities."}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "An explainable AI system combining a CNN classifier and Vision-Language Model achieves 96.5% accuracy in detecting AI-generated images, with fast inference suitable for edge devices, while providing artifact localization and semantic explanations.", "motivation": "Address the challenge of verifying visual authenticity due to increasingly realistic AI-generated imagery by creating an interpretable detection system.", "method": "Combines a lightweight convolutional classifier (Faster-Than-Lies) with Vision-Language Model (Qwen2-VL-7B), uses autoencoder-based reconstruction error maps for artifact localization, and categorizes 70 visual artifacts into 8 semantic groups.", "result": "Achieves 96.5% accuracy on extended CiFAKE dataset with adversarial perturbations, maintains 175ms inference time on 8-core CPUs, and generates explainable text for detected anomalies.", "conclusion": "Demonstrates feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery, with potential applications in forensics, industrial inspection, and social media moderation."}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "A Cosserat rod-based framework for modeling tendon-actuated concentric tube robots with n tubes and m_i tendons per tube, achieving <4% tip prediction error.", "motivation": "To address the lack of a complete mechanical model for tendon-actuated concentric tube mechanisms that combine advantages of both tendon-driven and concentric tube robots while overcoming their limitations.", "method": "Proposed a Cosserat rod-based framework that models n concentric tubes with m_i tendons per tube, allowing tube twisting and elongation while enforcing shared centerline for bending.", "result": "Validated with two-tube and three-tube assemblies achieving tip prediction errors <4% of robot's total length, and applied to existing robots with maximum tip deviations around 5% of total length.", "conclusion": "The model provides a foundation for accurate shape estimation and control of advanced tendon-actuated concentric tube robots."}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "AI's role in scientific problem-solving can displace disciplinary creativity, potentially diminishing the value of scientific pursuit.", "motivation": "To examine how artificial intelligence affects disciplinary creativity in scientific fields, distinguishing between creative approaches and products.", "method": "Drawing on philosophy of creativity concepts and analyzing two mathematical case studies to compare computational extensions versus AI-driven displacement of disciplinary creativity.", "result": "While computation can extend disciplinary creativity, certain AI approaches displace it, altering the nature of scientific work.", "conclusion": "AI's displacement of disciplinary creativity may diminish the value of scientific pursuit by changing how creative expertise is applied within disciplines."}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "CountFormer is a transformer-based framework that uses DINOv2 features and positional embeddings for class-agnostic object counting, achieving state-of-the-art performance on structurally complex scenes.", "motivation": "Humans can count objects by perceiving visual repetition and structural relationships, but existing counting models struggle with complex shapes, internal symmetry, or overlapping components.", "method": "Built on CounTR architecture, replaces visual encoder with self-supervised DINOv2 foundation model, incorporates positional embedding fusion, and uses lightweight convolutional decoder to produce density maps.", "result": "Achieves performance comparable to state-of-the-art methods on FSC-147 dataset, with superior accuracy on structurally intricate or densely packed scenes.", "conclusion": "Integrating foundation models like DINOv2 enables counting systems to approach human-like structural perception, advancing toward truly general and exemplar-free counting."}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "A soft robot finger with adaptive-twist deformation that can wrap around objects and grasp them from dense arrangements using a variable stiffness mechanism controlled by a single actuation source.", "motivation": "To enable soft hands to grasp individual objects from densely packed groups by inserting fingers into narrow gaps and maintaining twisted deformation for secure grasping.", "method": "Proposed a variable stiffness mechanism that adaptively changes stiffness with pressure, conducted FEA to determine design parameters, and developed a soft finger prototype.", "result": "Successfully demonstrated grasping various objects through wrapping, with the finger capable of deep insertion into limited gaps and maintaining appropriate grasping force.", "conclusion": "The adaptive-twist soft finger with variable stiffness mechanism enables effective grasping by wrapping in dense object arrangements using single actuation."}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "ME-POMDPs extend standard POMDPs with discrete model uncertainty, representing multiple possible POMDPs. The paper generalizes them to AB-POMDPs, shows reduction methods, and develops exact/approximate algorithms for computing robust policies.", "motivation": "To address scenarios where multiple domain experts disagree on problem modeling, requiring a single policy robust against all possible POMDP variations within a set.", "method": "Generalize ME-POMDPs to AB-POMDPs with sets of initial beliefs, prove reduction theorems to simplify model variations, and develop exact and point-based approximate algorithms for robust policy computation.", "result": "Successfully computed robust policies for standard POMDP benchmarks extended to multi-environment settings, demonstrating practical applicability of the approach.", "conclusion": "ME-POMDPs and their generalization to AB-POMDPs provide a principled framework for handling model uncertainty, with effective algorithms for computing worst-case optimal policies in multi-environment scenarios."}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "A novel framework using fixed cameras and deep learning for continuous monitoring of floating debris in rivers, with geometric modeling for object size estimation.", "motivation": "Floating anthropogenic debris in rivers negatively impacts biodiversity, water quality, and human activities like navigation and recreation, requiring effective monitoring solutions.", "method": "Utilizes fixed in-situ cameras with deep learning models for debris detection and quantification, plus geometric modeling using camera characteristics for object size estimation from 2D images.", "result": "Identified optimal deep learning models for accuracy and speed under complex conditions, demonstrated importance of dataset protocols including negative images and temporal leakage considerations.", "conclusion": "Feasible to develop robust, low-cost automated monitoring systems using projective geometry with regression corrections for urban aquatic environments."}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.", "AI": {"tldr": "This survey reviews multi-robot collaborative SLAM using 3D Gaussian Splatting (3DGS), analyzing architectures, core components, datasets, and future research directions.", "motivation": "3DGS enables real-time, high-fidelity rendering ideal for robotics, but its use in multi-robot systems introduces challenges in global consistency, communication management, and heterogeneous data fusion.", "method": "Systematically categorizes approaches by architecture (centralized, distributed) and analyzes core components including multi-agent consistency, communication efficiency, Gaussian representation, semantic distillation, fusion and pose optimization, and real-time scalability.", "result": "Provides comprehensive analysis of current approaches and identifies key performance metrics through dataset summaries and evaluation frameworks.", "conclusion": "Identifies critical open challenges and future directions including lifelong mapping, semantic association and mapping, multi-model robustness, and bridging the Sim2Real gap."}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "A framework using test-time tuning enhances pre-trained transformers for de novo molecular structure generation directly from tandem mass spectra and molecular formulas, outperforming state-of-the-art methods by significant margins.", "motivation": "Current methods rely on database matching or multi-step pipelines with intermediate predictions, making identification challenging for compounds absent from reference databases.", "method": "Leverages test-time tuning to enhance learning of pre-trained transformer models, enabling end-to-end de novo molecular structure generation directly from tandem mass spectra and molecular formulas.", "result": "Surpasses state-of-the-art DiffMS by 100% on NPLIB1 and 20% on MassSpecGym benchmarks. Test-time tuning provides 62% relative performance gain over conventional fine-tuning on MassSpecGym.", "conclusion": "The approach enables dynamic adaptation to novel spectra and generates structurally accurate molecular candidates even when predictions deviate from ground truth, providing valuable guidance for human interpretation and more reliable identification."}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow is a physics-aware super-resolution framework for remote sensing imagery that maintains geometric fidelity and semantic accuracy under out-of-distribution conditions, using dual-conditioning and uncertainty quantification to prevent feature hallucination.", "motivation": "Super-resolution for remote sensing imagery often fails under out-of-distribution conditions, producing visually plausible but physically inaccurate results, especially for rare geomorphic features captured by diverse sensors.", "method": "Uses dual-conditioning architecture with Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Includes multifaceted loss function for spectral/radiometric consistency and stochastic forward pass for uncertainty quantification.", "result": "In blind evaluations, geophysical experts rated outputs approaching ground truth fidelity, significantly outperforming state-of-the-art baselines with nearly 40% reduction in FID and superior perceptual metrics.", "conclusion": "RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift."}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco is a modular skill-selection framework that dynamically adapts locomotion strategies for legged robots by evaluating pre-trained policies for safety and energy efficiency on complex terrains like staircases.", "motivation": "Overcome limitations of end-to-end deep reinforcement learning approaches in legged robot locomotion, which lack safety and interpretability when generalizing to novel terrains.", "method": "A modular framework that evaluates pre-trained locomotion policies by predicting execution safety and energy consumption (cost of transport) over a planning horizon, enabling dynamic selection of safe and efficient policies based on local terrain perception.", "result": "Empirical evaluation on staircase locomotion tasks shows VOCALoco achieves improved robustness and safety during stair ascent and descent compared to conventional end-to-end DRL policies, validated in both simulated and real-world scenarios with a quadrupedal robot.", "conclusion": "VOCALoco provides a safer and more interpretable alternative to end-to-end DRL for legged robot locomotion, successfully demonstrating improved performance on complex terrain navigation tasks."}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "An AI system generates creative chess puzzles with aesthetic appeal and novel solutions, evaluated by world-renowned chess experts.", "motivation": "To investigate whether Generative AI can produce creative and novel outputs, specifically in the domain of chess puzzles.", "method": "Developed an AI system to generate chess puzzles characterized by aesthetic appeal, novelty, counter-intuitive and unique solutions, then evaluated by chess experts.", "result": "Three world-renowned chess experts (International Master Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler) selected their favorite AI-generated puzzles and explained their appeal based on creativity, challenge level, and aesthetic design.", "conclusion": "The AI system demonstrates the ability to generate creative chess puzzles that are appreciated by domain experts for their aesthetic qualities and innovative solutions."}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "A training-free method for 3D scene generation that repurposes text-to-3D object diffusion models as modular tile generators, enabling scalable synthesis of large, coherent scenes through multi-tile denoising and seamless blending.", "motivation": "Existing 3D scene generation methods are limited to single objects, require domain-specific training, or lack full 360-degree viewability, creating a need for more flexible and scalable approaches.", "method": "Reformulates scene generation as a multi-tile denoising problem using overlapping 3D regions independently generated by text-to-3D object diffusion models, with seamless blending via weighted averaging.", "result": "Enables scalable synthesis of large, coherent scenes with local semantic control, diverse scene layouts, efficient generation, and flexible editing without scene-level datasets or retraining.", "conclusion": "Establishes a simple yet powerful foundation for general-purpose, language-driven 3D scene construction that inherits generalization capabilities from object-level priors with minimal heuristics."}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "The paper proposes extending Boundary Vector Cells (BVCs) to 3D by incorporating vertical angular sensitivity, enabling more accurate spatial localization in complex environments while maintaining performance in simpler 2D-like scenarios.", "motivation": "Current BVC models are limited to 2D environments and suffer from spatial ambiguities due to horizontal symmetries, making them inadequate for real-world 3D navigation.", "method": "Incorporates vertical angular sensitivity into BVC framework and processes LiDAR data to capture vertical contours, allowing disambiguation of locations that would be indistinguishable in 2D representations.", "result": "In environments with minimal vertical variation, the 3D model matches 2D baseline performance; in complex 3D environments, it produces substantially more distinct place fields and significantly reduces spatial aliasing.", "conclusion": "Adding vertical dimension to BVC-based localization significantly enhances navigation and mapping in real-world 3D spaces while maintaining performance in simpler scenarios."}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "Current pathology foundation models fail to deliver expected breakthroughs in cancer diagnosis due to fundamental conceptual mismatches with tissue complexity, exhibiting low accuracy, poor robustness, and safety vulnerabilities.", "motivation": "To understand why foundation models that revolutionized other domains are failing in computational pathology despite rapid adoption, by examining the underlying conceptual mismatches.", "method": "Systematic evaluation and analysis of pathology foundation models to identify seven key causes of their shortcomings through conceptual examination of model assumptions versus tissue complexity.", "result": "Identified seven interrelated causes of failure: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and fundamental design flaw in patch size.", "conclusion": "Current pathology foundation models are conceptually misaligned with tissue morphology and require fundamental paradigm rethinking rather than incremental improvements."}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP is a training-free framework that improves CLIP for semantic segmentation by exploiting visual discriminability across layers, attention heads, and tokens through three techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement.", "motivation": "CLIP models struggle with semantic segmentation due to misalignment between image-level pre-training and pixel-level dense prediction needs. Prior methods inherit global alignment bias from preceding layers, leading to suboptimal performance.", "method": "Three training-free techniques: 1) Semantic-spatial reweighting to restore visual discriminability, 2) Selective head enhancement using consistently discriminative attention heads, 3) Abnormal token replacement based on sparse activation patterns.", "result": "Achieves state-of-the-art performance on 8 semantic segmentation benchmarks across diverse scenarios without additional training, auxiliary networks, or extensive hyperparameter tuning.", "conclusion": "LHT-CLIP effectively bridges the gap between CLIP's pre-training and segmentation requirements, demonstrating practical deployment potential through systematic exploitation of existing model capabilities."}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "SynAD is a framework that enhances end-to-end autonomous driving models using synthetic data by designating ego vehicles in multi-agent scenarios, projecting path-level scenarios onto maps, and integrating synthetic data with real-world data through a novel training strategy.", "motivation": "Real-world driving datasets limit scenario diversity for training end-to-end autonomous driving models. Synthetic scenario generation can enrich training data but hasn't been effectively applied to E2E AD due to lack of designated ego vehicles and sensor inputs in synthetic scenarios.", "method": "Designates agent with most comprehensive driving information as ego vehicle in multi-agent synthetic scenarios, projects path-level scenarios onto maps, uses Map-to-BEV Network to derive bird's-eye-view features without sensor inputs, and develops training strategy to integrate map-based synthetic data with real driving data.", "result": "SynAD effectively integrates all components and notably enhances safety performance in autonomous driving models.", "conclusion": "SynAD bridges synthetic scenario generation and E2E AD, paving the way for more comprehensive and robust autonomous driving models."}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP is a hierarchical framework for LLMs that improves long-horizon reasoning through plan-ahead decomposition, structured context re-injection, and memory-efficient execution, achieving significant performance gains on complex tasks.", "motivation": "Existing sequential prompting methods suffer from context drift, goal information loss, and failure cycles, while hierarchical methods weaken cross-level continuity or have high runtime overhead.", "method": "ReCAP combines three mechanisms: (1) plan-ahead decomposition - generate full subtask list, execute first item, refine remainder; (2) structured re-injection of parent plans - maintain consistent multi-level context during recursive return; (3) memory-efficient execution - bound active prompt for linear cost scaling with task depth.", "result": "ReCAP achieves 32% gain on synchronous Robotouille and 29% improvement on asynchronous Robotouille under strict pass@1 protocol, substantially improving subgoal alignment and success rates on various long-horizon reasoning benchmarks.", "conclusion": "ReCAP effectively aligns high-level goals with low-level actions, reduces redundant prompting, and preserves coherent context updates across recursion, making it a powerful framework for long-horizon reasoning tasks in LLMs."}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride is a pipeline for generating coherent scene-level captions in instructional videos without manual segmentation, using adaptive frame sampling and multimodal reasoning to improve temporal coherence and educational value.", "motivation": "Scene-level captioning in instructional videos enhances learning by aligning visual cues with textual guidance, but existing methods often fail to capture temporal structure, leading to incoherent captions that undermine educational intent.", "method": "Uses adaptive frame sampling and multimodal windowing to capture key transitions, employs multimodal chain-of-thought for action-object pairs, and refines them with dynamic stride window selection to balance temporal context and redundancy.", "result": "Outperforms strong baselines (VLLaMA3, GPT-4o) on both N-gram metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore), producing more temporally coherent and informative captions.", "conclusion": "DynaStride shows promising direction for improving AI-powered instructional content generation by effectively integrating visual semantics and temporal reasoning in scene-level captions."}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "A framework combining Language-Conditioned Visual Representation (LCVR) and Language-conditioned Mixture-of-Experts Density Policy (LMoE-DP) addresses perceptual ambiguity and task conflict in multitask robotic manipulation via imitation learning, achieving 79% average success rate.", "motivation": "Perceptual ambiguity and task conflict limit multitask robotic manipulation via imitation learning, as visually similar tasks can confuse models and different tasks may interfere with each other during training.", "method": "Proposes LCVR to resolve perceptual ambiguities by grounding visual features with language instructions, and LMoE-DP using sparse expert architecture with gradient modulation to specialize in distinct action distributions and mitigate task conflict.", "result": "LCVR boosts ACT and Diffusion Policy success rates by 33.75% and 25% respectively. The full framework achieves 79% average success, outperforming advanced baseline by 21% on real-robot benchmarks.", "conclusion": "Combining semantic grounding through language-conditioned visual representation and expert specialization via mixture-of-experts enables robust and efficient multi-task manipulation in robotics."}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "LLM-based agents achieve near-optimal performance in decentralized multi-agent goal assignment using structured environmental information and deterministic conflict resolution.", "motivation": "Addressing the challenge of coordinating multiple autonomous agents in shared environments under decentralized conditions without negotiation or iterative coordination.", "method": "Agents independently generate ranked preferences over goals using structured environmental representations (grid visualizations and scenario data), then exchange rankings and use deterministic conflict-resolution rules for assignment.", "result": "LLM-based agents with well-designed prompts and quantitative information achieve near-optimal makespans and consistently outperform traditional heuristics.", "conclusion": "Language models show strong potential for decentralized goal assignment in multi-agent path planning, with information structure being a critical factor for success."}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D enhances 3D portrait generation by combining image-to-avatar pipelines with diffusion models to refine noisy renders while maintaining 3D consistency and low latency.", "motivation": "Existing image-to-3D models for portraits produce visual artifacts and fail to preserve subject identity, while diffusion models lack 3D awareness and are computationally expensive.", "method": "Uses a feedforward image-to-avatar pipeline to generate initial 3D representations, then refines noisy renders with a single-step diffusion model conditioned on input images for multi-view consistency. Includes pre-training on synthetic multi-view data and fine-tuning on real images.", "result": "Qualitatively and quantitatively outperforms current state-of-the-art methods for portrait novel-view synthesis while being time-efficient.", "conclusion": "Image-space diffusion models can significantly enhance 3D portrait generation quality while maintaining 3D-awareness and low latency, achieving better identity preservation and detail than existing approaches."}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "Distributed multi-robot exploration planning using topological maps for balanced area partition and task allocation in obstacle-dense environments.", "motivation": "Address collaborative multi-robot autonomous online exploration with balanced exploration area partition and task allocation among mobile robots in non-convex environments.", "method": "Novel topological map structure for spatial connectivity and global exploration completeness; distributed weighted topological graph Voronoi algorithm for balanced graph space partitions; local planner for optimizing visitation sequences and generating motion trajectories.", "result": "Significant improvements in exploration efficiency, completeness, and workload balance across robot team compared to state-of-the-art methods.", "conclusion": "The proposed approach effectively solves distributed multi-robot exploration with theoretical guarantees for consensus convergence and equitable partitions, demonstrating superior performance in complex environments."}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM developed CUGA, a generalist agent that achieves SOTA on benchmarks and shows promise in enterprise BPO settings, addressing scalability, safety, and governance requirements.", "motivation": "Enterprises struggle to move AI agents from prototypes to production due to fragmented frameworks, slow development, and lack of standardized evaluation practices.", "method": "CUGA uses a hierarchical planner-executor architecture with strong analytical foundations, evaluated on AppWorld, WebArena, and a custom BPO-TA benchmark with 26 tasks.", "result": "CUGA achieved state-of-the-art performance on benchmarks and approached specialized agent accuracy in enterprise pilot while showing potential for reduced development time and cost.", "conclusion": "Generalist agents like CUGA can operate at enterprise scale, but require addressing technical and organizational challenges to advance from research to production systems."}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS enhances 3D Gaussian Splatting (3DGS) for indoor scenes by incorporating planar priors and geometric supervision to address ambiguous geometry in low-texture regions.", "motivation": "3DGS struggles with ambiguous geometry in large, low-texture indoor scenes due to reliance on photometric loss alone.", "method": "Uses Language-Prompted Planar Priors (LP3) with vision-language segmentation, cross-view fusion, and geometric priors. Adds planar consistency and geometric supervision terms to optimize Gaussians.", "result": "Significantly outperforms state-of-the-art methods on standard indoor benchmarks, achieving accurate and detailed 3D surface reconstruction.", "conclusion": "PlanarGS effectively addresses 3DGS limitations in indoor scenes through planar and geometric priors, enabling high-fidelity reconstruction."}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "A phase-based trajectory optimization method for legged robots that ensures dynamic feasibility and friction cone constraints by leveraging linear differential equations and B\u00e9zier polynomials.", "motivation": "To generate reliable motion for legged robots by simultaneously computing robot paths and contact sequences while accurately considering dynamics in the optimization formulation.", "method": "Uses superposition properties of linear differential equations to decouple translational dynamics for each contact point, employs B\u00e9zier polynomials to derive analytical relationships between position and force, and exploits convex closure properties to ensure friction cone constraints.", "result": "The framework successfully generates dynamically reliable motions with various gait sequences for legged robots, validated using a quadruped robot model.", "conclusion": "The proposed trajectory optimization approach effectively ensures dynamic feasibility and friction cone constraints throughout the entire trajectory, enabling reliable motion generation for legged robots."}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "This paper introduces an RL framework with novel rewards to generate creative chess puzzles that are unique, counter-intuitive, diverse, and realistic, achieving 10x improvement in counter-intuitive puzzle generation.", "motivation": "While Generative AI advances rapidly, generating truly creative, aesthetic, and counter-intuitive outputs remains challenging, particularly in the domain of chess puzzles.", "method": "The authors benchmark Generative AI architectures and introduce an RL framework with novel rewards based on chess engine search statistics to enhance puzzle uniqueness, counter-intuitiveness, diversity, and realism.", "result": "The RL approach dramatically increases counter-intuitive puzzle generation from 0.22% (supervised) to 2.5%, surpassing existing dataset rates (2.1%) and the best Lichess-trained model (0.4%). The puzzles meet novelty and diversity benchmarks, retain aesthetic themes, and are rated by human experts as more creative, enjoyable, and counter-intuitive than composed book puzzles.", "conclusion": "The final outcome is a curated booklet of AI-generated puzzles acknowledged for creativity by three world-renowned experts, demonstrating that the approach can generate puzzles approaching classic compositions in quality."}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe is an adaptive training scheme for implicit neural representations that uses neuron pruning to reduce redundancy and input frequency densification to improve capacity, achieving better size-quality trade-offs.", "motivation": "Current methods for implicit neural representations require manual frequency selection and architecture tuning through heavy hyperparameter optimization, which is inefficient and suboptimal.", "method": "AIRe employs neuron pruning to identify and remove less-contributory neurons via targeted weight decay and structured pruning, followed by input frequency densification in underfitting spectrum regions.", "result": "Experiments on images and SDFs show that AIRe reduces model size while maintaining or improving reconstruction quality compared to existing methods.", "conclusion": "AIRe provides an effective adaptive training approach that automatically refines INR architectures during optimization, eliminating the need for extensive hyperparameter tuning while achieving superior performance."}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS is the first end-to-end autonomous driving framework that eliminates imitation learning entirely, using only reinforcement learning with raw sensor inputs while achieving state-of-the-art performance on challenging benchmarks.", "motivation": "Existing autonomous driving approaches have limitations: Imitation Learning suffers from sub-optimal expert demonstrations and covariate shift, while Reinforcement Learning typically works only with low-dimensional symbolic inputs rather than raw sensor data.", "method": "ZTRS combines offline reinforcement learning with Exhaustive Policy Optimization (EPO), a policy gradient variant designed for enumerable actions and rewards, enabling direct learning from high-dimensional sensor inputs without imitation learning.", "result": "ZTRS achieves state-of-the-art performance on Navhard benchmark and outperforms IL-based baselines on HUGSIM, demonstrating strong performance across Navtest, Navhard, and HUGSIM benchmarks.", "conclusion": "ZTRS successfully bridges the gap between sensor-rich inputs and RL training, proving that end-to-end autonomous driving can be achieved without imitation learning while maintaining robust performance across diverse scenarios."}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "This paper compares digital twin approaches for dynamical system modeling and control using a miniature greenhouse testbed, evaluating four predictive models and three control strategies across interpolation/extrapolation scenarios.", "motivation": "To investigate the integration of physics-based, data-driven, and hybrid approaches with traditional and AI-driven controllers for digital twin applications in dynamical systems.", "method": "Developed and compared four predictive models (Linear, PBM, LSTM, HAM) and three control strategies (MPC, RL, LLM-based control) using a miniature greenhouse test platform under interpolation and extrapolation scenarios.", "result": "HAM provided the most balanced modeling performance across accuracy, generalization, and computational efficiency; LSTM achieved high precision but with greater resource cost. MPC delivered robust control performance, RL showed strong adaptability, and LLM-based controllers enabled flexible human-AI interaction.", "conclusion": "Hybrid approaches like HAM offer balanced modeling performance, while different control strategies provide complementary strengths - MPC for robustness, RL for adaptability, and LLM-based control for human-AI interaction when combined with predictive tools."}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "Neural USD is a framework for precise object editing in generative models using hierarchical scene representation inspired by Universal Scene Descriptor (USD), enabling per-object control over appearance, geometry, and pose.", "motivation": "Current controllable generative models often cause unintended global changes when trying to edit specific objects in scenes, lacking precise and iterative object editing capabilities.", "method": "Introduces Neural USD with structured hierarchical scene representation, accommodates diverse signals with minimal constraints, and applies fine-tuning to disentangle control signals for appearance, geometry, and pose.", "result": "The framework enables iterative and incremental workflows with per-object control, demonstrating effective disentanglement of control signals for precise editing.", "conclusion": "Neural USD represents a significant step forward in addressing precise object editing challenges in generative modeling through structured hierarchical scene representation."}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "The paper proposes a novel embodied agent framework using Vision-Language Models (VLMs) for intelligent robotic manipulation, achieving 28% higher task success rates compared to LLM+CLIP approaches.", "motivation": "To address the limitations of existing LLM-based embodied agents in planning and executing complex natural language control tasks online, and to better serve human welfare through Human-centered AI.", "method": "A three-module framework: human-robot voice interaction, vision-language agent (with task planner, instruction converter, and feedback evaluator), and action execution module.", "result": "Achieved 28% higher average task success rate in both simulated and real environments compared to LLM+CLIP approaches.", "conclusion": "The proposed VLM-based embodied agent framework significantly improves execution success rates for high-level natural language instruction tasks in robotics."}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "This survey paper examines security risks specific to agentic AI systems powered by LLMs, providing a taxonomy of threats, evaluation benchmarks, and defense strategies for these autonomous systems.", "motivation": "Agentic AI systems with planning, tool use, memory, and autonomy capabilities create new and amplified security risks that are distinct from traditional AI safety and conventional software security, requiring specialized analysis.", "method": "The paper conducts a comprehensive survey that outlines a taxonomy of agentic AI threats, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives.", "result": "The survey synthesizes current research on agentic AI security, identifying specific threats unique to these autonomous systems and providing frameworks for understanding and addressing them.", "conclusion": "The paper aims to support the development of secure-by-design agent systems by highlighting open challenges and providing a foundation for future research in agentic AI security."}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "SafeVision is a novel image guardrail system that integrates human-like reasoning to enhance adaptability and transparency, achieving state-of-the-art performance while being 16x faster than GPT-4o.", "motivation": "Traditional image guardrail models have limitations including misclassification due to pure feature-based learning without semantic reasoning, inability to adapt to emerging threats without costly retraining, and lack of transparency.", "method": "The approach includes an effective data collection and generation framework, policy-following training pipeline, customized loss function, and diverse QA generation and training strategy. It dynamically aligns with evolving safety policies at inference time without retraining.", "result": "SafeVision outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster. It achieves state-of-the-art performance on different benchmarks.", "conclusion": "SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats, addressing limitations of traditional models."}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo is a visual navigation system that uses language 3D Gaussian Splatting memory to handle multi-modal, open-vocabulary goal queries and multi-goal navigation, outperforming state-of-the-art methods.", "motivation": "To address practical demands for multi-modal, open-vocabulary goal queries and multi-goal visual navigation, overcoming limitations of classical methods restricted to single-goal, single-modality, and closed set goal settings.", "method": "Constructs a unified 3D language memory using language 3D Gaussian Splatting during exploration, then queries the memory for task goals, predicts candidate goal locations, and uses local perception-based verification to dynamically match and validate goals during navigation.", "result": "LagMemo's memory module enables effective multi-modal open-vocabulary goal localization and outperforms state-of-the-art methods in multi-goal visual navigation. Also created GOAT-Core benchmark for fair evaluation.", "conclusion": "LagMemo provides an effective solution for practical multi-modal, open-vocabulary multi-goal visual navigation by leveraging language-enhanced 3D memory and dynamic goal verification."}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "The paper proposes a novel training algorithm based on amortized variational inference to improve chain-of-thought reasoning in Large Vision-Language Models, addressing limitations of existing methods like SFT, PPO, and GRPO.", "motivation": "Existing training algorithms for chain-of-thought reasoning in LVLMs don't generalize well across unseen tasks and rely heavily on biased reward models, limiting their interpretability and reliability.", "method": "Reformulates reasoning as posterior inference using amortized variational inference, introduces diversity-seeking RL with sparse token-level rewards, and implements Bayesian inference-scaling with marginal likelihood for efficient rationale ranking.", "result": "Empirically demonstrates enhanced performance on seven reasoning benchmarks, improving state-of-the-art LVLMs in effectiveness, generalization, and interpretability.", "conclusion": "The proposed variational inference-based approach successfully addresses limitations of existing methods and significantly improves chain-of-thought reasoning capabilities in LVLMs."}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "A framework that brings chain-of-thought reasoning to chest X-ray interpretation, enabling transparent stepwise reasoning similar to clinicians while maintaining competitive classification accuracy.", "motivation": "Current vision-language models for medical image analysis are opaque and lack the transparent, stepwise reasoning that clinicians rely on for diagnosis and decision-making.", "method": "Two-stage training: reasoning-style supervised fine-tuning followed by reinforcement learning with verifiable rewards over X-ray abnormalities. Combines high-fidelity visual encoding with explicit reasoning traces.", "result": "Achieves competitive multi-label classification in out-of-distribution evaluation. In reader studies with expert radiologists, reasoning traces increased confidence, supported error auditing, and reduced report finalization time.", "conclusion": "The approach enables trustworthy, explainable AI in medical imaging where reasoning quality is as critical as prediction quality, supporting safer human-AI collaboration and clinical auditability."}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "Behavioral cloning with \"blindfolded\" experts who lack full task information generalizes better than cloning fully-informed experts, especially with fewer demonstrations.", "motivation": "To improve generalization in behavioral cloning for sequential decision-making by limiting the demonstrator's task information, forcing non-trivial exploration.", "method": "Hide some task information from demonstrators, creating \"blindfolded\" experts who must explore to solve tasks. Clone their behavior and compare with fully-informed experts.", "result": "Blindfolded expert cloning generalizes better to unseen tasks than fully-informed cloning in real-world robot peg insertion and Procgen videogames. Theoretical analysis shows generalization error scales with \u221a(I/m), where I is task information available.", "conclusion": "Cloning blindfolded experts improves generalization, especially with fewer demonstrated tasks, as demonstrated by experiments and theoretical analysis."}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "A decentralized causal discovery framework using judo calculus that handles context-dependent causal effects across different regimes through sheaf theory and j-stability.", "motivation": "Real-world causal effects depend on context (age, country, dose, genotype, etc.), requiring methods that can handle this regime dependence rather than assuming universal causal relationships.", "method": "Uses judo calculus (j-stable causal inference with j-do-calculus in a topos of sheaves) combined with standard causal discovery methods (score-based, constraint-based, gradient-based) to formalize context dependence as local truth across regimes.", "result": "Experimental results show computational efficiency from decentralized sheaf-theoretic approach and improved performance over classical causal discovery methods on synthetic and real-world datasets from biology and economics.", "conclusion": "The judo calculus framework provides a constructive, decentralized approach to causal discovery that effectively handles regime-dependent causal effects with better efficiency and performance than traditional methods."}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "Proposes joint prediction of multiple Fourier components for arbitrary-scale super-resolution to improve quality and efficiency over existing recurrent methods.", "motivation": "Existing methods predict Fourier components one by one using recurrent neural networks, leading to performance degradation and inefficiency due to independent prediction.", "method": "Predicting multiple Fourier components jointly instead of one by one.", "result": "Improves both quality and efficiency in arbitrary-scale super-resolution.", "conclusion": "Joint prediction of multiple components is more effective than independent prediction for super-resolution tasks."}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "HMAMP uses adversarial motion priors to learn human-style manipulation skills, enabling robots to perform tasks like hammering with motion patterns similar to humans.", "motivation": "To develop robots that can interact with humans more naturally by manipulating objects and tools in a human-like manner.", "method": "Uses adversarial networks to model tool/object manipulation dynamics and task aims, training a discriminator with real-world and simulation data to generate realistic human-like motion trajectories.", "result": "HMAMP outperforms baseline methods on hammering tasks and demonstrates potential for real-world applications through successful robot arm hammering experiments.", "conclusion": "HMAMP represents significant progress toward developing robots that can interact naturally with humans by learning human-style manipulation skills."}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "The paper proposes a new 'sign estimator' method for LLM alignment that replaces cross-entropy loss with binary classification loss, providing consistent ordinal alignment and reducing preference distortion compared to standard RLHF methods.", "motivation": "Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences and yield inconsistent estimates of population-average utility, which is a key measure of social welfare.", "method": "The sign estimator replaces cross-entropy with binary classification loss in the aggregation step of pairwise comparison data, providing a simple yet provably consistent and efficient estimator under mild assumptions.", "result": "In simulations using digital twins, the sign estimator reduced angular estimation error by nearly 35% and decreased disagreement with true population preferences from 12% to 8% compared to standard RLHF. It also outperformed panel data heuristics that explicitly model user heterogeneity.", "conclusion": "The sign estimator offers a simple, consistent, and efficient solution for LLM alignment that substantially reduces preference distortion while maintaining implementation simplicity comparable to existing alignment pipelines."}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo is a comprehensive benchmark for egocentric AI assistants that evaluates memory, understanding, and cross-memory reasoning across 14+ hours of synchronized multi-modal data in realistic streaming scenarios.", "motivation": "Existing benchmarks evaluate AI assistant capabilities in isolation, lack realistic streaming scenarios, or only support short-term tasks, failing to capture the real-world requirements of egocentric AI assistants.", "method": "Created a dataset with over 14 hours per participant of synchronized egocentric video, audio, and text across four domains, aligned on a unified timeline with human-refined visual narrations and speech transcripts. Defines 12 diagnostic subtasks across three core capabilities.", "result": "Contains 3,291 human-verified QA items spanning multiple question formats, evaluated in streaming settings with proposed metrics for real-time accuracy and memory persistence.", "conclusion": "TeleEgo provides a realistic and comprehensive evaluation framework to advance the development of practical AI assistants that can process multi-modal inputs, respond in real time, and retain long-term memory."}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend is a 3D-aware representation learning framework that learns geometry, semantics, and dynamics through masked reconstruction and future prediction using differentiable volumetric rendering, achieving improved robotic manipulation performance.", "motivation": "Current approaches rely on 2D vision pretraining that focuses on static semantics or scene geometry, or use video prediction models emphasizing 2D dynamics, failing to jointly learn the geometry, semantics, and dynamics needed for effective manipulation.", "method": "Pretrains on multi-view RGB-D video data using masked reconstruction and future prediction with differentiable volumetric rendering to learn unified triplane features that capture spatial geometry, future dynamics, and task semantics.", "result": "Substantial improvements in policy success rate, generalization to environmental perturbations, and real-world applicability across diverse manipulation tasks on RLBench, Colosseum benchmarks and real-world experiments.", "conclusion": "DynaRend effectively learns 3D-aware and dynamics-informed representations that can be transferred to downstream robotic manipulation tasks via action value map prediction, addressing key limitations of existing approaches."}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "This paper develops a conditioned deep learning model that incorporates individuals' social infrastructure resilience (SIR) and spatial context to predict post-disruption movement patterns using sparse individual-level data.", "motivation": "Predicting individual movement shifts after disruptive events is challenging due to lack of SIR measures, insufficient capture of movement-spatial context interactions, and spatial sparsity of individual movement data.", "method": "A conditioned deep learning model that incorporates individuals' SIR and captures complex relationships between movement patterns and local spatial context using large-scale sparse individual-level data.", "result": "Experiments show that incorporating SIR and spatial context enhances prediction of post-event movement patterns, and the model captures divergent shifts among individuals with similar pre-event patterns but different SIR.", "conclusion": "The conditioned model successfully predicts post-disruption movement patterns by accounting for individual SIR and spatial context, addressing key challenges in movement prediction."}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "Proposes AdvBlur, a novel diabetic retinopathy classification method that uses adversarial blurred images and dual-loss functions to improve domain generalization across different datasets and imaging conditions.", "motivation": "Existing deep learning models for diabetic retinopathy detection struggle with robustness due to distributional variations from different acquisition devices, demographic disparities, and imaging conditions.", "method": "Integrates adversarial blurred images into the dataset and employs a dual-loss function framework to address domain generalization challenges.", "result": "Achieves competitive performance compared to state-of-the-art domain generalization DR models on unseen external datasets, with comprehensive evaluations across multiple datasets.", "conclusion": "AdvBlur effectively mitigates the impact of unseen distributional variations and demonstrates strong domain generalization capabilities for diabetic retinopathy classification."}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "CoNi-OA is a real-time obstacle avoidance algorithm for UAV-UGV cooperative systems that uses single-frame LiDAR data to generate velocity modulation matrices, enabling collision-free navigation without global state estimation or obstacle prediction.", "motivation": "CoNi-MPC framework enables UAV control using relative states but lacks environmental awareness for obstacle avoidance, creating safety challenges in cooperative air-ground tasks.", "method": "Utilizes single-frame raw LiDAR data to generate modulation matrices that directly adjust quadrotor velocity, enabling real-time trajectory generation within UGV's non-inertial frame without obstacle modeling or prediction.", "result": "Achieves real-time performance with computational demands under 5 ms per iteration, maintains safety in dynamic environments, and enables collision-free trajectory generation without global state estimation.", "conclusion": "CoNi-OA provides an efficient, adaptable obstacle avoidance solution for UAV-UGV cooperation that works in both static and dynamic environments without requiring global states or obstacle prediction."}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "This paper introduces two new LLM-discovered heuristics (EDDC and MDDC) for the single-machine total tardiness problem that outperform traditional methods and remain competitive with exact approaches on large instances.", "motivation": "To leverage Large Language Models for discovering novel heuristics in combinatorial optimization, specifically for the NP-hard single-machine total tardiness problem where exact methods become computationally intractable for large instances.", "method": "Developed two LLM-discovered heuristics (EDDC and MDDC) inspired by EDD and MDD rules, benchmarked using rigorous criteria including optimality gaps and solution time from MIP formulation, and compared against state-of-the-art heuristics and exact methods across various job sizes.", "result": "For instances with more than 100 jobs, EDDC improves upon classic EDD rule and other algorithms, while MDDC consistently outperforms traditional heuristics and remains competitive with exact approaches, especially on larger instances up to 500 jobs.", "conclusion": "Human-LLM collaboration can produce scalable, high-performing heuristics for NP-hard constrained combinatorial optimization problems, even under limited computational resources when properly configured."}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "The SEG.A challenge introduced a large public dataset for aortic vessel tree segmentation, showing that 3D U-Net architectures with ensemble methods and custom post-processing achieved the best performance.", "motivation": "Automated analysis of aortic vessel trees from CT angiography has clinical potential but was hindered by lack of shared high-quality data.", "method": "Created a large multi-institutional dataset and benchmarked automated algorithms on hidden test sets, with optional surface meshing tasks for computational simulations.", "result": "Deep learning methods dominated, with 3D U-Net architectures performing best. Ensemble methods significantly outperformed individual models, and performance was linked to algorithmic design and training data characteristics.", "conclusion": "The challenge established a new performance benchmark and provides a lasting resource to drive future innovation toward clinically translatable tools."}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim is a framework that automatically creates large-scale, navigable indoor simulators from common image sequences, addressing visual artifacts and enabling mesh-free traversability checking.", "motivation": "To overcome the cost and scalability limitations of traditional 3D scanning methods for creating indoor navigation environments.", "method": "Uses Floor-Aware Gaussian Splatting to handle visual artifacts on sparsely observed floors and introduces a mesh-free traversability checking algorithm that constructs topological graphs by analyzing rendered views.", "result": "Successfully demonstrated the system's ability to generate valid, large-scale navigation graphs from real-world data.", "conclusion": "NVSim provides an efficient and scalable solution for creating navigable indoor simulators from image sequences, eliminating the need for expensive 3D scanning."}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast is a cross-domain time series forecasting framework that decomposes series into seasonal and trend components, modeling them separately with specialized modules for better generalization across domains.", "motivation": "Existing methods struggle with domain-specific trend shifts and inconsistent periodic patterns when forecasting across heterogeneous time series domains, as they treat temporal series as undifferentiated sequences without explicitly decoupling structural components.", "method": "OneCast decomposes time series into seasonal and trend components. Seasonal patterns are captured via lightweight projection with interpretable basis functions, while trends are encoded into discrete tokens via semantic-aware tokenizer and inferred through masked discrete diffusion mechanism.", "result": "Extensive experiments across eight domains demonstrate that OneCast mostly outperforms state-of-the-art baselines in cross-domain time series forecasting.", "conclusion": "The structured and modular approach of explicitly decoupling seasonal and trend components enables effective generalization across heterogeneous time series domains, addressing limitations of treating temporal series as undifferentiated sequences."}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "Mars-Bench is the first benchmark for evaluating foundation models on Mars-related tasks using orbital and surface imagery, addressing the lack of standardized evaluation frameworks in Mars science.", "motivation": "Foundation models have shown strong generalization in many domains but their application to Mars science remains limited due to the absence of standardized benchmarks and evaluation frameworks.", "method": "Created Mars-Bench with 20 datasets spanning classification, segmentation, and object detection tasks focused on key geologic features (craters, cones, boulders, frost), providing standardized datasets and baseline evaluations using various pre-trained models.", "result": "Results suggest Mars-specific foundation models may offer advantages over general-domain counterparts, motivating further exploration of domain-adapted pre-training.", "conclusion": "Mars-Bench establishes a standardized foundation for developing and comparing machine learning models for Mars science, with all data, models, and code publicly available."}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "An optimal trajectory generation method for 3D overhead cranes using differential flatness that incorporates complex constraints like nonlinear friction and collision avoidance, enabling aggressive movements with payload swing constrained only at the endpoint.", "motivation": "To enable fast and safe crane operations by addressing the limitations of existing methods that neglect important physical constraints like friction, which can lead to actuator saturation and collisions.", "method": "Leverages differential flatness to directly incorporate complex physical and dynamic constraints including nonlinear friction and collision avoidance for both payload and rope, while allowing aggressive movements by constraining payload swing only at the final point.", "result": "Comparative simulations validate the approach, showing that neglecting dry friction causes actuator saturation and collisions, while the proposed method enables fast and safe crane trajectories.", "conclusion": "Friction modeling is a fundamental requirement for generating fast and safe crane trajectories, and the differential flatness-based approach effectively handles complex constraints to enable aggressive crane movements."}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer is a clustering-based log analysis chatbot that combines LLMs and ML algorithms to simplify cybersecurity log analysis, overcoming LLM limitations and achieving 39-68% performance improvements over existing chatbots.", "motivation": "System log analysis is crucial for cybersecurity but challenging due to high costs, lack of expertise, and time constraints. Existing LLM-based solutions have limitations in context windows and structured text handling.", "method": "Uses a modular architecture with router, log recognizer, log parser, and search tools. Combines LLMs with ML algorithms through clustering-based approach to address LLM limitations.", "result": "Achieved 39-68% performance improvements over state-of-the-art LLM chatbots (ChatGPT, ChatPDF, NotebookLM) across four domain logs. Showed strong robustness with 93% reduction in interquartile range using ROUGE-1 scores.", "conclusion": "LLMLogAnalyzer effectively enhances LLM capabilities for structured text analysis, improving accuracy and robustness for cybersecurity log analysis, making it valuable for both experts and non-technical users."}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "APT is a black-box framework that uses LLMs to generate human-readable adversarial suffixes for text-to-image models, bypassing safety filters through dual-evasion strategy and achieving high transferability.", "motivation": "Current red-teaming methods for T2I models require white-box access, use inefficient per-prompt optimization, and generate meaningless prompts that are easily blocked by filters.", "method": "Uses alternating optimization-finetuning pipeline between adversarial suffix optimization and LLM fine-tuning, with dual-evasion strategy: perplexity-based filtering constraint and banned-token penalties.", "result": "Demonstrates excellent red-teaming performance with human-readable, filter-resistant prompts, and superior zero-shot transferability to unseen prompts and commercial APIs.", "conclusion": "APT effectively exposes critical vulnerabilities in T2I models through human-readable adversarial prompts that bypass safety mechanisms."}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "The paper presents a bilevel optimization method for estimating sensor noise covariances in state estimation, using a factorization approach that enables parallel computation and achieves higher efficiency than existing baselines.", "motivation": "Accurate sensor noise covariance specification is crucial for reliable state estimation but difficult in practice due to environmental variability and front-end preprocessing challenges.", "method": "Formulates noise covariance estimation as bilevel optimization with Bayesian factorization, using invariant extended Kalman filter with state augmentation at lower level and derivative filter for analytical gradients at upper level.", "result": "Experiments on synthetic and real-world datasets demonstrate that the proposed method achieves higher efficiency compared to existing baselines.", "conclusion": "The bilevel optimization approach with Bayesian factorization effectively addresses noise covariance estimation challenges while maintaining computational efficiency."}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "This study compares classical physics-based car following models (IDM, OVM, OVRV, CACC) with a machine learning approach (Random Forest) for modeling electric vehicle following behavior, finding that Random Forest significantly outperforms classical models across all gap scenarios.", "motivation": "The increasing adoption of electric vehicles requires better understanding of their driving behavior to enhance traffic safety and develop smart driving systems, especially in mixed autonomy traffic environments.", "method": "Used real-world EV following ICE vehicle data under varied driving conditions. Calibrated classical model parameters by minimizing RMSE, while Random Forest predicted acceleration using spacing, speed, and gap type as inputs.", "result": "Random Forest achieved superior accuracy with RMSEs of 0.0046 (medium gap), 0.0016 (long gap), and 0.0025 (extra long gap). Among classical models, CACC performed best with RMSE of 2.67 for long gaps.", "conclusion": "Machine learning models like Random Forest demonstrate significantly better performance than physics-based models for EV car following behavior, making them valuable for simulating EV behavior and analyzing mixed autonomy traffic dynamics."}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "Residual Networks (ResNet) with skip connections overcome vanishing gradient problems in deep CNNs, enabling training of very deep networks with better accuracy and faster convergence.", "motivation": "Training very deep convolutional neural networks is challenging due to vanishing gradient problems that prevent effective learning in deep layers.", "method": "ResNet uses skip connections (shortcuts) that bypass intermediate layers, allowing gradients to flow directly through the network and enabling training of networks with hundreds of layers.", "result": "On CIFAR-10 dataset, ResNet-18 achieved 89.9% accuracy compared to 84.1% for traditional deep CNN of similar depth, with faster convergence and more stable training.", "conclusion": "Residual Networks with skip connections effectively solve the vanishing gradient problem, enabling successful training of very deep neural networks with improved performance."}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "This paper extends the Team Orienteering Problem (TOP) to competitive multi-robot settings with Stochastic Prize-Collecting Games (SPCG), proposing ORS and FORL algorithms that achieve 87-95% optimality compared to TOP solutions.", "motivation": "Existing TOP formulations assume cooperative robots and don't handle competitive scenarios in reward-scarce environments where robots have self-interested objectives.", "method": "Proposed Stochastic Prize-Collecting Games (SPCG) with two algorithms: Ordinal Rank Search (ORS) for determining effective ranks in local neighborhoods, and Fictitious Ordinal Response Learning (FORL) for learning best-response policies against higher-ranked opponents.", "result": "Empirical evaluations show ORS enables more scalable learning to large team sizes, FORL generalizes better to imbalanced prize distributions, and learned policies achieve 87-95% optimality compared to TOP solutions.", "conclusion": "SPCG effectively extends TOP to competitive multi-robot settings, with the proposed algorithms enabling efficient learning and near-optimal performance in competitive routing scenarios."}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens is a transparent AI system for pathology that allows doctors to ask questions in plain English about tissue slides, provides structured reports with visual proofs, and focuses only on relevant tissue while ignoring background noise.", "motivation": "To create a trustworthy AI system that doctors can trust by making it transparent and collaborative, rather than a black box, so pathologists can understand the AI's reasoning like consulting a colleague.", "method": "Developed HistoLens which translates natural language questions into precise AI queries, provides structured reports with visual proofs (heatmaps showing exact cells/regions used for analysis), and trains the AI to focus only on patient tissue while ignoring background noise.", "result": "Creates a workflow where pathologists remain in charge as experts, using the AI assistant to verify insights and make faster, more confident diagnoses through transparent collaboration.", "conclusion": "HistoLens successfully bridges the trust gap in medical AI by providing transparency, visual explanations, and collaborative functionality that keeps the human expert in control while enhancing diagnostic confidence and efficiency."}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA is a one-stage parameter-efficient fine-tuning method that uses nonlinear kernel functions in low-rank decomposition and adaptive bi-level sparsity allocation to achieve state-of-the-art performance with significantly reduced memory usage compared to existing methods.", "motivation": "Current sparse tuning methods have two main limitations: they use gradient information to locate task-relevant weights which overlooks parameter adjustments during fine-tuning, and they require storing all weight matrices in the optimizer leading to high memory usage.", "method": "SNELLA uses a one-stage approach that selectively updates weight matrices by adding them to another sparse matrix merged by two low-rank learnable matrices with nonlinear kernel functions. It also employs an adaptive bi-level sparsity allocation mechanism that allows weights to compete across and inside layers based on importance scores in an end-to-end manner.", "result": "SNELLA achieves SOTA performance with 1.8% higher Top-1 accuracy on FGVC benchmark (91.9% vs 90.1%) compared to SPT-LoRA, and reduces memory usage by 31.1%-39.9% across models with parameter scales from 86M to 632M on classification, segmentation, and generation tasks.", "conclusion": "SNELLA successfully overcomes the limitations of current sparse tuning methods by providing a more efficient one-stage approach that achieves better performance with significantly reduced memory consumption through innovative low-rank decomposition with nonlinear kernels and adaptive sparsity allocation."}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM is a gravity-enhanced stereo visual inertial SLAM system that addresses underwater robot challenges by using stereo depth estimation, gravity initialization, and a 4-DOF PnP solver for improved accuracy and stability.", "motivation": "Underwater VI SLAM faces challenges due to visual degeneracy and insufficient IMU motion excitation, making accurate localization difficult in underwater environments.", "method": "Leverages stereo camera depth estimation to eliminate scale estimation during IMU initialization, uses gravity initialization to decouple pitch/roll for 4-DOF PnP solving with minimal 3-point solver, and implements bias-eliminated estimator with provable consistency. Also refines 6-DOF pose while estimating IMU covariance for adaptive gravity prior weighting.", "result": "Extensive experiments on simulated and real-world data show GeVI-SLAM achieves higher accuracy and greater stability compared to state-of-the-art methods.", "conclusion": "The proposed gravity-enhanced approach with 4-DOF PnP solver and adaptive weighting effectively addresses underwater VI SLAM challenges, providing superior performance in accuracy and stability."}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent is a lightweight, self-evolving multi-agent system for incident management that converts heterogeneous observability data into structured descriptions and uses multi-agent collaboration for transparent diagnostic inference, achieving state-of-the-art performance on the OPENRCA benchmark.", "motivation": "Manual incident management is labor-intensive and error-prone with massive observability data, while existing automated approaches struggle with generalization, interpretability, and high deployment costs.", "method": "Uses a training-free data processor to convert heterogeneous observability data into structured textual descriptions, employs multi-agent collaboration framework for transparent diagnostic inference, and implements dual self-evolution mechanism for continual capability growth.", "result": "Comprehensive experiments on OPENRCA benchmark demonstrate state-of-the-art performance, showing OpsAgent is generalizable, interpretable, cost-efficient, and self-evolving.", "conclusion": "OpsAgent is a practically deployable and sustainable solution for long-term operation in real-world cloud systems, addressing key limitations of existing incident management approaches."}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "COLA is a training-free framework that uses optimal transport to restore cross-modal alignment between image and text features in CLIP models under adversarial attacks, improving robustness without compromising clean accuracy.", "motivation": "Vision-language models like CLIP are vulnerable to adversarial perturbations due to misalignment between text and image features, which gets amplified under attacks. Existing methods overlook these feature gaps.", "method": "COLA projects adversarial image embeddings onto a subspace spanned by class text features to filter distortions, then models images/texts as distributions over augmented views and refines alignment via optimal transport with integrated subspace projection.", "result": "Extensive evaluations on 14 benchmarks show COLA improves robustness significantly, with average 6.7% improvement on ImageNet variants under PGD attacks while maintaining high clean accuracy.", "conclusion": "COLA effectively addresses adversarial misalignment in VLMs through optimal transport-based cross-modality alignment, providing training-free robustness enhancement compatible with existing fine-tuned models."}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "A hierarchical framework for robotic inspection that combines global view-planning with local view replanning to handle environment uncertainty in real-world settings like subterranean mines.", "motivation": "Existing robotic inspection methods rely on known environment models, but discrepancies between models and actual site conditions (due to natural or human activities) can alter surface morphology or introduce obstructions, making inspection unreliable.", "method": "The framework divides inspection into: (a) generating initial global view-plan for regions of interest using historical maps, and (b) local view replanning to adapt to current scene morphology. This preserves global coverage while enabling reactive adaptation.", "result": "The approach was validated through real-world deployments in subterranean mines using quadrupedal robots, showing robust performance against environment uncertainty.", "conclusion": "The hierarchical framework successfully enables robotic inspection systems to maintain global objectives while adapting locally to environmental changes, making inspection tasks more reliable in uncertain conditions."}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "Automated framework for generating high-difficulty multi-hop QA datasets that require complex reasoning over ambiguous, indirect cues, addressing the scarcity of training-ready data for retrieval-and-reasoning models.", "motivation": "Current multi-hop QA datasets are scarce and mostly designed for evaluation, not training. Manual curation of non-trivially retrievable questions is costly and doesn't scale, creating a data bottleneck for training retrieval-and-reasoning agents.", "method": "Three-step automated framework: (1) grows diverse evidence clusters using NLI-based relation typing and diversity-aware expansion, (2) applies reverse question construction to create oblique cues where isolated signals are underinformative but combined uniquely identify targets, (3) enforces quality with multi-model consensus filtering and structured constraint decomposition with evidence-based matching.", "result": "Scalable process that produces complex, retrieval-resistant yet verifiable questions suitable for both SFT/RL training and challenging evaluation, reducing human curation effort while maintaining difficulty comparable to strong evaluation benchmarks.", "conclusion": "The framework successfully addresses the critical data bottleneck for training high-capability retrieval-and-reasoning agents by automating the generation of training-ready multi-hop QA datasets that preserve the challenging characteristics of hard evaluation benchmarks."}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "BOB is a fine-tuning strategy for T2I models that extracts class-agnostic attributes (background, pose) from real examples, conditions on them during fine-tuning, and marginalizes them out during generation to improve synthetic data quality for fine-grained classification.", "motivation": "Fine-tuning T2I models with few real examples for synthetic dataset generation can cause overfitting and reduce diversity, limiting effectiveness for classification tasks.", "method": "Extract class-agnostic attributes from real examples, explicitly condition T2I model on these attributes during fine-tuning, and marginalize them out during generation to preserve generative prior and reduce overfitting.", "result": "BOB achieves state-of-the-art performance, outperforming DataDream by 7.4% on Aircraft dataset and performing better than using 10 real images in 3 out of 4 benchmarks with only 5 real images augmented with synthetic data.", "conclusion": "BOB effectively mitigates overfitting in T2I fine-tuning, preserves model diversity, and significantly improves synthetic data quality for fine-grained classification tasks."}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "A Unified Iterative Calibration (UIC) framework for underwater SLAM systems that jointly estimates extrinsic parameters and clock offsets between sensors using Maximum A Posteriori estimation with Gaussian Process motion priors.", "motivation": "Existing Doppler Velocity Log (DVL) calibration methods are limited to specific sensor configurations, rely on oversimplified assumptions, and fail to jointly estimate translational extrinsics and time offsets for high-accuracy underwater SLAM performance.", "method": "Proposes UIC framework using MAP estimation with GP motion prior for high-fidelity motion interpolation, alternating between GP-based motion state updates and gradient-based calibration variable updates, supported by provably consistent sequential initialization.", "result": "UIC can be applied to IMU, cameras and other sensor modalities, with open-source DVL-camera calibration toolbox released. Validated through simulations and real-world tests.", "conclusion": "The UIC framework successfully addresses underwater sensor calibration limitations and its components (GP priors for MAP calibration, reliable initialization) are broadly applicable to other multi-sensor calibration problems beyond underwater applications."}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM\u2081 is a multimodal spatial foundation model that unifies digital and physical space operations with cross-embodiment generalization, outperforming existing model families by ~6% in digital tasks and ~3% in physical tasks.", "motivation": "Current MLLMs have poor generalization across digital-physical spaces and embodiments, VLAs lack robust high-level reasoning, and ELLMs are constrained to digital space with poor physical world generalization. Unified models that work seamlessly across spaces and embodiments are missing.", "method": "Two-stage training: Stage I injects embodied knowledge into MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module via intent-bridging interface that extracts high-level semantics from MLLM to guide control without fine-tuning the MLLM backbone.", "result": "Single BLM\u2081 instance outperforms four model families (MLLMs, ELLMs, VLAs, GMLMs) with ~6% gains in digital tasks and ~3% gains in physical tasks across evaluations on digital and physical benchmarks.", "conclusion": "BLM\u2081 successfully integrates cross-space transfer, cross-task learning, and cross-embodiment generalization, demonstrating robust performance across digital and physical environments while maintaining instruction following and reasoning capabilities."}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "OmniText is a training-free generalist framework for Text Image Manipulation (TIM) that addresses limitations of existing text inpainting methods by enabling text removal, style control, and preventing duplicated letters through self-attention inversion and cross-attention redistribution.", "motivation": "Current diffusion-based text inpainting methods have three key limitations: inability to remove text, lack of style control over rendered text, and tendency to generate duplicated letters, which hinders their broader applicability to TIM tasks.", "method": "Uses self-attention inversion for text removal to reduce text hallucinations, redistributes cross-attention to reduce text hallucination, and introduces novel loss functions (cross-attention content loss and self-attention style loss) in a latent optimization framework for controllable inpainting.", "result": "OmniText achieves state-of-the-art performance across multiple TIM tasks and metrics, comparable with specialist methods, and introduces OmniText-Bench benchmark dataset for evaluating diverse TIM tasks.", "conclusion": "OmniText is the first generalist method capable of performing diverse TIM tasks including text removal, rescaling, repositioning, and insertion/editing with various styles, demonstrating superior performance over other text inpainting methods."}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "Curriculum-based reinforcement learning framework for training precise jumping policies for robot 'Olympus', achieving 1.25m horizontal jumps with centimeter accuracy and 1.0m vertical jumps.", "motivation": "To develop precise and high-performance jumping capabilities for robots, addressing the challenge of sparse rewards in jumping tasks and enabling versatile dynamic locomotion.", "method": "Uses curriculum-based RL with reward densification using projectile motion laws, reference state initialization for faster exploration, and separate policies for vertical/horizontal jumps combined with walking policy.", "result": "Achieved horizontal jumps up to 1.25m with centimeter accuracy and vertical jumps up to 1.0m, successfully crossing Sim2Real gap and enabling omnidirectional jumping with minor modifications.", "conclusion": "The proposed framework effectively trains precise jumping policies, demonstrates superior performance over previous works, and enables versatile dynamic locomotion capabilities for robots."}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner is the first autonomous vehicle planning framework for multi-dataset integration, using three innovations to achieve unified cross-dataset learning while maintaining planning robustness.", "motivation": "Existing deep learning planning methods are confined to single-dataset training, limiting their robustness. The discovery that vehicular trajectory distributions and history-future correlations are consistent across datasets motivates multi-dataset integration.", "method": "1) HFTDN aggregates history-future trajectory pairs using historical trajectory similarity for cross-dataset planning guidance. 2) GFTM learns robust history-future correlations with gradient-free design to prevent shortcut learning. 3) S2D paradigm uses adaptive dropout for robust training and full prior utilization during inference.", "result": "The framework achieves unified cross-dataset learning and maintains planning robustness while enabling knowledge transfer across different datasets.", "conclusion": "UniPlanner successfully addresses the limitations of single-dataset training in autonomous vehicle planning by leveraging multi-dataset integration through three synergistic components, demonstrating consistent trajectory patterns across datasets can be effectively utilized for robust planning."}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "The paper discovers a positive correlation between interpretability and classifiability in pre-trained vision models, proposes an Inherent Interpretability Score (IIS) to quantify interpretability, and shows that improving classifiability also enhances interpretability.", "motivation": "Pre-trained visual models prioritize classifiability but lack clear understanding of whether they can simultaneously achieve high interpretability and classifiability, which is crucial for widespread applications requiring interpretable representations.", "method": "Proposes Inherent Interpretability Score (IIS) that quantifies representation interpretability by measuring information loss and the ratio of interpretable semantics that can be captured by interpretations.", "result": "Surprisingly discovered positive correlation between interpretability and classifiability - representations with higher classifiability provide more interpretable semantics. Fine-tuning with interpretability maximization further improves classifiability, and predictions based on interpretations show less accuracy degradation.", "conclusion": "Practitioners can unify improvements in both interpretability and classifiability for pre-trained vision models, as these two properties are positively correlated rather than conflicting objectives."}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "GroundLoc is a LiDAR-only localization system for mobile robots in large outdoor environments that uses BEV image projection and place recognition (R2D2 or SIFT) for map registration, achieving sub-50cm accuracy with minimal storage requirements.", "motivation": "To develop an efficient LiDAR-only localization system for large-scale outdoor environments that can work with various sensor types while maintaining high accuracy and low storage requirements.", "method": "Projects LiDAR data into Bird's-Eye View images focusing on ground areas, then uses R2D2 network or SIFT for place recognition and keypoint selection to register with 2D raster image maps.", "result": "Outperforms state-of-the-art methods on SemanticKITTI and HeLiPR datasets, achieves Average Trajectory Error below 50 cm on all Ouster OS2 128 sequences while meeting online runtime requirements.", "conclusion": "GroundLoc provides an effective LiDAR-only localization solution that supports multiple sensor types, achieves high accuracy, and requires only 4 MB storage per square kilometer for prior maps."}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "MGA is a memory-driven GUI agent that uses an \"observe first, then decide\" approach with structured memory to improve robustness and generalization in GUI interactions.", "motivation": "Existing GUI agents suffer from error propagation due to dependence on historical trajectories and local exploration bias from \"decision-first, observation-later\" mechanisms that overlook critical interface cues.", "method": "MGA reframes GUI interaction around \"observe first, then decide\" principle, modeling each step as an independent state with current screenshot, task-agnostic spatial information, and dynamically updated structured memory.", "result": "Experiments on OSworld benchmarks, real desktop applications (Chrome, VSCode, VLC), and cross-task transfer show MGA achieves substantial gains in robustness, generalization, and efficiency compared to state-of-the-art baselines.", "conclusion": "The memory-driven approach with structured representation enables more effective GUI agent performance by addressing key limitations of existing methods."}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "UHKD is a knowledge distillation framework that uses frequency domain transformations to enable effective knowledge transfer between heterogeneous teacher-student model architectures, achieving significant performance improvements over existing methods.", "motivation": "Existing knowledge distillation methods struggle with architectural diversity between teacher and student models, especially when using intermediate features, due to semantic discrepancies. Most methods focus only on logits space and degrade in heterogeneous scenarios.", "method": "Proposes Unified Heterogeneous Knowledge Distillation (UHKD) using Fourier transform to capture global feature information in frequency domain. Includes Feature Transformation Module for teacher features and Feature Alignment Module for student features with multi-level matching. Uses joint objective combining MSE on intermediate features and KL divergence on logits.", "result": "Experiments on CIFAR-100 and ImageNet-1K show gains of 5.59% and 0.83% respectively over the latest method, demonstrating effectiveness in unifying heterogeneous representations.", "conclusion": "UHKD effectively addresses architectural heterogeneity in knowledge distillation by leveraging frequency domain representations, enabling efficient utilization of visual knowledge across different model architectures."}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "A Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model is proposed for generating realistic multi-agent traffic scenarios in roundabouts to support virtual testing of intelligent driving functions.", "motivation": "Traditional road testing for intelligent driving functions is time-consuming and expensive. Virtual testing offers better efficiency and reproducibility, but roundabouts with complex interactions remain underexplored.", "method": "Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model that generates multi-agent traffic scenarios in roundabouts, with KPIs to evaluate interactive behavior.", "result": "The model accurately reconstructs original scenarios and generates realistic, diverse synthetic scenarios. Latent space analysis shows partial disentanglement with interpretable effects on vehicle timing and velocity profiles.", "conclusion": "The model effectively generates scenarios for validating intelligent driving functions involving multi-agent interactions and can augment data for development and improvement."}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow is an automated pipeline that discovers MCP servers, synthesizes training data, and trains models to improve LLMs' ability to use external tools from the Model Contextual Protocol ecosystem.", "motivation": "LLMs struggle to utilize the rapidly expanding MCP ecosystem due to limited server coverage, costly manual curation, and lack of training support in existing research.", "method": "An automated web-agent-driven pipeline for large-scale server discovery (1166 servers, 11536 tools), data synthesis (68733 instruction-function pairs, 6439 trajectories), and model training.", "result": "MCP-Flow significantly outperforms prior work in scale and diversity, demonstrating superior MCP tool selection, function-call generation, and enhanced agentic task performance.", "conclusion": "MCP-Flow provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments, with publicly available implementation."}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo is a large-scale multi-view RGB-D video dataset of canine movements with 1.2k sequences from 10 dogs, addressing limitations of existing datasets. The authors introduce a three-stage optimization pipeline for motion recovery and establish four benchmark settings for evaluation.", "motivation": "Existing dog motion datasets lack multi-view and real 3D data, have limited scale and diversity. DogMo aims to overcome these limitations to advance research in dog motion recovery.", "method": "Three-stage instance-specific optimization pipeline: 1) coarse alignment, 2) dense correspondence supervision, 3) temporal regularization. Fits SMAL model to motion sequences. Dataset includes 1.2k sequences from 10 dogs with multi-view RGB-D data.", "result": "Created DogMo dataset with rich variation in motion and breed. Established four motion recovery benchmark settings (monocular/multi-view, RGB/RGB-D). Developed optimization pipeline for accurate motion recovery.", "conclusion": "DogMo provides a principled foundation for advancing dog motion recovery research and opens new directions at the intersection of computer vision, computer graphics, and animal behavior modeling."}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "Using an inertial sensor on the sound ankle and genetic algorithm-optimized neural networks to predict thigh and knee joint angles for obstacle-crossing in powered transfemoral prosthetics, achieving high accuracy in gait phase estimation and low prediction errors.", "motivation": "Amputees with powered transfemoral prosthetics face challenges navigating obstacles and complex terrain, requiring improved control methods for obstacle-crossing movements.", "method": "Inertial sensor on sound ankle guides obstacle-crossing; genetic algorithm computes optimal neural network structure; gait progression prediction algorithm determines actuation angles for prosthetic knee motor.", "result": "Method effectively eliminates noise interference when Gaussian noise standard deviation <1; achieves 100% accuracy in gait phase estimation under 150 Hz; thigh angle prediction error 8.71%; knee angle prediction error 6.78%.", "conclusion": "The method accurately predicts gait progression and joint angles, offering significant practical value for obstacle negotiation in powered transfemoral prosthetics."}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "The paper addresses the tie-breaking issue in MCTS when multiple actions from the same parent fall into the same abstract node, proposing and evaluating alternative intra-abstraction policies that outperform random tie-breaking.", "motivation": "MCTS suffers from sample inefficiency, which can be improved using state/action abstractions to share information among nodes. However, current abstraction methods like pruned OGA don't properly handle cases where multiple actions from the same parent share an abstract node, requiring tie-breaking.", "method": "The authors propose and empirically evaluate several alternative intra-abstraction policies to replace the implicit random tie-breaking used in state-of-the-art abstraction algorithms like pruned OGA.", "result": "Several of the proposed intra-abstraction policies outperform the random policy across a majority of environments and parameter settings.", "conclusion": "Alternative intra-abstraction policies can significantly improve MCTS performance when multiple actions from the same parent share abstract nodes, addressing a previously unnoticed limitation in existing abstraction algorithms."}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "ETC is a training-free framework that accelerates diffusion models by reusing model outputs with error control and trend consistency, achieving 2.65x speedup over FLUX with minimal quality degradation.", "motivation": "Diffusion models have excellent generative quality but suffer from slow iterative sampling. Existing training-free acceleration methods ignore denoising trends and lack error control, causing trajectory deviations and result inconsistencies.", "method": "ETC introduces (1) a consistent trend predictor that projects historical denoising patterns into stable future directions across multiple approximation steps, and (2) a model-specific error tolerance search mechanism that identifies transition points from semantic planning to quality refinement to derive corrective thresholds.", "result": "ETC achieves 2.65x acceleration over FLUX with only -0.074 SSIM score degradation in consistency, demonstrating significant speed improvement while maintaining high quality.", "conclusion": "The proposed ETC framework effectively accelerates diffusion models by leveraging trend consistency and error-aware mechanisms, providing a practical solution for fast diffusion sampling without compromising generation quality."}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare is a framework for creating failure-resilient imitation learning policies that can automatically detect and recover from out-of-distribution scenarios without using explicit failure data.", "motivation": "Imitation learning policies are effective for visual navigation but prone to unpredictable failures in out-of-distribution scenarios, requiring a solution that can both detect and automatically recover from failures.", "method": "Fare embeds OOD-detection and recognition capabilities into IL policies without using explicit failure data, and pairs them with recovery heuristics. It identifies failure-causing factors by pinpointing image regions that trigger failure detections.", "result": "Real-world experiments demonstrate that Fare enables failure recovery across two different policy architectures and supports robust long-range navigation in complex environments.", "conclusion": "The Fare framework successfully creates failure-resilient IL policies that can automatically detect and recover from failures, enabling more robust visual navigation in challenging scenarios."}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "The paper proposes Self-Indicator, a method that uses the rank of correlation matrices between input problems and output reasoning paths from LLMs to assess reasoning correctness without external resources.", "motivation": "Existing methods for checking LLM outputs rely heavily on external resources like trained verifiers or elaborate prompts, leading to high computational overhead and domain-specific limitations.", "method": "The method calculates the rank of correlation matrices between input problems and LLM reasoning paths, using this as an indicator of reasoning correctness. It then reweights candidate reasoning paths based on this indicator.", "result": "Self-Indicator achieves over 75% accuracy in distinguishing correct reasoning paths from incorrect ones and improves accuracy on three reasoning benchmarks by more than 8% with minimal computational overhead.", "conclusion": "The internal behaviors of LLMs contain reliable indicators of reasoning correctness, enabling effective self-verification without external resources."}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "A training-free framework that uses LLMs to generate explicit layouts and object-centric VLM reranking to improve text-to-image model compositionality while maintaining image quality.", "motivation": "Modern text-to-image models struggle with compositionality issues like accurate object counts, attributes, and spatial relations despite their realism.", "method": "Combines object-centric approach with self-refinement: uses LLMs to synthesize explicit layouts from prompts, injects layouts into generation process, and employs object-centric VLM to iteratively rerank candidates for prompt alignment.", "result": "Achieves stronger scene alignment with prompts compared to recent text-to-image models while preserving aesthetic quality.", "conclusion": "The framework successfully improves layout faithfulness in text-to-image generation through explicit layout-grounding and self-refine-based inference-time scaling."}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "A framework for analyzing object-aware controllers that help robots avoid collisions, focusing on kinematics, motion profiles, and virtual constraints, with experimental verification and comparison of three controllers.", "motivation": "Real-time control is essential for safe robot operation in dynamic environments with moving objects, requiring effective collision avoidance methods.", "method": "Developed an analysis framework focusing on three design considerations (kinematics, motion profiles, virtual constraints) and used fundamental robot-obstacle experimental scenarios to verify robot behaviors.", "result": "Comparison of three representative object-aware controllers revealed common deficiencies: lack of kinematic considerations, discontinuity in control points, and instability in movement profiles.", "conclusion": "The framework can be used for future design, comparison, and benchmarking of obstacle avoidance methods in robotics."}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "A multi-agent framework using LLMs for claim verification through quantitative bipolar argumentation frameworks (QBAFs), with experiments showing improved forecasting accuracy when combining evidence from multiple agents.", "motivation": "To improve judgmental forecasting by treating it as claim verification and leveraging multiple agents to provide diverse evidence and perspectives on future events.", "method": "Proposed multi-agent framework with three types of LLM-powered agents: ArgLLM (existing QBAF approach), RbAM (relation-based argument mining from external sources), and RAG-ArgLLM (retrieval-augmented generation of arguments). Experiments used 2-3 agents with 6 different base LLMs on standard judgmental forecasting datasets.", "result": "Combining evidence from multiple agents improved forecasting accuracy, especially with three agents, while providing explainable evidence combinations for claim verification.", "conclusion": "Multi-agent frameworks with LLMs can effectively support claim verification in judgmental forecasting by combining diverse evidence sources and improving accuracy while maintaining explainability."}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "VC4VG is a caption optimization framework for text-to-video generation that analyzes caption requirements, provides design methodology, and introduces a benchmark with T2V-specific metrics.", "motivation": "High-quality video-text pairs are crucial for text-to-video generation, but strategies for optimizing video captions specifically for T2V training remain underexplored.", "method": "Analyze caption content from T2V perspective, decompose essential elements for video reconstruction, propose principled caption design methodology, and construct VC4VG-Bench benchmark with fine-grained, multi-dimensional metrics.", "result": "Extensive T2V fine-tuning experiments show strong correlation between improved caption quality and video generation performance, validating the framework's effectiveness.", "conclusion": "VC4VG provides an effective framework for optimizing video captions for text-to-video generation, with released benchmark tools and code to support further research."}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "This perspective paper proposes a framework for embodying physical computing into soft robots using three strategies: analog oscillators, physical reservoir computing, and physical algorithmic computing to enable complex behaviors without traditional electronics.", "motivation": "To advance soft robotics towards robustness and intelligence for everyday use by integrating soft computers and controllers, moving beyond traditional CMOS-based electronics.", "method": "Proposes a framework for embodying physical computing into soft robots through three strategies: analog oscillators, physical reservoir computing, and physical algorithmic computing that encode inputs into mechanical computing kernels.", "result": "The embodied computers enable soft robots to perform complex behaviors including coordinated locomotion with obstacle avoidance, payload weight and orientation classification, and programmable operation based on logical rules.", "conclusion": "The paper presents a perspective framework for future development of physical computing in soft robotics, detailing working principles and surveying current state-of-the-art approaches."}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "gLLMs like ChatGPT are revolutionizing communication research by outperforming human coders in content analysis at lower cost and time, but face seven key challenges that need systematic guidance for proper implementation.", "motivation": "To address the underdeveloped integration of gLLMs into communication research methodology despite their potential for superior performance in content analysis tasks.", "method": "Synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide addressing seven critical challenges.", "result": "Identifies seven key challenges in gLLM-assisted content analysis: codebook development, prompt engineering, model selection, parameter tuning, iterative refinement, validation of reliability, and performance enhancement.", "conclusion": "Provides a framework to make gLLM-based content analysis more accessible while ensuring adherence to disciplinary standards of validity, reliability, reproducibility, and research ethics."}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "Proposed MSRANetV2, a CNN architecture with ResNet50V2 backbone enhanced with residual attention and SE blocks for colorectal cancer tissue classification, achieving exceptional performance (F1-score ~0.99) on two public datasets.", "motivation": "Colorectal cancer is a leading cause of cancer mortality, and conventional diagnostic methods like colonoscopy are subjective, time-consuming, and variable. Deep learning can enhance diagnostic precision and efficiency in digital pathology.", "method": "MSRANetV2 uses ResNet50V2 backbone extended with residual attention mechanisms and squeeze-and-excitation blocks to extract deep semantic and fine-grained spatial features. It employs channel alignment and upsampling to fuse multi-scale representations.", "result": "Achieved remarkable performance on CRC-VAL-HE-7K (Precision: 0.9884\u00b10.0151, Recall: 0.9900\u00b10.0151, F1: 0.9900\u00b10.0145, AUC: 0.9999\u00b10.00006, Accuracy: 0.9905\u00b10.0025) and NCT-CRC-HE-100K datasets with similar high metrics. Grad-CAM visualizations enhanced interpretability.", "conclusion": "MSRANetV2 is validated as a reliable, interpretable, and high-performing architectural model for classifying colorectal cancer tissues, demonstrating strong potential for improving diagnostic precision in digital pathology."}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents is a multi-agent system that embeds Predictability-Computability-Stability (PCS) principles into LLM-driven data science workflows, outperforming existing end-to-end systems like AutoKaggle and DataInterpreter.", "motivation": "Current LLM-driven data science systems rely solely on LLM reasoning without scientific guidance, limiting trustworthiness and robustness with real-world noisy datasets.", "method": "Multi-agent system implementing modular workflow (data cleaning, feature engineering, modeling, evaluation) guided by PCS principles, incorporating perturbation analysis, unit testing, and model validation.", "result": "Outperformed AutoKaggle and DataInterpreter on nine diverse datasets using DeepSeek-V3 and GPT-4o backends.", "conclusion": "Successfully demonstrates feasibility of embedding PCS principles into LLM-driven data science automation to improve system performance and trustworthiness."}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "A systematic framework for autonomous driving scene understanding using Vision-Language Models, featuring task-specific prompting, visual assembly, and optimized inference parameters, achieving over 70% accuracy on both clean and corrupted data.", "motivation": "To enhance Vision-Language Models' performance on safety-critical autonomous driving tasks by addressing challenges in perception, prediction, planning, and corruption detection across diverse question types.", "method": "Four-component framework: 1) Mixture-of-Prompts router for question classification, 2) Task-specific prompts with coordinate systems and reasoning techniques, 3) Visual assembly module for multi-view image composition, 4) Optimized inference parameters per task.", "result": "Achieved 70.87% average accuracy on Phase-1 (clean data) and 72.85% on Phase-2 (corrupted data) using Qwen2.5-VL-72B model.", "conclusion": "Structured prompting and spatial grounding significantly improve VLM performance on autonomous driving tasks, demonstrating the effectiveness of systematic framework design for safety-critical applications."}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "GenTrack is a novel multi-object tracking method that combines stochastic and deterministic approaches to handle varying numbers of targets, using PSO with fitness measures and social interactions to improve tracking accuracy and reduce ID switches.", "motivation": "To address challenges in multi-object tracking including unknown and time-varying numbers of targets, maintaining target identity consistency, handling nonlinear dynamics, and improving performance with weak/noisy detectors during occlusions.", "method": "Hybrid tracking using stochastic and deterministic approaches, PSO with fitness measures to guide particles, integration of social interactions among targets, comprehensive state and observation model with space consistency, appearance, detection confidence, track penalties, and social scores.", "result": "Superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with reduced ID switches and track loss especially during occlusions.", "conclusion": "GenTrack provides an effective solution for robust multi-object tracking with publicly available source code, offering three variants for flexible implementation and setting a new baseline for visual MOT systems."}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "The paper introduces Brain-like Space, a unified geometric framework that maps AI models' intrinsic spatial attention organization to human brain networks, enabling comparison across different modalities and revealing a continuous arc-shaped geometry of brain-likeness.", "motivation": "To understand whether artificial neural networks organize information similarly to the human brain, and to create a common ground for comparing AI models across different modalities (vision, language, multimodal) that is not bound to specific inputs or tasks.", "method": "Developed Brain-like Space concept by mapping AI models' intrinsic spatial attention topological organization onto canonical human functional brain networks. Analyzed 151 Transformer-based models including large vision models, large language models, and large multimodal models.", "result": "Uncovered a continuous arc-shaped geometry reflecting gradual increase in brain-likeness. Found that brain-likeness is shaped by pretraining paradigm (global semantic abstraction emphasis) and positional encoding schemes (facilitating cross-modal fusion), not just modality. Brain-likeness and downstream task performance are not identical.", "conclusion": "Brain-like Space provides the first unified framework for situating, quantifying, and comparing intelligence across domains, revealing deep organizational principles that bridge machines and the brain."}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "UAP-SAM2 is a universal adversarial attack method designed to target SAM2 (Segment Anything Model 2) by addressing two key challenges: prompt dependency and semantic entanglement across frames, achieving superior performance over existing attacks.", "motivation": "SAM2's robustness against adversarial attacks remains unexplored, and existing attacks on SAM may not transfer effectively due to architectural differences in SAM2, particularly regarding prompt guidance and frame-to-frame semantic consistency.", "method": "The method uses a target-scanning strategy to divide frames into regions with random prompts to reduce prompt dependency, and a dual semantic deviation framework that distorts semantics within frames and disrupts consistency across consecutive frames to optimize a Universal Adversarial Perturbation (UAP).", "result": "Extensive experiments on six datasets across two segmentation tasks show that UAP-SAM2 significantly outperforms state-of-the-art attacks by a large margin, demonstrating its effectiveness against SAM2.", "conclusion": "UAP-SAM2 successfully addresses the unique challenges of attacking SAM2, providing a robust universal adversarial attack method that effectively compromises SAM2's segmentation capabilities across various prompts and frames."}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "A visual multi-object tracking method combining stochastic particle filtering with deterministic association for consistent tracking under nonlinear dynamics and varying target numbers, with particle swarm optimization for better sampling and novel identity preservation during occlusions.", "motivation": "To address the challenges of maintaining identifier consistency in multi-object tracking with unknown and time-varying target numbers under nonlinear dynamics, where traditional methods struggle with track divergence during occlusions and interactions.", "method": "Joint stochastic-deterministic approach: particle filter with PSO optimization using motion consistency, appearance similarity, and social-interaction cues; deterministic association with cost matrix considering spatial consistency, detection confidence, and track penalties; smooth state updating with identity preservation; velocity regression for trend-seed velocities.", "result": "Experimental results show superior performance compared to state-of-the-art trackers, with the method working effectively for both pre-recorded videos and live camera streams.", "conclusion": "The proposed hybrid approach successfully maintains identifier consistency in challenging tracking scenarios with nonlinear dynamics and varying target numbers, demonstrating improved performance over existing methods while being flexible for different video sources."}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "Proposes a multi-agent AI ecosystem for N-of-1 medical decision support that moves beyond average patient models to provide personalized, transparent care for individual patients including those with rare conditions or underrepresented demographics.", "motivation": "Current AI systems in medicine serve the average patient but fail at the margins - patients with rare variants, multimorbidity, or underrepresented demographics. This 'average patient fallacy' erodes equity and trust in medical AI.", "method": "A multi-agent ecosystem where agents clustered by organ systems, patient populations, and analytic modalities use shared models and evidence synthesis tools. Results converge in a coordination layer that weighs reliability, uncertainty, and data density to create decision-support packets with risk estimates, confidence ranges, outlier flags, and linked evidence.", "result": "Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in small groups, and risk-coverage trade-offs. The system provides transparent, equitable care centered on individual patients.", "conclusion": "By moving from monolithic models to orchestrated intelligence, this approach aligns medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual, addressing computational demands, automation bias, and regulatory challenges through caching, consensus checks, and adaptive trials."}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "CLFSeg is an encoder-decoder framework that combines fuzzy logic with convolutional layers to improve polyp and cardiac segmentation, addressing uncertainty and boundary ambiguity while maintaining computational efficiency.", "motivation": "Traditional CNN models have limited generalizability, robustness, and inability to handle uncertainty in medical image segmentation, which affects performance for early cancer detection and treatment planning.", "method": "Proposes CLFSeg framework with Fuzzy-Convolutional (FC) module that aggregates convolutional layers and fuzzy logic to identify local/global features while minimizing uncertainty and noise. Uses binary cross-entropy with dice loss to handle class imbalance and focus on tiny/boundary regions.", "result": "Exceptional performance on four public datasets (CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, ACDC), surpassing existing SOTA methods and effectively focusing on relevant anatomical regions of interest.", "conclusion": "CLFSeg improves segmentation performance while ensuring computational efficiency, making it a potential solution for real-world medical diagnostic scenarios."}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "This paper addresses challenges in training Spiking Neural Networks (SNNs) for robotic control by analyzing surrogate gradient slopes and proposing a novel training approach using privileged guiding policies, achieving significant performance improvements in real-world drone control tasks.", "motivation": "SNNs offer energy efficiency for robotics but face challenges: non-differentiable spiking neurons require surrogate gradients with unclear optimization properties, and stateful dynamics require sequence training hindered by limited sequence lengths in reinforcement learning.", "method": "Systematically analyzed surrogate gradient slope settings and proposed a novel training approach using privileged guiding policies to bootstrap learning while exploiting online environment interactions with spiking policies, combined with adaptive slope schedules.", "result": "Shallower slopes or scheduled slopes led to 2.1x improvement in RL performance. The combined method achieved 400 points average return in drone position control, substantially outperforming prior techniques like Behavioral Cloning and TD3BC which achieved at most -200 points.", "conclusion": "This work advances both theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers, demonstrating successful application in real-world robotic systems."}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards are a machine-readable standard for defining operational, regulatory, and ethical constraints that AI agents must follow at runtime, enabling verifiable compliance and accountable autonomy.", "motivation": "To address the need for practical mechanisms that integrate AI governance with engineering practice, ensuring autonomous agents follow required constraints and enabling distributed assurance in multi-agent ecosystems.", "method": "Create a deployment-layer standard that defines a normative layer with allow/deny rules, obligations, evidentiary requirements, and mappings to assurance frameworks like NIST AI RMF, ISO/IEC 42001, and EU AI Act.", "result": "Policy Cards can be automatically validated, version-controlled, and linked to runtime enforcement or continuous-audit pipelines, forming a foundation for distributed assurance.", "conclusion": "Policy Cards provide a practical solution for verifiable compliance in autonomous agents, bridging high-level governance with hands-on engineering to enable accountable autonomy at scale."}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "MC-SJD is a training-free, lossless parallel decoding framework that accelerates autoregressive visual generation by extending Speculative Jacobi Decoding (SJD) with coupling-based token sampling to improve acceptance rates.", "motivation": "Autoregressive modeling for visual generation suffers from slow inference speed due to per-token generation requiring thousands of steps, limiting practical adoption despite its strong generation capabilities.", "method": "Extends SJD with MC-SJD, an information-theoretic approach using coupling to maximize probability of sampling identical draft tokens across iterations while preserving lossless property, requiring only single-line modification to existing algorithm.", "result": "Achieves up to ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, with no degradation in output quality.", "conclusion": "MC-SJD provides substantial performance gains for AR visual generation through simple yet effective coupling-based approach that maintains lossless decoding while dramatically improving inference speed."}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion is a novel LLM reasoning framework that enables dependency-aware query decomposition and logic-parallel content expansion to achieve both high efficiency and quality for real-time web applications.", "motivation": "Current LLM reasoning struggles to meet the dual requirements of high-quality complex reasoning and low-latency/high-throughput for interactive web services, creating a critical bottleneck.", "method": "Orion decomposes queries into two phases: key point generation (retrieval-augmented few-shot prompting) and content parallel expansion (concurrent elaboration based on dependency graphs), with pipeline scheduling for cross-query parallelism.", "result": "Orion achieves up to 4.33x higher token generation speed, 3.42x lower answer latency, and improves reasoning quality by up to 18.75% compared to baselines.", "conclusion": "Orion successfully addresses the LLM reasoning bottleneck for web applications by enabling efficient parallel processing while maintaining logical consistency and improving reasoning quality."}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "ID\u00b2Face is a training-centric face anonymization framework that uses a conditional diffusion model with identity-masked learning to disentangle identity from non-identity attributes, enabling direct anonymization without inference-time optimization.", "motivation": "Existing diffusion-based anonymization methods rely on inference-time interventions that cause distribution shifts and entangle identity with non-identity attributes, degrading visual quality and data utility.", "method": "Uses a conditional diffusion model with identity-masked learning, an Identity-Decoupled Latent Recomposer with Identity VAE, bidirectional latent alignment, and an Identity-Guided Latent Harmonizer with soft-gating. Includes Orthogonal Identity Mapping to suppress identity leakage.", "result": "ID\u00b2Face outperforms existing methods in visual quality, identity suppression, and utility preservation.", "conclusion": "The proposed training-centric approach effectively disentangles identity and non-identity features, enabling high-quality anonymization without inference-time optimization."}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "APTBench is a new benchmark framework that converts real-world agent tasks into multiple-choice/text completion questions to evaluate agentic capabilities during LLM pre-training, focusing on planning and action abilities in software engineering and deep research scenarios.", "motivation": "Current pre-training benchmarks focus on isolated static skills and fail to assess agentic capabilities, while agent benchmarks require multi-turn execution that base models can't handle, creating a gap in evaluating agentic potential during pre-training.", "method": "APTBench converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models, focusing on core agentic abilities like planning and action across software engineering and deep research scenarios.", "result": "APTBench provides more predictive signals of downstream agent performance compared to general-purpose benchmarks, while being significantly more lightweight and cost-effective than full-scale end-to-end agent evaluations after post-training.", "conclusion": "APTBench addresses the critical need for evaluating agentic potential during pre-training, offering a practical framework to guide model training more effectively by bridging the gap between static skill benchmarks and complex agent evaluations."}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "SCOPE is a novel visual token pruning method for MLLMs that jointly optimizes saliency and coverage to preserve semantic completeness while reducing computational overhead.", "motivation": "Existing visual token pruning methods focus only on saliency (attention scores), leading to semantic incompleteness in selected tokens despite computational savings.", "method": "Proposes SCOPE score that integrates saliency with token-coverage gain, iteratively selecting tokens based on both their importance and ability to cover semantic relationships with other tokens.", "result": "Extensive experiments on vision-language benchmarks using LLaVA-1.5 and LLaVA-Next show consistent outperformance over prior pruning approaches.", "conclusion": "SCOPE effectively balances computational efficiency with semantic preservation by considering both saliency and coverage in visual token selection."}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "MobileRisk-Live is a dynamic sandbox environment with safety detection benchmark for mobile agents, and OS-Sentinel is a hybrid framework combining formal verification and VLM-based contextual judgment to detect safety risks.", "motivation": "Vision-Language Model (VLM) agents operating mobile environments pose potential safety risks like system compromise and privacy leakage, but detecting these risks across complex mobile operational spaces remains underexplored.", "method": "Introduced MobileRisk-Live benchmark with realistic trajectories and fine-grained annotations, and proposed OS-Sentinel framework that combines Formal Verifier for system-level violations with VLM-based Contextual Judge for contextual risk assessment.", "result": "OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics, demonstrating superior safety detection performance.", "conclusion": "The work establishes foundation for mobile agent safety research and provides critical insights for developing safer autonomous mobile agents."}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "This paper introduces the first event-based microsaccade dataset using simulated eye movements and evaluates spiking neural networks for microsaccade classification, achieving ~90% accuracy.", "motivation": "Traditional microsaccade studies using eye trackers or frame-based analysis are costly, limited in scalability and temporal resolution. Event-based sensing offers high-speed, low-latency alternative for capturing fine-grained spatiotemporal changes.", "method": "Used Blender to render high-fidelity eye movement scenarios simulating microsaccades (0.5-2.0\u00b0 angular displacement, 7 classes), converted to event streams using v2e. Evaluated with Spiking-VGG11/13/16 models and proposed Spiking-VGG16Flow (optical-flow-enhanced variant) in SpikingJelly.", "result": "Models achieved around 90% average accuracy, successfully classifying microsaccades by angular displacement independent of event count or duration.", "conclusion": "Demonstrates potential of spiking neural networks for fine motion recognition and establishes benchmark for event-based vision research. Dataset, code, and trained models will be publicly available."}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "This study compares logical and abstract reasoning abilities of multiple LLMs against human performance using custom reasoning questions, revealing significant gaps in LLMs' deduction capabilities.", "motivation": "To evaluate whether LLMs truly understand information and can perform logical inferences beyond linguistic tasks, advancing AI reasoning capabilities.", "method": "Used eight custom-designed reasoning questions to test several LLMs (GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral, Perplexity, Sabi'a) and benchmarked results against human performance on the same tasks.", "result": "Revealed significant differences between LLM and human performance, indicating areas where LLMs struggle with deduction and logical reasoning.", "conclusion": "LLMs show limitations in logical and abstract reasoning compared to humans, highlighting the need for improved reasoning capabilities in artificial intelligence systems."}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "The paper proposes Lipschitz-regularized object detection (LROD) to address the functional mismatch between image restoration and detection networks, improving detection robustness in adverse conditions like haze and low light.", "motivation": "Traditional cascade frameworks that use image restoration as pre-processing for object detection suffer from functional mismatch, where restoration networks perform smooth transformations while detectors have discontinuous decision boundaries, causing instability and sensitivity to minor perturbations.", "method": "The authors analyze the functional differences through Lipschitz continuity and propose LROD framework that integrates image restoration directly into detector's feature learning. They implement this as LR-YOLO, which harmonizes Lipschitz continuity of both tasks during training.", "result": "Extensive experiments on haze and low-light benchmarks show that LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy compared to traditional approaches.", "conclusion": "The proposed Lipschitz-regularized framework effectively addresses the functional mismatch between restoration and detection networks, providing a more stable and robust solution for object detection in adverse conditions."}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "Law in Silico is an LLM-based agent framework that simulates legal societies with individual decision-making and institutional mechanisms, showing it can reproduce real-world crime trends and provide insights about legal system effectiveness.", "motivation": "Real-world legal experiments are costly or infeasible, so simulating legal societies with AI provides an effective alternative for legal theory verification and development, as well as supporting legal administration.", "method": "Introduces Law in Silico, an LLM-based agent framework for simulating legal scenarios with individual decision-making and institutional mechanisms of legislation, adjudication, and enforcement.", "result": "Experiments comparing simulated crime rates with real-world data show LLM-based agents can largely reproduce macro-level crime trends and provide insights aligning with real-world observations. Micro-level simulations reveal that well-functioning, transparent, and adaptive legal systems better protect vulnerable individuals' rights.", "conclusion": "LLM-based agents are effective for legal society simulation, capable of reproducing real-world crime patterns and demonstrating the importance of well-functioning legal systems for protecting vulnerable populations."}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "DeshadowMamba introduces a selective state space model (Mamba) with CrossGate modulation and ColorShift regularization for shadow removal, achieving state-of-the-art performance by preserving structural integrity and chromatic consistency.", "motivation": "Existing attention-based shadow removal models mix illumination cues from irrelevant regions, causing distorted structures and inconsistent colors. The authors revisit shadow removal from a sequence modeling perspective to address these limitations.", "method": "Proposed DeshadowMamba uses Mamba (selective state space model) with CrossGate directional modulation to inject shadow-aware similarity into input gates, and ColorShift regularization with contrastive learning to suppress color contamination.", "result": "Extensive experiments on public benchmarks show DeshadowMamba achieves state-of-the-art visual quality and strong quantitative performance in shadow removal.", "conclusion": "The proposed method successfully adapts sequence modeling to shadow removal requirements, achieving robust structural integrity and chromatic consistency through directional state transitions and color-aware regularization."}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "This paper introduces a pattern language with two architectural patterns for building actionable world models from structured data: DOM Transduction Pattern for simplifying web page complexity and Hypermedia Affordances Recognition Pattern for dynamically discovering web service capabilities.", "motivation": "Software agents need actionable world models from structured data like DOM and web service descriptions, but face challenges with HTML verbosity and static API integrations that prevent adaptation to evolving services.", "method": "Proposes two complementary architectural patterns: 1) DOM Transduction Pattern that distills verbose DOM into compact, task-relevant representations, and 2) Hypermedia Affordances Recognition Pattern that enables dynamic discovery and integration of unknown web service capabilities at runtime.", "result": "The patterns provide a robust framework for engineering agents that can efficiently construct and maintain accurate world models from structured data sources.", "conclusion": "This pattern language enables scalable, adaptive, and interoperable automation across the web and its extended resources by allowing agents to efficiently build and maintain actionable world models from complex structured data."}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "UtilGen is a utility-centric data augmentation framework that optimizes synthetic data generation for downstream tasks using task feedback, achieving 3.87% average accuracy improvement over SOTA methods.", "motivation": "Most data augmentation methods focus on visual quality (fidelity/diversity) but neglect task-specific requirements, which vary across different tasks and network architectures.", "method": "Uses a weight allocation network to evaluate task-specific utility of synthetic samples, then iteratively refines generation via dual-level optimization: model-level (tailors generative model) and instance-level (adjusts prompt embeddings and initial noise).", "result": "Achieved superior performance on 8 benchmark datasets with 3.87% average accuracy improvement over SOTA, producing more impactful and task-relevant synthetic data.", "conclusion": "The paradigm shift from visual characteristics-centric to task utility-centric data augmentation is effective, as demonstrated by UtilGen's consistent performance improvements across diverse datasets."}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "A training-free one-shot attribution method using image resynthesis to identify the source of synthetic images, outperforming existing techniques in few-shot scenarios.", "motivation": "Synthetic image source attribution is challenging in data scarcity conditions, requiring few-shot or zero-shot classification capabilities.", "method": "Generate a prompt describing the image, resynthesize it with all candidate sources, and attribute to the model producing the closest resynthesis in feature space.", "result": "The proposed resynthesis method outperforms state-of-the-art few-shot approaches when only a few training samples are available.", "conclusion": "The method is effective for synthetic image attribution in data-scarce conditions, and the new dataset provides a valuable benchmark for future few-shot and zero-shot methods."}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "Proposes a two-stage pipeline for cost-efficient in-context learning that reduces LLM dependency by using cross-task examples for pseudo-labeling and graph-based label propagation.", "motivation": "Collecting high-quality examples for new or challenging tasks is costly and labor-intensive, creating a need for more efficient ICL methods.", "method": "Two-stage pipeline: 1) Use cross-task examples to prompt LLM for pseudo-labeling small target dataset, 2) Graph-based label propagation to spread labels to remaining examples without additional LLM queries.", "result": "Experiments across five tasks show strong performance while significantly reducing labeling costs compared to traditional approaches.", "conclusion": "The proposed method effectively combines cross-task supervision flexibility with LLM-free propagation scalability for cost-efficient in-context learning."}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "ViPER is a self-bootstrapping framework that enhances fine-grained visual perception in VLMs through a coarse-to-fine progressive learning approach with self-critiquing and self-prediction, achieving significant performance gains while maintaining generalizability.", "motivation": "Address the critical bottleneck of limited fine-grained visual perception in Vision-Language Models (VLMs) caused by scarcity of high-quality data and limitations of existing methods like SFT (compromises general capabilities) and RFT (prioritizes textual reasoning over visual perception).", "method": "Two-stage task structuring visual perception as coarse-to-fine progressive process; ViPER framework with self-critiquing and self-prediction; synergistic integration of image-level and instance-level reconstruction with two-stage reinforcement learning strategy; closed-loop training paradigm using internally synthesized data.", "result": "Applied to Qwen2.5-VL family, produced Qwen-Viper series with average gain of 1.7% on seven comprehensive benchmarks across various tasks and up to 6.0% on fine-grained perception; consistently superior performance across different vision-language scenarios while maintaining generalizability.", "conclusion": "ViPER enables self-improvement in perceptual capabilities and provides concrete evidence for the reciprocal relationship between generation and understanding, representing a breakthrough for developing more autonomous and capable VLMs."}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "The paper proposes a data-centric paradigm for deploying Generative AI in healthcare, repositioning the medical data ecosystem as the foundational substrate to support GenAI systems through effective data processing pipelines and knowledge retrieval.", "motivation": "GenAI promises transformative opportunities in healthcare but requires understanding of what can and cannot be achieved. Current deployments need better integration with healthcare tasks and data management.", "method": "Proposes repositioning the medical data ecosystem as the foundational substrate for GenAI systems, with semantic vector search, contextual querying, and effective data processing pipelines to support both upstream model training and downstream clinical applications.", "result": "The approach enables GenAI-powered operations that supply foundation models with high-quality multimodal data for pretraining and fine-tuning, while serving as a knowledge retrieval backend for task-specific inference via agentic layers.", "conclusion": "The data-centric paradigm enables high-quality and effective deployment of GenAI for healthcare delivery by making the medical data ecosystem the core foundation for generative healthcare systems."}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "The paper explores prompt learning as an efficient adaptation strategy for few-shot remote sensing scene classification, demonstrating that it consistently outperforms traditional baselines like zero-shot CLIP and linear probes.", "motivation": "Remote sensing applications face challenges due to limited labeled data and high annotation costs across diverse domains. Direct application of vision-language models like CLIP is suboptimal due to domain gaps and need for task-specific semantic adaptation.", "method": "Systematically evaluated several prompt learning methods: Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. Compared against zero-shot CLIP with hand-crafted prompts and linear probe on frozen CLIP features.", "result": "Prompt learning consistently outperforms both baselines in few-shot scenarios across multiple benchmark datasets. Prompting with Self-Regulating Constraints achieved the most robust cross-domain performance in cross-dataset generalization tests.", "conclusion": "Prompt learning serves as a scalable and efficient solution for bridging domain gaps in satellite and aerial imagery, providing a strong foundation for future research in remote sensing applications."}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT is a novel data synthesis framework that generates high-quality multi-turn function calling training data through environment-API graph interactions, advanced tool-query synthesis, and guided iterative chains, enabling smaller models to achieve state-of-the-art performance on function calling benchmarks.", "motivation": "Existing data synthesis methods for function calling are insufficient for real-world environments due to challenges in targeted model training, tool architecture isolation, and multi-turn logical dependencies. High-quality multi-turn training data is critical for developing advanced AI systems' tool-using capabilities.", "method": "FunReason-MT employs three key components: 1) Environment-API Graph Interactions to collect diverse high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify complex query construction, and 3) Guided Iterative Chain for sophisticated chain-of-thought generation.", "result": "A 4B parameter model trained on FunReason-MT generated data achieved state-of-the-art performance on Berkeley Function-Calling Leaderboard (BFCLv3), outperforming most comparable-sized models and even closed-source models. Further improvements were demonstrated on BFCLv4.", "conclusion": "FunReason-MT provides a reliable and robust framework for generating high-quality multi-turn function calling data, enabling effective agentic learning and addressing the structural deficiencies in existing data synthesis methods."}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "A novel switching Dual-Student architecture with Loss-Aware Exponential Moving Average for semi-supervised medical image segmentation, outperforming state-of-the-art methods.", "motivation": "To overcome limitations in teacher-student frameworks caused by strong correlation and unreliable knowledge transfer between teacher and student networks in semi-supervised medical image segmentation.", "method": "Introduces a switching Dual-Student architecture that selects the most reliable student at each iteration, and a Loss-Aware Exponential Moving Average strategy to dynamically ensure the teacher absorbs meaningful information from students.", "result": "Extensively evaluated on 3D medical image segmentation datasets, the method outperforms state-of-the-art semi-supervised methods.", "conclusion": "The plug-and-play framework effectively improves segmentation accuracy under limited supervision by enhancing dual-student collaboration and preventing error reinforcement."}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "Foundation models (FMs) are transforming site-specific disease management in crops through vision-language models, enabling text-based symptom interpretation and interactive QA, with digital twins and reinforcement learning emerging for targeted spraying applications.", "motivation": "To advance site-specific disease management (SSDM) in crops by leveraging foundation models that can integrate visual and textual data, interpret symptoms, reason about symptom-management relationships, and support interactive systems for growers.", "method": "Reviewed approximately 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and analyzing their role in adaptive learning, reinforcement learning, and digital twin frameworks for targeted spraying.", "result": "Key findings: FMs are gaining traction with surging literature in 2023-24; VLMs outpace LLMs with 5-10x increase in publications; RL and AL are still nascent for smart spraying; digital twins with RL can simulate targeted spraying; addressing sim-to-real gap is critical; human-robot collaboration remains limited.", "conclusion": "Multi-modal FMs with real-time feedback will drive next-generation SSDM, with continued development needed in human-in-the-loop approaches and bridging the sim-to-real gap for practical field deployment."}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "W2-Net addresses the overlooked challenge in Referring Expression Counting (REC) where annotation points focus on class-representative locations, causing models to neglect attribute information. It introduces dual-query mechanism and Subclass Separable Matching to improve subclass discrimination and counting accuracy.", "motivation": "Current REC approaches suffer from annotation bias where points are placed on class-representative locations (e.g., heads), forcing models to focus on class-level features while ignoring attribute information from other visual regions (e.g., legs for 'walking'). This fundamental challenge has been overlooked.", "method": "W2-Net decouples REC into 'what to count' and 'where to see' via dual-query mechanism: what-to-count (w2c) queries localize objects, while where-to-see (w2s) queries seek attribute-specific visual regions. Also introduces Subclass Separable Matching (SSM) with repulsive force to enhance inter-subclass separability during label assignment.", "result": "Significantly outperforms state-of-the-art on REC-8K dataset: reduces counting error by 22.5% (validation) and 18.0% (test), and improves localization F1 by 7% and 8% respectively.", "conclusion": "W2-Net effectively addresses the annotation bias problem in REC by explicitly decoupling object localization and attribute feature extraction, achieving substantial improvements in both counting accuracy and localization precision through its dual-query mechanism and novel matching strategy."}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG is a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) to address multi-turn tool interactions, with experiments showing it provides a challenging benchmark and that graph-based rewards enhance RLVR training.", "motivation": "Most existing work on agentic tool use overlooks the complexity of multi-turn tool interactions, creating a gap in understanding and benchmarking these complex interactions.", "method": "Introduce OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) with controllable complexity, and propose a graph-based reward to enhance RLVR training.", "result": "The dataset presents a challenging but solvable benchmark, and the proposed graph-based reward is effective when combined with GRPO-style algorithms.", "conclusion": "Leveraging topological structure and data complexity is important for advancing multi-turn tool use capabilities in agentic systems."}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "StrokeSeg is a lightweight, modular framework that converts research-grade stroke lesion segmentation models into deployable clinical applications with equivalent performance to original PyTorch pipelines.", "motivation": "Deep learning frameworks like nnU-Net achieve state-of-the-art performance but are difficult to deploy clinically due to heavy dependencies and monolithic design.", "method": "Decouples preprocessing, inference, and postprocessing: preprocessing uses Anima toolbox with BIDS-compliant outputs, inference uses ONNX Runtime with Float16 quantization (reducing model size by ~50%), and provides both GUI and CLI interfaces distributed as Python scripts and standalone Windows executable.", "result": "On 300 sub-acute and chronic stroke subjects, segmentation performance was equivalent to original PyTorch pipeline (Dice difference <10^-3).", "conclusion": "High-performing research pipelines can be successfully transformed into portable, clinically usable tools while maintaining equivalent performance."}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "A framework that integrates tool and document knowledge graphs to improve exemplar artifact generation through dependency modeling and deep-sparse integration.", "motivation": "To enhance exemplar artifact generation by uncovering and exploiting dependencies among tools and documents, addressing the need for better tool-augmented reasoning and planning.", "method": "Constructs tool knowledge graph from tool schemas using DeepResearch-inspired analysis, derives complementary knowledge graph from internal documents/SOPs, fuses both graphs, and uses deep-sparse integration to align structural tool dependencies with procedural knowledge.", "result": "Experiments show the unified framework effectively models tool interactions and improves plan generation.", "conclusion": "Linking tool graphs with domain knowledge graphs provides significant benefits for tool-augmented reasoning and planning."}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "Proposes MLSN, a luminance-aware multi-scale network for polarization image fusion that handles complex lighting environments through dynamic luminance injection, global-local feature fusion, and brightness enhancement.", "motivation": "Polarization image fusion combines S0 and DOLP images to reveal surface properties, but faces challenges with inherent contrast differences in polarized images and complex luminance environments.", "method": "Uses multi-scale spatial weight matrix with brightness branch for dynamic luminance injection, windowed self-attention for global-local feature fusion, and Brightness-Enhancement module for nonlinear luminance correction.", "result": "Outperforms state-of-the-art methods on MSP, PIF and GAND datasets with MS-SSIM and SD metrics higher than average by 8.57%-63.53%. Also introduces MSP dataset with 1000 polarized image pairs.", "conclusion": "MLSN effectively handles complex lighting in polarization image fusion through luminance-aware architecture and achieves superior performance compared to existing methods."}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "This paper systematically studies how radiology reports can be used during pre-training and fine-tuning to improve medical image classification, finding that text-based pre-training helps when labels are well-represented in text but can be detrimental otherwise, and that fine-tuning with reports can provide significant benefits.", "motivation": "Radiology reports contain rich expert annotations but require manual work from radiologists. The paper aims to determine when and how these reports can be leveraged during training to improve image-only classification, especially for tasks where labels are weakly associated with text.", "method": "Conducted systematic study of using radiology reports during both pre-training and fine-tuning across diagnostic and prognostic tasks (e.g., 12-month readmission), varying training set sizes. Compared explicit image-text alignment approaches.", "result": "Found that: (1) Pre-training with reports helps when labels are well-represented in text but can be detrimental otherwise; (2) Fine-tuning with reports can lead to significant improvements and sometimes has larger impact than pre-training method.", "conclusion": "Provides actionable insights into when and how to leverage privileged text data for medical image classifiers, highlighting gaps in current research and showing that fine-tuning with reports can be more impactful than pre-training in certain settings."}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "REFLECT, a flow-based generative model, is evaluated for unsupervised detection of focal and non-lesional abnormalities in post-stroke patients, showing that training on healthy controls improves detection performance compared to training on lesion-free stroke patient data.", "motivation": "Post-stroke MRI reveals secondary structural changes like atrophy and ventricular enlargement that are poorly captured by supervised segmentation methods but serve as important imaging biomarkers for recovery and outcome.", "method": "Used REFLECT, a flow-based generative model, for unsupervised anomaly detection. Trained two models: one on lesion-free slices from stroke patients (ATLAS) and another on healthy controls (IXI). Performance was evaluated using dual-expert annotations and Free-Response ROC analysis for anomaly maps.", "result": "The IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43) compared to the ATLAS-trained model.", "conclusion": "Training on fully healthy anatomy improves the modeling of normal variability, enabling broader and more reliable detection of structural abnormalities in post-stroke patients."}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "A sensor-free approach using satellite imagery and machine learning to monitor reservoir volume in Lebanon, achieving over 95% accuracy in water segmentation and less than 1.5% volume estimation error.", "motivation": "Sustainable management of Qaraaoun Reservoir requires reliable monitoring despite frequent sensor malfunctions and limited maintenance capacity in Lebanon.", "method": "Integrates open-source satellite imagery (Sentinel-2, Landsat), a new water segmentation index, and Support Vector Regression machine learning to estimate reservoir volume from surface area without ground measurements.", "result": "Water segmentation aligns with ground truth for >95% of shoreline; SVR model achieves <1.5% error of full capacity and R\u00b2 >0.98; generates 50 years of time-series data.", "conclusion": "The method provides robust, cost-effective, sensor-independent monitoring that can be replicated for other water bodies and supports climate change research."}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "This paper introduces a comprehensive evaluation framework for explainable AI (XAI) methods in semantic segmentation, addressing the gap in evaluation strategies for this specific computer vision task.", "motivation": "There's a need for transparency and trust in AI models, especially in safety-critical domains. While XAI evaluation has progressed for classification tasks, evaluation strategies for semantic segmentation remain underexplored despite its spatial and contextual complexities.", "method": "The authors propose a systematic evaluation framework specifically designed for XAI in semantic segmentation, using pixel-level evaluation strategies and carefully designed metrics to account for spatial and contextual task complexities.", "result": "Simulation results using class activation mapping (CAM)-based XAI schemes demonstrate the efficiency, robustness, and reliability of the proposed methodology.", "conclusion": "The framework contributes to advancing transparent, trustworthy, and accountable semantic segmentation models by providing fine-grained interpretability insights."}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "DCIC-sgp is a learned image compression framework that uses self-generated priors to separate global structures from local textures, achieving better rate-distortion performance and reducing geometric deformation artifacts at low bitrates.", "motivation": "Current learned image compression methods struggle to model complex correlation structures in natural images, particularly the entanglement of invariant global structures with transient local textures, leading to severe geometric deformation at low bitrates.", "method": "The framework uses functional decomposition with self-generated priors that first encode structural backbone information, then holistically modulate the entire compression pipeline. This deep conditioning allows the analysis transform to focus on residual high-entropy details, effectively disentangling information streams.", "result": "The method substantially mitigates geometric deformation artifacts at low bitrates and achieves significant BD-rate reductions of 14.4%, 15.7%, and 15.1% against VVC test model VTM-12.1 on Kodak, CLIC, and Tecnick datasets respectively.", "conclusion": "The deeply-conditioned approach with self-generated priors effectively disentangles information streams in image compression, leading to superior performance and reduced artifacts compared to conventional methods."}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "Video Diffusion Models (VDMs) pretrained on spatiotemporal data show better data efficiency and compositional understanding than LLMs in visual tasks, suggesting VDMs as promising visual foundation models.", "motivation": "LLMs have succeeded in language tasks with little supervision but struggle in the visual domain with compositional understanding and sample efficiency. The authors investigate if VDMs can bridge this gap.", "method": "The authors compare pretrained LLMs and VDMs equipped with lightweight adapters on various visual tasks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata.", "result": "VDMs demonstrate higher data efficiency than LLMs across all benchmarks, indicating stronger inductive biases for structure and dynamics from spatiotemporal pretraining.", "conclusion": "Video pretraining provides inductive biases that support progress toward effective visual foundation models, outperforming language-based approaches in visual reasoning tasks."}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "This paper proposes a deep learning-based system for detecting and segmenting three types of tea leaf diseases (Red Rust, Helopeltis, Red spider mite) using object detection models and custom damage area calculation.", "motivation": "To develop an automated system for identifying tea leaf diseases caused by pests and pathogens, and to quantify the damaged area on leaves for better disease management.", "method": "Evaluated SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for object detection, and used Mask R-CNN for instance segmentation with a custom method to calculate damaged leaf portions.", "result": "Faster R-CNN ResNet50 V1 performed better with 25% mAP compared to SSD MobileNet V2's 20.9% mAP. Both models showed low precision and recall values on IOU range 0.50:0.95.", "conclusion": "Faster R-CNN ResNet50 V1 is more effective for tea leaf disease detection than SSD MobileNet V2, and the custom segmentation method enables quantification of disease damage areas."}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo is a fully automatic, calibration-free pipeline for markerless motion capture from unsynchronized, uncalibrated consumer RGB cameras that simultaneously calibrates cameras and reconstructs 3D keypoints at metric scale with high accuracy and efficiency.", "motivation": "Existing calibration-free motion capture approaches suffer from high computational cost and reduced reconstruction accuracy, while traditional methods require precise camera calibration that limits accessibility for non-experts and in-the-wild captures.", "method": "Leverages 2D keypoints from off-the-shelf detectors with confidence-driven spatio-temporal keypoint sampling and graph-based global optimization to simultaneously calibrate cameras (including distortion coefficients) and reconstruct 3D keypoints and dense scene point maps. Introduces pairwise reprojection consensus score for reliability quantification.", "result": "Substantial improvements over prior methods: reduces camera translation error by 83-85%, camera angular error by 86-92%, and world mean-per-joint error by 83-91%. Processes multi-view sequences faster than their duration in specific configurations (36min for 1h20min footage).", "conclusion": "Kineo provides an efficient, accurate, and accessible solution for calibration-free motion capture that outperforms state-of-the-art methods while being practical for real-world applications, with open-source release to promote adoption."}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "Decoupled MeanFlow converts pretrained flow models into flow map models without architectural changes, enabling high-quality image generation in 1-4 steps with over 100x faster inference.", "motivation": "Existing denoising generative models require many steps due to discretization error, and flow map training typically requires incompatible architectural modifications with pretrained models.", "method": "A decoding strategy that conditions final blocks of diffusion transformers on subsequent timesteps, converting flow models to flow maps without architectural changes, combined with enhanced training techniques.", "result": "Achieved 1-step FID of 2.16 on ImageNet 256x256 and 2.12 on 512x512, surpassing prior art. With 4 steps, achieved FID of 1.51 and 1.68, nearly matching flow model performance.", "conclusion": "Training flow models first then converting them to flow maps is more efficient than training flow maps from scratch, enabling high-quality fast sampling with minimal architectural overhead."}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "NeuralRTI improves reflectance transformation imaging quality but is computationally expensive. DisK-NeuralRTI uses knowledge distillation to reduce computational costs while maintaining quality.", "motivation": "Traditional RTI methods (PTM, HSH) struggle with complex reflectance fields using few coefficients, causing artifacts. NeuralRTI provides superior quality but is computationally expensive for interactive relighting on limited hardware.", "method": "Proposed DisK-NeuralRTI uses knowledge distillation to reduce computational costs of NeuralRTI while maintaining rendering quality, addressing the limitation of expensive custom decoder networks.", "result": "The method successfully reduces computational costs compared to NeuralRTI while preserving rendering quality, making interactive relighting feasible on limited hardware.", "conclusion": "Knowledge distillation enables efficient NeuralRTI rendering without sacrificing quality, making high-quality reflectance transformation imaging more accessible."}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Latent Sketchpad is a framework that enhances Multimodal Large Language Models (MLLMs) by adding visual planning capabilities through internal visual scratchpads, enabling models to interleave textual reasoning with visual latent generation.", "motivation": "MLLMs excel at visual understanding but struggle with complex scenarios requiring visual planning and imagination. Inspired by human sketching as visual thinking, the framework aims to equip MLLMs with generative visual thought capabilities without compromising reasoning ability.", "method": "Integrates visual generation directly into MLLMs' native autoregressive reasoning process using two components: Context-Aware Vision Head for autoregressive visual representation production, and pretrained Sketch Decoder for rendering visual latents into human-interpretable images.", "result": "Experiments on MazePlanning dataset show Latent Sketchpad delivers comparable or superior reasoning performance to backbone MLLMs, and generalizes across distinct frontier MLLMs including Gemma3 and Qwen2.5-VL.", "conclusion": "By extending textual reasoning to visual thinking, the framework opens new opportunities for richer human-computer interaction and broader applications in multimodal AI."}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "OSWorld-MCP is the first comprehensive benchmark for evaluating multimodal agents' tool invocation, GUI operation, and decision-making abilities in real-world computer environments, addressing the gap in fair assessment of tool usage capabilities.", "motivation": "Past evaluations focused mainly on GUI interaction skills while overlooking tool invocation abilities enabled by Model Context Protocol (MCP), creating unfair comparisons between agents with integrated tools and those evaluated only on GUI interaction.", "method": "Developed an automated code-generation pipeline to create tools and combined them with curated existing tools, resulting in 158 high-quality tools across 7 common applications that were manually validated for functionality, applicability, and versatility.", "result": "MCP tools improved task success rates (e.g., from 8.3% to 20.4% for OpenAI o3, from 40.1% to 43.3% for Claude 4 Sonnet), but even the strongest models had relatively low tool invocation rates (only 36.3%), indicating significant room for improvement.", "conclusion": "OSWorld-MCP provides a fair and comprehensive benchmark that deepens understanding of multimodal agents' tool usage capabilities and sets a new standard for evaluating performance in complex, tool-assisted environments."}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "A deep learning method using Kolmogorov-Arnold Networks with Gaussian Radial Basis Functions to correct scatter artifacts in cone-beam CT by modeling the rotational symmetry of scatter distribution in the projection domain.", "motivation": "CBCT suffers from scatter artifacts during data acquisition, which cause CT value bias and reduced tissue contrast, degrading diagnostic accuracy in reconstructed images.", "method": "The method models point scatter function using Gaussian RBFs embedded into KAN layers, leveraging rotational symmetry of scatter distribution in projection domain and KAN's nonlinear mapping capabilities for learning high-dimensional scatter features.", "result": "Experimental validation on synthetic and real-scan data shows the model effectively corrects scatter artifacts in reconstructed images and outperforms current methods in quantitative metrics.", "conclusion": "The proposed deep learning approach successfully addresses CBCT scatter artifacts by combining physical prior knowledge of scatter distribution with KAN's powerful function mapping capabilities, achieving superior correction performance."}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "Proposes a dual-branch CNN for face forgery detection using spatial and frequency domain features with adaptive fusion and unified loss function, achieving strong performance across multiple forgery types.", "motivation": "Address the growing threat of realistic face forgery techniques (face swapping, attribute editing, diffusion-based synthesis) used for misinformation, identity fraud, and defamation, highlighting the urgent need for robust detection methods.", "method": "Dual-branch CNN with RGB branch for semantic information and frequency branch for high-frequency artifacts, using channel attention for adaptive feature fusion and FSC Loss (focal loss + supervised contrastive loss + frequency center margin loss) for enhanced class separability.", "result": "Achieves strong performance on DiFF benchmark across four forgery types (text-to-image, image-to-image, face swap, face edit) and outperforms average human accuracy.", "conclusion": "The proposed method demonstrates effectiveness in detecting various face forgery types and has potential to contribute to safeguarding AI ecosystems against visual forgery attacks."}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "PathoGaze1.0 is a comprehensive behavioral dataset capturing pathologists' visual search and decision-making during cancer diagnosis from whole-slide images, including eye-tracking, mouse interactions, and diagnostic decisions from 19 pathologists interpreting 397 WSIs.", "motivation": "Pathologists' diagnostic accuracy averages only 70% and adding a second pathologist doesn't improve consistency. The field lacks behavioral data to explain diagnostic errors and inconsistencies in WSI interpretation.", "method": "Collected 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data from 19 pathologists interpreting 397 WSIs using an ecologically valid testbed called PTAH.", "result": "Recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. Created a comprehensive dataset that captures the full diagnostic workflow dynamics.", "conclusion": "PathoGaze1.0 provides valuable behavioral data that can help explain diagnostic errors and inconsistencies, and could improve training for both pathologists and AI systems supporting human experts."}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "Proposes Group Relative Attention Guidance (GRAG), a method to enable continuous and fine-grained control over editing intensity in Diffusion-in-Transformer models by reweighting token delta values, achieving smoother control than Classifier-Free Guidance.", "motivation": "Existing image editing methods based on Diffusion-in-Transformer models lack effective control over editing degree, limiting customization capabilities.", "method": "Analyzes MM-Attention mechanism in DiT models, identifies bias vectors as inherent editing behavior, and proposes GRAG to reweight token delta values to modulate model focus between input image and editing instruction.", "result": "GRAG can be integrated with minimal code (4 lines), consistently enhances editing quality, and provides smoother and more precise control over editing degree compared to Classifier-Free Guidance.", "conclusion": "GRAG enables continuous and fine-grained control over editing intensity without tuning, improving editing quality and control precision in Diffusion-in-Transformer based image editing."}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "SAGE is a zero-shot method for video transitions that combines structural guidance with generative synthesis to create smooth, semantically consistent transitions between diverse clips without fine-tuning.", "motivation": "Current video transition methods struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, creating a need for content-aware and visually coherent transitions.", "method": "SAGE combines structural guidance (line maps and motion flow) with generative synthesis in a zero-shot approach, drawing on artistic workflows like aligning silhouettes and interpolating salient features.", "result": "Extensive experiments show SAGE outperforms both classical and generative baselines (FILM, TVG, DiffMorpher, VACE, GI) on quantitative metrics and user studies for transitions between diverse clips.", "conclusion": "SAGE provides an effective zero-shot solution for creating smooth, semantically consistent video transitions that bridge diverse clips with large temporal or semantic gaps."}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "MIC-BEV is a Transformer-based BEV perception framework for infrastructure-based multi-camera 3D object detection that supports variable cameras and shows strong robustness under sensor degradation.", "motivation": "Existing camera-based detection models underperform in infrastructure scenarios due to multi-view setups, diverse camera configurations, degraded visual inputs, and various road layouts.", "method": "Uses a graph-enhanced fusion module to integrate multi-view image features into BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. Also introduces M2I synthetic dataset for training.", "result": "Achieves state-of-the-art performance in 3D object detection on both M2I and RoScenes datasets, with strong robustness under challenging conditions including extreme weather and sensor degradation.", "conclusion": "MIC-BEV demonstrates potential for real-world deployment in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy."}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "Vision Transformers naturally develop object binding capabilities during self-supervised pretraining, with over 90% accuracy in determining whether two patches belong to the same object, particularly in models like DINO, MAE, and CLIP.", "motivation": "To investigate whether object binding - the brain's ability to group features into coherent objects - naturally emerges in pre-trained Vision Transformers, challenging the view that ViTs lack this capability.", "method": "Used similarity probes to decode 'IsSameObject' property from patch embeddings across ViT layers, comparing self-supervised (DINO, MAE, CLIP) and ImageNet-supervised models, and performed ablation studies.", "result": "Object binding emerges reliably in self-supervised ViTs with over 90% accuracy, but is markedly weaker in ImageNet-supervised models. IsSameObject is encoded in a low-dimensional subspace and actively guides attention.", "conclusion": "Object binding is not an architectural artifact but an ability acquired through specific pretraining objectives, challenging the view that ViTs lack object binding and showing symbolic knowledge emerges naturally in connectionist systems."}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "ProMoE is a Mixture-of-Experts framework for Diffusion Transformers that uses a two-step router with explicit routing guidance to address the limitations of applying MoE to visual tokens, achieving state-of-the-art performance on ImageNet.", "motivation": "Existing attempts to apply Mixture-of-Experts (MoE) to Diffusion Transformers (DiTs) have yielded limited gains due to fundamental differences between language and visual tokens. Visual tokens exhibit spatial redundancy and functional heterogeneity, which hinders expert specialization in vision MoE.", "method": "ProMoE features a two-step router with explicit routing guidance: (1) conditional routing partitions image tokens into conditional and unconditional sets based on functional roles, and (2) prototypical routing refines assignments of conditional tokens using learnable prototypes based on semantic content. It also includes a routing contrastive loss to enhance intra-expert coherence and inter-expert diversity.", "result": "Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives.", "conclusion": "The proposed ProMoE framework effectively addresses the challenges of applying MoE to vision tasks by providing explicit semantic guidance through prototypical routing, which is crucial for vision MoE and enables superior performance compared to existing methods."}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "URSA is a discrete generative modeling framework that bridges the performance gap with continuous approaches for scalable video generation through iterative global refinement of discrete tokens with specialized designs for efficiency.", "motivation": "Discrete approaches for video generation lag behind continuous-space methods due to error accumulation and long-context inconsistency issues.", "method": "URSA formulates video generation as iterative global refinement of discrete spatiotemporal tokens using Linearized Metric Path and Resolution-dependent Timestep Shifting mechanisms, plus asynchronous temporal fine-tuning for unified task handling.", "result": "URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods on challenging video and image generation benchmarks.", "conclusion": "URSA successfully bridges the performance gap between discrete and continuous approaches for scalable video generation, enabling efficient high-resolution synthesis and long-duration generation with fewer inference steps."}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "Generative View Stitching (GVS) enables collision-free camera-guided video generation by sampling entire sequences in parallel with future conditioning, overcoming limitations of autoregressive models that fail with predefined camera trajectories.", "motivation": "Autoregressive video diffusion models cannot incorporate future conditioning from predefined camera trajectories, leading to collisions and generation collapse when following complex camera paths.", "method": "Proposes GVS sampling algorithm that extends diffusion stitching methods to video generation, works with any off-the-shelf video model trained with Diffusion Forcing, and introduces Omni Guidance for temporal consistency and loop-closing mechanisms.", "result": "GVS achieves stable, collision-free, frame-to-frame consistent camera-guided video generation that successfully handles complex predefined camera paths including impossible geometries like the Impossible Staircase.", "conclusion": "GVS provides a practical solution for camera-guided video generation that maintains coherence across long sequences while faithfully following complex camera trajectories, demonstrating superior performance over autoregressive approaches."}}
