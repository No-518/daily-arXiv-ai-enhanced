<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.AI](#cs.AI) [Total: 34]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [iFlyBot-VLA Technical Report](https://arxiv.org/abs/2511.01914)
*Yuan Zhang,Chenyu Xue,Wenjie Xu,Chao Ji,Jiajia wu,Jia Pan*

Main category: cs.CV

TL;DR: iFlyBot-VLA is a Vision-Language-Action model that uses dual-level action representation (latent actions and discrete action tokens) trained on large-scale manipulation videos, achieving competitive performance on robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a VLA model that can effectively bridge language, vision, and action for robotic manipulation by aligning their representation spaces through dual supervision.

Method: Uses a latent action model trained on manipulation videos, dual-level action representation framework (latent actions + discrete action tokens), and mixed training strategy combining robot trajectory data with QA datasets.

Result: Demonstrates superiority on LIBERO Franka benchmark and achieves competitive success rates in real-world manipulation tasks across diverse and challenging scenarios.

Conclusion: The proposed framework successfully aligns language, vision, and action representations, enabling direct VLM contribution to action generation, with plans to open-source datasets for community research.

Abstract: We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model
trained under a novel framework. The main contributions are listed as follows:
(1) a latent action model thoroughly trained on large-scale human and robotic
manipulation videos; (2) a dual-level action representation framework that
jointly supervises both the Vision-Language Model (VLM) and the action expert
during training; (3) a mixed training strategy that combines robot trajectory
data with general QA and spatial QA datasets, effectively enhancing the 3D
perceptual and reasoning capabilities of the VLM backbone. Specifically, the
VLM is trained to predict two complementary forms of actions: latent actions,
derived from our latent action model pretrained on cross-embodiment
manipulation data, which capture implicit high-level intentions; and structured
discrete action tokens, obtained through frequency-domain transformations of
continuous control signals, which encode explicit low-level dynamics. This dual
supervision aligns the representation spaces of language, vision, and action,
enabling the VLM to directly contribute to action generation. Experimental
results on the LIBERO Franka benchmark demonstrate the superiority of our
frame-work, while real-world evaluations further show that iFlyBot-VLA achieves
competitive success rates across diverse and challenging manipulation tasks.
Furthermore, we plan to open-source a portion of our self-constructed dataset
to support future research in the community

</details>


### [2] [Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound](https://arxiv.org/abs/2511.01915)
*Edoardo Conti,Riccardo Rosati,Lorenzo Federici,Adriano Mancini,Maria Chiara Fiorentin*

Main category: cs.CV

TL;DR: First comprehensive evaluation of foundation models in fetal ultrasound imaging under low inter-class variability, showing domain-specific pretraining is essential for distinguishing anatomically similar fetal brain planes.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models like DINOv3 show transferability in medical domains, but their ability to discriminate anatomically similar structures with overlapping features hasn't been systematically investigated, particularly for fetal brain standard planes critical for biometric assessment.

Method: Curated and aggregated all public fetal ultrasound datasets into FetalUS-188K benchmark (188K+ images). DINOv3 pretrained self-supervised for ultrasound-aware representations, evaluated via linear probing and fine-tuning with two initialization schemes: fetal ultrasound pretraining vs natural-image initialization.

Result: Models pretrained on fetal ultrasound data consistently outperformed natural-image initialized models, with up to 20% F1-score improvement. Domain-adaptive pretraining preserved subtle echogenic and structural cues crucial for distinguishing intermediate planes.

Conclusion: Generic foundation models fail to generalize under low inter-class variability, while domain-specific pretraining is essential for achieving robust and clinically reliable representations in fetal brain ultrasound imaging.

Abstract: Purpose: This study provides the first comprehensive evaluation of foundation
models in fetal ultrasound (US) imaging under low inter-class variability
conditions. While recent vision foundation models such as DINOv3 have shown
remarkable transferability across medical domains, their ability to
discriminate anatomically similar structures has not been systematically
investigated. We address this gap by focusing on fetal brain standard
planes--transthalamic (TT), transventricular (TV), and transcerebellar
(TC)--which exhibit highly overlapping anatomical features and pose a critical
challenge for reliable biometric assessment.
  Methods: To ensure a fair and reproducible evaluation, all publicly available
fetal ultrasound datasets were curated and aggregated into a unified
multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated
images from heterogeneous acquisition settings. DINOv3 was pretrained in a
self-supervised manner to learn ultrasound-aware representations. The learned
features were then evaluated through standardized adaptation protocols,
including linear probing with frozen backbone and full fine-tuning, under two
initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization
from natural-image DINOv3 weights.
  Results: Models pretrained on fetal ultrasound data consistently outperformed
those initialized on natural images, with weighted F1-score improvements of up
to 20 percent. Domain-adaptive pretraining enabled the network to preserve
subtle echogenic and structural cues crucial for distinguishing intermediate
planes such as TV.
  Conclusion: Results demonstrate that generic foundation models fail to
generalize under low inter-class variability, whereas domain-specific
pretraining is essential to achieve robust and clinically reliable
representations in fetal brain ultrasound imaging.

</details>


### [3] [Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users](https://arxiv.org/abs/2511.01990)
*Saurabh Kaushik,Lalit Maurya,Elizabeth Tellman,ZhiJie Zhang*

Main category: cs.CV

TL;DR: Geo-Foundational Models (GFMs) show competitive performance for flood inundation mapping across different satellite sensors, with Clay emerging as the best performer due to its balance of accuracy, computational efficiency, and few-shot learning capabilities.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic comparison between GFMs and traditional models like U-Net for flood mapping across different sensors and data availability scenarios, making it difficult for end-users to select appropriate models.

Method: Evaluated three GFMs (Prithvi 2.0, Clay V1.5, DOFA) and UViT against traditional models (TransNorm, U-Net, Attention U-Net) using PlanetScope, Sentinel-1, and Sentinel-2 data. Conducted leave-one-region-out cross-validation across five regions and few-shot experiments.

Result: Clay outperformed others on PlanetScope (0.79 mIoU) and Sentinel-2 (0.70), while Prithvi led on Sentinel-1 (0.57). In cross-validation, Clay showed slightly better performance across all sensors. Clay achieved 0.64 mIoU with just five training images, significantly outperforming other GFMs. Clay is computationally efficient with 26M parameters, making it 3x faster than Prithvi.

Conclusion: GFMs offer small to moderate improvements in flood mapping accuracy at lower computational cost and labeling effort compared to traditional U-Net, with Clay being the recommended choice due to its balanced performance across accuracy, efficiency, and data requirements.

Abstract: Geo-Foundational Models (GFMs) enable fast and reliable extraction of
spatiotemporal information from satellite imagery, improving flood inundation
mapping by leveraging location and time embeddings. Despite their potential, it
remains unclear whether GFMs outperform traditional models like U-Net. A
systematic comparison across sensors and data availability scenarios is still
lacking, which is an essential step to guide end-users in model selection. To
address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a
Prithvi variant), against TransNorm, U-Net, and Attention U-Net using
PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance
among all GFMs, with only 2-5% variation between the best and worst models
across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and
Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In
leave-one-region-out cross-validation across five regions, Clay shows slightly
better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07),
0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA
(0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and
Sentinel-1, respectively. Across all 19 sites, leave-one-region-out
cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual
inspection highlights Clay's superior ability to retain fine details. Few-shot
experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training
images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational
time, Clay is a better choice due to its smaller model size (26M parameters),
making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M).
Contrary to previous findings, our results suggest GFMs offer small to moderate
improvements in flood mapping accuracy at lower computational cost and labeling
effort compared to traditional U-Net.

</details>


### [4] [Locally-Supervised Global Image Restoration](https://arxiv.org/abs/2511.01998)
*Benjamin Walder,Daniel Toader,Robert Nuster,Günther Paltauf,Peter Burgholzer,Gregor Langer,Lukas Krainer,Markus Haltmeier*

Main category: cs.CV

TL;DR: The paper presents a learning-based method for image reconstruction from incomplete measurements (upsampling and inpainting) that exploits multiple invariances of image distributions to achieve performance comparable to fully supervised approaches, with validation on photoacoustic microscopy upsampling.


<details>
  <summary>Details</summary>
Motivation: To address image reconstruction from incomplete measurements with fixed deterministic sampling patterns that have inherently incomplete coverage, overcoming limitations of conventional supervised methods that require fully sampled ground truth and self-supervised methods that rely on random sampling.

Method: A learning-based framework that exploits multiple invariances of the underlying image distribution, enabling reconstruction from fixed deterministic sampling patterns with incomplete coverage.

Result: Competitive or superior results to fully supervised approaches on optical-resolution image upsampling in photoacoustic microscopy, while requiring substantially less ground truth data.

Conclusion: The method successfully overcomes limitations of conventional approaches by leveraging multiple image distribution invariances, achieving high-quality reconstruction from incomplete measurements with reduced ground truth requirements.

Abstract: We address the problem of image reconstruction from incomplete measurements,
encompassing both upsampling and inpainting, within a learning-based framework.
Conventional supervised approaches require fully sampled ground truth data,
while self-supervised methods allow incomplete ground truth but typically rely
on random sampling that, in expectation, covers the entire image. In contrast,
we consider fixed, deterministic sampling patterns with inherently incomplete
coverage, even in expectation. To overcome this limitation, we exploit multiple
invariances of the underlying image distribution, which theoretically allows us
to achieve the same reconstruction performance as fully supervised approaches.
We validate our method on optical-resolution image upsampling in photoacoustic
microscopy (PAM), demonstrating competitive or superior results while requiring
substantially less ground truth data.

</details>


### [5] [Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2511.02014)
*Tuan Truong,Guillermo Jimenez Perez,Pedro Osorio,Matthias Lenga*

Main category: cs.CV

TL;DR: This paper benchmarks Large Multimodal Models (LMMs) for PHI detection in medical imaging, finding that while LMMs show superior OCR performance over traditional methods, this doesn't always translate to better overall PHI detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional PHI detection methods rely on OCR with named entity recognition, but recent LMM advancements offer opportunities for improved text extraction and semantic analysis in medical imaging privacy protection.

Method: Systematic benchmarking of three LMMs (GPT-4o, Gemini 2.5 Flash, Qwen 2.5 7B) using two pipeline configurations: text-only analysis and combined OCR+semantic analysis, comparing against conventional OCR models like EasyOCR.

Result: LMMs demonstrated superior OCR performance (WER: 0.03-0.05, CER: 0.02-0.03) but this didn't consistently improve overall PHI detection. Best gains were on complex imprint patterns, while readable text regions showed similar performance across pipelines.

Conclusion: The study provides empirical recommendations for LMM selection based on operational constraints and proposes a scalable, modular deployment strategy for PHI detection in medical imaging.

Abstract: The detection of Protected Health Information (PHI) in medical imaging is
critical for safeguarding patient privacy and ensuring compliance with
regulatory frameworks. Traditional detection methodologies predominantly
utilize Optical Character Recognition (OCR) models in conjunction with named
entity recognition. However, recent advancements in Large Multimodal Model
(LMM) present new opportunities for enhanced text extraction and semantic
analysis. In this study, we systematically benchmark three prominent closed and
open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing
two distinct pipeline configurations: one dedicated to text analysis alone and
another integrating both OCR and semantic analysis. Our results indicate that
LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to
conventional models like EasyOCR. However, this improvement in OCR performance
does not consistently correlate with enhanced overall PHI detection accuracy.
The strongest performance gains are observed on test cases with complex imprint
patterns. In scenarios where text regions are well readable with sufficient
contrast, and strong LMMs are employed for text analysis after OCR, different
pipeline configurations yield similar results. Furthermore, we provide
empirically grounded recommendations for LMM selection tailored to specific
operational constraints and propose a deployment strategy that leverages
scalable and modular infrastructure.

</details>


### [6] [StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities](https://arxiv.org/abs/2511.02027)
*Zeyu Yang,Clayton Souza Leite,Yu Xiao*

Main category: cs.CV

TL;DR: StrengthSense is an open dataset containing IMU signals from 11 strength-demanding activities and 2 non-strength activities, collected from 29 subjects using 10 IMUs, with video-annotated validation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive datasets capturing strength-demanding activities with wearable sensors, which are crucial for monitoring muscular strength, endurance, and power.

Method: Collected data from 29 healthy subjects using 10 IMUs placed on limbs and torso, annotated using video recordings as references. Conducted comparative analysis between IMU-estimated joint angles and video-extracted angles.

Result: Created a comprehensive dataset with technical validation showing accuracy and reliability of sensor data through comparative analysis with video data.

Conclusion: StrengthSense enables researchers and developers to advance human activity recognition algorithms and create fitness/health monitoring applications.

Abstract: Tracking strength-demanding activities with wearable sensors like IMUs is
crucial for monitoring muscular strength, endurance, and power. However, there
is a lack of comprehensive datasets capturing these activities. To fill this
gap, we introduce \textit{StrengthSense}, an open dataset that encompasses IMU
signals capturing 11 strength-demanding activities, such as sit-to-stand,
climbing stairs, and mopping. For comparative purposes, the dataset also
includes 2 non-strength demanding activities. The dataset was collected from 29
healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was
annotated using video recordings as references. This paper provides a
comprehensive overview of the data collection, pre-processing, and technical
validation. We conducted a comparative analysis between the joint angles
estimated by IMUs and those directly extracted from video to verify the
accuracy and reliability of the sensor data. Researchers and developers can
utilize \textit{StrengthSense} to advance the development of human activity
recognition algorithms, create fitness and health monitoring applications, and
more.

</details>


### [7] [Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis](https://arxiv.org/abs/2511.02046)
*Soham Joshi,Shwet Kamal Mishra,Viswanath Gopalakrishnan*

Main category: cs.CV

TL;DR: Proposes an automated pipeline for synthesizing large-scale text-VQA datasets using OCR, ROI detection, caption generation, and question generation models.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for text-VQA datasets is tedious and challenging; need for automated pipeline leveraging foundation models and mature OCR systems.

Method: End-to-end pipeline combining OCR detection/recognition, ROI detection, caption generation, and question generation models to synthesize QA pairs from scene text in images.

Result: Successfully created large-scale text-VQA dataset with ~72K QA pairs from ~44K images - first automated pipeline of its kind.

Conclusion: Proposed pipeline enables scalable, automated creation of faithful text-VQA datasets, addressing annotation challenges through systematic integration of vision-language models.

Abstract: Creation of large-scale databases for Visual Question Answering tasks
pertaining to the text data in a scene (text-VQA) involves skilful human
annotation, which is tedious and challenging. With the advent of foundation
models that handle vision and language modalities, and with the maturity of OCR
systems, it is the need of the hour to establish an end-to-end pipeline that
can synthesize Question-Answer (QA) pairs based on scene-text from a given
image. We propose a pipeline for automated synthesis for text-VQA dataset that
can produce faithful QA pairs, and which scales up with the availability of
scene text data. Our proposed method harnesses the capabilities of multiple
models and algorithms involving OCR detection and recognition (text spotting),
region of interest (ROI) detection, caption generation, and question
generation. These components are streamlined into a cohesive pipeline to
automate the synthesis and validation of QA pairs. To the best of our
knowledge, this is the first pipeline proposed to automatically synthesize and
validate a large-scale text-VQA dataset comprising around 72K QA pairs based on
around 44K images.

</details>


### [8] [Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study](https://arxiv.org/abs/2511.02086)
*Yue Yang,Fabian Necker,Christoph Leuze,Michelle Chen,Andrey Finegersh,Jake Lee,Vasu Divi,Bruce Daniel,Brian Hargreaves,Jie Ying Wu,Fred M Baik*

Main category: cs.CV

TL;DR: Developed a depth-only markerless AR registration pipeline for HoloLens 2 that achieved 3-4 mm median error in live surgical settings without fiducials, validated on small/low-curvature anatomies like feet, ears, and legs.


<details>
  <summary>Details</summary>
Motivation: To enable accurate augmented reality guidance in surgical settings without requiring fiducial markers, particularly for small or low-curvature anatomical targets where traditional methods may struggle.

Method: Used HoloLens 2 with AHAT depth tracking aligned to CT-derived skin meshes via depth-bias correction, human-in-the-loop initialization, and global-to-local registration. Validated with AR-tracked tools comparing skin-to-bone distances to CT ground truth.

Result: Preclinical validation showed median errors of 0.78-0.80 mm (RMSE 0.97-1.20 mm). Clinical trials achieved median per-point error of 3.9 mm across feet (3.2 mm), ear (4.3 mm), and lower leg (5.3 mm), with 5 mm coverage rates of 92-95%, 84-90%, and 72-86% respectively.

Conclusion: The depth-only markerless AR pipeline achieved clinically relevant accuracy approaching typical error thresholds for moderate-risk surgical tasks, demonstrating improved clinical readiness for markerless AR guidance through human-guided initialization and global-to-local registration.

Abstract: Purpose: In this paper, we develop and clinically evaluate a depth-only,
markerless augmented reality (AR) registration pipeline on a head-mounted
display, and assess accuracy across small or low-curvature anatomies in
real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd
Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i)
depth-bias correction, (ii) brief human-in-the-loop initialization, (iii)
global and local registration. We validated the surface-tracing error metric by
comparing "skin-to-bone" relative distances to CT ground truth on leg and foot
models, using an AR-tracked tool. We then performed seven intraoperative target
trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap
harvest and mandibular reconstruction surgery, and collected 500+ data per
trial. Results: Preclinical validation showed tight agreement between AR-traced
and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm,
1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by
anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm
coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed
significantly (Delta median ~1.1 mm; p < 0.001). Conclusion: A depth-only,
markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear,
and lower leg in live surgical settings without fiducials, approaching typical
clinical error thresholds for moderate-risk tasks. Human-guided initialization
plus global-to-local registration enabled accurate alignment on small or
low-curvature targets, improving the clinical readiness of markerless AR
guidance.

</details>


### [9] [From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera](https://arxiv.org/abs/2511.02142)
*Huahua Lin,Xiaohao Cai,Mark Nixon,James M. Mulqueeney,Thomas H. G. Ezard*

Main category: cs.CV

TL;DR: Proposes an automated pipeline combining instance segmentation and chamber ordering to reconstruct 3D growth trajectories of planktonic foraminifera from CT scans, reducing manual effort while maintaining biological accuracy.


<details>
  <summary>Details</summary>
Motivation: Automated tracing of foraminifera chamber growth is largely unexplored, with existing methods relying on time-consuming manual segmentation. Understanding their growth trajectory provides insights into environmental adaptation.

Method: End-to-end pipeline integrating instance segmentation (computer vision technique) with dedicated chamber ordering algorithm to reconstruct 3D growth trajectories from high-resolution CT scans. Evaluates multiple segmentation methods optimized for different chamber features.

Result: Pipeline substantially reduces manual effort while maintaining biologically meaningful accuracy. Chamber-ordering algorithm remains robust even with partial segmentation, achieving consistent reconstruction of developmental trajectories despite some under-segmentation in smaller chambers.

Conclusion: Provides first fully automated and reproducible pipeline for digital foraminiferal growth analysis, establishing foundation for large-scale, data-driven ecological studies.

Abstract: Planktonic foraminifera, marine protists characterized by their intricate
chambered shells, serve as valuable indicators of past and present
environmental conditions. Understanding their chamber growth trajectory
provides crucial insights into organismal development and ecological adaptation
under changing environments. However, automated tracing of chamber growth from
imaging data remains largely unexplored, with existing approaches relying
heavily on manual segmentation of each chamber, which is time-consuming and
subjective. In this study, we propose an end-to-end pipeline that integrates
instance segmentation, a computer vision technique not extensively explored in
foraminifera, with a dedicated chamber ordering algorithm to automatically
reconstruct three-dimensional growth trajectories from high-resolution computed
tomography scans. We quantitatively and qualitatively evaluate multiple
instance segmentation methods, each optimized for distinct spatial features of
the chambers, and examine their downstream influence on growth-order
reconstruction accuracy. Experimental results on expert-annotated datasets
demonstrate that the proposed pipeline substantially reduces manual effort
while maintaining biologically meaningful accuracy. Although segmentation
models exhibit under-segmentation in smaller chambers due to reduced voxel
fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm
remains robust, achieving consistent reconstruction of developmental
trajectories even under partial segmentation. This work provides the first
fully automated and reproducible pipeline for digital foraminiferal growth
analysis, establishing a foundation for large-scale, data-driven ecological
studies.

</details>


### [10] [Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis](https://arxiv.org/abs/2511.02144)
*Zhicheng Wang,Junbiao Pang*

Main category: cs.CV

TL;DR: A cascaded PCA and RPCA framework for accurate pavement crack width measurement that handles complex crack morphologies and enables rapid measurements from arbitrary pixel locations.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with complex, non-uniform crack boundaries and lack rapid measurement capabilities needed for comprehensive pavement condition assessment.

Method: Three-stage framework: (1) crack segmentation using detection algorithms, (2) PCA for orientation axis of quasi-parallel cracks, (3) RPCA for Main Propagation Axis of irregular cracks.

Result: Superior performance in computational efficiency and measurement accuracy compared to state-of-the-art techniques across three public datasets.

Conclusion: The proposed cascaded PCA-RPCA framework effectively addresses limitations in crack width quantification, providing accurate and efficient measurements for pavement condition evaluation.

Abstract: Accurate quantification of pavement crack width plays a pivotal role in
assessing structural integrity and guiding maintenance interventions. However,
achieving precise crack width measurements presents significant challenges due
to: (1) the complex, non-uniform morphology of crack boundaries, which limits
the efficacy of conventional approaches, and (2) the demand for rapid
measurement capabilities from arbitrary pixel locations to facilitate
comprehensive pavement condition evaluation. To overcome these limitations,
this study introduces a cascaded framework integrating Principal Component
Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from
digital images. The proposed methodology comprises three sequential stages: (1)
initial crack segmentation using established detection algorithms to generate a
binary representation, (2) determination of the primary orientation axis for
quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation
Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations
were conducted across three publicly available datasets, demonstrating that the
proposed approach achieves superior performance in both computational
efficiency and measurement accuracy compared to existing state-of-the-art
techniques.

</details>


### [11] [Autobiasing Event Cameras for Flickering Mitigation](https://arxiv.org/abs/2511.02180)
*Mehdi Sefidgar Dilmaghani,Waseem Shariff,Cian Ryan,Joe Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: An autonomous bias tuning mechanism for event cameras that uses CNN to detect flicker and dynamically adjust biases, reducing flicker effects across 25-500 Hz range and improving face detection performance.


<details>
  <summary>Details</summary>
Motivation: Address flicker effects caused by rapid light variations in event cameras, which degrade performance in diverse environments, without relying on additional hardware or software filters.

Method: Uses Convolutional Neural Networks to identify flicker in spatial space and dynamically adjust event camera bias settings to minimize flicker impact, tested with face detector framework under various lighting conditions.

Result: Significant improvements: enhanced YOLO confidence for face detection, increased percentage of frames with detected faces, and 38.2% reduction in average gradient (flicker indicator) in well-lit conditions and 53.6% reduction in low-light conditions.

Conclusion: The autobiasing system effectively improves event camera functionality in adverse lighting scenarios by autonomously mitigating flicker through bias adjustments, demonstrating practical potential for real-world applications.

Abstract: Understanding and mitigating flicker effects caused by rapid variations in
light intensity is critical for enhancing the performance of event cameras in
diverse environments. This paper introduces an innovative autonomous mechanism
for tuning the biases of event cameras, effectively addressing flicker across a
wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on
additional hardware or software for flicker filtering, our approach leverages
the event cameras inherent bias settings. Utilizing a simple Convolutional
Neural Networks -CNNs, the system identifies instances of flicker in a spatial
space and dynamically adjusts specific biases to minimize its impact. The
efficacy of this autobiasing system was robustly tested using a face detector
framework under both well-lit and low-light conditions, as well as across
various frequencies. The results demonstrated significant improvements:
enhanced YOLO confidence metrics for face detection, and an increased
percentage of frames capturing detected faces. Moreover, the average gradient,
which serves as an indicator of flicker presence through edge detection,
decreased by 38.2 percent in well-lit conditions and by 53.6 percent in
low-light conditions. These findings underscore the potential of our approach
to significantly improve the functionality of event cameras in a range of
adverse lighting scenarios.

</details>


### [12] [Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2511.02182)
*Jinhwan Seo,Yoonki Cho,Junhyug Noh,Sung-eui Yoon*

Main category: cs.CV

TL;DR: A three-stage pipeline for Grounded Video Question Answering that uses a trigger moment from CORTEX prompt to identify the most visible object frame, achieving significant improvement in HOTA score from 0.2704 to 0.4968.


<details>
  <summary>Details</summary>
Motivation: The GVQA task requires multimodal models capable of complex video reasoning, visual grounding, and temporal object tracking, which existing approaches struggle to handle effectively.

Method: Decomposes GVQA into three stages: Video Reasoning & QA, Spatio-temporal Grounding, and Tracking, with key innovation being a trigger moment derived from CORTEX prompt to identify the most visible object frame.

Result: Achieved HOTA score of 0.4968, representing a significant improvement over previous year's winning score of 0.2704 on the GVQA task.

Conclusion: The proposed three-stage pipeline with trigger moment anchoring effectively addresses the challenges of GVQA, demonstrating substantial performance gains in multimodal video understanding and object tracking.

Abstract: In this technical report, we introduce a framework to address Grounded Video
Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The
GVQA task demands robust multimodal models capable of complex reasoning over
video content, grounding the resulting answers visually, and tracking the
referenced objects temporally. To achieve this capability, our proposed
approach decomposes the GVQA task into a three-stage pipeline: (1) Video
Reasoning \& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key
contribution is the introduction of a trigger moment, derived from our proposed
CORTEX prompt, which pinpoints the single most visible frame of a target object
to serve as a robust anchor for grounding and tracking. To this end, we achieve
the HOTA score of 0.4968, which marks a significant improvement over the
previous year's winning score of 0.2704 on GVQA task.

</details>


### [13] [MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation](https://arxiv.org/abs/2511.02193)
*Jiawen Liu,Yuanbo Zeng,Jiaming Liang,Yizhen Yang,Yiheng Zhang,Enhui Cai,Xiaoqi Sheng,Hongmin Cai*

Main category: cs.CV

TL;DR: MM-UNet is a novel deep learning architecture for retinal vessel segmentation that uses Morph Mamba Convolution layers and Reverse Selective State Guidance modules to improve segmentation accuracy for thin, branching vascular structures.


<details>
  <summary>Details</summary>
Motivation: Retinal vessel segmentation is crucial for clinical diagnosis of ocular diseases, but existing methods struggle with the unique characteristics of retinal vasculature - extremely thin, branching structures with significant global morphological variations across images, which challenge segmentation precision and robustness.

Method: Proposes MM-UNet with two key innovations: 1) Morph Mamba Convolution layers that replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling; 2) Reverse Selective State Guidance modules that integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency.

Result: Extensive experiments on two public retinal vessel segmentation datasets show superior performance. MM-UNet achieves F1-score gains of 1.64% on DRIVE and 1.25% on STARE compared to existing approaches.

Conclusion: MM-UNet demonstrates effectiveness and advancement in retinal vessel segmentation, with publicly available code, making it a valuable contribution to quantitative analysis of vascular morphology in clinical diagnosis.

Abstract: Accurate detection of retinal vessels plays a critical role in reflecting a
wide range of health status indicators in the clinical diagnosis of ocular
diseases. Recently, advances in deep learning have led to a surge in retinal
vessel segmentation methods, which have significantly contributed to the
quantitative analysis of vascular morphology. However, retinal vasculature
differs significantly from conventional segmentation targets in that it
consists of extremely thin and branching structures, whose global morphology
varies greatly across images. These characteristics continue to pose challenges
to segmentation precision and robustness. To address these issues, we propose
MM-UNet, a novel architecture tailored for efficient retinal vessel
segmentation. The model incorporates Morph Mamba Convolution layers, which
replace pointwise convolutions to enhance branching topological perception
through morph, state-aware feature sampling. Additionally, Reverse Selective
State Guidance modules integrate reverse guidance theory with state-space
modeling to improve geometric boundary awareness and decoding efficiency.
Extensive experiments conducted on two public retinal vessel segmentation
datasets demonstrate the superior performance of the proposed method in
segmentation accuracy. Compared to the existing approaches, MM-UNet achieves
F1-score gains of 1.64 $\%$ on DRIVE and 1.25 $\%$ on STARE, demonstrating its
effectiveness and advancement. The project code is public via
https://github.com/liujiawen-jpg/MM-UNet.

</details>


### [14] [Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers](https://arxiv.org/abs/2511.02206)
*Zhengjie Zhang,Xiaoxie Mao,Qihao Guo,Shaoting Zhang,Qi Huang,Mu Zhou,Fang Xie,Mianxin Liu*

Main category: cs.CV

TL;DR: A language-enhanced generative model synthesizes realistic Abeta-PET images from blood biomarkers and MRI, enabling accurate Alzheimer's diagnosis without expensive PET scans.


<details>
  <summary>Details</summary>
Motivation: Alzheimer's disease diagnosis relies on expensive and inaccessible Abeta-PET imaging. This study aims to predict PET spatial patterns from more accessible blood biomarkers and MRI scans to improve diagnostic accessibility.

Method: Developed a language-enhanced generative model using large language models and multimodal fusion to synthesize PET images from blood biomarkers and T1-weighted MRI scans of 566 participants. Evaluated synthetic images for quality, diagnostic consistency, and clinical applicability.

Result: Synthetic PET images closely matched real PET scans (SSIM = 0.920, Pearson's r = 0.955) with high diagnostic agreement (accuracy = 0.80). Synthetic PET-based models (AUC = 0.78) outperformed MRI-only (AUC = 0.68) and biomarker-only (AUC = 0.73) models, with further improvement when combined with biomarkers (AUC = 0.79).

Conclusion: The language-enhanced generative model successfully synthesizes realistic PET images, enhancing the utility of MRI and blood biomarkers for Alzheimer's disease assessment and improving diagnostic workflows.

Abstract: Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta
positron emission tomography (Abeta-PET), which is limited by high cost and
limited accessibility. This study explores whether Abeta-PET spatial patterns
can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We
collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566
participants. A language-enhanced generative model, driven by a large language
model (LLM) and multimodal information fusion, was developed to synthesize PET
images. Synthesized images were evaluated for image quality, diagnostic
consistency, and clinical applicability within a fully automated diagnostic
pipeline. Findings: The synthetic PET images closely resemble real PET scans in
both structural details (SSIM = 0.920 +/- 0.003) and regional patterns
(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show
high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic
PET, we developed a fully automatic AD diagnostic pipeline integrating PET
synthesis and classification. The synthetic PET-based model (AUC = 0.78)
outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while
combining synthetic PET and BBMs further improved performance (AUC = 0.79).
Ablation analysis supports the advantages of LLM integration and prompt
engineering. Interpretation: Our language-enhanced generative model synthesizes
realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial
pattern assessment and improving the diagnostic workflow for Alzheimer's
disease.

</details>


### [15] [Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping](https://arxiv.org/abs/2511.02207)
*Jiajia Li,Keyi Zhu,Qianwen Zhang,Dong Chen,Qi Sun,Zhaojian Li*

Main category: cs.CV

TL;DR: Proposes an object-centric 3D reconstruction framework using SAM-2 and alpha masking for clean strawberry plant reconstructions, enabling automatic plant trait estimation with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional plant phenotyping methods are time-consuming and destructive, while current 3DGS applications reconstruct entire scenes including background noise, complicating trait analysis and increasing computational costs.

Method: Uses Segment Anything Model v2 (SAM-2) and alpha channel background masking for preprocessing, then applies 3D Gaussian Splatting for object-centric reconstruction, followed by DBSCAN clustering and PCA for automatic trait estimation.

Result: Method outperforms conventional pipelines in both accuracy and efficiency, producing cleaner reconstructions with reduced computational time and enabling automatic estimation of plant height and canopy width.

Conclusion: Provides a scalable, non-destructive solution for strawberry plant phenotyping that addresses limitations of current neural rendering approaches in agricultural applications.

Abstract: Strawberries are among the most economically significant fruits in the United
States, generating over $2 billion in annual farm-gate sales and accounting for
approximately 13% of the total fruit production value. Plant phenotyping plays
a vital role in selecting superior cultivars by characterizing plant traits
such as morphology, canopy structure, and growth dynamics. However, traditional
plant phenotyping methods are time-consuming, labor-intensive, and often
destructive. Recently, neural rendering techniques, notably Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful
frameworks for high-fidelity 3D reconstruction. By capturing a sequence of
multi-view images or videos around a target plant, these methods enable
non-destructive reconstruction of complex plant architectures. Despite their
promise, most current applications of 3DGS in agricultural domains reconstruct
the entire scene, including background elements, which introduces noise,
increases computational costs, and complicates downstream trait analysis. To
address this limitation, we propose a novel object-centric 3D reconstruction
framework incorporating a preprocessing pipeline that leverages the Segment
Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean
strawberry plant reconstructions. This approach produces more accurate
geometric representations while substantially reducing computational time. With
a background-free reconstruction, our algorithm can automatically estimate
important plant traits, such as plant height and canopy width, using DBSCAN
clustering and Principal Component Analysis (PCA). Experimental results show
that our method outperforms conventional pipelines in both accuracy and
efficiency, offering a scalable and non-destructive solution for strawberry
plant phenotyping.

</details>


### [16] [Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning](https://arxiv.org/abs/2511.02210)
*Anders Austlid Taskén,Thierry Judge,Erik Andreas Rye Berg,Jinyang Yu,Bjørnar Grenne,Frank Lindseth,Svend Aakhus,Pierre-Marc Jodoin,Nicolas Duchateau,Olivier Bernard,Gabriel Kiss*

Main category: cs.CV

TL;DR: This paper introduces autoStrain, the first automated pipeline for segmental longitudinal strain estimation in transesophageal echocardiography using deep learning methods for motion estimation.


<details>
  <summary>Details</summary>
Motivation: Current techniques for strain estimation require significant manual intervention and expertise, limiting efficiency and making them too resource-intensive for monitoring purposes.

Method: Used two deep learning approaches (TeeFlow based on RAFT optical flow model and TeeTracker based on CoTracker point trajectory model) trained on synthetic TEE data generated using SIMUS simulation pipeline with ground truth myocardial motion.

Result: TeeTracker outperformed TeeFlow with mean distance error of 0.65mm on synthetic test data. Clinical validation on 16 patients showed SLS estimation aligned with clinical references (mean difference 1.09% with 95% limits of agreement -8.90% to 11.09%).

Conclusion: Integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.

Abstract: Segmental longitudinal strain (SLS) of the left ventricle (LV) is an
important prognostic indicator for evaluating regional LV dysfunction, in
particular for diagnosing and managing myocardial ischemia. Current techniques
for strain estimation require significant manual intervention and expertise,
limiting their efficiency and making them too resource-intensive for monitoring
purposes. This study introduces the first automated pipeline, autoStrain, for
SLS estimation in transesophageal echocardiography (TEE) using deep learning
(DL) methods for motion estimation. We present a comparative analysis of two DL
approaches: TeeFlow, based on the RAFT optical flow model for dense
frame-to-frame predictions, and TeeTracker, based on the CoTracker point
trajectory model for sparse long-sequence predictions.
  As ground truth motion data from real echocardiographic sequences are hardly
accessible, we took advantage of a unique simulation pipeline (SIMUS) to
generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with
ground truth myocardial motion to train and evaluate both models. Our
evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a
mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.
  Clinical validation on 16 patients further demonstrated that SLS estimation
with our autoStrain pipeline aligned with clinical references, achieving a mean
difference (95\% limits of agreement) of 1.09% (-8.90% to 11.09%).
Incorporation of simulated ischemia in the synTEE data improved the accuracy of
the models in quantifying abnormal deformation. Our findings indicate that
integrating AI-driven motion estimation with TEE can significantly enhance the
precision and efficiency of cardiac function assessment in clinical settings.

</details>


### [17] [Can Foundation Models Revolutionize Mobile AR Sparse Sensing?](https://arxiv.org/abs/2511.02215)
*Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: Foundation models can significantly improve mobile sparse sensing by enhancing geometry-aware image warping and 3D scene reconstruction, overcoming traditional trade-offs between sensing quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Mobile sensing systems face fundamental trade-offs between sensing quality and efficiency due to computational and power constraints. Existing sparse sensing methods often suffer from reduced accuracy due to missing information across space and time.

Method: Investigate whether foundation models can improve mobile sparse sensing using real-world mobile AR data, focusing on geometry-aware image warping and 3D scene reconstruction.

Result: Foundation models offer significant improvements in geometry-aware image warping and demonstrate leading performance in 3D scene reconstruction, showing scalability of foundation model-based sparse sensing.

Conclusion: Foundation models show promise for transforming mobile sparse sensing but also reveal open challenges for integration into mobile systems.

Abstract: Mobile sensing systems have long faced a fundamental trade-off between
sensing quality and efficiency due to constraints in computation, power, and
other limitations. Sparse sensing, which aims to acquire and process only a
subset of sensor data, has been a key strategy for maintaining performance
under such constraints. However, existing sparse sensing methods often suffer
from reduced accuracy, as missing information across space and time introduces
uncertainty into many sensing systems. In this work, we investigate whether
foundation models can change the landscape of mobile sparse sensing. Using
real-world mobile AR data, our evaluations demonstrate that foundation models
offer significant improvements in geometry-aware image warping, a central
technique for enabling accurate reuse of cross-frame information. Furthermore,
our study demonstrates the scalability of foundation model-based sparse sensing
and shows its leading performance in 3D scene reconstruction. Collectively, our
study reveals critical aspects of the promises and the open challenges of
integrating foundation models into mobile sparse sensing systems.

</details>


### [18] [Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2511.02228)
*Delin Ma,Menghui Zhou,Jun Qi,Yun Yang,Po Yang*

Main category: cs.CV

TL;DR: Proposes a Collaborative Attention and Consistent-Guided Fusion framework for Alzheimer's disease diagnosis using MRI and PET neuroimaging, addressing limitations in existing multimodal fusion approaches.


<details>
  <summary>Details</summary>
Motivation: Early Alzheimer's disease diagnosis is crucial, and multimodal neuroimaging fusion shows promise but existing methods overlook modality-specific features and suffer from distributional differences between modalities causing biased representations.

Method: Uses learnable parameter representation block to handle missing modality information, shared and modality-independent encoders to preserve both shared and specific representations, and consistency-guided mechanism to align latent distributions across modalities.

Result: Experimental results on ADNI dataset demonstrate superior diagnostic performance compared to existing fusion strategies.

Conclusion: The proposed framework effectively addresses challenges in multimodal neuroimaging fusion for Alzheimer's disease diagnosis by considering both shared and modality-specific features while aligning cross-modal distributions.

Abstract: Alzheimer's disease (AD) is the most prevalent form of dementia, and its
early diagnosis is essential for slowing disease progression. Recent studies on
multimodal neuroimaging fusion using MRI and PET have achieved promising
results by integrating multi-scale complementary features. However, most
existing approaches primarily emphasize cross-modal complementarity while
overlooking the diagnostic importance of modality-specific features. In
addition, the inherent distributional differences between modalities often lead
to biased and noisy representations, degrading classification performance. To
address these challenges, we propose a Collaborative Attention and
Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The
proposed model introduces a learnable parameter representation (LPR) block to
compensate for missing modality information, followed by a shared encoder and
modality-independent encoders to preserve both shared and specific
representations. Furthermore, a consistency-guided mechanism is employed to
explicitly align the latent distributions across modalities. Experimental
results on the ADNI dataset demonstrate that our method achieves superior
diagnostic performance compared with existing fusion strategies.

</details>


### [19] [Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization](https://arxiv.org/abs/2511.02329)
*Shaohan Li,Yunpeng Shi,Gilad Lerman*

Main category: cs.CV

TL;DR: Cycle-Sync is a robust global framework for camera pose estimation that uses modified message-passing least squares with cycle consistency and robust loss functions, achieving state-of-the-art performance without bundle adjustment.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and global camera pose estimation framework that leverages cycle consistency and avoids the computational burden of bundle adjustment while achieving strong theoretical guarantees.

Method: Adapts message-passing least squares (MPLS) for camera location estimation with cycle consistency, redefines cycle consistencies using estimated distances, incorporates Welsch-type robust loss, and adds outlier rejection via robust subspace recovery.

Result: Establishes the strongest known deterministic exact-recovery guarantee for camera location estimation, with cycle consistency alone achieving the lowest known sample complexity. Outperforms leading pose estimators including full structure-from-motion pipelines with bundle adjustment.

Conclusion: Cycle-Sync provides a robust, globally optimal camera pose estimation framework that leverages cycle consistency and avoids bundle adjustment while achieving superior performance compared to existing methods.

Abstract: We introduce Cycle-Sync, a robust and global framework for estimating camera
poses (both rotations and locations). Our core innovation is a location solver
that adapts message-passing least squares (MPLS) -- originally developed for
group synchronization -- to camera location estimation. We modify MPLS to
emphasize cycle-consistent information, redefine cycle consistencies using
estimated distances from previous iterations, and incorporate a Welsch-type
robust loss. We establish the strongest known deterministic exact-recovery
guarantee for camera location estimation, showing that cycle consistency alone
-- without access to inter-camera distances -- suffices to achieve the lowest
sample complexity currently known. To further enhance robustness, we introduce
a plug-and-play outlier rejection module inspired by robust subspace recovery,
and we fully integrate cycle consistency into MPLS for rotation
synchronization. Our global approach avoids the need for bundle adjustment.
Experiments on synthetic and real datasets show that Cycle-Sync consistently
outperforms leading pose estimators, including full structure-from-motion
pipelines with bundle adjustment.

</details>


### [20] [Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency](https://arxiv.org/abs/2511.02247)
*Hao Li,Daiwei Lu,Jesse d'Almeida,Dilara Isik,Ehsan Khodapanah Aghdam,Nick DiSanto,Ayberk Acar,Susheela Sharma,Jie Ying Wu,Robert J. Webster III,Ipek Oguz*

Main category: cs.CV

TL;DR: A latent feature alignment method for monocular depth estimation in endoscopic videos that reduces domain gap between synthetic and real images using adversarial learning and directional feature consistency.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised domain adaptation methods for monocular depth estimation in medical endoscopy still leave a domain gap between translated synthetic images and real endoscopic frames, limiting accurate absolute depth estimation.

Method: Proposes a latent feature alignment approach that takes both translated synthetic and real endoscopic frames as input, learning domain-invariant features via adversarial learning and directional feature consistency, independent of the image translation process.

Result: Achieves superior performance on both absolute and relative depth metrics compared to state-of-the-art methods, with consistent improvements across various backbones and pretrained weights on endoscopic videos of central airway phantoms.

Conclusion: The proposed feature-level domain adaptation method effectively reduces the domain gap and improves absolute depth estimation for endoscopic medical robotics applications.

Abstract: Monocular depth estimation (MDE) is a critical task to guide autonomous
medical robots. However, obtaining absolute (metric) depth from an endoscopy
camera in surgical scenes is difficult, which limits supervised learning of
depth on real endoscopic images. Current image-level unsupervised domain
adaptation methods translate synthetic images with known depth maps into the
style of real endoscopic frames and train depth networks using these translated
images with their corresponding depth maps. However a domain gap often remains
between real and translated synthetic images. In this paper, we present a
latent feature alignment method to improve absolute depth estimation by
reducing this domain gap in the context of endoscopic videos of the central
airway. Our methods are agnostic to the image translation process and focus on
the depth estimation itself. Specifically, the depth network takes translated
synthetic and real endoscopic frames as input and learns latent
domain-invariant features via adversarial learning and directional feature
consistency. The evaluation is conducted on endoscopic videos of central airway
phantoms with manually aligned absolute depth maps. Compared to
state-of-the-art MDE methods, our approach achieves superior performance on
both absolute and relative depth metrics, and consistently improves results
across various backbones and pretrained weights. Our code is available at
https://github.com/MedICL-VU/MDE.

</details>


### [21] [Synthetic Crop-Weed Image Generation and its Impact on Model Generalization](https://arxiv.org/abs/2511.02417)
*Garen Boyadjian,Cyrille Pierre,Johann Laconte,Riccardo Bertoglio*

Main category: cs.CV

TL;DR: A pipeline for generating synthetic crop-weed images using Blender reduces annotation costs for agricultural weeding robots, achieving 10% sim-to-real gap and outperforming real datasets in cross-domain scenarios.


<details>
  <summary>Details</summary>
Motivation: Training deep learning models for crop-weed segmentation requires large annotated datasets, which are costly to obtain in real agricultural fields. Synthetic data can help but faces the challenge of the simulation-to-reality gap.

Method: Developed a pipeline for procedural generation of synthetic crop-weed images using Blender, creating annotated datasets under diverse conditions including plant growth stages, weed density, lighting, and camera angles.

Result: Training on synthetic images achieved a 10% sim-to-real gap, surpassing previous state-of-the-art methods. Synthetic data demonstrated better generalization than real datasets in cross-domain scenarios.

Conclusion: Synthetic agricultural datasets show strong potential and support hybrid training strategies for more efficient model development in agricultural robotics applications.

Abstract: Precise semantic segmentation of crops and weeds is necessary for
agricultural weeding robots. However, training deep learning models requires
large annotated datasets, which are costly to obtain in real fields. Synthetic
data can reduce this burden, but the gap between simulated and real images
remains a challenge. In this paper, we present a pipeline for procedural
generation of synthetic crop-weed images using Blender, producing annotated
datasets under diverse conditions of plant growth, weed density, lighting, and
camera angle. We benchmark several state-of-the-art segmentation models on
synthetic and real datasets and analyze their cross-domain generalization. Our
results show that training on synthetic images leads to a sim-to-real gap of
10%, surpassing previous state-of-the-art methods. Moreover, synthetic data
demonstrates good generalization properties, outperforming real datasets in
cross-domain scenarios. These findings highlight the potential of synthetic
agricultural datasets and support hybrid strategies for more efficient model
training.

</details>


### [22] [Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework](https://arxiv.org/abs/2511.02271)
*Yucheng Song,Yifan Ge,Junhao Li,Zhining Liao,Zhifang Liao*

Main category: cs.CV

TL;DR: Proposes HTSC-CIF framework that hierarchically decomposes MRG challenges into low-, mid-, and high-level tasks to address domain knowledge gaps, cross-modal alignment, and spurious correlations simultaneously.


<details>
  <summary>Details</summary>
Motivation: Medical Report Generation faces three key challenges: insufficient domain knowledge understanding, poor text-visual entity alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges.

Method: HTSC-CIF framework with hierarchical task decomposition: 1) Low-level: align medical entities with spatial locations; 2) Mid-level: Prefix Language Modeling and Masked Image Modeling for cross-modal alignment; 3) High-level: cross-modal causal intervention via front-door intervention.

Result: Extensive experiments show HTSC-CIF significantly outperforms state-of-the-art MRG methods.

Conclusion: The proposed hierarchical approach effectively addresses all three key challenges in MRG and demonstrates superior performance over existing methods.

Abstract: Medical Report Generation (MRG) is a key part of modern medical diagnostics,
as it automatically generates reports from radiological images to reduce
radiologists' burden. However, reliable MRG models for lesion description face
three main challenges: insufficient domain knowledge understanding, poor
text-visual entity embedding alignment, and spurious correlations from
cross-modal biases. Previous work only addresses single challenges, while this
paper tackles all three via a novel hierarchical task decomposition approach,
proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into
low-, mid-, and high-level tasks: 1) Low-level: align medical entity features
with spatial locations to enhance domain knowledge for visual encoders; 2)
Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling
(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a
cross-modal causal intervention module (via front-door intervention) to reduce
confounders and improve interpretability. Extensive experiments confirm
HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)
MRG methods. Code will be made public upon paper acceptance.

</details>


### [23] [From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427)
*Nicolas Schuler,Lea Dewald,Nick Baldig,Jürgen Graf*

Main category: cs.CV

TL;DR: This paper investigates small Visual Language Models (VLMs) for scene interpretation and action recognition on edge devices in mobile robotics, evaluating their capabilities, challenges, and biases in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: While LLMs and VLMs show remarkable advancements in video understanding and commonsense reasoning, their computational complexity poses challenges for deployment on edge devices in mobile robotics, creating a trade-off between accuracy and inference time.

Method: The authors propose a pipeline that evaluates state-of-the-art small VLMs capable of edge deployment, testing them on a diverse dataset of real-world cityscape, on-campus, and indoor scenarios.

Result: The experimental evaluation discusses the potential of small VLMs on edge devices, with particular emphasis on identifying challenges, weaknesses, inherent model biases, and practical applications of the gained information.

Conclusion: Small VLMs show promise for scene interpretation and action recognition in mobile robotics applications on edge devices, though they face specific challenges related to computational constraints and inherent biases that need to be addressed.

Abstract: Video Understanding, Scene Interpretation and Commonsense Reasoning are
highly challenging tasks enabling the interpretation of visual information,
allowing agents to perceive, interact with and make rational decisions in its
environment. Large Language Models (LLMs) and Visual Language Models (VLMs)
have shown remarkable advancements in these areas in recent years, enabling
domain-specific applications as well as zero-shot open vocabulary tasks,
combining multiple domains. However, the required computational complexity
poses challenges for their application on edge devices and in the context of
Mobile Robotics, especially considering the trade-off between accuracy and
inference time. In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics. The proposed pipeline is
evaluated on a diverse dataset consisting of various real-world cityscape,
on-campus and indoor scenarios. The experimental evaluation discusses the
potential of these small models on edge devices, with particular emphasis on
challenges, weaknesses, inherent model biases and the application of the gained
information. Supplementary material is provided via the following repository:
https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/

</details>


### [24] [Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?](https://arxiv.org/abs/2511.02277)
*Giorgos Sfikas,Konstantina Nikolaidou,Foteini Papadopoulou,George Retsinas,Anastasios L. Kesidis*

Main category: cs.CV

TL;DR: The paper explores using Euler angles parameterization for Normalizing Flows models in object pose estimation, arguing that despite their limitations, Euler angles can lead to useful probabilistic pose estimation models compared to more complex parameterizations.


<details>
  <summary>Details</summary>
Motivation: Probabilistic pose estimation is beneficial when pose is ambiguous due to sensor constraints, projection limitations, or object symmetries. The authors want to investigate if the simpler Euler angles parameterization can be effective for Normalizing Flows models despite known shortcomings.

Method: The researchers use Euler angles parameterization as the basis for a Normalizing Flows model for pose estimation, comparing it against more complex parameterizations of 3D pose.

Result: The paper finds that Euler angles, despite their well-known limitations, can lead to useful models for probabilistic pose estimation in various aspects.

Conclusion: Euler angles parameterization, while having shortcomings, can be effectively used in Normalizing Flows models for object pose estimation and may offer advantages over more complex parameterizations in certain scenarios.

Abstract: Object pose estimation is a task that is of central importance in 3D Computer
Vision. Given a target image and a canonical pose, a single point estimate may
very often be sufficient; however, a probabilistic pose output is related to a
number of benefits when pose is not unambiguous due to sensor and projection
constraints or inherent object symmetries. With this paper, we explore the
usefulness of using the well-known Euler angles parameterisation as a basis for
a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation,
3D pose has been parameterized in a number of ways, either in or out of the
context of parameter estimation. We explore the idea that Euler angles, despite
their shortcomings, may lead to useful models in a number of aspects, compared
to a model built on a more complex parameterisation.

</details>


### [25] [Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems](https://arxiv.org/abs/2511.02507)
*Nicolas Schuler,Lea Dewald,Jürgen Graf*

Main category: cs.CV

TL;DR: Proposes a privacy-preserving pipeline for automated natural language report generation in mobile robotics using local edge computing models instead of external services.


<details>
  <summary>Details</summary>
Motivation: Need for automated evaluation of heterogeneous data in critical robotics applications (autonomous driving, service robotics) while preserving privacy and enabling deployment on edge devices.

Method: Developed a pipeline using multi-modal sensors and local models capable of running on edge computing devices, eliminating dependency on external services.

Result: Evaluated implementation on diverse dataset spanning indoor, outdoor and urban environments, providing both quantitative and qualitative results with example reports available publicly.

Conclusion: Successfully demonstrated a privacy-preserving automated report generation system for mobile robotics that can operate locally without external dependencies.

Abstract: Recent advancements in Deep Learning enable hardware-based cognitive systems,
that is, mechatronic systems in general and robotics in particular with
integrated Artificial Intelligence, to interact with dynamic and unstructured
environments. While the results are impressive, the application of such systems
to critical tasks like autonomous driving as well as service and care robotics
necessitate the evaluation of large amount of heterogeneous data. Automated
report generation for Mobile Robotics can play a crucial role in facilitating
the evaluation and acceptance of such systems in various domains. In this
paper, we propose a pipeline for generating automated reports in natural
language utilizing various multi-modal sensors that solely relies on local
models capable of being deployed on edge computing devices, thus preserving the
privacy of all actors involved and eliminating the need for external services.
In particular, we evaluate our implementation on a diverse dataset spanning
multiple domains including indoor, outdoor and urban environments, providing
quantitative as well as qualitative evaluation results. Various generated
example reports and other supplementary materials are available via a public
repository.

</details>


### [26] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL is a reinforcement learning framework that enhances multimodal LLMs' reasoning by teaching when and how to think, using dual rewards for reasoning quality and adaptive thinking strategies.


<details>
  <summary>Details</summary>
Motivation: Existing approaches suffer from outcome-only supervision (rewarding correct answers without ensuring sound reasoning) and uniform thinking strategies (leading to overthinking on simple tasks and underthinking on complex ones).

Method: SAIL-RL uses a dual reward system: Thinking Reward evaluates reasoning quality (factual grounding, logical coherence, answer consistency), and Judging Reward adaptively determines whether deep reasoning or direct answering is appropriate.

Result: Experiments on SAIL-VL2 show improved reasoning and multimodal understanding benchmarks at 4B and 8B scales, competitive performance against GPT-4o, and substantial reduction in hallucinations.

Conclusion: SAIL-RL establishes a principled framework for building more reliable and adaptive multimodal large language models.

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [27] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: A GNN-based approach for HME recognition that models expressions as graphs, using BLSTM for symbol processing and a 2D-CFG parser with GNN link prediction to refine spatial relations.


<details>
  <summary>Details</summary>
Motivation: To improve Handwritten Mathematical Expression recognition by effectively modeling spatial dependencies between symbols through graph structures.

Method: Uses deep BLSTM for symbol segmentation/recognition, 2D-CFG parser for spatial relations, and GNN link prediction to refine the graph structure by removing unnecessary connections.

Result: Experimental results show promising performance in HME structure recognition, demonstrating the effectiveness of the approach.

Conclusion: The proposed GNN-based method with graph modeling and link prediction effectively recognizes HME structures by capturing spatial dependencies between symbols.

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [28] [GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection](https://arxiv.org/abs/2511.02335)
*Kun Zou,Yongheng Xu,Jianxing Yu,Yan Pan,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: GAFD-CC is a novel OOD detection method that performs global-aware feature decoupling guided by classification weights and fuses decoupled features with multi-scale logit-based confidence for robust OOD detection.


<details>
  <summary>Details</summary>
Motivation: Existing post-hoc OOD detection methods overlook the inherent correlation between features and logits, which is crucial for effective OOD detection.

Method: 1) Global-aware feature decoupling guided by classification weights to extract positively and negatively correlated features; 2) Adaptive fusion of decoupled features with multi-scale logit-based confidence.

Result: Extensive experiments on large-scale benchmarks demonstrate GAFD-CC's competitive performance and strong generalization ability compared to state-of-the-art methods.

Conclusion: GAFD-CC effectively addresses the limitation of existing methods by leveraging feature-logit correlation and achieves superior OOD detection performance.

Abstract: Out-of-distribution (OOD) detection is paramount to ensuring the reliability
and robustness of learning models in real-world applications. Existing post-hoc
OOD detection methods detect OOD samples by leveraging their features and
logits information without retraining. However, they often overlook the
inherent correlation between features and logits, which is crucial for
effective OOD detection. To address this limitation, we propose Global-Aware
Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to
refine decision boundaries and increase discriminative performance. Firstly, it
performs global-aware feature decoupling guided by classification weights. This
involves aligning features with the direction of global classification weights
to decouple them. From this, GAFD-CC extracts two types of critical
information: positively correlated features that promote in-distribution
(ID)/OOD boundary refinement and negatively correlated features that suppress
false positives and tighten these boundaries. Secondly, it adaptively fuses
these decoupled features with multi-scale logit-based confidence for
comprehensive and robust OOD detection. Extensive experiments on large-scale
benchmarks demonstrate GAFD-CC's competitive performance and strong
generalization ability compared to those of state-of-the-art methods.

</details>


### [29] [M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings](https://arxiv.org/abs/2511.02349)
*Jiankai Tang,Tao Zhang,Jia Li,Yiru Zhang,Mingyu Zhang,Kegang Wang,Yuming Hao,Bolin Wang,Haiyang Li,Xingyao Wang,Yuanchun Shi,Yuntao Wang,Sichong Qian*

Main category: cs.CV

TL;DR: The paper introduces M3PD, the first dual-view mobile photoplethysmography dataset with synchronized facial and fingertip videos from 60 participants (47 cardiovascular patients), and proposes F3Mamba, a Mamba-based model that fuses both views to reduce heart-rate error by 21.9-30.2% over single-view methods.


<details>
  <summary>Details</summary>
Motivation: Current portable physiological monitoring methods have accessibility limitations and require impractical postures. Video-based photoplethysmography on smartphones faces reliability challenges from motion artifacts, lighting variations, and single-view constraints, with limited application to cardiovascular patients and no cross-device accuracy datasets.

Method: Created M3PD dataset with synchronized dual-view videos (facial and fingertip) captured via front and rear smartphone cameras. Proposed F3Mamba model that fuses both views using Mamba-based temporal modeling for improved heart rate estimation.

Result: F3Mamba reduces heart-rate error by 21.9% to 30.2% compared to existing single-view baselines, demonstrating improved robustness in challenging real-world scenarios.

Conclusion: The dual-view approach with Mamba-based temporal modeling significantly improves the reliability and accuracy of mobile photoplethysmography for cardiovascular monitoring, addressing key limitations of single-view methods.

Abstract: Portable physiological monitoring is essential for early detection and
management of cardiovascular disease, but current methods often require
specialized equipment that limits accessibility or impose impractical postures
that patients cannot maintain. Video-based photoplethysmography on smartphones
offers a convenient noninvasive alternative, yet it still faces reliability
challenges caused by motion artifacts, lighting variations, and single-view
constraints. Few studies have demonstrated reliable application to
cardiovascular patients, and no widely used open datasets exist for
cross-device accuracy. To address these limitations, we introduce the M3PD
dataset, the first publicly available dual-view mobile photoplethysmography
dataset, comprising synchronized facial and fingertip videos captured
simultaneously via front and rear smartphone cameras from 60 participants
(including 47 cardiovascular patients). Building on this dual-view setting, we
further propose F3Mamba, which fuses the facial and fingertip views through
Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to
30.2 percent over existing single-view baselines while improving robustness in
challenging real-world scenarios. Data and code:
https://github.com/Health-HCI-Group/F3Mamba.

</details>


### [30] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa is a novel vision-language framework that uses continuous cross-modal reasoning through iterative refinement of latent thought vectors, improving performance and efficiency over traditional discrete token-based models.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are constrained to discrete linguistic token reasoning, which bottlenecks the rich, high-dimensional nature of visual perception and fails to capture tacit thought processes beyond verbal expression.

Method: Uses iterative reasoning cycle with Latent Q-Former (LQ-Former) as dynamic reasoning engine, token selection for salient regions, and multi-task training combining contrastive learning and diffusion-based reconstruction for grounding.

Result: Improves accuracy and token efficiency over baselines; 1.5B model competes with 7B-9B models; 7B version remains competitive with SOTA; latent space captures interpretable reasoning patterns.

Conclusion: CoCoVa successfully bridges the representational gap between discrete language processing and continuous visual understanding, demonstrating potential for more human-like multimodal reasoning.

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [31] [RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](https://arxiv.org/abs/2511.02384)
*Jiahe Song,Chuang Wang,Bowen Jiang,Yinfan Wang,Hao Zheng,Xingjian Wei,Chengjin Liu,Junyuan Gao,Yubin Wang,Lijun Wu,Jiang Wu,Qian Yu,Conghui He*

Main category: cs.CV

TL;DR: RxnCaption reformulates chemical reaction diagram parsing as an image captioning task using Large Vision-Language Models, achieving state-of-the-art performance with a novel BBox and Index as Visual Prompt strategy.


<details>
  <summary>Details</summary>
Motivation: Existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models, creating a need for automated parsing methods.

Method: The framework uses a "BBox and Index as Visual Prompt" (BIVP) strategy with MolYOLO molecular detector to pre-draw bounding boxes and indices on input images, transforming parsing into a natural-language description problem handled by LVLMs.

Result: RxnCaption achieves state-of-the-art performance on multiple metrics and constructs the RxnCaption-11k dataset, which is an order of magnitude larger than prior real-world literature benchmarks.

Conclusion: The method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry.

Abstract: Large-scale chemical reaction datasets are crucial for AI research in
chemistry. However, existing chemical reaction data often exist as images
within papers, making them not machine-readable and unusable for training
machine learning models. In response to this challenge, we propose the
RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).
Our framework reformulates the traditional coordinate prediction driven parsing
process into an image captioning problem, which Large Vision-Language Models
(LVLMs) handle naturally. We introduce a strategy termed "BBox and Index as
Visual Prompt" (BIVP), which uses our state-of-the-art molecular detector,
MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the
input image. This turns the downstream parsing into a natural-language
description problem. Extensive experiments show that the BIVP strategy
significantly improves structural extraction quality while simplifying model
design. We further construct the RxnCaption-11k dataset, an order of magnitude
larger than prior real-world literature benchmarks, with a balanced test subset
across four layout archetypes. Experiments demonstrate that RxnCaption-VL
achieves state-of-the-art performance on multiple metrics. We believe our
method, dataset, and models will advance structured information extraction from
chemical literature and catalyze broader AI applications in chemistry. We will
release data, models, and code on GitHub.

</details>


### [32] [Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2511.02395)
*Leon Schwarzer,Matthias Zeller,Daniel Casado Herraez,Simon Dierl,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: Self-supervised moving object segmentation for sparse radar point clouds using contrastive learning with cluster refinement to reduce annotation costs and improve performance.


<details>
  <summary>Details</summary>
Motivation: Radar sensors provide direct Doppler velocity measurements for single-scan moving object segmentation, but sparse and noisy radar data makes supervised learning annotation tedious and expensive.

Method: Two-step approach: contrastive self-supervised representation learning with cluster-based loss and dynamic points removal, followed by supervised fine-tuning with limited annotated data.

Result: Method improves label efficiency after fine-tuning and boosts state-of-the-art performance through self-supervised pretraining.

Conclusion: Self-supervised pretraining enables effective moving object segmentation in sparse radar data while reducing annotation costs.

Abstract: Moving object segmentation is a crucial task for safe and reliable autonomous
mobile systems like self-driving cars, improving the reliability and robustness
of subsequent tasks like SLAM or path planning. While the segmentation of
camera or LiDAR data is widely researched and achieves great results, it often
introduces an increased latency by requiring the accumulation of temporal
sequences to gain the necessary temporal context. Radar sensors overcome this
problem with their ability to provide a direct measurement of a point's Doppler
velocity, which can be exploited for single-scan moving object segmentation.
However, radar point clouds are often sparse and noisy, making data annotation
for use in supervised learning very tedious, time-consuming, and
cost-intensive. To overcome this problem, we address the task of
self-supervised moving object segmentation of sparse and noisy radar point
clouds. We follow a two-step approach of contrastive self-supervised
representation learning with subsequent supervised fine-tuning using limited
amounts of annotated data. We propose a novel clustering-based contrastive loss
function with cluster refinement based on dynamic points removal to pretrain
the network to produce motion-aware representations of the radar data. Our
method improves label efficiency after fine-tuning, effectively boosting
state-of-the-art performance by self-supervised pretraining.

</details>


### [33] [A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds](https://arxiv.org/abs/2511.02397)
*Kuo-Liang Chung,Ting-Chung Tang*

Main category: cs.CV

TL;DR: Proposes a grouping-based hybrid color correction algorithm for color point clouds that adaptively partitions points based on overlapping rate and applies different correction methods (KBI, JKHE, HE) to each group.


<details>
  <summary>Details</summary>
Motivation: Color consistency correction is fundamental for 3D rendering and compression, but most existing methods focus on color images rather than point clouds.

Method: Estimates overlapping rate between source and target point clouds, then adaptively partitions target points into 2-3 proximity groups (close, moderate, distant). Uses K-nearest neighbors bilateral interpolation for close points, joint KBI and histogram equalization for moderate points, and histogram equalization for distant points.

Result: Tested on 1086 color point cloud pairs and demonstrated superior performance against state-of-the-art methods.

Conclusion: The proposed grouping-based hybrid approach effectively achieves color consistency correction for point clouds with adaptive grouping and specialized correction methods for different proximity levels.

Abstract: Color consistency correction for color point clouds is a fundamental yet
important task in 3D rendering and compression applications. In the past, most
previous color correction methods aimed at correcting color for color images.
The purpose of this paper is to propose a grouping-based hybrid color
correction algorithm for color point clouds. Our algorithm begins by estimating
the overlapping rate between the aligned source and target point clouds, and
then adaptively partitions the target points into two groups, namely the close
proximity group Gcl and the moderate proximity group Gmod, or three groups,
namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated
overlapping rate is low or high, respectively. To correct color for target
points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method
is proposed. To correct color for target points in Gmod, a joint KBI and the
histogram equalization (JKHE) method is proposed. For target points in Gdist, a
histogram equalization (HE) method is proposed for color correction. Finally,
we discuss the grouping-effect free property and the ablation study in our
algorithm. The desired color consistency correction benefit of our algorithm
has been justified through 1086 testing color point cloud pairs against the
state-of-the-art methods. The C++ source code of our algorithm can be accessed
from the website: https://github.com/ivpml84079/Point-cloud-color-correction.

</details>


### [34] [Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs](https://arxiv.org/abs/2511.02404)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.CV

TL;DR: Self-supervised Vision Transformers (DINO) show the highest representational alignment between feline and human visual systems, outperforming supervised ViTs, CNNs, and windowed transformers in bridging species-specific visual statistics.


<details>
  <summary>Details</summary>
Motivation: To understand how feline ocular specializations (vertically elongated pupils for ambush predation) manifest in visual representations compared to humans, and to quantify cross-species representational alignment using modern computer vision models.

Method: Used a frozen-encoder benchmark with layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO).

Result: DINO ViT-B/16 achieved the highest alignment (mean CKA-RBF ≈0.814, mean CKA-linear ≈0.745, mean RSA ≈0.698), peaking at early blocks. Supervised ViTs were competitive on CKA but showed weaker geometric correspondence than DINO. CNNs were strong baselines but below plain ViTs, and windowed transformers underperformed.

Conclusion: Self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than CNNs and windowed Transformers, providing neuroscientific hypotheses about cross-species visual convergence.

Abstract: Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic
cats) have vertically elongated pupils linked to ambush predation; yet, how
such specializations manifest in downstream visual representations remains
incompletely understood. We present a unified, frozen-encoder benchmark that
quantifies feline-human cross-species representational alignment in the wild,
across convolutional networks, supervised Vision Transformers, windowed
transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel
Alignment (linear and RBF) and Representational Similarity Analysis, with
additional distributional and stability tests reported in the paper. Across
models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF
$\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$),
peaking at early blocks, indicating that token-level self-supervision induces
early-stage features that bridge species-specific statistics. Supervised ViTs
are competitive on CKA yet show weaker geometric correspondence than DINO
(e.g., ViT-B/16 RSA $\approx0.53$ at block8; ViT-L/16 $\approx0.47$ at
block14), revealing depth-dependent divergences between similarity and
representational geometry. CNNs remain strong baselines but below plain ViTs on
alignment, and windowed transformers underperform plain ViTs, implicating
architectural inductive biases in cross-species alignment. Results indicate
that self-supervision coupled with ViT inductive biases yields representational
geometries that more closely align feline and human visual systems than widely
used CNNs and windowed Transformers, providing testable neuroscientific
hypotheses about where and how cross-species visual computations converge. We
release our code and dataset for reference and reproducibility.

</details>


### [35] [IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition](https://arxiv.org/abs/2511.02411)
*Wenyang Wei,Yang yang,Xixi Jia,Xiangchu Feng,Weiwei Wang,Renzhen Wang*

Main category: cs.CV

TL;DR: IllumFlow is a novel framework that combines conditional Rectified Flow with Retinex theory for low-light image enhancement, addressing illumination variations and noise through separate optimization of illumination and reflectance components.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of low-light image enhancement, including handling wide dynamic range of illumination variations and complex noise in reflectance components, while preserving color fidelity and enabling customizable brightness enhancement.

Method: The framework first decomposes input images into reflectance and illumination components using Retinex theory. It then uses conditional rectified flow to model illumination changes as continuous flow fields, and employs a denoising network with flow-derived data augmentation to remove reflectance noise and chromatic aberration.

Result: Extensive experiments on low-light enhancement and exposure correction demonstrate superior quantitative and qualitative performance over existing methods.

Conclusion: IllumFlow enables precise illumination adaptation across lighting conditions while naturally supporting customizable brightness enhancement, achieving state-of-the-art performance in low-light image enhancement tasks.

Abstract: We present IllumFlow, a novel framework that synergizes conditional Rectified
Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our
model addresses low-light enhancement through separate optimization of
illumination and reflectance components, effectively handling both lighting
variations and noise. Specifically, we first decompose an input image into
reflectance and illumination components following Retinex theory. To model the
wide dynamic range of illumination variations in low-light images, we propose a
conditional rectified flow framework that represents illumination changes as a
continuous flow field. While complex noise primarily resides in the reflectance
component, we introduce a denoising network, enhanced by flow-derived data
augmentation, to remove reflectance noise and chromatic aberration while
preserving color fidelity. IllumFlow enables precise illumination adaptation
across lighting conditions while naturally supporting customizable brightness
enhancement. Extensive experiments on low-light enhancement and exposure
correction demonstrate superior quantitative and qualitative performance over
existing methods.

</details>


### [36] [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](https://arxiv.org/abs/2511.02415)
*Duo Xu,Hao Cheng,Xin Lin,Zhen Xie,Hao Wang*

Main category: cs.CV

TL;DR: This paper proposes an automated code-driven pipeline to generate ChartM³, a comprehensive dataset for complex chart understanding that improves multimodal models' reasoning capabilities and enables smaller models to match larger models' performance.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models have limited coverage of complex chart scenarios and computation-intensive reasoning tasks found in real-world applications, creating a need for better training datasets.

Method: Developed an automated multi-stage code-driven pipeline using retrieval-augmented generation (RAG) for chart templates and chain-of-thought (CoT) strategies to generate reasoning codes that drive chart rendering and statistical computations.

Result: Created ChartM³ dataset with 38K charts and 142K Q&A pairs for training, plus 2,871 evaluation samples. Supervised fine-tuning and reinforcement learning experiments showed significant improvements in reasoning capabilities and cross-domain generalization.

Conclusion: The proposed pipeline and dataset significantly enhance multimodal models' chart understanding abilities, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension tasks.

Abstract: Complex chart understanding tasks demand advanced visual recognition and
reasoning capabilities from multimodal large language models (MLLMs). However,
current research provides limited coverage of complex chart scenarios and
computation-intensive reasoning tasks prevalent in real-world applications.
This study proposes an automated multi-stage code-driven pipeline for
systematically generating visual reasoning datasets to address these
limitations. The pipeline integrates retrieval-augmented generation (RAG) to
retrieve professional chart templates and employs chain-of-thought (CoT)
strategies to generate reasoning codes that simulate real data distributions,
thereby driving chart rendering and question-related statistical computations.
Through model-based evaluation, the pipeline enhances chart diversity and data
quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and
multi-step dataset containing 38K charts and 142K Q&A pairs for training, along
with 2,871 high-quality evaluation samples for enabling practical performance
assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)
experiments demonstrate that our dataset significantly improves reasoning
capabilities and cross-domain generalization performance, enabling smaller
models to achieve performance comparable to larger-scale models in complex
chart comprehension.

</details>


### [37] [KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image](https://arxiv.org/abs/2511.02462)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: KAO is a novel framework using Kernel-Adaptive Optimization in diffusion models for satellite image inpainting, achieving state-of-the-art performance on VHR datasets like DeepGlobe and Massachusetts Roads Dataset.


<details>
  <summary>Details</summary>
Motivation: Satellite image inpainting is crucial for remote sensing analysis, but existing methods face challenges with very high-resolution datasets - preconditioned models require extensive retraining while postconditioned models have high computational overhead.

Method: KAO introduces Latent Space Conditioning to optimize a compact latent space for efficient inpainting, and incorporates Explicit Propagation with forward-backward fusion to improve stability and precision in the diffusion process.

Result: Experimental results show KAO sets a new benchmark for VHR satellite image restoration, providing scalable high-performance solution that balances efficiency of preconditioned models with flexibility of postconditioned models.

Conclusion: KAO offers an effective framework for satellite image inpainting that addresses the specific challenges of VHR datasets through innovative latent space optimization and diffusion process enhancements.

Abstract: Satellite image inpainting is a crucial task in remote sensing, where
accurately restoring missing or occluded regions is essential for robust image
analysis. In this paper, we propose KAO, a novel framework that utilizes
Kernel-Adaptive Optimization within diffusion models for satellite image
inpainting. KAO is specifically designed to address the challenges posed by
very high-resolution (VHR) satellite datasets, such as DeepGlobe and the
Massachusetts Roads Dataset. Unlike existing methods that rely on
preconditioned models requiring extensive retraining or postconditioned models
with significant computational overhead, KAO introduces a Latent Space
Conditioning approach, optimizing a compact latent space to achieve efficient
and accurate inpainting. Furthermore, we incorporate Explicit Propagation into
the diffusion process, facilitating forward-backward fusion, which improves the
stability and precision of the method. Experimental results demonstrate that
KAO sets a new benchmark for VHR satellite image restoration, providing a
scalable, high-performance solution that balances the efficiency of
preconditioned models with the flexibility of postconditioned models.

</details>


### [38] [MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer](https://arxiv.org/abs/2511.02473)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shotaro Tora*

Main category: cs.CV

TL;DR: MVAFormer is a novel transformer-based multi-view action recognition method designed for spatio-temporal action recognition (STAR) settings, using feature maps with preserved spatial information and divided self-attention mechanisms to effectively model relationships between different camera views.


<details>
  <summary>Details</summary>
Motivation: Previous multi-view action recognition methods focused only on recognizing single actions from entire videos and were not applicable to the STAR setting where each person's action is recognized sequentially. They also used embedding vectors that lost spatial information, which is crucial for STAR.

Method: Proposes MVAFormer with a transformer-based cooperation module that utilizes feature maps (preserving spatial information) and divides self-attention into same-view and different-view components to effectively model relationships between multiple camera views.

Result: MVAFormer outperforms comparison baselines by approximately 4.4 points on the F-measure in experiments using a newly collected dataset.

Conclusion: The proposed MVAFormer successfully addresses the limitations of previous methods by preserving spatial information through feature maps and effectively modeling multi-view relationships, making it suitable for spatio-temporal action recognition settings.

Abstract: Multi-view action recognition aims to recognize human actions using multiple
camera views and deals with occlusion caused by obstacles or crowds. In this
task, cooperation among views, which generates a joint representation by
combining multiple views, is vital. Previous studies have explored promising
cooperation methods for improving performance. However, since their methods
focus only on the task setting of recognizing a single action from an entire
video, they are not applicable to the recently popular spatio-temporal action
recognition~(STAR) setting, in which each person's action is recognized
sequentially. To address this problem, this paper proposes a multi-view action
recognition method for the STAR setting, called MVAFormer. In MVAFormer, we
introduce a novel transformer-based cooperation module among views. In contrast
to previous studies, which utilize embedding vectors with lost spatial
information, our module utilizes the feature map for effective cooperation in
the STAR setting, which preserves the spatial information. Furthermore, in our
module, we divide the self-attention for the same and different views to model
the relationship between multiple views effectively. The results of experiments
using a newly collected dataset demonstrate that MVAFormer outperforms the
comparison baselines by approximately $4.4$ points on the F-measure.

</details>


### [39] [OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control](https://arxiv.org/abs/2511.02483)
*Xilong Zhou,Jianchun Chen,Pramod Rao,Timo Teufel,Linjie Lyu,Tigran Minasian,Oleksandr Sotnychenko,Xiaoxiao Long,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: OLATverse is a large-scale dataset with 9M images of 765 real-world objects captured under controlled lighting conditions, providing high-fidelity appearance data for inverse rendering and relighting research.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current techniques that rely heavily on synthetic datasets and small-scale real-world datasets, which restricts realism and generalization in object-centric inverse rendering, novel view synthesis, and relighting.

Method: Captured 765 real-world objects using 35 DSLR cameras and 331 individually controlled light sources, providing well-calibrated camera parameters, object masks, photometric surface normals, and diffuse albedo as auxiliary resources.

Result: Created a comprehensive real-world object-centric benchmark for inverse rendering and normal estimation, with extensive evaluation set and publicly available dataset.

Conclusion: OLATverse represents a pivotal step toward integrating next-generation inverse rendering and relighting methods with real-world data, bridging the gap between synthetic training and real-world application.

Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions. While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization. To address this gap, OLATverse
offers two key advantages over existing datasets: large-scale coverage of real
objects and high-fidelity appearance under precisely controlled illuminations.
Specifically, OLATverse contains 765 common and uncommon real-world objects,
spanning a wide range of material categories. Each object is captured using 35
DSLR cameras and 331 individually controlled light sources, enabling the
simulation of diverse illumination conditions. In addition, for each object, we
provide well-calibrated camera parameters, accurate object masks, photometric
surface normals, and diffuse albedo as auxiliary resources. We also construct
an extensive evaluation set, establishing the first comprehensive real-world
object-centric benchmark for inverse rendering and normal estimation. We
believe that OLATverse represents a pivotal step toward integrating the next
generation of inverse rendering and relighting methods with real-world data.
The full dataset, along with all post-processing workflows, will be publicly
released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.

</details>


### [40] [Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization](https://arxiv.org/abs/2511.02489)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: This paper presents a cross-view UAV localization framework using object detection and graph neural networks to address localization in GNSS-denied areas, achieving strong performance through instance extraction and graph-based similarity metrics.


<details>
  <summary>Details</summary>
Motivation: With the growth of low-altitude economy and UAV usage, there's a need for reliable localization in GNSS-denied areas where satellite-based methods fail. Existing approaches face limitations with cross-domain differences, misalignment, and content loss.

Method: Leverages modern object detection to extract salient instances from UAV and satellite images, integrates graph neural network to reason about inter-image and intra-image node relationships, and uses fine-grained graph-based node-similarity metric for retrieval and localization.

Result: Extensive experiments on public and real-world datasets show the method effectively handles heterogeneous appearance differences, generalizes well, and is applicable to scenarios with larger modality gaps like infrared-visible image matching.

Conclusion: The proposed cross-view UAV localization framework using object detection and graph neural networks provides an effective solution for GNSS-denied environments, demonstrating strong performance and generalization capabilities across different modalities.

Abstract: With the rapid growth of the low-altitude economy, UAVs have become crucial
for measurement and tracking in patrol systems. However, in GNSS-denied areas,
satellite-based localization methods are prone to failure. This paper presents
a cross-view UAV localization framework that performs map matching via object
detection, aimed at effectively addressing cross-temporal, cross-view,
heterogeneous aerial image matching. In typical pipelines, UAV visual
localization is formulated as an image-retrieval problem: features are
extracted to build a localization map, and the pose of a query image is
estimated by matching it to a reference database with known poses. Because
publicly available UAV localization datasets are limited, many approaches
recast localization as a classification task and rely on scene labels in these
datasets to ensure accuracy. Other methods seek to reduce cross-domain
differences using polar-coordinate reprojection, perspective transformations,
or generative adversarial networks; however, they can suffer from misalignment,
content loss, and limited realism. In contrast, we leverage modern object
detection to accurately extract salient instances from UAV and satellite
images, and integrate a graph neural network to reason about inter-image and
intra-image node relationships. Using a fine-grained, graph-based
node-similarity metric, our method achieves strong retrieval and localization
performance. Extensive experiments on public and real-world datasets show that
our approach handles heterogeneous appearance differences effectively and
generalizes well, making it applicable to scenarios with larger modality gaps,
such as infrared-visible image matching. Our dataset will be publicly available
at the following URL: https://github.com/liutao23/ODGNNLoc.git.

</details>


### [41] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: Introduces DetectiumFire, a large-scale multi-modal dataset with 22.5k fire images and 2.5k videos to address the lack of annotated fire data for AI applications.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal models struggle with fire domain applications due to limited publicly available datasets with high-quality fire annotations.

Method: Created DetectiumFire dataset with high-resolution fire images and videos annotated with computer vision labels and detailed textual prompts covering diverse fire types and scenarios.

Result: The dataset enables successful applications in object detection, diffusion-based image generation, and vision-language reasoning, showing reduced redundancy and enhanced real-world coverage.

Conclusion: DetectiumFire advances fire-related AI research and supports intelligent safety system development, with the dataset publicly released to the community.

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [42] [Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](https://arxiv.org/abs/2511.02503)
*Robinson Umeike,Neil Getty,Yin Xiangyu,Yi Jiang*

Main category: cs.CV

TL;DR: PtychoBench benchmark compares SFT vs ICL for microscopy workflow automation, finding task-dependent optimal strategies: SFT+ICL best for visual artifact detection, while ICL alone works best for textual parameter recommendation.


<details>
  <summary>Details</summary>
Motivation: To determine optimal domain adaptation strategies for foundation models (LLMs/VLMs) in specialized scientific tasks like microscopy workflow automation, where general-purpose models need specialization but the best approach is unclear.

Method: Introduced PtychoBench multi-modal benchmark to systematically compare Supervised Fine-Tuning (SFT) and In-Context Learning (ICL) strategies on visual artifact detection (VLMs) and textual parameter recommendation (LLMs) tasks in data-scarce conditions.

Result: For visual tasks, SFT combined with ICL achieved best performance (Micro-F1 0.728). For textual tasks, ICL on large base models was superior (Micro-F1 0.847), outperforming SFT models. Also identified context-aware prompting superiority and contextual interference in fine-tuned models.

Conclusion: Optimal specialization strategy depends on task modality, providing a framework for developing effective science-based AI agentic systems. Visual tasks benefit from SFT+ICL combination, while textual tasks perform best with ICL alone.

Abstract: The automation of workflows in advanced microscopy is a key goal where
foundation models like Language Models (LLMs) and Vision-Language Models (VLMs)
show great potential. However, adapting these general-purpose models for
specialized scientific tasks is critical, and the optimal domain adaptation
strategy is often unclear. To address this, we introduce PtychoBench, a new
multi-modal, multi-task benchmark for ptychographic analysis. Using this
benchmark, we systematically compare two specialization strategies: Supervised
Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies
on a visual artifact detection task with VLMs and a textual parameter
recommendation task with LLMs in a data-scarce regime. Our findings reveal that
the optimal specialization pathway is task-dependent. For the visual task, SFT
and ICL are highly complementary, with a fine-tuned model guided by
context-aware examples achieving the highest mean performance (Micro-F1 of
0.728). Conversely, for the textual task, ICL on a large base model is the
superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a
powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm
the superiority of context-aware prompting and identify a consistent contextual
interference phenomenon in fine-tuned models. These results, benchmarked
against strong baselines including GPT-4o and a DINOv3-based classifier, offer
key observations for AI in science: the optimal specialization path in our
benchmark is dependent on the task modality, offering a clear framework for
developing more effective science-based agentic systems.

</details>


### [43] [A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding](https://arxiv.org/abs/2511.02565)
*Jingyu Lu,Haonan Wang,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: VCFlow is a subject-agnostic brain decoding framework that reconstructs visual experiences from fMRI by modeling the ventral-dorsal visual system architecture, achieving fast reconstruction without subject-specific training.


<details>
  <summary>Details</summary>
Motivation: To enable clinical applications of brain decoding by overcoming challenges in cross-subject generalization and complex brain signal processing, while avoiding the need for extensive per-subject data and computation.

Method: Hierarchical decoding framework that disentangles features from early visual cortex, ventral, and dorsal streams, combined with feature-level contrastive learning for subject-invariant semantic representations.

Result: Achieves only 7% accuracy loss compared to conventional methods while generating reconstructed videos in 10 seconds without retraining, using significantly less data than traditional approaches that require over 12 hours per subject.

Conclusion: VCFlow provides a fast, clinically scalable solution for subject-agnostic visual reconstruction from fMRI, making brain decoding more practical for real-world applications.

Abstract: Subject-agnostic brain decoding, which aims to reconstruct continuous visual
experiences from fMRI without subject-specific training, holds great potential
for clinical applications. However, this direction remains underexplored due to
challenges in cross-subject generalization and the complex nature of brain
signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a
novel hierarchical decoding framework that explicitly models the ventral-dorsal
architecture of the human visual system to learn multi-dimensional
representations. By disentangling and leveraging features from early visual
cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary
cognitive information essential for visual reconstruction. Furthermore, we
introduce a feature-level contrastive learning strategy to enhance the
extraction of subject-invariant semantic representations, thereby enhancing
subject-agnostic applicability to previously unseen subjects. Unlike
conventional pipelines that need more than 12 hours of per-subject data and
heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates
each reconstructed video in 10 seconds without any retraining, offering a fast
and clinically scalable solution. The source code will be released upon
acceptance of the paper.

</details>


### [44] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: An energy-based optimization method for video shot assembly that learns from reference videos to automatically arrange shots according to specific narrative requirements and artistic styles, enabling users without editing experience to create compelling videos.


<details>
  <summary>Details</summary>
Motivation: Traditional shot assembly requires manual work by experienced editors, and current automated video editing technologies fail to capture creators' unique artistic expression in shot sequencing and arrangement.

Method: 1) Visual-semantic matching between LLM-generated scripts and video library for candidate shots; 2) Shot segmentation and attribute extraction (shot size, camera motion, semantics) from reference videos; 3) Energy-based models to learn from attributes and score sequences; 4) Optimization combining multiple syntax rules.

Result: The method automates shot arrangement and combination according to specific logic, narrative requirements, or artistic styles, learning assembly styles from reference videos to create coherent visual sequences.

Conclusion: The proposed energy-based shot assembly system enables even inexperienced users to create visually compelling videos by learning and applying artistic assembly styles from reference videos.

Abstract: Shot assembly is a crucial step in film production and video editing,
involving the sequencing and arrangement of shots to construct a narrative,
convey information, or evoke emotions. Traditionally, this process has been
manually executed by experienced editors. While current intelligent video
editing technologies can handle some automated video editing tasks, they often
fail to capture the creator's unique artistic expression in shot assembly.To
address this challenge, we propose an energy-based optimization method for
video shot assembly. Specifically, we first perform visual-semantic matching
between the script generated by a large language model and a video library to
obtain subsets of candidate shots aligned with the script semantics. Next, we
segment and label the shots from reference videos, extracting attributes such
as shot size, camera motion, and semantics. We then employ energy-based models
to learn from these attributes, scoring candidate shot sequences based on their
alignment with reference styles. Finally, we achieve shot assembly optimization
by combining multiple syntax rules, producing videos that align with the
assembly style of the reference videos. Our method not only automates the
arrangement and combination of independent shots according to specific logic,
narrative requirements, or artistic styles but also learns the assembly style
of reference videos, creating a coherent visual sequence or holistic visual
expression. With our system, even users with no prior video editing experience
can create visually compelling videos. Project page:
https://sobeymil.github.io/esa.com

</details>


### [45] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai,Ryugo Morita,Shunsuke Kitada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: TAUE is a training-free diffusion model framework that enables zero-shot, layer-wise image generation through Noise Transplantation and Cultivation (NTC), achieving semantic coherence across layers without fine-tuning or datasets.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models produce single flattened images, limiting professional applications requiring layer-wise control. Existing solutions either need fine-tuning with inaccessible datasets or generate only isolated foreground elements without complete scenes.

Method: Uses Noise Transplantation and Cultivation (NTC) to extract intermediate latent representations from foreground and composite generation processes, transplanting them into initial noise for subsequent layers to ensure semantic and structural coherence.

Result: Achieves performance comparable to fine-tuned methods while maintaining high image quality and fidelity, enabling consistent multi-layered outputs without training requirements.

Conclusion: TAUE eliminates costly training and dataset needs while enabling novel applications like complex compositional editing, making generative workflows more accessible and controllable.

Abstract: Despite the remarkable success of text-to-image diffusion models, their
output of a single, flattened image remains a critical bottleneck for
professional applications requiring layer-wise control. Existing solutions
either rely on fine-tuning with large, inaccessible datasets or are
training-free yet limited to generating isolated foreground elements, failing
to produce a complete and coherent scene. To address this, we introduce the
Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a
novel framework for zero-shot, layer-wise image generation. Our core technique,
Noise Transplantation and Cultivation (NTC), extracts intermediate latent
representations from both foreground and composite generation processes,
transplanting them into the initial noise for subsequent layers. This ensures
semantic and structural coherence across foreground, background, and composite
layers, enabling consistent, multi-layered outputs without requiring
fine-tuning or auxiliary datasets. Extensive experiments show that our
training-free method achieves performance comparable to fine-tuned methods,
enhancing layer-wise consistency while maintaining high image quality and
fidelity. TAUE not only eliminates costly training and dataset requirements but
also unlocks novel downstream applications, such as complex compositional
editing, paving the way for more accessible and controllable generative
workflows.

</details>


### [46] [LLEXICORP: End-user Explainability of Convolutional Neural Networks](https://arxiv.org/abs/2511.02720)
*Vojtěch Kůr,Adam Bajger,Adam Kukučka,Marek Hradil,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LLEXICORP automates concept relevance propagation (CRP) by using multimodal large language models to automatically name concepts and generate natural language explanations, making AI interpretability more accessible.


<details>
  <summary>Details</summary>
Motivation: Current CRP workflows are manual and require experts to inspect activation images and synthesize explanations, limiting accessibility and scalability of AI interpretability.

Method: A modular pipeline that couples CRP with a multimodal large language model, using crafted prompts to teach CRP semantics and separating naming and explanation tasks to ensure faithfulness.

Result: The method automatically assigns descriptive names to concept prototypes and generates tailored natural-language explanations for different audiences, evaluated qualitatively on ImageNet with VGG16.

Conclusion: Integrating concept-based attribution methods with large language models significantly lowers the barrier to interpreting deep neural networks, enabling more transparent AI systems.

Abstract: Convolutional neural networks (CNNs) underpin many modern computer vision
systems. With applications ranging from common to critical areas, a need to
explain and understand the model and its decisions (XAI) emerged. Prior works
suggest that in the top layers of CNNs, the individual channels can be
attributed to classifying human-understandable concepts. Concept relevance
propagation (CRP) methods can backtrack predictions to these channels and find
images that most activate these channels. However, current CRP workflows are
largely manual: experts must inspect activation images to name the discovered
concepts and must synthesize verbose explanations from relevance maps, limiting
the accessibility of the explanations and their scalability.
  To address these issues, we introduce Large Language model EXplaIns COncept
Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a
multimodal large language model. Our approach automatically assigns descriptive
names to concept prototypes and generates natural-language explanations that
translate quantitative relevance distributions into intuitive narratives. To
ensure faithfulness, we craft prompts that teach the language model the
semantics of CRP through examples and enforce a separation between naming and
explanation tasks. The resulting text can be tailored to different audiences,
offering low-level technical descriptions for experts and high-level summaries
for non-technical stakeholders.
  We qualitatively evaluate our method on various images from ImageNet on a
VGG16 model. Our findings suggest that integrating concept-based attribution
methods with large language models can significantly lower the barrier to
interpreting deep neural networks, paving the way for more transparent AI
systems.

</details>


### [47] [LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization](https://arxiv.org/abs/2511.02510)
*Jee Won Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: LiteVoxel is a self-tuning training pipeline for sparse-voxel rasterization that addresses underfitting of low-frequency content, brittle pruning heuristics, and VRAM overgrowth while maintaining performance comparable to SVRaster.


<details>
  <summary>Details</summary>
Motivation: Sparse-voxel rasterization suffers from underfitting low-frequency content, dependence on brittle pruning heuristics, and VRAM overgrowth that inflates memory usage.

Method: Uses inverse-Sobel reweighting with mid-training gamma-ramp for low-frequency awareness, depth-quantile pruning with EMA-hysteresis guards, and ray-footprint-based priority-driven subdivision under explicit growth budget.

Result: Reduces peak VRAM by 40%-60%, mitigates errors in low-frequency regions and boundary instability while maintaining comparable PSNR/SSIM, training time, and FPS to SVRaster.

Conclusion: LiteVoxel enables more predictable, memory-efficient training without sacrificing perceptual quality by preserving low-frequency detail that prior methods miss.

Abstract: Sparse-voxel rasterization is a fast, differentiable alternative for
optimization-based scene reconstruction, but it tends to underfit low-frequency
content, depends on brittle pruning heuristics, and can overgrow in ways that
inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
makes SV rasterization both steadier and lighter. Our loss is made
low-frequency aware via an inverse-Sobel reweighting with a mid-training
gamma-ramp, shifting gradient budget to flat regions only after geometry
stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
logic on maximum blending weight, stabilized by EMA-hysteresis guards and
refines structure through ray-footprint-based, priority-driven subdivision
under an explicit growth budget. Ablations and full-system results across
Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
of errors in low-frequency regions and boundary instability while keeping
PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
detail that prior setups miss, enabling more predictable, memory-efficient
training without sacrificing perceptual quality.

</details>


### [48] [Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data](https://arxiv.org/abs/2511.02541)
*Jessica Plassmann,Nicolas Schuler,Georg von Freymann,Michael Schuth*

Main category: cs.CV

TL;DR: This study explores unsupervised learning methods for automated anomaly detection in shearographic images, evaluating three architectures trained solely on defect-free data to enable industrial adoption without expert interpretation.


<details>
  <summary>Details</summary>
Motivation: Shearography is limited in industrial adoption due to the need for expert interpretation. This research aims to reduce reliance on labeled data and manual evaluation through unsupervised learning methods.

Method: Three unsupervised architectures were evaluated: fully connected autoencoder, convolutional autoencoder, and student-teacher feature matching model. All models were trained on defect-free data, with controlled datasets including both ideal and realistic deformation conditions.

Result: The student-teacher approach achieved superior classification robustness and precise spatial defect localization, showing improved feature separability compared to autoencoder-based models. A YOLOv8 model served as reference for benchmarking localization quality.

Conclusion: The study demonstrates the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments, with the student-teacher model showing particular promise.

Abstract: Shearography is a non-destructive testing method for detecting subsurface
defects, offering high sensitivity and full-field inspection capabilities.
However, its industrial adoption remains limited due to the need for expert
interpretation. To reduce reliance on labeled data and manual evaluation, this
study explores unsupervised learning methods for automated anomaly detection in
shearographic images. Three architectures are evaluated: a fully connected
autoencoder, a convolutional autoencoder, and a student-teacher feature
matching model. All models are trained solely on defect-free data. A controlled
dataset was developed using a custom specimen with reproducible defect
patterns, enabling systematic acquisition of shearographic measurements under
both ideal and realistic deformation conditions. Two training subsets were
defined: one containing only undistorted, defect-free samples, and one
additionally including globally deformed, yet defect-free, data. The latter
simulates practical inspection conditions by incorporating deformation-induced
fringe patterns that may obscure localized anomalies. The models are evaluated
in terms of binary classification and, for the student-teacher model, spatial
defect localization. Results show that the student-teacher approach achieves
superior classification robustness and enables precise localization. Compared
to the autoencoder-based models, it demonstrates improved separability of
feature representations, as visualized through t-SNE embeddings. Additionally,
a YOLOv8 model trained on labeled defect data serves as a reference to
benchmark localization quality. This study underscores the potential of
unsupervised deep learning for scalable, label-efficient shearographic
inspection in industrial environments.

</details>


### [49] [Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction](https://arxiv.org/abs/2511.02558)
*Ali Farki,Elaheh Moradi,Deepika Koundal,Jussi Tohka*

Main category: cs.CV

TL;DR: Deep learning models can predict future brain MRI scans from baseline images with high fidelity, enabling personalized neurodegenerative disease prognosis.


<details>
  <summary>Details</summary>
Motivation: Predicting future brain states from baseline MRI is crucial for studying neurodegenerative diseases like Alzheimer's, but most existing methods focus on cognitive scores rather than full image-to-image prediction.

Method: Implemented and evaluated five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, ODE-UNet) on two longitudinal cohorts (ADNI and AIBL) for MRI image-to-image prediction.

Result: Best performing models achieved high-fidelity predictions, and all models generalized well to independent external datasets, demonstrating robust cross-cohort performance.

Conclusion: Deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis in neurodegenerative diseases.

Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI)
is a central challenge in neuroimaging and has important implications for
studying neurodegenerative diseases such as Alzheimer's disease (AD). Most
existing approaches predict future cognitive scores or clinical outcomes, such
as conversion from mild cognitive impairment to dementia. Instead, here we
investigate longitudinal MRI image-to-image prediction that forecasts a
participant's entire brain MRI several years into the future, intrinsically
modeling complex, spatially distributed neurodegenerative patterns. We
implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR,
Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL).
Predicted follow-up MRIs are directly compared with the actual follow-up scans
using metrics that capture global similarity and local differences. The best
performing models achieve high-fidelity predictions, and all models generalize
well to an independent external dataset, demonstrating robust cross-cohort
performance. Our results indicate that deep learning can reliably predict
participant-specific brain MRI at the voxel level, offering new opportunities
for individualized prognosis.

</details>


### [50] [The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic](https://arxiv.org/abs/2511.02563)
*Akash Sharma,Chinmay Mhatre,Sankalp Gawali,Ruthvik Bokkasam,Brij Kishore,Vishwajeet Pattanaik,Tarun Rambha,Abdul R. Pinjari,Vijay Kovvali,Anirban Chakraborty,Punit Rathore,Raghu Krishnapuram,Yogesh Simmhan*

Main category: cs.CV

TL;DR: UVH-26 is the first large-scale Indian traffic dataset with 26,646 images from Bengaluru CCTV cameras, annotated with 1.8M bounding boxes across 14 vehicle classes, showing 8.4-31.5% improvement over COCO-trained models.


<details>
  <summary>Details</summary>
Motivation: Address the lack of domain-specific training data for Indian traffic scenarios in existing global benchmarks like COCO, which don't capture the heterogeneity of Indian urban mobility.

Method: Collected 26,646 high-resolution images from 2800 Bengaluru CCTV cameras over 4 weeks, annotated through crowdsourced hackathon with 565 students, derived consensus ground truth using Majority Voting and STAPLE algorithms, and trained multiple detectors (YOLO11, RT-DETR, DAMO-YOLO).

Result: Models trained on UVH-26 achieved 8.4-31.5% improvements in mAP50:95 over COCO-trained baselines, with RT-DETR-X performing best at 0.67 mAP50:95 vs 0.40 for COCO on common classes.

Conclusion: Domain-specific training data significantly improves detection performance for Indian traffic scenarios, and UVH-26 fills a critical gap for advancing intelligent transportation systems in emerging nations with complex traffic conditions.

Abstract: This report describes the UVH-26 dataset, the first public release by
AIM@IISc of a large-scale dataset of annotated traffic-camera images from
India. The dataset comprises 26,646 high-resolution (1080p) images sampled from
2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently
annotated through a crowdsourced hackathon involving 565 college students from
across India. In total, 1.8 million bounding boxes were labeled across 14
vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler
(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,
Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k
consensus ground truth bounding boxes and labels were derived for distinct
objects in the 26k images using Majority Voting and STAPLE algorithms. Further,
we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,
and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,
mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in
mAP50:95 over equivalent baseline models trained on COCO dataset, with
RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40
for COCO-trained weights for common classes (Car, Bus, and Truck). This
demonstrates the benefits of domain-specific training data for Indian traffic
scenarios. The release package provides the 26k images with consensus
annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the
6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the
heterogeneity of Indian urban mobility directly from operational traffic-camera
streams, UVH-26 addresses a critical gap in existing global benchmarks, and
offers a foundation for advancing detection, classification, and deployment of
intelligent transportation systems in emerging nations with complex traffic
conditions.

</details>


### [51] [Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification](https://arxiv.org/abs/2511.02564)
*Md Rashidunnabi,Kailash A. Hambarde,Vasco Lopes,Joao C. Neves,Hugo Proenca*

Main category: cs.CV

TL;DR: MTF-CVReID is a parameter-efficient framework for video-based person re-identification in cross-view domains that introduces seven complementary modules over a ViT-B/16 backbone to address extreme viewpoint shifts, scale disparities, and temporal inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Video-based person ReID in cross-view domains (e.g., aerial-ground surveillance) faces challenges from extreme viewpoint shifts, scale disparities, and temporal inconsistencies, making it an open problem that needs robust solutions.

Method: Proposes MTF-CVReID with seven modules: Cross-Stream Feature Normalization, Multi-Resolution Feature Harmonization, Identity-Aware Memory Module, Temporal Dynamics Modeling, Inter-View Feature Alignment, Hierarchical Temporal Pattern Learning, and Multi-View Identity Consistency Learning using contrastive learning.

Result: Achieves state-of-the-art performance on AG-VPReID benchmark across all altitude levels, with strong cross-dataset generalization to G2A-VReID and MARS datasets. Maintains real-time efficiency (189 FPS) while adding only ~2M parameters and 0.7 GFLOPs over baseline.

Conclusion: Carefully designed adapter-based modules can substantially enhance cross-view robustness and temporal consistency without compromising computational efficiency, demonstrating the effectiveness of parameter-efficient approaches for cross-view video ReID.

Abstract: Video-based person re-identification (ReID) in cross-view domains (for
example, aerial-ground surveillance) remains an open problem because of extreme
viewpoint shifts, scale disparities, and temporal inconsistencies. To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone. Specifically,
we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and
view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale
stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to
reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for
motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment
(IVFA) for perspective-invariant representation alignment; (6) Hierarchical
Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;
and (7) Multi-View Identity Consistency Learning (MVICL) that enforces
cross-view identity coherence using a contrastive learning paradigm. Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets. These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency. The source code is available at
https://github.com/MdRashidunnabi/MTF-CVReID

</details>


### [52] [Zero-Shot Multi-Animal Tracking in the Wild](https://arxiv.org/abs/2511.02591)
*Jan Frederik Meier,Timo Lüddecke*

Main category: cs.CV

TL;DR: Zero-shot multi-animal tracking framework using Grounding Dino detector and SAM 2 tracker that works across diverse species without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-animal tracking requires extensive model fine-tuning and heuristic design for each scenario, limiting practical application.

Method: Combines Grounding Dino object detector with Segment Anything Model 2 (SAM 2) tracker using carefully designed heuristics for zero-shot application.

Result: Achieves strong and consistent performance on ChimpAct, Bird Flock Tracking, AnimalTrack, and GMOT-40 datasets across diverse species and environments.

Conclusion: Vision foundation models enable effective zero-shot multi-animal tracking without retraining, demonstrating broad applicability across ecological scenarios.

Abstract: Multi-animal tracking is crucial for understanding animal ecology and
behavior. However, it remains a challenging task due to variations in habitat,
motion patterns, and species appearance. Traditional approaches typically
require extensive model fine-tuning and heuristic design for each application
scenario. In this work, we explore the potential of recent vision foundation
models for zero-shot multi-animal tracking. By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation. Evaluations on
ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate
strong and consistent performance across diverse species and environments. The
code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.

</details>


### [53] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: UniChange is the first MLLM-based unified change detection model that integrates both binary change detection (BCD) and semantic change detection (SCD) tasks using special tokens and text prompts, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current change detection models are limited to single-type annotated data and cannot leverage diverse BCD and SCD datasets simultaneously, leading to poor generalization and limited versatility.

Method: Leverages Multimodal Large Language Models (MLLMs) with language priors, introduces three special tokens ([T1], [T2], [CHANGE]), and uses text prompts to guide change category identification without predefined classification heads.

Result: Achieved SOTA performance on four benchmarks: WHU-CD (90.41 IoU), S2Looking (53.04 IoU), LEVIR-CD+ (78.87 IoU), and SECOND (57.62 IoU), surpassing all previous methods.

Conclusion: UniChange successfully unifies BCD and SCD tasks, enables knowledge acquisition from multi-source datasets with conflicting class definitions, and demonstrates superior generalization capabilities through MLLM integration.

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [54] [Robust Face Liveness Detection for Biometric Authentication using Single Image](https://arxiv.org/abs/2511.02645)
*Poulami Raha,Yeongnam Chae*

Main category: cs.CV

TL;DR: A lightweight CNN framework for detecting face spoofing attacks (print/display, video, wrap attacks) that provides fast liveness detection in 1-2 seconds on CPU, with a new dataset of 500+ videos from 60 subjects.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to presentation attacks where malicious actors use spoofing techniques to gain illegitimate access to secure systems.

Method: Proposes a novel lightweight CNN framework specifically designed to identify various types of spoof attacks including print/display, video, and wrap attacks.

Result: The architecture provides seamless liveness detection with fast biometric authentication (1-2 seconds on CPU) and includes a newly created 2D spoof attack dataset with 500+ videos from 60 subjects.

Conclusion: The proposed robust architecture effectively detects presentation attacks and demonstrates practical viability through real-time detection capabilities and comprehensive dataset validation.

Abstract: Biometric technologies are widely adopted in security, legal, and financial
systems. Face recognition can authenticate a person based on the unique facial
features such as shape and texture. However, recent works have demonstrated the
vulnerability of Face Recognition Systems (FRS) towards presentation attacks.
Using spoofing (aka.,presentation attacks), a malicious actor can get
illegitimate access to secure systems. This paper proposes a novel light-weight
CNN framework to identify print/display, video and wrap attacks. The proposed
robust architecture provides seamless liveness detection ensuring faster
biometric authentication (1-2 seconds on CPU). Further, this also presents a
newly created 2D spoof attack dataset consisting of more than 500 videos
collected from 60 subjects. To validate the effectiveness of this architecture,
we provide a demonstration video depicting print/display, video and wrap attack
detection approaches. The demo can be viewed in the following link:
https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa

</details>


### [55] [Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models](https://arxiv.org/abs/2511.02650)
*Tianfan Peng,Yuntao Du,Pengzhou Ji,Shijie Dong,Kailin Jiang,Mingchuan Ma,Yijun Tian,Jinhe Bi,Qian Li,Wei Du,Feng Xiao,Lizhen Cui*

Main category: cs.CV

TL;DR: UniPruneBench is a unified benchmark for evaluating visual token pruning methods in multimodal LLMs, covering 6 ability dimensions, 10 datasets, 10 compression algorithms, and 3 LMM families, with key findings including random pruning as a strong baseline and pruning ratio as the dominant performance factor.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models suffer from severe inference inefficiency due to large numbers of visual tokens, and existing token compression methods have fragmented and inconsistent evaluation.

Method: Developed UniPruneBench - a unified benchmark with standardized protocols across multiple dimensions, incorporating system-level metrics like runtime and prefilling latency alongside task accuracy.

Result: Key findings: random pruning is surprisingly strong, no single method consistently outperforms others, pruning sensitivity varies significantly (OCR most vulnerable), and pruning ratio is the dominant performance factor.

Conclusion: UniPruneBench provides a reliable foundation for future research on efficient multimodal modeling and will help standardize evaluation of visual token compression methods.

Abstract: Large multimodal models (LMMs) often suffer from severe inference
inefficiency due to the large number of visual tokens introduced by image
encoders. While recent token compression methods, such as pruning and merging,
have shown promise in reducing redundancy, their evaluation remains fragmented
and inconsistent. In this work, we present UniPruneBench, a unified and
extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
provides standardized protocols across six ability dimensions and ten datasets,
covering ten representative compression algorithms and three families of LMMs
(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
system-level metrics such as runtime and prefilling latency to provide a
holistic view. Our experiments uncover several key findings: (1) random pruning
is a surprisingly strong baseline, (2) no single method consistently
outperforms others across scenarios, (3) pruning sensitivity varies
significantly across tasks, with OCR being most vulnerable, and (4) pruning
ratio is the dominant factor governing performance degradation. We believe
UniPruneBench will serve as a reliable foundation for future research on
efficient multimodal modeling.

</details>


### [56] [Differentiable Hierarchical Visual Tokenization](https://arxiv.org/abs/2511.02652)
*Marius Aasan,Martine Hjelkrem-Tan,Nico Catalano,Changkyu Choi,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: The paper introduces a differentiable tokenizer that adapts to image content at pixel-level granularity while maintaining compatibility with existing Vision Transformer architectures, enabling retrofitting of pretrained models.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers use fixed patch tokens that ignore spatial and semantic structure of images, limiting their ability to adapt to image content effectively.

Method: End-to-end differentiable tokenizer with hierarchical model selection using information criteria, providing pixel-level granularity adaptation to image content.

Result: Competitive performance in both image-level classification and dense-prediction tasks, with additional capability for out-of-the-box raster-to-vector conversion.

Conclusion: The proposed differentiable tokenizer successfully addresses limitations of fixed patch tokens in Vision Transformers while maintaining backward compatibility and expanding functionality to vector conversion tasks.

Abstract: Vision Transformers rely on fixed patch tokens that ignore the spatial and
semantic structure of images. In this work, we introduce an end-to-end
differentiable tokenizer that adapts to image content with pixel-level
granularity while remaining backward-compatible with existing architectures for
retrofitting pretrained models. Our method uses hierarchical model selection
with information criteria to provide competitive performance in both
image-level classification and dense-prediction tasks, and even supports
out-of-the-box raster-to-vector conversion.

</details>


### [57] [Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.02685)
*Chao Yuan,Zanwu Liu,Guiwei Zhang,Haoxuan Xu,Yujian Zhao,Guanglin Niu,Bo Li*

Main category: cs.CV

TL;DR: A novel VI-ReID framework using Modality-Transition Representation Learning (MTRL) with generated intermediate images as transmitters between visible and infrared modalities, achieving state-of-the-art performance without additional parameters.


<details>
  <summary>Details</summary>
Motivation: Existing VI-ReID methods have limitations: they create substantial modality gaps, rely on intermediate representations that don't fully utilize intermediate features, and often require additional parameters or lack interpretability.

Method: Proposes MTRL framework with middle generated images as transmitters between modalities, using modality-transition contrastive loss and modality-query regularization loss for training without additional parameters.

Result: Significantly and consistently outperforms existing state-of-the-art methods on three typical VI-ReID datasets while maintaining the same inference speed as the backbone.

Conclusion: The proposed MTRL framework effectively aligns cross-modal features through modality-transition representation learning, achieving superior VI-ReID performance without computational overhead.

Abstract: Visible-infrared person re-identification (VI-ReID) technique could associate
the pedestrian images across visible and infrared modalities in the practical
scenarios of background illumination changes. However, a substantial gap
inherently exists between these two modalities. Besides, existing methods
primarily rely on intermediate representations to align cross-modal features of
the same person. The intermediate feature representations are usually create by
generating intermediate images (kind of data enhancement), or fusing
intermediate features (more parameters, lack of interpretability), and they do
not make good use of the intermediate features. Thus, we propose a novel
VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a
middle generated image as a transmitter from visible to infrared modals, which
are fully aligned with the original visible images and similar to the infrared
modality. After that, using a modality-transition contrastive loss and a
modality-query regularization loss for training, which could align the
cross-modal features more effectively. Notably, our proposed framework does not
need any additional parameters, which achieves the same inference speed to the
backbone while improving its performance on VI-ReID task. Extensive
experimental results illustrate that our model significantly and consistently
outperforms existing SOTAs on three typical VI-ReID datasets.

</details>


### [58] [VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models](https://arxiv.org/abs/2511.02712)
*Zhicheng Zhang,Weicheng Wang,Yongjie Zhu,Wenyu Qin,Pengfei Wan,Di Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: Proposes VidEmo framework for video emotion analysis using affective cues-guided reasoning and a two-stage tuning process with curriculum learning and reinforcement learning, achieving competitive performance on 15 face perception tasks.


<details>
  <summary>Details</summary>
Motivation: Current video emotion analysis faces challenges due to emotions' dynamic and cues-dependent nature, making it difficult to understand complex emotional states with reasonable rationale.

Method: Affective cues-guided reasoning framework unifying attribute perception, expression analysis, and emotional understanding. Uses VidEmo foundation models with two-stage tuning: curriculum emotion learning followed by affective-tree reinforcement learning.

Result: Achieves competitive performance across 15 face perception tasks, setting new milestones in video emotion understanding.

Conclusion: The proposed framework effectively addresses the challenges in video emotion analysis through structured reasoning and comprehensive data infrastructure, demonstrating significant improvements in emotion understanding tasks.

Abstract: Understanding and predicting emotion from videos has gathered significant
attention in recent studies, driven by advancements in video large language
models (VideoLLMs). While advanced methods have made progress in video emotion
analysis, the intrinsic nature of emotions poses significant challenges.
Emotions are characterized by dynamic and cues-dependent properties, making it
difficult to understand complex and evolving emotional states with reasonable
rationale. To tackle these challenges, we propose a novel affective cues-guided
reasoning framework that unifies fundamental attribute perception, expression
analysis, and high-level emotional understanding in a stage-wise manner. At the
core of our approach is a family of video emotion foundation models (VidEmo),
specifically designed for emotion reasoning and instruction-following. These
models undergo a two-stage tuning process: first, curriculum emotion learning
for injecting emotion knowledge, followed by affective-tree reinforcement
learning for emotion reasoning. Moreover, we establish a foundational data
infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)
consisting of 2.1M diverse instruction-based samples. Emo-CFG includes
explainable emotional question-answering, fine-grained captions, and associated
rationales, providing essential resources for advancing emotion understanding
tasks. Experimental results demonstrate that our approach achieves competitive
performance, setting a new milestone across 15 face perception tasks.

</details>


### [59] [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)
*Tyler Zhu,Tengda Han,Leonidas Guibas,Viorica Pătrăucean,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: This paper presents the first comprehensive study of video-text representation alignment, revealing that cross-modal alignment depends on visual and text data richness, correlates with downstream task performance, and serves as a zero-shot probe for encoder representation power.


<details>
  <summary>Details</summary>
Motivation: While image-text alignment has been well-studied, the temporal nature of video data remains largely unexplored in cross-modal representation alignment, creating a gap in understanding video encoders' capabilities.

Method: The study conducts comprehensive analysis of video-text alignment using modern video and language encoders, proposes parametric test-time scaling laws, and investigates correlations between semantic alignment and downstream task performance.

Result: Findings show cross-modal alignment highly depends on visual and text data richness, with proposed scaling laws demonstrating strong predictive power. Strong alignment correlates with general-purpose video representation and understanding, and temporal reasoning correlates with cross-modal alignment.

Conclusion: Video-text alignment serves as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data, providing insights into video understanding capabilities.

Abstract: The alignment of representations from different modalities has recently been
shown to provide insights on the structural similarities and downstream
capabilities of different encoders across diverse data types. While significant
progress has been made in aligning images with text, the temporal nature of
video data remains largely unexplored in this context. In this work, we conduct
the first comprehensive study of video-text representation alignment, probing
the capabilities of modern video and language encoders. Our findings reveal
several key insights. First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs. multi-frame videos)
and text (single caption vs. a collection) data provided at test time,
especially when using state-of-the-art video encoders. We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations. Secondly, we investigate the
correlation between semantic alignment and performance on both semantic and
non-semantic downstream tasks, providing initial evidence that strong alignment
against text encoders may be linked to general-purpose video representation and
understanding. Finally, we correlate temporal reasoning with cross-modal
alignment providing a challenging test-bed for vision and language models.
Overall, our work introduces video-text alignment as an informative zero-shot
way to probe the representation power of different encoders for spatio-temporal
data. Project page can be found at https://video-prh.github.io/

</details>


### [60] [PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing](https://arxiv.org/abs/2511.02777)
*Antonio Oroz,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: PercHead is a unified method for single-image 3D head reconstruction and semantic 3D editing that uses perceptual supervision from DINOv2 and SAM2.1, achieving state-of-the-art novel-view synthesis with exceptional robustness to extreme viewing angles, and enables intuitive 3D editing through segmentation maps and text/image prompts.


<details>
  <summary>Details</summary>
Motivation: Single-image 3D head reconstruction and semantic 3D editing are challenging due to severe view occlusions, weak perceptual supervision, and ambiguity in 3D editing. There is a need for a unified approach that can handle both tasks effectively.

Method: Uses a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. Employs perceptual supervision based on DINOv2 and SAM2.1 for geometric and appearance fidelity. For editing, swaps encoder and finetunes network to disentangle geometry (via segmentation maps) and style (via text prompts or reference images).

Result: Achieves state-of-the-art performance in novel-view synthesis with exceptional robustness to extreme viewing angles compared to established baselines. Enables intuitive 3D editing through a lightweight interactive GUI where users can sculpt geometry and stylize appearance.

Conclusion: PercHead provides a unified solution for both 3D head reconstruction and semantic editing, demonstrating superior performance in view synthesis and offering powerful, intuitive editing capabilities through disentangled geometry and style control.

Abstract: We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE

</details>


### [61] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: VCode introduces SVG code as a visual representation benchmark that reframes multimodal understanding as code generation, with a novel evaluation protocol (CodeVQA) and an agentic framework (VCoder) that improves SVG generation through iterative revision and visual tools.


<details>
  <summary>Details</summary>
Motivation: Current progress in AI has focused on language-centric coding tasks while leaving visual-centric coding underexplored, despite code being an executable medium for reasoning and action in the agent era.

Method: VCode benchmark reframes multimodal understanding as SVG code generation across three domains. VCoder framework augments VLMs with Thinking with Revision (iterative analysis and refinement) and Acting with Visual Tools (detectors and parsers for structured cues).

Result: Frontier VLMs struggle with faithful SVG generation, revealing a gap between language-centric and visual-centric coding. VCoder delivers 12.3-point overall gain over top-performing Claude-4-Opus. Both humans and VLMs perform worse on rendered SVGs but show consistency.

Conclusion: SVG code shows promise as a symbolic visual representation, and the gap in visual-centric coding can be addressed through agentic frameworks that combine iterative reasoning with visual tools.

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>


### [62] [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://arxiv.org/abs/2511.02779)
*Yiyang Zhou,Haoqin Tu,Zijun Wang,Zeyu Wang,Niklas Muennighoff,Fan Nie,Yejin Choi,James Zou,Chaorui Deng,Shen Yan,Haoqi Fan,Cihang Xie,Huaxiu Yao,Qinghao Ye*

Main category: cs.CV

TL;DR: MIRA is a new benchmark that evaluates models' ability to generate and use intermediate visual images (like sketches and diagrams) for reasoning, similar to how humans "draw to think". It contains 546 multimodal problems and shows that current models struggle with text-only reasoning but improve significantly when visual cues are provided.


<details>
  <summary>Details</summary>
Motivation: Traditional chain-of-thought methods rely solely on text, but many complex problems require visual reasoning through intermediate images. The motivation is to create a benchmark that mirrors how humans solve problems by "drawing to think" and evaluate models' ability to generate and utilize visual representations for reasoning.

Method: Created MIRA benchmark with 546 multimodal problems annotated with intermediate visual images and final answers. Proposed unified evaluation protocol with three input levels: direct input, text-only CoT, and Visual-CoT with annotated image clues. Used pass@k and majority voting accuracies to measure performance.

Result: Existing multimodal models perform poorly with text-only prompts but show consistent improvement (average 33.7% relative gain) when intermediate visual cues are provided. Expanding search space and designing textual prompts aligned with Visual-CoT yielded only limited improvements compared to Visual-CoT setting.

Conclusion: Visual information plays a critical role in enabling successful reasoning on complex problems. The results demonstrate that imagined visual information is essential for reasoning tasks that involve complex structures, spatial relationships, or reasoning steps difficult to express through language alone.

Abstract: We propose MIRA, a new benchmark designed to evaluate models in scenarios
where generating intermediate visual images is essential for successful
reasoning. Unlike traditional CoT methods that rely solely on text, tasks in
MIRA require models to generate and utilize intermediate images - such as
sketches, structural diagrams, or path drawings - to guide their reasoning
process. This setup closely mirrors how humans solve complex problems through
"drawing to think". To solve this, MIRA focuses on tasks that are intrinsically
challenging and involve complex structures, spatial relationships, or reasoning
steps that are difficult to express through language alone. To ensure that our
evaluation data is of high-quality, we include 546 multimodal problems,
annotated with intermediate visual images and final answers. We also propose a
unified evaluation protocol for MIRA that spans three levels of evaluation
input: direct input with image and question only, text-only CoT input with
image and thinking prompts, and Visual-CoT input with both annotated image
clues and textual thinking prompts. To probe the upper bound of model capacity
on our benchmark, we also report pass@k and majority voting accuracies under
different k settings. Experimental results show that existing multimodal large
language models, including strongest private models as well as strong
open-weight models, perform poorly when relying solely on textual prompts.
However, when intermediate visual cues are provided, model performance improves
consistently, yielding an average relative gain of 33.7% across all models and
tasks. We also probe the upper bound by expanding the search space and
designing textual prompts aligned with Visual-CoT, but both yield only limited
improvements compared to our Visual-CoT setting. These results underscore the
critical role of imagined visual information in enabling successful reasoning
on MIRA.

</details>


### [63] [AI-Generated Image Detection: An Empirical Study and Future Research Directions](https://arxiv.org/abs/2511.02791)
*Nusrat Tasnim,Kutub Uddin,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: This paper introduces a unified benchmarking framework to systematically evaluate AI-generated media forensic methods, addressing gaps in non-standardized benchmarks, inconsistent training protocols, and limited evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The threats from AI-generated media like deepfakes are eroding public trust and increasing fraud, while existing forensic methods suffer from inconsistent benchmarks, training protocols, and evaluation metrics that hinder fair comparison and deployment.

Method: The authors developed a unified benchmarking framework that evaluates ten state-of-the-art forensic methods (using scratch, frozen, and fine-tuned approaches) across seven publicly available datasets (GAN and diffusion-generated images) under controlled conditions.

Result: Evaluations revealed substantial variability in generalization, with some methods showing strong in-distribution performance but poor cross-model transferability. Performance was measured using accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity.

Conclusion: This study provides guidance for developing more robust, generalizable, and explainable forensic solutions by systematically understanding the strengths and limitations of current approaches through comprehensive benchmarking.

Abstract: The threats posed by AI-generated media, particularly deepfakes, are now
raising significant challenges for multimedia forensics, misinformation
detection, and biometric system resulting in erosion of public trust in the
legal system, significant increase in frauds, and social engineering attacks.
Although several forensic methods have been proposed, they suffer from three
critical gaps: (i) use of non-standardized benchmarks with GAN- or
diffusion-generated images, (ii) inconsistent training protocols (e.g.,
scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail
to capture generalization and explainability. These limitations hinder fair
comparison, obscure true robustness, and restrict deployment in
security-critical applications. This paper introduces a unified benchmarking
framework for systematic evaluation of forensic methods under controlled and
reproducible conditions. We benchmark ten SoTA forensic methods (scratch,
frozen, and fine-tuned) and seven publicly available datasets (GAN and
diffusion) to perform extensive and systematic evaluations. We evaluate
performance using multiple metrics, including accuracy, average precision,
ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model
interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations
demonstrate substantial variability in generalization, with certain methods
exhibiting strong in-distribution performance but degraded cross-model
transferability. This study aims to guide the research community toward a
deeper understanding of the strengths and limitations of current forensic
approaches, and to inspire the development of more robust, generalizable, and
explainable solutions.

</details>


### [64] [PLUTO-4: Frontier Pathology Foundation Models](https://arxiv.org/abs/2511.02826)
*Harshith Padigela,Shima Nofallah,Atchuth Naveen Chilaparasetti,Ryun Han,Andrew Walker,Judy Shen,Chintan Shah,Blake Martin,Aashish Sood,Elliot Miller,Ben Glass,Andy Beck,Harsha Pokkalla,Syed Ashar Javed*

Main category: cs.CV

TL;DR: PLUTO-4 introduces two complementary pathology foundation models - a compact PLUTO-4S for efficient deployment and a frontier-scale PLUTO-4G for maximum performance - achieving state-of-the-art results across various pathology tasks.


<details>
  <summary>Details</summary>
Motivation: To build on the progress of foundation models in pathology by creating next-generation models that can handle diverse histopathology tasks with improved efficiency and performance.

Method: Developed two Vision Transformer architectures: PLUTO-4S (compact, multi-scale with FlexiViT and 2D-RoPE) and PLUTO-4G (frontier-scale, single patch size). Pretrained using self-supervised DINOv2 objective on a large corpus of 551,164 WSIs from 137,144 patients across 50+ institutions.

Result: Achieves state-of-the-art performance on patch-level classification, segmentation, and slide-level diagnosis. PLUTO-4S provides high-throughput deployment, while PLUTO-4G shows 11% improvement in dermatopathology diagnosis and establishes new performance frontiers.

Conclusion: PLUTO-4 has strong potential to transform real-world pathology applications as a backbone for both translational research and diagnostic use cases, with complementary models addressing different deployment needs.

Abstract: Foundation models trained on large-scale pathology image corpora have
demonstrated strong transfer capabilities across diverse histopathology tasks.
Building on this progress, we introduce PLUTO-4, our next generation of
pathology foundation models that extend the Pathology-Universal Transformer
(PLUTO) to frontier scale. We share two complementary Vision Transformer
architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model
optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE
embeddings, and a frontier-scale PLUTO-4G model trained with a single patch
size to maximize representation capacity and stability. Both models are
pretrained using a self-supervised objective derived from DINOv2 on a large
multi-institutional corpus containing 551,164 WSIs from 137,144 patients across
over 50 institutions, spanning over 60 disease types and over 100 stains.
Comprehensive evaluation across public and internal benchmarks demonstrates
that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying
spatial and biological context, including patch-level classification,
segmentation, and slide-level diagnosis. The compact PLUTO-4S provides
high-throughput and robust performance for practical deployment, while PLUTO-4G
establishes new performance frontiers across multiple pathology benchmarks,
including an 11% improvement in dermatopathology diagnosis. These diverse
improvements underscore PLUTO-4's potential to transform real-world
applications as a backbone for translational research and diagnostic use cases.

</details>


### [65] [Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks](https://arxiv.org/abs/2511.02830)
*Dmitrii Pozdeev,Alexey Artemov,Ananta R. Bhattarai,Artem Sevastopolsky*

Main category: cs.CV

TL;DR: DenseMarks is a learned 3D embedding representation for human heads that enables high-quality dense correspondences across diverse head images by mapping pixels to a canonical 3D unit cube space.


<details>
  <summary>Details</summary>
Motivation: To create robust dense correspondences for human head images that work across different poses, individuals, and cover the entire head including hair, overcoming limitations of existing methods.

Method: Uses Vision Transformer to predict 3D embeddings for each pixel mapped to a canonical unit cube, trained with contrastive loss on pairwise point matches from talking head videos, plus multi-task learning with face landmarks, segmentation, and spatial continuity constraints.

Result: Achieves state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models, demonstrating robustness to pose variations and full head coverage.

Conclusion: DenseMarks provides an interpretable and queryable canonical space representation that enables various applications including semantic part matching, face/head tracking, and stereo reconstruction with consistent performance across diverse poses and individuals.

Abstract: We propose DenseMarks - a new learned representation for human heads,
enabling high-quality dense correspondences of human head images. For a 2D
image of a human head, a Vision Transformer network predicts a 3D embedding for
each pixel, which corresponds to a location in a 3D canonical unit cube. In
order to train our network, we collect a dataset of pairwise point matches,
estimated by a state-of-the-art point tracker over a collection of diverse
in-the-wild talking heads videos, and guide the mapping via a contrastive loss,
encouraging matched points to have close embeddings. We further employ
multi-task learning with face landmarks and segmentation constraints, as well
as imposing spatial continuity of embeddings through latent cube features,
which results in an interpretable and queryable canonical space. The
representation can be used for finding common semantic parts, face/head
tracking, and stereo reconstruction. Due to the strong supervision, our method
is robust to pose variations and covers the entire head, including hair.
Additionally, the canonical space bottleneck makes sure the obtained
representations are consistent across diverse poses and individuals. We
demonstrate state-of-the-art results in geometry-aware point matching and
monocular head tracking with 3D Morphable Models. The code and the model
checkpoint will be made available to the public.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [66] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE introduces a textual Chain of Reasoning approach to improve VLM-based robotic manipulation by externalizing spatial reasoning before action, achieving state-of-the-art performance on placement benchmarks.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle to translate high-level instructions into precise spatial affordances for robotic manipulation, and existing visual Chain-of-Thought methods are computationally intensive.

Method: TRACE integrates textual Chain of Reasoning into affordance prediction, creates a large-scale dataset via autonomous pipeline pairing instructions with textual rationales, and fine-tunes VLMs to externalize spatial reasoning.

Result: Achieves 48.1% accuracy on Where2Place benchmark (9.6% relative improvement) and 55.0% on W2P(h) subset. Performance scales with reasoning data amount, and attention maps show interpretable dynamic reasoning process.

Conclusion: Training VLMs to generate textual Chain of Reasoning is an effective strategy for enhancing precision, reliability, and interpretability of VLM-based robot control.

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [67] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: SOPPI combines MPPI with SVGD to optimize trajectory sampling, reducing sample deprivation and improving performance without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional MPPI relies on random Gaussian sampling which causes sample deprivation and suboptimal results by under-representing possible trajectories.

Method: Introduces SVGD updates between MPPI environment steps to dynamically update noise distributions at runtime, shaping more optimal trajectory representations.

Result: Demonstrated improved performance over standard MPPI in systems from Cart-Pole to 2D bipedal walking, with feasibility at lower particle counts across various hyper-parameters.

Conclusion: SOPPI shows promise for higher DOF systems and potential integration with differentiable simulators, offering efficient trajectory optimization.

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [68] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap is a GPU-accelerated and CPU-optimized local mapping module for visual SLAM that achieves significant speed improvements while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address key performance bottlenecks in the local mapping process for visual SLAM systems through targeted GPU and CPU optimizations.

Method: Offload map point triangulation and fusion to GPU, accelerate redundant keyframe culling on CPU, integrate GPU-accelerated solver for local bundle adjustment, built on ORB-SLAM3 using CUDA.

Result: Achieves average speedup of 1.3x in EuRoC dataset and 1.6x in TUM-VI dataset for local mapping module on both desktop and embedded platforms while maintaining original system accuracy.

Conclusion: TurboMap successfully demonstrates that GPU acceleration and CPU optimization can significantly improve local mapping performance in visual SLAM systems without sacrificing accuracy.

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [69] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: TACO is a framework that optimizes quadrotor controller parameters online using trajectory and state information, enabling real-time performance improvements over static parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Standard quadrotor controllers use fixed parameters that sacrifice task-specific performance, creating a need for adaptive parameter tuning that can respond to varying trajectories and states.

Method: Uses a learned predictive model with lightweight optimization to adapt controller gains in real-time, and can also adapt trajectories for dynamic feasibility while maintaining smoothness. Includes a parallelized simulator for large-scale training.

Result: Outperforms conventional static parameter tuning and operates orders of magnitude faster than black-box optimization baselines. Successfully deployed on physical quadrotor with significant tracking error reduction.

Conclusion: TACO enables practical real-time adaptive control for quadrotors, improving trajectory tracking performance through online parameter optimization and trajectory adaptation.

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [70] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: This survey paper examines world models in robotics, analyzing their core capabilities through robotic manipulation methods rather than imposing a fixed definition, and aims to outline a roadmap for developing practical world models.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents need to understand world dynamics beyond reactive control, requiring world models that encode environmental states, capture dynamics, and enable prediction and reasoning. However, definitions and capabilities of world models remain ambiguous.

Method: The authors review methods in robotic manipulation that exhibit core world model capabilities, analyzing their roles across perception, prediction, and control, and identifying key challenges and solutions.

Result: The survey distills the core components, capabilities, and functions that a real world model should possess, providing insights into what constitutes effective world modeling in robotics.

Conclusion: Building on the analysis, the paper outlines a roadmap for developing generalizable and practical world models for robotics applications.

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [71] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: This paper presents a layered model for multi-robot autonomy that combines collective decision-making using census-based opinion dynamics with individual multi-objective behavior optimization, enabling new collaborative behaviors in marine robotics applications.


<details>
  <summary>Details</summary>
Motivation: Multi-robot systems operating in marine environments can benefit from collaboration through improved efficiency and robustness, but modeling and designing these systems is challenging due to the interplay between collective and individual behaviors.

Method: The model uses census (weighted neighbor inputs) for collective teaming decisions via nonlinear opinion dynamics, and multi-objective behavior optimization for individual action decisions using interval programming. A distributed optimization method for subgroup allocation is introduced using gradient descent with neighbor influence.

Result: The model can recover foundational distributed optimization and control algorithms, and enables new collective behaviors. Experimental validation with autonomous surface vehicles shows utility in adaptive sampling, unit protection, and capture-the-flag scenarios.

Conclusion: The layered autonomy model successfully bridges collective and individual decision-making in multi-robot systems, demonstrating practical utility across diverse marine robotics applications through experimental validation.

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [72] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: A pipeline integrating 3D generative AI with vision-language models enables robotic assembly of multi-component objects from natural language by decomposing AI-generated meshes into structural and panel components.


<details>
  <summary>Details</summary>
Motivation: To address challenges in creating multi-component physical objects from text prompts using 3D generative AI, particularly in determining appropriate component types for different object regions.

Method: Leverages vision-language models for zero-shot, multi-modal reasoning about geometry and functionality to decompose AI-generated meshes into multi-component 3D models using predefined structural and panel components.

Result: Evaluation shows users preferred VLM-generated assignments 90.6% of the time, compared to 59.4% for rule-based and 2.5% for random assignment. The system also allows conversational feedback for refinement.

Conclusion: The integration of VLMs with 3D generative AI enables effective robotic assembly of multi-component objects while providing human control through conversational refinement.

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [73] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: A 7-DOF robotic arm with RCM and ergonomic design for laparoscopic surgery improves targeting accuracy by over 50%, reduces task time, and decreases surgeon muscle strain compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: To enhance surgical precision, reduce surgeon fatigue, and improve surgeon interaction in minimally invasive surgery through optimized robotic assistance.

Method: Developed a 7-DOF robotic arm system with remote center of motion at instrument insertion point and ergonomic considerations, implemented on general-purpose platform and evaluated through simulated surgical tasks.

Result: Significantly improved targeting accuracy (error reduced by over 50%), shorter task completion times, and substantially lower operator muscle strain and discomfort compared to conventional manual laparoscopy.

Conclusion: Kinematic optimization and human-centered ergonomic design are crucial for enhancing robot-assisted surgery performance, providing guidance for next-generation surgical robots to improve surgical outcomes and operating team ergonomics.

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [74] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: This paper compares centralized vs distributed MARL for soft robotic arm control, finding centralized better for simple systems (n≤2), no significant difference for n≤4, and distributed more sample-efficient for complex systems (4<n≤12) but centralized trains faster.


<details>
  <summary>Details</summary>
Motivation: To systematically compare centralized and distributed multi-agent reinforcement learning architectures for controlling soft robotic arms modeled as Cosserat rods, providing design guidance for sim-to-real transfer.

Method: Used PyElastica and OpenAI Gym to train both global PPO (centralized) and MAPPO (distributed) controllers under identical budgets, varying the number of controlled sections n and evaluating performance in baseline, disturbance recovery, and actuator failure scenarios.

Result: Distributed policy shows high sample efficiency for complex systems (4<n≤12) with better success rate, resilience, and robustness under local observability, while centralized policies train much faster and outperform distributed in very simple systems (n≤2).

Conclusion: There are trade-offs between centralized and distributed policies in RL-based soft robotic control - centralized offers time efficiency while distributed provides sample efficiency for complex systems, providing actionable design guidance for future applications.

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [75] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY introduces a bidirectional language-action framework that learns both language-to-action (L2A) and action-to-language (A2L) mappings within a single vision-language model, enabling self-supervised improvement through semantic consistency verification.


<details>
  <summary>Details</summary>
Motivation: Current one-way language-to-action paradigms produce policies that lack deeper contextual understanding, limiting generalization and explainability. The complementary action-to-language skill is essential for developing holistic grounding and richer internal representations.

Method: Joint training on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between language descriptions (L2C). Uses self-improving cycle with active augmentation targeting low-confidence cases.

Result: Improves task success rates by 56.46% on average in pick-and-place tasks across simulation and real-world experiments, yielding more robust language-action grounding for robotic manipulation.

Conclusion: Bidirectional language-action mappings enable more holistic grounding and self-supervised learning, significantly improving robotic manipulation performance and robustness without additional human labels.

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [76] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: SuckTac is a novel intelligent sucker that integrates camera-based tactile sensing within an optimized structure, inspired by cephalopod suckers, enabling high-density perception and robust suction for various robotic applications.


<details>
  <summary>Details</summary>
Motivation: Existing suckers lack high-fidelity perceptual and tactile sensing, limiting their ability to resolve fine-grained geometric features and interaction status, which impedes robust performance with irregular objects in complex environments.

Method: Joint structure design and optimization using multi-material integrated casting to embed camera and light source into the sucker, combined with mechanical optimizations including refined profile, compliant lip, and surface microstructure.

Result: The system enables in-situ, high-density perception of fine details like surface shape, texture and roughness, and demonstrates superior performance in challenging tasks including robotic cloth manipulation and soft mobile robot inspection.

Conclusion: SuckTac provides a novel solution that integrates high-density perception with robust suction capabilities, showing broad applicability for various robotic applications requiring adaptive interaction with diverse surfaces.

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [77] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: The ZJUNlict team improved their robot hardware by adding an IMU for better posture accuracy and angular velocity planning, and optimized software modules for enhanced decision making, ball pursuit prediction, and ball possession prediction to handle high-tempo games.


<details>
  <summary>Details</summary>
Motivation: To enhance robot performance in high-tempo game dynamics by improving both hardware capabilities and software efficiency.

Method: Integrated an IMU into the v2023 robot for hardware improvements and optimized key software modules including strategy and CUDA modules.

Result: Achieved better posture accuracy, angular velocity planning, and significant improvements in decision making efficiency, ball pursuit prediction, and ball possession prediction.

Conclusion: The combined hardware and software advancements enable the ZJUNlict team to better adapt to and perform in high-tempo game environments.

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [78] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: A framework for aerial manipulators using superquadrics for accurate geometric modeling, combining maximum-clearance planning with safety-critical control to achieve safe, smooth trajectories in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Planning safe trajectories for aerial manipulators is challenging due to whole-body collision avoidance and the conservativeness of common geometric abstractions like bounding boxes or ellipsoids.

Method: Uses superquadrics (SQs) for geometry-accurate modeling, fuses Voronoi diagrams with equilibrium-manifold formulation for planning, and employs high-order control barrier functions for safety-critical control.

Result: Outperforms sampling-based planners in simulation, producing faster, safer, and smoother trajectories with better geometric fidelity than ellipsoid-based baselines. Hardware experiments confirm feasibility and robustness.

Conclusion: The SQ-based framework enables effective whole-body motion planning and safety-critical control for aerial manipulators, demonstrating consistent performance across simulation and hardware.

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [79] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: OmniPianist is the first agent capable of performing nearly 1000 music pieces through scalable, demonstration-free learning using Optimal Transport for fingering, large-scale RL training, and Flow Matching Transformer for imitation learning.


<details>
  <summary>Details</summary>
Motivation: To achieve human-level dexterity in robotics, particularly for the challenging task of bimanual robotic piano playing which is high-dimensional, contact-rich, and requires fast, precise control.

Method: Three core components: 1) Automatic fingering strategy using Optimal Transport for autonomous discovery of efficient piano-playing strategies, 2) Large-scale RL training of 2000+ specialized agents aggregated into RP1M++ dataset, 3) Flow Matching Transformer for imitation learning using the RP1M++ dataset.

Result: Created OmniPianist agent capable of performing a wide range of musical pieces, with extensive experiments and ablation studies demonstrating effectiveness and scalability.

Conclusion: The approach advances dexterous robotic piano playing at scale through scalable, human-demonstration-free learning methods.

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [80] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: This paper demonstrates closed-loop position control of conductive non-magnetic objects using oscillating magnetic fields, building on previous open-loop work to enable 3-DOF position control of induced magnetic dipoles.


<details>
  <summary>Details</summary>
Motivation: To extend magnetic manipulation to conductive non-magnetic objects, enabling applications like space debris recycling where low inductive forces are suitable, and to overcome limitations of previous open-loop methods.

Method: Uses oscillating magnetic fields to induce eddy currents in conductive objects, creating opposing dipole moments for manipulation. Explores various force inversion methods and implements closed-loop position control of a semi-buoyant aluminum sphere in lab tests.

Result: Successfully demonstrated closed-loop position control of conductive objects using induced magnetic dipoles, representing a critical advancement from previous open-loop methods.

Conclusion: The closed-loop methods developed represent a crucial first step toward broader applications for 3-degree-of-freedom position control of induced magnetic dipoles in conductive non-magnetic materials.

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [81] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1 is a vision-language-action model that introduces Unified Vision-Motion Codes (UVMC) to address challenges in precise low-level action generation and cross-domain generalization across heterogeneous robotic datasets.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models struggle with producing precise low-level actions from high-dimensional observations and bridging domain gaps across diverse robot embodiments and human demonstrations, failing to fully exploit complementary multi-modal knowledge in large-scale datasets.

Method: XR-1 uses a dual-branch VQ-VAE to learn Unified Vision-Motion Codes (UVMC) that jointly encode visual dynamics and robotic motion, with a three-stage training paradigm: self-supervised UVMC learning, UVMC-guided pretraining, and task-specific post-training.

Result: XR-1 outperforms state-of-the-art baselines (π₀.₅, π₀, RDT, UniVLA, GR00T-N1.5) in extensive real-world experiments with 14,000+ rollouts on 6 robot embodiments across 120+ manipulation tasks, demonstrating strong generalization to novel objects, backgrounds, distractors, and illumination changes.

Conclusion: XR-1 provides a versatile and scalable framework for VLA learning that effectively addresses key challenges in robotic manipulation through unified multi-modal representation learning and cross-embodiment training.

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [82] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2 is a portable, mocap-free humanoid teleoperation system that enables real-time whole-body control using VR and a custom robot neck, achieving high success rates in data collection and enabling hierarchical visuomotor policies for autonomous humanoid control.


<details>
  <summary>Details</summary>
Motivation: Humanoid robotics lacks effective data collection frameworks compared to other robotics domains, with existing systems using decoupled control or expensive motion capture setups.

Method: Leverages PICO4U VR for real-time whole-body human motion capture and a custom 2-DoF robot neck for egocentric vision, enabling holistic human-to-humanoid control without motion capture.

Result: Demonstrated long-horizon dexterous and mobile humanoid skills, collected 100 demonstrations in 15 minutes with almost 100% success rate, and developed a hierarchical visuomotor policy for autonomous whole-body control.

Conclusion: TWIST2 provides a scalable, portable solution for humanoid teleoperation and data collection, with fully reproducible and open-sourced system and dataset.

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Mirror-Neuron Patterns in AI Alignment](https://arxiv.org/abs/2511.01885)
*Robyn Wyrick*

Main category: cs.AI

TL;DR: This paper explores whether artificial neural networks can develop mirror neuron-like patterns to enable intrinsic AI alignment through empathy-like mechanisms, using a Frog and Toad game framework to demonstrate cooperative behaviors.


<details>
  <summary>Details</summary>
Motivation: As AI advances toward superhuman capabilities, current alignment strategies relying on external constraints may prove insufficient against super-intelligent AI. The research investigates whether intrinsic alignment through mirror neuron patterns could complement existing techniques.

Method: Used a novel Frog and Toad game framework designed to promote cooperative behaviors, identified conditions for mirror-neuron pattern emergence, evaluated their influence on action circuits, and introduced the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency.

Result: Findings show that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior.

Conclusion: Intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures, potentially providing more robust alignment against future super-intelligent systems.

Abstract: As artificial intelligence (AI) advances toward superhuman capabilities,
aligning these systems with human values becomes increasingly critical. Current
alignment strategies rely largely on externally specified constraints that may
prove insufficient against future super-intelligent AI capable of circumventing
top-down controls.
  This research investigates whether artificial neural networks (ANNs) can
develop patterns analogous to biological mirror neurons cells that activate
both when performing and observing actions, and how such patterns might
contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in
empathy, imitation, and social cognition in humans. The study therefore asks:
(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these
patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative
behaviors, we identify conditions under which mirror-neuron patterns emerge,
evaluate their influence on action circuits, introduce the Checkpoint Mirror
Neuron Index (CMNI) to quantify activation strength and consistency, and
propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and
self/other coupling foster shared neural representations in ANNs similar to
biological mirror neurons. These empathy-like circuits support cooperative
behavior and suggest that intrinsic motivations modeled through mirror-neuron
dynamics could complement existing alignment techniques by embedding
empathy-like mechanisms directly within AI architectures.

</details>


### [84] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: The paper introduces human-AI co-embodied intelligence, a physical AI system that integrates humans, agentic AI, and wearable hardware for real-world experiments and manufacturing, demonstrated through the APEX system in cleanroom electronics fabrication.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models are confined to virtual domains while real-world experiments and manufacturing still require human supervision, creating a gap that limits reproducibility, scalability, and accessibility.

Method: Developed the Agentic-Physical Experimentation (APEX) system that couples agentic reasoning with physical execution through mixed-reality. The system observes human actions, aligns them with procedures, provides 3D visual guidance, and analyzes steps using wearable interfaces.

Result: APEX achieved context-aware reasoning with accuracy exceeding general multimodal LLMs, corrected errors in real-time, and successfully transferred expertise to beginners in flexible electronics fabrication.

Conclusion: Establishes a new class of agentic-physical-human intelligence that extends agentic reasoning into the physical domain, transforming scientific research and manufacturing into autonomous, traceable, interpretable, and scalable processes.

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [85] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: Foundation models can automatically generate reward functions from text instructions to train competitive RL agents for autonomous racing in Gran Turismo 7, matching champion-level performance.


<details>
  <summary>Details</summary>
Motivation: Designing reward functions for RL agents is challenging, especially in complex environments like autonomous racing. Manual reward design is difficult and time-consuming.

Method: Combines LLM-based reward generation from text instructions, VLM preference-based evaluation, and human feedback to automatically search over reward function spaces.

Result: Produced racing agents competitive with GT Sophy (champion-level RL racing agent) and generated novel behaviors in Gran Turismo 7.

Conclusion: This approach enables practical automated reward design for real-world RL applications, reducing the burden of manual reward engineering.

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [86] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: The Deep Value Benchmark (DVB) is a framework that tests whether LLMs learn fundamental human values or just surface-level preferences, revealing that current models generalize deep values less than chance (average DVGR = 0.30).


<details>
  <summary>Details</summary>
Motivation: To distinguish between AI systems that learn fundamental human values (robust alignment) versus those that only capture superficial patterns in preference data (risking misaligned behavior).

Method: Uses controlled confounding between deep values (moral principles) and shallow features (superficial attributes) in training, then breaks these correlations in testing to measure Deep Value Generalization Rate (DVGR).

Result: Across 9 models, average DVGR is 0.30 (below chance), with larger models performing slightly worse than smaller ones. All models generalize deep values less than chance.

Conclusion: Current LLMs fail to learn fundamental human values and instead rely on superficial patterns, highlighting a critical alignment gap that DVB can measure interpretably.

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [87] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: This paper proposes InsurAgent, an LLM-powered agent that simulates human flood insurance decision-making by combining retrieval-augmented generation with reasoning capabilities to overcome LLMs' limitations in quantitative probability estimation.


<details>
  <summary>Details</summary>
Motivation: Flood insurance participation rates remain low despite its effectiveness, highlighting the need to understand behavioral mechanisms behind insurance decisions. LLMs offer promising tools for simulating human decision-making but have limitations in quantitative probability estimation.

Method: Developed InsurAgent with five modules: perception, retrieval (using RAG to ground decisions in empirical survey data), reasoning (leveraging LLM common sense), action, and memory (supporting temporal decision evolution simulation).

Result: InsurAgent achieved accurate estimation of marginal and bivariate probabilities, captured contextual information intractable for traditional models, and successfully simulated temporal decision evolutions through life trajectory scenarios.

Conclusion: InsurAgent provides a valuable tool for behavioral modeling and policy analysis by combining the strengths of LLMs with empirical data grounding to simulate complex human decision-making processes in insurance contexts.

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [88] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC is an adaptive reward prediction method that predicts expected future rewards based on thinking tokens, enabling early stopping of unpromising reasoning chains, optimized model selection, and adaptive test-time scaling.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs while maintaining or improving reasoning accuracy in large language models by dynamically controlling reasoning length and model selection.

Method: Trains a lightweight adapter on reasoning models to predict expected future rewards as a function of thinking tokens, enabling adaptive reasoning control.

Result: Achieved 26% compute reduction while maintaining accuracy, 4% higher accuracy at equal compute, 55% less compute at equal accuracy, and 7-11% accuracy improvements in different compute regimes.

Conclusion: Re-FORC enables efficient dynamic reasoning with upfront computation estimation and significant computational savings while maintaining or improving performance.

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [89] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: ATHENA is a framework that combines symbolic utility functions with LLM-based semantic adaptation to model personalized human decision-making, outperforming existing models in travel mode and vaccine choice tasks.


<details>
  <summary>Details</summary>
Motivation: Individual decision-making often differs from population-level predictions, especially in high-stakes scenarios like vaccine uptake, due to unique personal factors including numerical attributes and linguistic influences.

Method: Two-stage approach: 1) LLM-augmented symbolic discovery for group-level utility functions, 2) Individual-level semantic adaptation using personalized semantic templates guided by optimal utility.

Result: Consistently outperforms utility-based, machine learning, and other LLM-based models, improving F1 score by at least 6.5% over strongest baseline models in travel mode and vaccine choice tasks.

Conclusion: ATHENA provides an effective framework for human-centric decision modeling by integrating symbolic utility modeling with semantic adaptation, with both stages being critical and complementary.

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [90] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: STRMAC is a state-aware routing framework for multi-agent systems that adaptively selects the most suitable agent at each step, achieving 23.8% performance improvement and 90.1% data collection reduction.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems with LLMs have rigid scheduling and inefficient coordination that fails to adapt to evolving task requirements, limiting their full potential.

Method: Separately encodes interaction history and agent knowledge to power a router that adaptively selects the most suitable single agent at each step, plus a self-evolving data generation approach for efficient training.

Result: Achieves state-of-the-art performance with up to 23.8% improvement over baselines and reduces data collection overhead by up to 90.1% compared to exhaustive search.

Conclusion: STRMAC provides an efficient and adaptive collaboration framework that significantly enhances multi-agent system performance while reducing training data requirements.

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [91] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: PPP is a multi-objective reinforcement learning approach that jointly optimizes productivity (task completion), proactivity (asking essential questions), and personalization (adapting to user preferences) for AI agents, achieving significant improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing work focuses primarily on task success, but effective real-world agents need to optimize three key dimensions: productivity, proactivity, and personalization for better user interaction.

Method: Introduced UserVille (interactive environment with LLM-based user simulators) and PPP (multi-objective reinforcement learning approach) that jointly optimizes productivity, proactivity, and personalization.

Result: Agents trained with PPP achieved substantial improvements over strong baselines like GPT-5 (+21.6 on average), showing ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success.

Conclusion: Explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [92] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: Proposes a framework called \method for complex tabular numerical reasoning that decomposes queries, sanitizes tables, and uses program-of-thoughts reasoning to generate executable code, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large language models often underperform in complex tabular reasoning due to complex queries, noisy data, and limited numerical capabilities, requiring a more robust approach.

Method: Three-component framework: (1) query decomposer that breaks down complex questions, (2) table sanitizer that cleans and filters noisy tables, and (3) program-of-thoughts-based reasoner that generates executable code to derive final answers.

Result: Achieved SOTA performance with 8.79%, 6.08%, and 19.87% accuracy improvements on TAT-QA, TableBench, and \method datasets respectively. Also introduced CalTab151 dataset for unbiased evaluation.

Conclusion: The framework effectively enhances LLM performance for complex tabular numerical reasoning and integrates seamlessly with mainstream LLMs, providing a robust solution.

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [93] [Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network](https://arxiv.org/abs/2511.02238)
*Keyu Zhao,Weiquan Lin,Qirui Zheng,Fengli Xu,Yong Li*

Main category: cs.AI

TL;DR: Deep Ideation framework integrates scientific concept networks with LLMs to generate novel research ideas through an explore-expand-evolve workflow, improving idea quality by 10.67% compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous research ideation methods rely on simplistic keyword co-occurrence or semantic similarity, overlooking complex contextual relationships between scientific concepts. LLM-driven methods also fail to effectively utilize scientific concept networks, limiting idea grounding in established research.

Method: Proposes Deep Ideation framework with scientific network capturing keyword co-occurrence and contextual relationships. Uses explore-expand-evolve workflow with Idea Stack for progress tracking and critic engine trained on real-world reviewer feedback for continuous evaluation of novelty and feasibility.

Result: Improves quality of generated ideas by 10.67% compared to other methods, with ideas surpassing top conference acceptance levels. Human evaluation confirms practical value in scientific research, and ablation studies validate effectiveness of each workflow component.

Conclusion: Deep Ideation successfully integrates scientific networks with LLMs to generate high-quality research ideas, demonstrating significant improvements over existing methods and practical applicability in scientific research.

Abstract: Novel research ideas play a critical role in advancing scientific inquiries.
Recent advancements in Large Language Models (LLMs) have demonstrated their
potential to generate novel research ideas by leveraging large-scale scientific
literature. However, previous work in research ideation has primarily relied on
simplistic methods, such as keyword co-occurrence or semantic similarity. These
approaches focus on identifying statistical associations in the literature but
overlook the complex, contextual relationships between scientific concepts,
which are essential to effectively leverage knowledge embedded in human
literature. For instance, papers that simultaneously mention "keyword A" and
"keyword B" often present research ideas that integrate both concepts.
Additionally, some LLM-driven methods propose and refine research ideas using
the model's internal knowledge, but they fail to effectively utilize the
scientific concept network, limiting the grounding of ideas in established
research. To address these challenges, we propose the Deep Ideation framework
to address these challenges, integrating a scientific network that captures
keyword co-occurrence and contextual relationships, enriching LLM-driven
ideation. The framework introduces an explore-expand-evolve workflow to
iteratively refine research ideas, using an Idea Stack to track progress. A
critic engine, trained on real-world reviewer feedback, guides the process by
providing continuous feedback on the novelty and feasibility of ideas. Our
experiments show that our approach improves the quality of generated ideas by
10.67% compared to other methods, with ideas surpassing top conference
acceptance levels. Human evaluation highlights their practical value in
scientific research, and ablation studies confirm the effectiveness of each
component in the workflow. Code repo is available at
https://github.com/kyZhao-1/Deep-Ideation.

</details>


### [94] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: This paper introduces a framework to analyze how multimodal LLMs resolve conflicts between modalities, decomposing modality following into relative reasoning uncertainty and inherent modality preference using entropy-based uncertainty metrics.


<details>
  <summary>Details</summary>
Motivation: Prior work only measured modality following with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning and failing to disentangle modality bias from unimodal capabilities.

Method: Constructed a controllable dataset that systematically varies visual and textual reasoning difficulty, used entropy as uncertainty metric, and analyzed layer-wise predictions to understand internal mechanisms.

Result: Discovered a universal law: probability of following a modality decreases monotonically as its relative uncertainty increases. Identified balance points that reveal inherent modality preferences and showed models oscillate between modalities in ambiguous regions.

Conclusion: Established relative uncertainty and inherent preference as the two governing principles of modality following, providing both quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [95] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: The paper addresses lazy agent behavior in multi-agent LLM systems where one agent dominates collaboration, proposes methods to measure causal influence and implement verifiable rewards to encourage balanced participation and improve reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM systems suffer from lazy agent behavior where one agent dominates while the other contributes little, undermining collaboration and reducing the setup to an ineffective single-agent system.

Method: The paper provides theoretical analysis of lazy behavior, introduces stable causal influence measurement, and proposes a verifiable reward mechanism that allows reasoning agents to discard noisy outputs, consolidate instructions, and restart reasoning when necessary.

Result: Extensive experiments demonstrate that the proposed framework successfully alleviates lazy agent behavior and unlocks the full potential of multi-agent frameworks for complex reasoning tasks.

Conclusion: The proposed methods effectively mitigate lazy agent behavior in multi-agent LLM systems, enabling more balanced collaboration and improved performance on complex reasoning tasks through verifiable rewards and causal influence measurement.

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [96] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: ProQ-BERT is a transformer-based framework that predicts CKD progression using multi-modal EHR data, achieving superior performance (ROC-AUC up to 0.995) compared to existing methods through quantization-based tokenization and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Chronic Kidney Disease affects 10% of global population and accurate prognosis prediction is crucial for timely interventions and resource optimization in healthcare.

Method: Transformer-based framework integrating demographic, clinical, and laboratory data using quantization-based tokenization for continuous lab values, pretrained with masked language modeling and fine-tuned for binary classification of CKD progression from stage 3a to 5.

Result: Model consistently outperformed CEHR-BERT, achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction on a cohort of 91,816 patients.

Conclusion: Transformer architectures with temporal design choices are effective for clinical prognosis modeling, offering promising direction for personalized CKD care.

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [97] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: A fuzzy soft set theory-based expert system for breast cancer risk assessment using clinical parameters like BMI, insulin, leptin, adiponectin levels, and age.


<details>
  <summary>Details</summary>
Motivation: Early breast cancer detection is critical but challenging due to disease complexity and variable risk factors. The study aims to provide a non-invasive preliminary assessment tool using routine blood analysis parameters.

Method: Developed a fuzzy soft set theory-based expert system that integrates BMI, insulin level, leptin level, adiponectin level, and age through fuzzy inference rules and soft set computations.

Result: The system can assess breast cancer risk using measurable clinical parameters obtained from routine blood analyses, enabling non-invasive preliminary assessment.

Conclusion: The proposed expert system supports healthcare professionals in identifying high-risk patients and determining the need for further diagnostic procedures like biopsies.

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [98] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: The paper presents a new framework for estimating entire Precision-Recall curves for generative models using a binary classification approach, extending beyond existing metrics that only cover extreme curve values.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for generative models rely on scalar metrics, but Precision-Recall curves offer richer analysis. However, existing PR estimation methods face challenges and are often limited to extreme values of the curve.

Method: A binary classification-based framework for estimating entire PR curves, with thorough statistical analysis including minimax upper bounds on PR estimation risk.

Result: The framework extends several landmark PR metrics from literature and enables comprehensive PR curve estimation beyond extreme values. Experimental studies show different curve behaviors in various settings.

Conclusion: The proposed binary classification framework provides a robust method for estimating complete PR curves, addressing limitations of existing approaches and enabling richer analysis of generative models.

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [99] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: ReAcTree is a hierarchical task-planning method that decomposes complex goals into manageable subgoals using a dynamically constructed agent tree, outperforming existing methods like ReAct on embodied AI tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based planning methods struggle with complex, long-horizon tasks because they rely on monolithic trajectories that entangle all past decisions, making it difficult to handle complex tasks effectively.

Method: ReAcTree uses a hierarchical approach with dynamically constructed agent trees where each subgoal is handled by an LLM agent node. It includes control flow nodes for coordination and integrates episodic memory for goal-specific examples and working memory for environment observations.

Result: Experiments on WAH-NL and ALFRED datasets show ReAcTree consistently outperforms baselines like ReAct. On WAH-NL, it achieves 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.

Conclusion: The hierarchical decomposition approach with dynamic agent trees and complementary memory systems effectively addresses limitations of monolithic planning methods for complex embodied AI tasks.

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [100] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: This paper introduces Verifiable Multiple-Choice Reformulation (VMR), a training strategy that adapts RLVR to open-ended tasks by converting them into verifiable multiple-choice formats, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: RLVR has shown success in reasoning tasks with verifiable answers, but cannot be directly applied to open-ended tasks lacking ground truth. The authors aim to explore whether strengthening reasoning capabilities can improve performance in open-ended domains like creative writing and instruction following.

Method: Proposed Verifiable Multiple-Choice Reformulation (VMR) - a training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling RLVR training even without explicit ground truth solutions.

Result: Experimental results show VMR delivers an average gain of 5.99 points over baseline across eight open-ended benchmarks, validating its effectiveness in improving LLM performance on open-ended tasks.

Conclusion: VMR successfully enables the transfer of RLVR paradigm to open-ended domains, demonstrating that strengthening reasoning capabilities can indeed improve performance in tasks lacking standard answers.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [101] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero,Luis A. Hernández Gómez,Luis Mendo Tomás,Zoraida Frias Barroso*

Main category: cs.AI

TL;DR: This paper introduces Agentic AI as a solution for automating complex 5G/6G network optimization, proposes a framework for its core components, and demonstrates its application through a 5G RAN case study.


<details>
  <summary>Details</summary>
Motivation: The complexity of 5G and upcoming 6G networks makes manual optimization ineffective, requiring autonomous AI systems with human-level cognitive abilities for dynamic RAN environments.

Method: The paper outlines Agentic AI's core concepts, design patterns (reflection, planning, tool use, multi-agent collaboration), and applies them to mobile networks through a practical 5G RAN case study using time-series analytics and LAM-driven agents.

Result: The paper provides a framework for Agentic AI systems and demonstrates how LAM-driven agents can collaborate for KPI-based autonomous decision-making in RAN optimization.

Conclusion: Agentic AI represents a promising paradigm for automating complex network systems, with the proposed framework and practical case study contributing to ongoing research in 5G/6G network optimization.

Abstract: Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [102] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: KLPEG framework uses knowledge graphs and LLMs to enable efficient, targeted testing for incremental game updates by accumulating knowledge across versions and generating tailored test cases.


<details>
  <summary>Details</summary>
Motivation: Address challenges in automated game testing due to frequent updates, where current LLM-based methods lack structured knowledge accumulation for precise testing of incremental changes.

Method: Constructs and maintains Knowledge Graph to model game elements, dependencies, and causal relationships; uses LLMs to parse update logs and perform multi-hop reasoning on KG to identify impact scope and generate tailored test cases.

Result: Experiments in Overcooked and Minecraft show KLPEG can more accurately locate affected functionalities and complete tests in fewer steps, improving both effectiveness and efficiency.

Conclusion: KLPEG framework successfully addresses incremental game testing challenges through structured knowledge accumulation and reuse, enabling precise and efficient testing tailored to specific updates.

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [103] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA Benchmark evaluates LLMs on multi-domain quantitative reasoning using 500 real-life tasks from finance, physics, health, and statistics. State-of-the-art models achieved only 45-63% accuracy, with errors mainly from rounding (35%) and calculation mistakes (33%).


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' quantitative reasoning capabilities across real-world domains using verified calculator outputs, addressing limitations of standard math datasets by evaluating step-by-step reasoning, numerical precision, and domain generalization.

Method: Created ORCA Benchmark with 500 natural-language tasks across finance, physics, health, and statistics domains, using verified outputs from Omni's calculator engine to evaluate five state-of-the-art LLMs.

Result: Models achieved 45-63% accuracy with strengths in mathematics/engineering but weaknesses in physics/natural sciences. Error analysis showed 35% rounding errors and 33% calculation mistakes. Correlation analysis (r≈0.40-0.65) revealed partial complementarity among models.

Conclusion: Current LLMs have significant limitations in quantitative reasoning across real-world domains, particularly in numerical precision and domain-specific knowledge, highlighting the need for improved calculation capabilities and domain generalization.

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [104] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: This paper introduces the first adaptive shielding framework for reinforcement learning that automatically repairs GR(1) specifications online when environment assumptions are violated, ensuring both safety and liveness properties while maintaining near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Classical shielding approaches in RL are static and fail to adapt when environment assumptions are violated, leading to suboptimal performance and potential safety issues in dynamic environments.

Method: The framework uses runtime detection of environment assumption violations and employs Inductive Logic Programming (ILP) to automatically repair GR(1) specifications online in a systematic and interpretable way, ensuring graceful evolution of the shield.

Result: In case studies with Minepump and Atari Seaquest, RL agents with adaptive shields maintained near-optimal reward and perfect logical compliance, while static shields were severely suboptimal when optimizing for auxiliary rewards.

Conclusion: Adaptive shielding provides a superior approach to static shielding by enabling online specification repair, ensuring both safety compliance and performance optimization in dynamic environments.

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [105] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu,Jiarui Tong,Sheng Xu*

Main category: cs.AI

TL;DR: A multi-agent psychological simulation system that models internal cognitive-affective processes to generate believable human behaviors for training and education in human-centered fields.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of realistic simulations of human behavior in training and education, particularly in human-centered fields where authentic practice is essential.

Method: Uses a multi-agent system grounded in established psychological theories (self-efficacy, mindset, social constructivism) that simulates an 'inner parliament' of agents representing key psychological factors that deliberate and interact to determine output behavior.

Result: The system enables unprecedented transparency and alignment with human psychology, generating believable human behaviors for applications in teacher training and research.

Conclusion: The system embodies principles of social learning, cognitive apprenticeship, deliberate practice, and meta-cognition, providing a psychologically-grounded approach to human behavior simulation.

Abstract: Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [106] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR is a large benchmark dataset and generation framework for analyzing compositional spatial reasoning in LLMs, featuring over 5M datapoints with independently controllable aspects of compositionality.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous benchmarking tool that can independently analyze different aspects of compositional reasoning (productivity, substitutivity, overgeneralization, systematicity) in spatial reasoning tasks for LLMs.

Method: Built procedurally to be correct by construction, with independent verification using symbolic solvers. The framework allows independent variation of compositionality aspects: reasoning depth, entity/linguistic variability, input order/distractors, and novel linguistic elements.

Result: LLMs struggle with productive and systematic generalization in spatial reasoning tasks but are more robust to linguistic variation. The dataset provides provably correct benchmarking with fine-grained compositional analysis capabilities.

Conclusion: DecompSR enables robust and fine-grained probing of LLMs' compositional reasoning abilities through its independently controllable aspects of compositionality and provably correct construction.

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [107] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: The paper proposes a collaborative maze-solving benchmark to evaluate AI agent collaboration, revealing a 'collaboration gap' where models that perform well alone degrade in collaborative settings, and suggests relay inference and collaboration-aware training strategies.


<details>
  <summary>Details</summary>
Motivation: As AI systems increasingly rely on heterogeneous agent collaboration under partial observability, there's a need for empirical evaluation of agent-agent collaboration at scale, which current studies lack.

Method: Developed a collaborative maze-solving benchmark that isolates collaborative capabilities, modulates complexity, enables automated grading, and preserves ecological plausibility. Evaluated 32 leading models in solo, homogeneous, and heterogeneous pairings.

Result: Revealed a 'collaboration gap' - models performing well solo often degrade substantially when collaborating. Small distilled models that solve mazes well alone may fail completely in certain pairings. Starting with stronger agents improves outcomes, motivating 'relay inference' approach.

Conclusion: Findings argue for collaboration-aware evaluation, training strategies to enhance collaborative capabilities, and interaction design that reliably elicits agents' latent skills, applicable to both AI-AI and human-AI collaboration.

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [108] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: CostBench is a new benchmark that evaluates LLM agents' cost-aware planning and replanning abilities in travel planning scenarios with dynamic events, revealing significant gaps in economic reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent evaluations focus mainly on task completion while neglecting resource efficiency and adaptability to changing environments, missing crucial economic reasoning capabilities.

Method: Developed CostBench - a scalable cost-centric benchmark in travel planning domain with tasks solvable via multiple tool sequences with customizable costs, and four types of dynamic blocking events (tool failures, cost changes) to simulate real-world unpredictability.

Result: Evaluation of leading models shows substantial gaps in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings (GPT-5 achieves <75% exact match on hardest tasks), with performance dropping ~40% under dynamic conditions.

Conclusion: CostBench diagnoses key weaknesses in LLM agents' economic reasoning and lays groundwork for developing more economically rational and robust agents.

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [109] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: Span queries generalize inference server interfaces beyond chat completion, enabling optimization for diverse workloads like RAG, inference-time scaling, and agentic tasks through commutativity constraints and KV cache locality improvements.


<details>
  <summary>Details</summary>
Motivation: Current inference servers are heavily optimized for chat completion but clients are evolving to use various inference-time scaling and deep reasoning techniques. Prior work only addressed RAG optimization, leaving other non-chat use cases unoptimized.

Method: Introduce span queries as expression trees of inference calls with commutativity constraints, modify vLLM (492 lines) to support high-performance execution, and optimize for KV cache locality and attention locality.

Result: Span queries achieve 10-20x reductions in Time To First Token (TTFT) for non-chat use cases, and attention-optimized span queries on 2b parameter models outperform stock inference servers using 8b models.

Conclusion: Span queries provide a generalized interface that enables efficient optimization across diverse inference workloads, significantly improving performance for non-chat use cases while maintaining compatibility with existing chat completion.

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [110] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: LLM-supported semi-automated method for generating formal knowledge representations in control engineering, combining human readability with machine interpretability using the PyIRK framework.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of research output in control engineering requires new approaches to structure and formalize domain knowledge for better accessibility and verifiability.

Method: Uses language models to transform natural-language descriptions and mathematical definitions (LaTeX source code) into formalized knowledge graphs based on the Imperative Representation of Knowledge (PyIRK) framework.

Result: Developed an "interactive semantic layer" to enhance source documents and facilitate knowledge transfer in control engineering.

Conclusion: This approach contributes to creating easily accessible, collaborative, and verifiable knowledge bases for the control engineering domain.

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [111] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: The paper introduces modality sabotage as a diagnostic failure mode in MLLMs where unimodal errors override other evidence, and proposes a lightweight evaluation layer to analyze fusion dynamics by treating modalities as agents.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models' reasoning traces are opaque - it's unclear which modality drives predictions, how conflicts are resolved, or when one stream dominates, making it difficult to understand failure modes.

Method: A lightweight, model-agnostic evaluation layer that treats each modality as an agent producing candidate labels and self-assessments, with a simple fusion mechanism to aggregate outputs and identify contributors vs saboteurs.

Result: Application to multimodal emotion recognition benchmarks revealed systematic reliability profiles, providing insights into whether failures stem from dataset artifacts or model limitations.

Conclusion: The framework offers a diagnostic scaffold for multimodal reasoning that supports principled auditing of fusion dynamics and informs potential interventions.

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [112] [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.AI

TL;DR: Orion-MSP is a novel tabular in-context learning architecture that addresses limitations of current models through multi-scale processing, block-sparse attention, and Perceiver-style memory for efficient hierarchical feature interaction capture.


<details>
  <summary>Details</summary>
Motivation: Current tabular ICL architectures have limitations including single-scale feature processing, quadratic scaling dense attention, and strictly sequential component processing that prevents iterative refinement and cross-component communication.

Method: Introduces three key innovations: 1) multi-scale processing for hierarchical feature interactions, 2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency, and 3) Perceiver-style memory enabling bidirectional information flow across components.

Result: Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables.

Conclusion: Orion-MSP establishes a new standard for efficient tabular in-context learning, addressing key architectural limitations of previous approaches.

Abstract: Tabular data remain the predominant format for real-world applications. Yet,
developing effective neural models for tabular data remains challenging due to
heterogeneous feature types and complex interactions occurring at multiple
scales. Recent advances in tabular in-context learning (ICL), such as TabPFN
and TabICL, have achieved state-of-the-art performance comparable to
gradient-boosted trees (GBTs) without task-specific fine-tuning. However,
current architectures exhibit key limitations: (1) single-scale feature
processing that overlooks hierarchical dependencies, (2) dense attention with
quadratic scaling in table width, and (3) strictly sequential component
processing that prevents iterative representation refinement and
cross-component communication. To address these challenges, we introduce
Orion-MSP, a tabular ICL architecture featuring three key innovations: (1)
multi-scale processing to capture hierarchical feature interactions; (2)
block-sparse attention combining windowed, global, and random patterns for
scalable efficiency and long-range connectivity; and (3) a Perceiver-style
memory enabling safe bidirectional information flow across components. Across
diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance
while scaling effectively to high-dimensional tables, establishing a new
standard for efficient tabular in-context learning. The model is publicly
available at https://github.com/Lexsi-Labs/Orion-MSP .

</details>


### [113] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: This paper proposes a method to optimize AI attack policies for control evaluations by decomposing attack capability into five skills and using probabilistic modeling to overcome data limitations in complex environments.


<details>
  <summary>Details</summary>
Motivation: As AI deployments become more complex and high-stakes, there's a growing need to estimate their risk through control evaluations, which require strong attack policies. This is challenging in complex agentic environments with compute constraints that leave us data-poor.

Method: The authors decompose attack capability into five constituent skills (suspicion modeling, attack selection, plan synthesis, execution, and subtlety) and optimize each component individually. They develop a probabilistic model of attack dynamics to overcome data limitations, optimize attack hyperparameters using simulation, and transfer results to SHADE-Arena environments.

Result: The method achieves substantial improvement in attack strength, reducing safety score from a baseline of 0.87 to 0.41 using their scaffold approach.

Conclusion: The proposed framework successfully optimizes attack policies for AI control evaluations by skill decomposition and probabilistic modeling, enabling more effective risk assessment in complex AI deployments despite data constraints.

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [114] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener,Angela Yiu,Benjamin Chang,Mathieu Bourdenx,Tyler Nadolski,Arvis Sulovari,Eric C. Landsness,Daniel L. Barabasi,Siddharth Narayanan,Nicky Evans,Shriya Reddy,Martha Foiani,Aizad Kamal,Leah P. Shriver,Fang Cao,Asmamaw T. Wassie,Jon M. Laurent,Edwin Melville-Green,Mayk Caldas,Albert Bou,Kaleigh F. Roberts,Sladjana Zagorac,Timothy C. Orr,Miranda E. Orr,Kevin J. Zwezdaryk,Ali E. Ghareeb,Laurie McCoy,Bruna Gomes,Euan A. Ashley,Karen E. Duff,Tonio Buonassisi,Tom Rainforth,Randall J. Bateman,Michael Skarlinski,Samuel G. Rodriques,Michaela M. Hinks,Andrew D. White*

Main category: cs.AI

TL;DR: Kosmos is an AI scientist that automates data-driven discovery through iterative cycles of data analysis, literature search, and hypothesis generation, maintaining coherence over extended periods using a structured world model.


<details>
  <summary>Details</summary>
Motivation: Current AI agents for scientific research are limited in the number of actions they can take before losing coherence, restricting the depth of their findings and scientific discovery capabilities.

Method: Kosmos uses a structured world model to share information between data analysis and literature search agents, enabling coherent pursuit of objectives over 200 agent rollouts (executing ~42,000 lines of code and reading ~1,500 papers per run).

Result: Kosmos generated scientific reports with 79.4% accuracy according to independent scientists, performing equivalent of 6 months of human research time in a single 20-cycle run. It made 7 discoveries across multiple fields, with 3 independently reproducing unpublished findings and 4 making novel contributions.

Conclusion: Kosmos demonstrates scalable automated scientific discovery with traceable reasoning, showing that valuable scientific findings scale linearly with computational cycles, enabling deeper exploration of scientific questions than previously possible with AI systems.

Abstract: Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [115] [Neurosymbolic Deep Learning Semantics](https://arxiv.org/abs/2511.02825)
*Artur d'Avila Garcez,Simon Odense*

Main category: cs.AI

TL;DR: This paper argues that logic provides a semantic framework for deep learning and neurosymbolic AI, addressing the lack of semantics in current AI-based science. The authors introduce a framework for semantic encoding that explicitly maps neural networks to logic.


<details>
  <summary>Details</summary>
Motivation: AI lacks semantics, making its scientific discoveries unsatisfactory. The paper aims to provide a formal framework that can translate AI insights into comprehensible scientific knowledge through logical semantics.

Method: The authors use logic in a neurosymbolic framework to provide semantics for deep learning. They introduce a framework for semantic encoding that makes explicit the mapping between neural networks and logic, characterizing common ingredients of existing approaches.

Result: The paper provides a formal definition of the semantic encoding framework and reviews prominent approaches for neural encoding and knowledge extraction. It demonstrates how logical semantics and neural networks are linked through this framework.

Conclusion: Logic offers an adequate framework for providing semantics to AI-based science, particularly for deep learning and neurosymbolic AI. The semantic encoding framework helps bridge the gap between neural networks and comprehensible scientific knowledge.

Abstract: Artificial Intelligence (AI) is a powerful new language of science as
evidenced by recent Nobel Prizes in chemistry and physics that recognized
contributions to AI applied to those areas. Yet, this new language lacks
semantics, which makes AI's scientific discoveries unsatisfactory at best. With
the purpose of uncovering new facts but also improving our understanding of the
world, AI-based science requires formalization through a framework capable of
translating insight into comprehensible scientific knowledge. In this paper, we
argue that logic offers an adequate framework. In particular, we use logic in a
neurosymbolic framework to offer a much needed semantics for deep learning, the
neural network-based technology of current AI. Deep learning and neurosymbolic
AI lack a general set of conditions to ensure that desirable properties are
satisfied. Instead, there is a plethora of encoding and knowledge extraction
approaches designed for particular cases. To rectify this, we introduced a
framework for semantic encoding, making explicit the mapping between neural
networks and logic, and characterizing the common ingredients of the various
existing approaches. In this paper, we describe succinctly and exemplify how
logical semantics and neural networks are linked through this framework, we
review some of the most prominent approaches and techniques developed for
neural encoding and knowledge extraction, provide a formal definition of our
framework, and discuss some of the difficulties of identifying a semantic
encoding in practice in light of analogous problems in the philosophy of mind.

</details>


### [116] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: Agent-Omni framework coordinates existing foundation models through a master-agent system for flexible multimodal reasoning without retraining, achieving state-of-the-art performance across text, image, audio, and video tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs are limited to fixed modality pairs, require costly fine-tuning, and lack robust reasoning support for fully omni-capable models integrating text, images, audio, and video.

Method: A master-agent system where the master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses without retraining existing models.

Result: Extensive experiments show Agent-Omni consistently achieves state-of-the-art performance across text, image, audio, video, and omni benchmarks, particularly on complex cross-modal reasoning tasks.

Conclusion: The agent-based design enables seamless integration of specialized foundation models, maintains adaptability to diverse inputs, and provides transparency and interpretability while being modular and easily extensible for future improvements.

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>
