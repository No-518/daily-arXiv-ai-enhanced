{"id": "2602.06966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06966", "abs": "https://arxiv.org/abs/2602.06966", "authors": ["Kai Xu", "Hang Zhao", "Ruizhen Hu", "Min Yang", "Hao Liu", "Hui Zhang", "Haibin Yu"], "title": "Embodied Intelligence for Flexible Manufacturing: A Survey", "comment": "in chinese language. ROBOT", "summary": "Driven by breakthroughs in next-generation artificial intelligence, embodied intelligence is rapidly advancing into industrial manufacturing. In flexible manufacturing, industrial embodied intelligence faces three core challenges: accurate process modeling and monitoring under limited perception, dynamic balancing between flexible adaptation and high-precision control, and the integration of general-purpose skills with specialized industrial operations. Accordingly, this survey reviews existing work from three viewpoints: Industrial Eye, Industrial Hand, and Industrial Brain. At the perception level (Industrial Eye), multimodal data fusion and real-time modeling in complex dynamic settings are examined. At the control level (Industrial Hand), flexible, adaptive, and precise manipulation for complex manufacturing processes is analyzed. At the decision level (Industrial Brain), intelligent optimization methods for process planning and line scheduling are summarized. By considering multi-level collaboration and interdisciplinary integration, this work reveals the key technological pathways of embodied intelligence for closed-loop optimization of perception-decision-execution in manufacturing systems. A three-stage evolution model for the development of embodied intelligence in flexible manufacturing scenarios, comprising cognition enhancement, skill transition, and system evolution, is proposed, and future development trends are examined, to offer both a theoretical framework and practical guidance for the interdisciplinary advancement of industrial embodied intelligence in the context of flexible manufacturing.", "AI": {"tldr": "Survey paper reviewing embodied intelligence in flexible manufacturing, analyzing challenges in perception, control, and decision-making, and proposing a three-stage evolution model for industrial AI systems.", "motivation": "Address three core challenges in industrial embodied intelligence: accurate process modeling under limited perception, balancing flexible adaptation with high-precision control, and integrating general-purpose skills with specialized industrial operations.", "method": "Survey approach organized around three viewpoints: Industrial Eye (perception), Industrial Hand (control), and Industrial Brain (decision-making). Examines multimodal data fusion, adaptive manipulation, and intelligent optimization methods.", "result": "Identifies key technological pathways for closed-loop optimization in manufacturing systems and proposes a three-stage evolution model: cognition enhancement, skill transition, and system evolution.", "conclusion": "Provides theoretical framework and practical guidance for interdisciplinary advancement of industrial embodied intelligence in flexible manufacturing, with insights into future development trends."}}
{"id": "2602.06967", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06967", "abs": "https://arxiv.org/abs/2602.06967", "authors": ["Siqi Song", "Xuanbing Xie", "Zonglin Li", "Yuqiang Li", "Shijie Wang", "Biqing Qi"], "title": "Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models", "comment": "20 pages, 12 figures, Under Review", "summary": "Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.", "AI": {"tldr": "CLiMRS is a framework that uses LLM agents paired with robots to enable adaptive group negotiation for heterogeneous multi-robot collaboration through dynamic subgroup formation and perception-driven discussions.", "motivation": "Multi-robot collaboration requires heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. While LLMs excel at reasoning and planning, their potential for coordinated control in multi-robot systems hasn't been fully explored.", "method": "Pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within subgroups, a subgroup manager leads perception-driven multi-LLM discussions to generate action commands. Uses a grouping-planning-execution-feedback loop with feedback from both robot execution outcomes and environment changes.", "result": "CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones, as evaluated on CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks.", "conclusion": "Leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration, demonstrating that LLM-driven adaptive group negotiation is effective for complex multi-robot tasks."}}
{"id": "2602.06968", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06968", "abs": "https://arxiv.org/abs/2602.06968", "authors": ["Xubo Luo", "Zhaojin Li", "Xue Wan", "Wei Zhang", "Leizheng Shu"], "title": "Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing", "comment": "8 pages, accepted by RA-L", "summary": "Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.", "AI": {"tldr": "KANLoc: A monocular localization framework for autonomous lunar landing that combines visual odometry with a KAN-based absolute pose regressor to achieve drift-free, real-time 6-DoF localization.", "motivation": "Existing lunar landing localization approaches have limitations - visual odometry drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. There's a need for accurate, real-time 6-DoF localization for mission-critical autonomous lunar landing.", "method": "KANLoc uses a Kolmogorov-Arnold Network (KAN) to learn mapping from image features to map coordinates, producing sparse but reliable global pose anchors. These anchors are fused into a bundle adjustment framework that tightly couples VO with the absolute pose regressor, effectively canceling drift while maintaining local motion precision.", "result": "On both synthetic and real lunar landing datasets, KANLoc reduces average translation error by 32% and rotation error by 45%, with per-trajectory gains up to 45%/48%. It achieves real-time performance (\u226515 FPS) and outperforms strong baselines.", "conclusion": "KANLoc successfully addresses the limitations of existing lunar landing localization methods by combining the strengths of visual odometry and absolute pose regression through a novel KAN-based approach, achieving accurate, drift-free, real-time 6-DoF localization for autonomous lunar missions."}}
{"id": "2602.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06969", "abs": "https://arxiv.org/abs/2602.06969", "authors": ["Roshan Kumar Chhetri", "Sarocha Jetawatthana", "Thanakorn Khamvilai"], "title": "A Survey of Medical Drones from Flight Dynamics, Guidance, Navigation, and Control Perspectives", "comment": null, "summary": "The integration of drones into the medical field has revolutionized healthcare delivery by enabling rapid transportation of medical supplies, organs, and even emergency assistance in remote or disaster-stricken areas. While other survey papers focus on the healthcare supply chain, operations, and medical emergency response aspects, this paper provides a comprehensive review of medical drones from the perspectives of flight dynamics and guidance, navigation, and control (GNC) systems. We first discuss the medical aerial delivery mission requirements and suitable uncrewed aerial system (UAS) configurations. We then address payload container design and optimization, and its effect on supplies and overall flight dynamics. We also explore the fundamental principles of GNC in the context of medical drone operations, highlighting key challenges arising from vibration, air temperature, pressure, and humidity, which affect the quality of medical supplies. The paper examines various GNC algorithms that can mitigate these challenges, as well as the algorithms' limitations. With these considerations, this survey aims to provide insights into optimizing GNC frameworks for medical drones, emphasizing research gaps and directions to improve real-world healthcare applications.", "AI": {"tldr": "This paper provides a comprehensive review of medical drones focusing specifically on flight dynamics and guidance, navigation, and control (GNC) systems, rather than healthcare logistics or emergency response aspects.", "motivation": "While existing survey papers cover healthcare supply chain and emergency response aspects of medical drones, there's a gap in understanding the flight dynamics and GNC systems specifically tailored for medical delivery missions, which have unique requirements and challenges.", "method": "The paper conducts a comprehensive review by examining: 1) medical aerial delivery mission requirements and suitable UAS configurations, 2) payload container design/optimization and its effects on flight dynamics, and 3) GNC principles and algorithms in medical drone operations, including challenges from environmental factors.", "result": "The survey identifies key challenges in medical drone operations including vibration, temperature, pressure, and humidity effects on medical supplies, and examines various GNC algorithms that can mitigate these issues while highlighting their limitations.", "conclusion": "The paper provides insights for optimizing GNC frameworks for medical drones and identifies research gaps and directions to improve real-world healthcare applications through better flight dynamics and control systems."}}
{"id": "2602.07006", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "A hierarchical Bayesian model for quantifying the rarity of accidental patterns on shoe soles to improve forensic evidence strength assessment.", "motivation": "Shoe print evidence is crucial in forensics, but matching make/model alone is insufficient since thousands of identical shoes exist. Accidentals (cuts, scrapes, wear patterns) that accumulate after purchase can be distinctive, but quantifying their rarity is essential for accurate evidence evaluation.", "method": "Developed a hierarchical Bayesian model with two key advancements: 1) Framed as a latent Gaussian model for efficient inference on large collections via integrated nested Laplace approximations, 2) Incorporated spatially varying coefficients to model relationships between shoe tread patterns and accidental locations.", "result": "Demonstrated superior performance on held-out data compared to existing methods, enhancing accuracy and reliability in forensic shoe print analysis.", "conclusion": "The proposed hierarchical Bayesian model with spatial modeling and efficient inference provides improved quantification of accidental pattern rarity, strengthening forensic shoe print evidence evaluation."}}
{"id": "2602.07032", "categories": ["cs.AI", "cs.AR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07032", "abs": "https://arxiv.org/abs/2602.07032", "authors": ["Yuheng Wu", "Berk Gokmen", "Zhouhua Xie", "Peijing Li", "Caroline Trippel", "Priyanka Raina", "Thierry Tambe"], "title": "LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation", "comment": null, "summary": "Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.", "AI": {"tldr": "LLM-FSM is an automated benchmark for evaluating LLMs' ability to translate natural-language FSM specifications to correct RTL implementations, showing performance declines with complexity but improvements with fine-tuning and test-time compute.", "motivation": "Finite-state reasoning is crucial for hardware design, but existing benchmarks rely on manually constructed examples. There's a need for automated, scalable evaluation of LLMs' ability to recover FSM behavior from natural language and implement it correctly in RTL.", "method": "Automated pipeline that: 1) constructs FSMs with configurable state counts and constrained transitions, 2) prompts LLMs to express FSMs in structured YAML with application context, 3) converts YAML to natural-language specifications, 4) synthesizes reference RTL and testbenches correct-by-construction, and 5) verifies 1,000 problems using LLM-based and SAT-solver checks.", "result": "Strongest LLMs show sharply declining accuracy as FSM complexity increases. Supervised fine-tuning generalizes effectively to out-of-distribution tasks, and increasing test-time compute improves reasoning reliability. The benchmark is extensible and scales with future model capabilities.", "conclusion": "LLM-FSM provides an automated, scalable benchmark for evaluating LLMs' finite-state reasoning capabilities, revealing current limitations in handling complex FSMs while showing promising directions for improvement through fine-tuning and increased compute."}}
{"id": "2602.06971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06971", "abs": "https://arxiv.org/abs/2602.06971", "authors": ["Anastasios Manganaris", "Vittorio Giammarino", "Ahmed H. Qureshi", "Suresh Jagannathan"], "title": "Formal Methods in Robot Policy Learning and Verification: A Survey on Current Techniques and Future Directions", "comment": "19 Pages. 6 Figures. Published in Transactions on Machine Learning Research", "summary": "As hardware and software systems have grown in complexity, formal methods have been indispensable tools for rigorously specifying acceptable behaviors, synthesizing programs to meet these specifications, and validating the correctness of existing programs. In the field of robotics, a similar trend of rising complexity has emerged, driven in large part by the adoption of deep learning. While this shift has enabled the development of highly performant robot policies, their implementation as deep neural networks has posed challenges to traditional formal analysis, leading to models that are inflexible, fragile, and difficult to interpret. In response, the robotics community has introduced new formal and semi-formal methods to support the precise specification of complex objectives, guide the learning process to achieve them, and enable the verification of learned policies against them. In this survey, we provide a comprehensive overview of how formal methods have been used in recent robot learning research. We organize our discussion around two pillars: policy learning and policy verification. For both, we highlight representative techniques, compare their scalability and expressiveness, and summarize how they contribute to meaningfully improving realistic robot safety and correctness. We conclude with a discussion of remaining obstacles for achieving that goal and promising directions for advancing formal methods in robot learning.", "AI": {"tldr": "Survey of formal methods in robot learning, covering policy learning and verification to address challenges from deep learning complexity.", "motivation": "Deep learning has enabled high-performance robot policies but created challenges for traditional formal analysis, making models inflexible, fragile, and hard to interpret. Formal methods are needed to specify complex objectives, guide learning, and verify policies.", "method": "Comprehensive survey organizing discussion around two pillars: policy learning (specifying objectives and guiding learning) and policy verification (validating learned policies). Analyzes representative techniques, compares scalability and expressiveness.", "result": "Overview of how formal methods have been used in recent robot learning research, highlighting techniques that contribute to improving robot safety and correctness in realistic settings.", "conclusion": "Identifies remaining obstacles for achieving meaningful robot safety/correctness and suggests promising directions for advancing formal methods in robot learning."}}
{"id": "2602.07008", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "Proposes attribution-based human prior alignment method that penalizes models when their decision evidence deviates from human-provided regions (e.g., bounding boxes), improving both accuracy and decision reasonability.", "motivation": "Conventional supervised learning with only class-level labels allows models to achieve high accuracy through shortcut correlations rather than intended evidence. Models need to justify decisions with acceptable evidence aligned with human perception, but aligning models to human priors is challenging due to representation divergence.", "method": "Encode human priors as input regions (e.g., bounding boxes) that models should rely on. Use subset-selection-based attribution to expose model's decision evidence during training. Penalize reliance on off-prior evidence when attribution deviates from prior regions, encouraging attribution shift toward intended regions via training objective with attribution constraints.", "result": "Validated on image classification and click decision tasks in MLLM-based GUI agent models. Human prior alignment consistently improves task accuracy while enhancing model's decision reasonability across both conventional classification and autoregressive generation settings.", "conclusion": "Attribution-based human prior alignment effectively constrains models to rely on intended evidence rather than shortcut correlations, improving both performance and interpretability by aligning model decisions with human perception."}}
{"id": "2602.07034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07034", "abs": "https://arxiv.org/abs/2602.07034", "authors": ["Jinxiu Qu", "Zirui Tang", "Hongzhang Huang", "Boyu Niu", "Wei Zhou", "Jiannan Wang", "Yitong Song", "Guoliang Li", "Xuanhe Zhou", "Fan Wu"], "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA", "comment": null, "summary": "Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.", "AI": {"tldr": "ST-Raptor is an agentic system for semi-structured table QA that combines visual editing, tree-based structural modeling, and agent-driven query resolution to overcome limitations of existing methods.", "motivation": "Semi-structured table QA is challenging due to the need to extract precise cell contents/positions and recover implicit logical structures, hierarchical relationships, and semantic associations. Manual interpretation is labor-intensive, while existing automated methods (Text-to-SQL, Text-to-Code, multimodal LLM-based QA) suffer from information loss or struggle with complex layouts.", "method": "ST-Raptor offers an interactive analysis environment combining three key components: visual editing, tree-based structural modeling, and agent-driven query resolution. This integrated approach supports accurate and user-friendly table understanding.", "result": "Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability.", "conclusion": "ST-Raptor provides an effective solution for semi-structured table QA by addressing the limitations of existing approaches through its integrated visual, structural, and agentic components, offering improved performance and user experience."}}
{"id": "2602.06974", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06974", "abs": "https://arxiv.org/abs/2602.06974", "authors": ["Faith Johnson", "Bryan Bo Cao", "Shubham Jain", "Ashwin Ashok", "Kristin Dana"], "title": "FeudalNav: A Simple Framework for Visual Navigation", "comment": "8 Pages, 6 figures and 4 tables. arXiv admin note: substantial text overlap with arXiv:2411.09893, arXiv:2402.12498", "summary": "Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.", "AI": {"tldr": "Hierarchical visual navigation framework with waypoint selection network and visual similarity-based memory, achieving competitive results without odometry and enabling interactive navigation with minimal human intervention.", "motivation": "Traditional metric map-based methods fail in unseen, unmapped, or GPS-denied environments, requiring learning-based approaches that can navigate with minimal exploration using visual cues and memory like humans.", "method": "Hierarchical framework decomposing navigation into multiple levels: learns to select subgoals via transferable waypoint selection network, uses latent-space memory module organized by visual similarity (not graph-based), and operates without odometry in training or inference.", "result": "Competitive results with state-of-the-art methods in Habitat AI environments, demonstrating a compact, lightweight, simple-to-train navigator that can reach goals in novel locations without odometry.", "conclusion": "Visual similarity-based memory is sufficient for navigation tasks, and the interpretable framework enables interactive navigation where minimal human intervention significantly enhances performance, answering how much direction interaction is needed for success."}}
{"id": "2602.07011", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "MAU-Set dataset and MAU-GPT model for multi-type industrial anomaly understanding, outperforming SOTA across domains.", "motivation": "Industrial manufacturing scaling requires automated fine-grained product image analysis, but existing approaches suffer from limited dataset coverage and poor generalization across diverse anomaly patterns.", "method": "Introduce MAU-Set dataset spanning multiple industrial domains with hierarchical tasks, plus MAU-GPT multimodal large model with novel AMoE-LoRA mechanism unifying anomaly-aware and generalist experts adaptation.", "result": "MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable automated industrial inspection.", "conclusion": "The proposed dataset and model address key challenges in industrial anomaly understanding, enabling more comprehensive and generalizable automated quality control systems."}}
{"id": "2602.07035", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07035", "abs": "https://arxiv.org/abs/2602.07035", "authors": ["Jiahao Zhao", "Shaoxuan Xu", "Zhongxiang Sun", "Fengqi Zhu", "Jingyang Ou", "Yuling Shi", "Chongxuan Li", "Xiao Zhang", "Jun Xu"], "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents", "comment": null, "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C", "AI": {"tldr": "DLLM-Searcher is an optimization framework for diffusion-based LLM search agents that addresses latency and capability challenges through specialized training and parallel reasoning paradigms.", "motivation": "Current search agents suffer from high latency due to serial execution in ReAct paradigm, while diffusion LLMs have efficiency advantages but lack reasoning/tool-calling capabilities needed for agent tasks.", "method": "Two-stage post-training (Agentic SFT + Agentic VRPO) enhances dLLM capabilities, plus P-ReAct paradigm that prioritizes tool_call decoding to enable parallel thinking during tool execution.", "result": "DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents while P-ReAct delivers ~15% inference acceleration.", "conclusion": "The framework successfully bridges the gap between dLLM efficiency and agent capabilities, enabling practical deployment of efficient search agents through parallel reasoning and specialized training."}}
{"id": "2602.06977", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06977", "abs": "https://arxiv.org/abs/2602.06977", "authors": ["Shifa Sulaiman", "Francesco Schetter", "Tobias Jensen", "Simon B\u00f8gh", "Fanny Ficuciello"], "title": "Autonomous Manipulation of Hazardous Chemicals and Delicate Objects in a Self-Driving Laboratory: A Sliding Mode Approach", "comment": null, "summary": "Precise handling of chemical instruments and materials within a self-driving laboratory environment using robotic systems demands advanced and reliable control strategies. Sliding Mode Control (SMC) has emerged as a robust approach for managing uncertainties and disturbances in manipulator dynamics, providing superior control performance compared to traditional methods. This study implements a model-based SMC (MBSMC) utilizing a hyperbolic tangent function to regulate the motion of a manipulator mounted on a mobile platform operating inside a self-driving chemical laboratory. Given the manipulator's role in transporting fragile glass vessels filled with hazardous chemicals, the controller is specifically designed to minimize abrupt transitions and achieve gentle, accurate trajectory tracking. The proposed controller is benchmarked against a non-model-based SMC (NMBSMC) and a Proportional-Integral-Derivative (PID) controller using a comprehensive set of joint and Cartesian metrics. Compared to PID and NMBSMC, MBSMC achieved significantly smoother motion and up to 90% lower control effort, validating its robustness and precision for autonomous laboratory operations. Experimental trials confirmed successful execution of tasks such as vessel grasping and window operation, which failed under PID control due to its limited ability to handle nonlinear dynamics and external disturbances, resulting in substantial trajectory tracking errors. The results validate the controller's effectiveness in achieving smooth, precise, and safe manipulator motions, supporting the advancement of intelligent mobile manipulators in autonomous laboratory environments.", "AI": {"tldr": "Model-based sliding mode control with hyperbolic tangent function enables smooth, precise robotic manipulation in self-driving chemical labs, outperforming PID and non-model-based SMC with 90% lower control effort.", "motivation": "Precise handling of fragile chemical vessels in self-driving labs requires robust control strategies that can manage uncertainties and disturbances while ensuring gentle, safe manipulation to prevent accidents with hazardous materials.", "method": "Implemented model-based sliding mode control (MBSMC) using hyperbolic tangent function for a manipulator on mobile platform in self-driving chemical lab. Compared against non-model-based SMC (NMBSMC) and PID controller using joint and Cartesian metrics.", "result": "MBSMC achieved significantly smoother motion and up to 90% lower control effort compared to PID and NMBSMC. Successfully executed tasks like vessel grasping and window operation that failed under PID control due to nonlinear dynamics handling limitations.", "conclusion": "Model-based sliding mode control with hyperbolic tangent function provides robust, precise, and safe manipulator control for autonomous laboratory operations, advancing intelligent mobile manipulators in chemical research environments."}}
{"id": "2602.07012", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM is a comprehensive retinal segmentation and quantification framework that transforms fundus images into standardized biomarkers for large-scale ophthalmic research.", "motivation": "Retinal imaging offers valuable structural and vascular signals for health assessment, but large-scale analysis is limited by scarce multi-label datasets and lack of unified segmentation-to-quantification pipelines.", "method": "RetSAM uses a multi-stage training strategy on over 200,000 fundus images to segment anatomical structures, phenotypic patterns, and lesions, then converts these into standardized biomarkers capturing morphology, vascular geometry, and degenerative changes.", "result": "RetSAM achieves superior performance on 17 public datasets, improving by 3.9 percentage points in DSC on average (up to 15 points on challenging tasks), and generalizes well across diverse populations and imaging devices.", "conclusion": "RetSAM enables systematic correlation analyses across major ophthalmic diseases and transforms fundus images into interpretable quantitative phenotypes for large-scale research and clinical translation."}}
{"id": "2602.07040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07040", "abs": "https://arxiv.org/abs/2602.07040", "authors": ["Emmett Bicker"], "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods", "comment": "Available at www.asterlab.ai, 25 pages, 8 figures, 4 tables", "summary": "We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.", "AI": {"tldr": "Aster is an AI agent for autonomous scientific discovery that operates 20x faster than existing frameworks, achieving SOTA results across multiple domains including mathematics, GPU engineering, biology, neuroscience, and language model training.", "motivation": "The need for faster autonomous scientific discovery systems that can handle tasks with long evaluation durations (like multi-hour ML training runs) and expand the domain of tractable problems.", "method": "Aster iteratively improves programs given a task, initial program, and evaluation script, using an AI agent approach that significantly reduces the number of iterations required for novel discovery.", "result": "Aster achieved SOTA results in all tested domains except ZAPBench (where it matched human performance with <1/190th compute), including: Erdos minimum overlap problem, TriMul kernel optimization, single-cell analysis denoising, neural activity prediction, and NanoGPT Speedrun Competition.", "conclusion": "Aster demonstrates unprecedented speed and effectiveness in autonomous scientific discovery, making previously intractable problems tractable and available via web interface at asterlab.ai."}}
{"id": "2602.06991", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.06991", "abs": "https://arxiv.org/abs/2602.06991", "authors": ["Seongbo Ha", "Sibaek Lee", "Kyungsu Kang", "Joonyeol Choi", "Seungjun Tak", "Hyeonwoo Yu"], "title": "LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM", "comment": "17 pages, 4 figures", "summary": "In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.", "AI": {"tldr": "RGB-D SLAM system that reconstructs language-aligned dense feature fields in real-time (15 FPS) using Top-K rendering, multi-criteria map management, and hybrid field optimization.", "motivation": "To bridge the gap between 3D perception and language-based reasoning by enabling online SLAM with dense, uncompressed language-aligned feature fields, which has been challenging due to computational constraints.", "method": "Three key components: 1) Top-K Rendering pipeline for efficient semantic-distortion-free feature map rendering, 2) multi-criteria map management to prune redundant/inconsistent Gaussians while preserving scene integrity, and 3) hybrid field optimization framework that decouples geometric and semantic optimization frequencies.", "result": "Achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS, demonstrating feasibility of online SLAM with dense language-aligned feature fields.", "conclusion": "Online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, successfully bridging 3D perception with language-based reasoning in real-time applications."}}
{"id": "2602.07013", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "CR-VLM introduces a configurable refusal mechanism for Vision Language Models that adapts to user needs and contexts, preventing both under- and over-refusal through activation steering.", "motivation": "Existing refusal strategies in VLMs are one-size-fits-all and fail to adapt to diverse user needs and contextual constraints, leading to problematic under-refusal or over-refusal scenarios.", "method": "Three integrated components: (1) configurable refusal vector extraction via teacher-forced mechanism to amplify refusal signals, (2) gating mechanism to preserve acceptance for in-scope queries and prevent over-refusal, (3) counterfactual vision enhancement module aligning visual representations with refusal requirements.", "result": "Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals.", "conclusion": "CR-VLM offers a scalable path toward user-adaptive safety alignment in Vision Language Models through its configurable refusal approach."}}
{"id": "2602.07055", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07055", "abs": "https://arxiv.org/abs/2602.07055", "authors": ["Pingyue Zhang", "Zihan Huang", "Yue Wang", "Jieyu Zhang", "Letian Xue", "Zihan Wang", "Qineng Wang", "Keshigeyan Chandrasegaran", "Ruohan Zhang", "Yejin Choi", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Manling Li"], "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?", "comment": "published at iclr 2026", "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.", "AI": {"tldr": "Foundation models struggle with active spatial exploration and maintaining coherent spatial beliefs, showing significant performance gaps compared to passive perception tasks.", "motivation": "Spatial embodied intelligence requires active information acquisition under partial observability, but current multimodal foundation models excel mainly at passive perception while their capacity for self-directed exploration remains understudied.", "method": "Proposed Theory of Space framework to evaluate agents' ability to actively acquire information through self-directed exploration and construct spatial beliefs. Created a benchmark for curiosity-driven exploration to build cognitive maps, with key innovation of spatial belief probing to reveal internal spatial representations at each step.", "result": "Identified critical bottlenecks: 1) Active-Passive Gap - significant performance drop when agents must autonomously gather information; 2) High inefficiency - unsystematic exploration compared to program-based proxies; 3) Belief instability - spatial knowledge degrades over time; 4) Belief Inertia - failure to update obsolete priors with new evidence, particularly severe in vision-based models.", "conclusion": "Current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration, revealing fundamental limitations in their capacity for spatial embodied intelligence."}}
{"id": "2602.06995", "categories": ["cs.RO", "cs.CV", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.06995", "abs": "https://arxiv.org/abs/2602.06995", "authors": ["Konstantinos Gounis", "Sotiris A. Tegos", "Dimitrios Tyrovolas", "Panagiotis D. Diamantoulakis", "George K. Karagiannidis"], "title": "When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey", "comment": null, "summary": "The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.", "AI": {"tldr": "Survey paper exploring bidirectional relationship between SLAM (especially visual SLAM) and wireless communications, showing how each can benefit the other for improved localization, sensing, and autonomous systems.", "motivation": "Advancements in wireless communication/sensing equipment and intelligent autonomous systems create opportunities for robust joint communications and SLAM, but integrated solutions are still in early stages.", "method": "Comprehensive survey analyzing state-of-the-art at SLAM-wireless communications nexus, covering wireless signal propagation, geometric channel modeling, RF-based localization/sensing, image processing for landmark detection, and mathematical approaches including probabilistic models and spatial signal processing.", "result": "Identified bidirectional benefits: monocular V-SLAM benefits from RF information for scale ambiguity resolution, while 5G+ wireless communications can benefit from visual odometry. Found that integrated joint communications-SLAM solutions are still immature and require theoretical/practical advancements.", "conclusion": "The intersection of SLAM and wireless communications offers promising synergies but requires further research to develop integrated solutions with higher-level localization and semantic perception capabilities for RF and multi-antenna technologies."}}
{"id": "2602.07014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "Vectra is a reference-free MLLM framework for assessing visual quality in e-commerce In-Image Machine Translation, featuring a multidimensional scoring system, large-scale dataset, and 4B-parameter model that outperforms leading MLLMs.", "motivation": "Current visual quality assessment methods for e-commerce IIMT lack explainability and fine-grained reward signals, especially when dealing with context-dense product imagery and multimodal defects. Existing reference-based methods (SSIM, FID) are not explainable, while model-as-judge approaches lack domain-specific, detailed feedback.", "method": "Vectra introduces three components: (1) Vectra Score - a 14-dimensional interpretable quality metric system with spatially-aware Defect Area Ratio quantification; (2) Vectra Dataset - built from 1.1M real product images via diversity-aware sampling, including 2K benchmark, 30K reasoning annotations, and 3.5K expert preferences; (3) Vectra Model - a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning.", "result": "Vectra achieves state-of-the-art correlation with human rankings and outperforms leading MLLMs (including GPT-5 and Gemini-3) in scoring performance. The model generates both quantitative scores and diagnostic reasoning for visual quality assessment.", "conclusion": "Vectra successfully bridges the gap in visual quality assessment for e-commerce IIMT by providing an explainable, reference-free framework with fine-grained evaluation capabilities. The comprehensive dataset and model will be released to advance research in this domain."}}
{"id": "2602.07153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07153", "abs": "https://arxiv.org/abs/2602.07153", "authors": ["Jinbiao Wei", "Yilun Zhao", "Kangqi Ni", "Arman Cohan"], "title": "ANCHOR: Branch-Point Data Generation for GUI Agents", "comment": null, "summary": "End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.", "AI": {"tldr": "Anchor is a framework that expands desktop GUI interaction data from a few seed demonstrations by identifying branch points, proposing state-grounded task variants, and verifying execution to create scalable, high-quality training data for GUI agents.", "motivation": "Collecting human demonstrations for GUI agents is expensive, and existing synthetic pipelines have limited task diversity or produce noisy, goal-drifting trajectories that don't maintain coherent intent.", "method": "The framework starts with verified seed demonstrations, identifies branch points with meaningful state changes, proposes new state-grounded task variants conditioned on GUI context, uses an executing agent to generate trajectories, and applies verification with state-aware checks and trajectory-level consistency. It also uses task-conditioned step-level filtering to remove ungrounded actions and denoises post-branch segments.", "result": "Models fine-tuned on the expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines on standard desktop benchmarks (OSWorld and WindowsAgentArena), and generalize across applications and operating systems.", "conclusion": "Anchor provides an effective framework for bootstrapping scalable desktop supervision from limited seed demonstrations, addressing data quality and diversity challenges in GUI agent training."}}
{"id": "2602.07005", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07005", "abs": "https://arxiv.org/abs/2602.07005", "authors": ["Shifa Sulaiman", "Tobias Jensen", "Francesco Schetter", "Simon B\u00f8gh"], "title": "Admittance-Based Motion Planning with Vision-Guided Initialization for Robotic Manipulators in Self-Driving Laboratories", "comment": null, "summary": "Self driving laboratories (SDLs) are highly automated research environments that leverage advanced technologies to conduct experiments and analyze data with minimal human involvement. These environments often involve delicate laboratory equipment, unpredictable environmental interactions, and occasional human intervention, making compliant and force aware control essential for ensuring safety, adaptability, and reliability. This paper introduces a motion-planning framework centered on admittance control to enable adaptive and compliant robotic manipulation. Unlike conventional schemes, the proposed approach integrates an admittance controller directly into trajectory execution, allowing the manipulator to dynamically respond to external forces during interaction. This capability enables human operators to override or redirect the robot's motion in real time. A vision algorithm based on structured planar pose estimation is employed to detect and localize textured planar objects through feature extraction, homography estimation, and depth fusion, thereby providing an initial target configuration for motion planning. The vision based initialization establishes the reference trajectory, while the embedded admittance controller ensures that trajectory execution remains safe, adaptive, and responsive to external forces or human intervention. The proposed strategy is validated using textured image detection as a proof of concept. Future work will extend the framework to SDL environments involving transparent laboratory objects where compliant motion planning can further enhance autonomy, safety, and human-robot collaboration.", "AI": {"tldr": "A motion-planning framework using admittance control for compliant robotic manipulation in self-driving labs, enabling real-time human override and safe interaction with delicate equipment.", "motivation": "Self-driving labs need compliant force-aware control for safety with delicate equipment, unpredictable environments, and human intervention. Current approaches lack real-time adaptability to external forces.", "method": "Integrates admittance controller directly into trajectory execution for dynamic response to external forces. Uses vision algorithm with structured planar pose estimation (feature extraction, homography estimation, depth fusion) for object detection and motion planning initialization.", "result": "Validated using textured image detection as proof of concept. Framework enables real-time human override, safe trajectory execution, and adaptive response to external forces.", "conclusion": "Proposed admittance-based motion planning enhances safety and adaptability in SDLs. Future work will extend to transparent laboratory objects to further improve autonomy and human-robot collaboration."}}
{"id": "2602.07015", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07015", "abs": "https://arxiv.org/abs/2602.07015", "authors": ["Subreena", "Mohammad Amzad Hossain", "Mirza Raquib", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Muhammad Hanif", "Nick Rahimi"], "title": "Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach", "comment": null, "summary": "Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.", "AI": {"tldr": "A hybrid CNN model combining MobileNetV3-Large and EfficientNetB0 achieves high accuracy for Bangladeshi banknote recognition, addressing fraud risks for visually impaired individuals through a comprehensive dataset and explainable AI methods.", "motivation": "Visually impaired individuals face fraud risks when relying on others to identify banknotes. Current recognition models have limitations, and there's a need for accurate, efficient systems suitable for resource-constrained devices.", "method": "Built a new Bangladeshi banknote dataset with controlled/real-world scenarios, augmented with four additional datasets. Proposed hybrid CNN architecture combining MobileNetV3-Large and EfficientNetB0 for feature extraction, followed by MLP classifier. Used five-fold cross-validation and seven evaluation metrics, plus explainable AI methods (LIME and SHAP).", "result": "Achieved 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. Model performance was thoroughly evaluated with multiple metrics and explainable AI methods.", "conclusion": "The proposed hybrid CNN model provides an effective, efficient solution for banknote recognition that addresses fraud risks for visually impaired individuals while maintaining computational efficiency for resource-constrained devices."}}
{"id": "2602.07187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07187", "abs": "https://arxiv.org/abs/2602.07187", "authors": ["Hanyu Wang", "Yuanpu Cao", "Lu Lin", "Jinghui Chen"], "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents", "comment": null, "summary": "Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.", "AI": {"tldr": "PreFlect introduces prospective reflection for LLM agents, shifting from post-hoc error correction to pre-execution plan criticism and refinement, improving agent performance on complex tasks.", "motivation": "Existing reflective approaches for LLM agents are retrospective - they only analyze and correct errors after failures occur. This reactive approach is inefficient compared to preventing errors before execution through foresight.", "method": "PreFlect uses prospective reflection to criticize and refine agent plans before execution. It distills planning errors from historical agent trajectories to capture recurring success/failure patterns, and includes dynamic re-planning for execution-time plan updates when unexpected deviations occur.", "result": "Evaluations on different benchmarks show PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures.", "conclusion": "Shifting from retrospective to prospective reflection enables more effective LLM agents by preventing errors before execution rather than correcting them after failure, leading to better performance on complex tasks."}}
{"id": "2602.07007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07007", "abs": "https://arxiv.org/abs/2602.07007", "authors": ["Dongsheng Chen", "Yuxuan Li", "Yi Lin", "Guanhua Chen", "Jiaxin Zhang", "Xiangyu Zhao", "Lei Ma", "Xin Yao", "Xuetao Wei"], "title": "ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning", "comment": null, "summary": "Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.", "AI": {"tldr": "ARGOS framework uses attribute-guided combinatorial reasoning to generate physically-grounded hazard scenarios and functional safety requirements for Embodied AI systems operating on natural language instructions.", "motivation": "Traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale for Embodied AI in open-world environments because they rely on enumerating risks for finite, pre-defined functions, while Embodied AI operates on open-ended natural language instructions with combinatorial interaction risks. LLMs offer scalability but lack physical grounding, producing superficial hazard descriptions.", "method": "ARGOS (AttRibute-Guided cOmbinatorial reaSoning) bridges open-ended instructions and concrete physical attributes by dynamically decomposing entities into fine-grained properties. It grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios, then instantiates abstract safety standards (like ISO 13482) into context-specific Functional Safety Requirements by integrating scenarios with robot capabilities.", "result": "Extensive experiments validate that ARGOS produces high-quality Functional Safety Requirements and outperforms baselines in identifying long-tail risks.", "conclusion": "ARGOS paves the way for systematic and grounded functional safety requirement generation, a critical step toward safe industrial deployment of Embodied AI systems."}}
{"id": "2602.07016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07016", "abs": "https://arxiv.org/abs/2602.07016", "authors": ["Mohsen Mostafa"], "title": "Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency", "comment": "10 pages, 3 figures, https://www.kaggle.com/code/babydriver1233/optimized-pipeline-for-the-image-matching-challeng, https://www.kaggle.com/code/babydriver1233/integrating-lejepa-for-enhanced-image-matching", "summary": "Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.", "AI": {"tldr": "LeJEPA-inspired Gaussian constraints on image embeddings improve unsupervised 3D scene reconstruction from unstructured image collections, particularly for scene discovery and camera pose estimation in visually ambiguous settings.", "motivation": "Unsupervised 3D scene reconstruction from unstructured image collections remains challenging, especially with images from multiple unrelated scenes and visual ambiguity. The IMC2025 challenge highlights these difficulties requiring both scene discovery and camera pose estimation under real-world conditions with outliers and mixed content.", "method": "Three progressively refined pipelines culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. The method empirically evaluates how these constraints influence clustering consistency and pose estimation robustness rather than introducing new theoretical guarantees.", "result": "Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings.", "conclusion": "Theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines."}}
{"id": "2602.07238", "categories": ["cs.AI", "cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.07238", "abs": "https://arxiv.org/abs/2602.07238", "authors": ["Matthias Mertens", "Natalia Fischl-Lanzoni", "Neil Thompson"], "title": "Is there \"Secret Sauce'' in Large Language Model Development?", "comment": null, "summary": "Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.", "AI": {"tldr": "LLM performance at the frontier is driven primarily by compute scaling, not proprietary technology, but away from the frontier, proprietary techniques significantly reduce compute requirements for fixed capabilities.", "motivation": "To determine whether LLM performance differences stem from proprietary \"secret sauce\" technologies or simply from scaling up compute resources, examining the relative importance of developer-specific advantages versus compute scaling.", "method": "Analyzed 809 models released between 2022-2025 using scaling-law regressions with release-date and developer fixed effects to estimate the contributions of compute scaling versus developer-specific efficiency advantages.", "result": "At the frontier, 80-90% of performance differences explained by higher training compute (scale drives advances). Away from frontier, proprietary techniques reduce compute requirements by 40x+ for same capabilities, with substantial efficiency variation even within companies.", "conclusion": "Scale, not proprietary technology, drives frontier LLM advances, but proprietary techniques enable more efficient training away from frontier, with implications for AI leadership and capability diffusion."}}
{"id": "2602.07024", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07024", "abs": "https://arxiv.org/abs/2602.07024", "authors": ["Valerio Belcamino", "Nhat Minh Dinh Le", "Quan Khanh Luu", "Alessandro Carf\u00ec", "Van Anh Ho", "Fulvio Mastrogiovanni"], "title": "A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration", "comment": null, "summary": "Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.", "AI": {"tldr": "Multi-modal HAR system using data glove with IMUs and vision-based tactile sensor achieves high accuracy for hand activity recognition in human-robot collaboration scenarios.", "motivation": "Human activity recognition is crucial for human-robot collaboration to enable robots to respond and adapt to human intentions, requiring accurate sensing of hand activities during contact interactions.", "method": "Combines modular data glove with Inertial Measurement Units and vision-based tactile sensor to capture hand activities in contact with robots. Tested under three conditions: offline classification of segmented sequences, real-time classification under static conditions, and realistic HRC scenarios.", "result": "Experimental results show high accuracy for all tasks, demonstrating the effectiveness of the multi-modal approach across different collaborative settings.", "conclusion": "The multi-modal HAR system combining IMU-based data glove and vision-based tactile sensing is effective for hand activity recognition in human-robot collaboration, with potential benefits for various collaborative settings."}}
{"id": "2602.07017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07017", "abs": "https://arxiv.org/abs/2602.07017", "authors": ["Thuraya Alzubaidi", "Sana Ammar", "Maryam Alsharqi", "Islem Rekik", "Muzammil Behzad"], "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models", "comment": null, "summary": "Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\\% reduction in runtime, a 44.6\\% improvement in dice score, and a 96.7\\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.", "AI": {"tldr": "XAI-CLIP: ROI-guided perturbation framework using multimodal vision-language embeddings to generate clearer, boundary-aware saliency maps for medical image segmentation with improved efficiency and interpretability.", "motivation": "Transformer-based medical image segmentation models lack interpretability, hindering clinical trust and deployment. Existing XAI methods are computationally expensive, require many forward passes, and produce noisy or anatomically irrelevant explanations.", "method": "Proposes XAI-CLIP framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions, integrates language-informed region localization with segmentation, and applies targeted region-aware perturbations to generate boundary-aware saliency maps.", "result": "Achieves 60% runtime reduction, 44.6% improvement in dice score, and 96.7% increase in IoU for occlusion-based explanations compared to conventional methods on FLARE22 and CHAOS datasets. Produces cleaner, anatomically consistent attribution maps with fewer artifacts.", "conclusion": "Incorporating multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, enabling transparent and clinically deployable medical image segmentation systems."}}
{"id": "2602.07253", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07253", "abs": "https://arxiv.org/abs/2602.07253", "authors": ["Litian Liu", "Reza Pourreza", "Yubing Jian", "Yao Qin", "Roland Memisevic"], "title": "From Out-of-Distribution Detection to Hallucination Detection: A Geometric View", "comment": null, "summary": "Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.", "AI": {"tldr": "Treating hallucination detection as out-of-distribution detection yields training-free, single-sample detectors that work well for reasoning tasks.", "motivation": "Existing hallucination detection methods perform well on question-answering but struggle with reasoning tasks, creating a critical safety gap for LLMs.", "method": "Reframe hallucination detection as out-of-distribution detection by treating next-token prediction as classification and adapting OOD techniques for LLM structure.", "result": "OOD-based approaches achieve strong accuracy in hallucination detection for reasoning tasks while being training-free and requiring only single samples.", "conclusion": "Treating hallucination detection as OOD detection provides a promising, scalable pathway toward improving language model safety."}}
{"id": "2602.07074", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.07074", "abs": "https://arxiv.org/abs/2602.07074", "authors": ["H. Emre Tekaslan", "Ella M. Atkins"], "title": "Airspace-aware Contingency Landing Planning", "comment": null, "summary": "This paper develops a real-time, search-based aircraft contingency landing planner that minimizes traffic disruptions while accounting for ground risk. The airspace model captures dense air traffic departure and arrival flows, helicopter corridors, and prohibited zones and is demonstrated with a Washington, D.C., area case study. Historical Automatic Dependent Surveillance-Broadcast (ADS-B) data are processed to estimate air traffic density. A low-latency computational geometry algorithm generates proximity-based heatmaps around high-risk corridors and restricted regions. Airspace risk is quantified as the cumulative exposure time of a landing trajectory within congested regions, while ground risk is assessed from overflown population density to jointly guide trajectory selection. A landing site selection module further mitigates disruption to nominal air traffic operations. Benchmarking against minimum-risk Dubins solutions demonstrates that the proposed planner achieves lower joint risk and reduced airspace disruption while maintaining real-time performance. Under airspace-risk-only conditions, the planner generates trajectories within an average of 2.9 seconds on a laptop computer. Future work will incorporate dynamic air traffic updates to enable spatiotemporal contingency landing planning that minimizes the need for real-time traffic rerouting.", "AI": {"tldr": "Real-time aircraft contingency landing planner that minimizes traffic disruptions and ground risk using search-based methods and computational geometry.", "motivation": "Need for real-time contingency landing planning that minimizes disruptions to dense air traffic operations while accounting for ground risk from population overflight.", "method": "Search-based planning with airspace modeling using historical ADS-B data, computational geometry algorithms for proximity heatmaps, and joint risk assessment combining airspace congestion exposure and ground population density.", "result": "Planner achieves lower joint risk and reduced airspace disruption compared to minimum-risk Dubins solutions, with real-time performance (average 2.9 seconds on laptop).", "conclusion": "The proposed planner effectively balances airspace and ground risks while maintaining real-time performance, with future work planned for dynamic air traffic updates."}}
{"id": "2602.07019", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07019", "abs": "https://arxiv.org/abs/2602.07019", "authors": ["Elaheh Sabziyan Varnousfaderani", "Syed A. M. Shihab", "Jonathan King"], "title": "Deep Learning Based Multi-Level Classification for Aviation Safety", "comment": null, "summary": "Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.", "AI": {"tldr": "Proposes CNN-based bird classification system for aviation safety that identifies species, flock formation, and size to improve bird strike prevention beyond current radar-only approaches.", "motivation": "Bird strikes threaten aviation safety with loss of life, aircraft damage, and financial costs. Current radar systems can't identify bird species, which is critical since different species have distinct flight behaviors and altitude preferences.", "method": "Image-based bird classification framework using Convolutional Neural Networks (CNNs) working with camera systems for autonomous visual detection. Includes species identification CNN plus dedicated classifiers for flock formation type and flock size estimation.", "result": "The proposed system provides species identification for species-specific flight path prediction, plus flock type and size information that offers insights into collective flight behavior, trajectory dispersion, and potential impact severity.", "conclusion": "CNN-based visual classification system addresses limitations of current radar-only bird strike prevention by providing species identification and flock characteristics for more accurate risk assessment and flight path prediction."}}
{"id": "2602.07259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07259", "abs": "https://arxiv.org/abs/2602.07259", "authors": ["Cheol Woo Kim", "Davin Choo", "Tzeh Yuan Neoh", "Milind Tambe"], "title": "Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective", "comment": null, "summary": "As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.", "AI": {"tldr": "AI safety requires strategic oversight of human/institutional incentives, not just model alignment. The paper proposes using Stackelberg Security Games to frame AI oversight as adversarial resource allocation between defenders and attackers across the AI lifecycle.", "motivation": "Current AI safety frameworks treat alignment as static optimization, overlooking dynamic adversarial incentives in data collection, model evaluation, and deployment. There's a need to address how malicious actors and institutional failures can undermine safety.", "method": "Proposes using Stackelberg Security Games (SSGs) - game-theoretic models for adversarial resource allocation under uncertainty. Frames AI oversight as strategic interaction between defenders (auditors, evaluators, deployers) and attackers (malicious actors, worst-case failures).", "result": "The SSG framework provides a unifying approach for incentive design, limited oversight capacity, and adversarial uncertainty. It informs three applications: training-time auditing against poisoning, pre-deployment evaluation with constrained resources, and robust multi-model deployment in adversarial environments.", "conclusion": "Bridges algorithmic alignment with institutional oversight design, showing how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation. Provides a strategic framework for addressing adversarial incentives throughout the AI lifecycle."}}
{"id": "2602.07158", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07158", "abs": "https://arxiv.org/abs/2602.07158", "authors": ["Deniz Kerimoglu", "Ismail Uyanik"], "title": "A compliant ankle-actuated compass walker with triggering timing control", "comment": "6 figures, 6 pages", "summary": "Passive dynamic walkers are widely adopted as a mathematical model to represent biped walking. The stable locomotion of these models is limited to tilted surfaces, requiring gravitational energy. Various techniques, such as actuation through the ankle and hip joints, have been proposed to extend the applicability of these models to level ground and rough terrain with improved locomotion efficiency. However, most of these techniques rely on impulsive energy injection schemes and torsional springs, which are quite challenging to implement in a physical platform. Here, a new model is proposed, named triggering controlled ankle actuated compass gait (TC-AACG), which allows non-instantaneous compliant ankle pushoff. The proposed technique can be implemented in physical platforms via series elastic actuators (SEAs). Our systematic examination shows that the proposed approach extends the locomotion capabilities of a biped model compared to impulsive ankle pushoff approach. We provide extensive simulation analysis investigating the locomotion speed, mechanical cost of transport, and basin of attraction of the proposed model.", "AI": {"tldr": "Proposes TC-AACG model with compliant ankle pushoff using SEAs, extending biped locomotion capabilities beyond impulsive energy schemes.", "motivation": "Passive dynamic walkers need tilted surfaces for stable locomotion; existing actuation techniques use impulsive energy injection and torsional springs that are hard to implement physically.", "method": "Introduces TC-AACG model with non-instantaneous compliant ankle pushoff that can be implemented using series elastic actuators (SEAs).", "result": "The approach extends locomotion capabilities compared to impulsive ankle pushoff, with analysis showing improvements in speed, mechanical cost of transport, and basin of attraction.", "conclusion": "TC-AACG provides a physically implementable solution for biped locomotion on level ground and rough terrain with improved efficiency."}}
{"id": "2602.07025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07025", "abs": "https://arxiv.org/abs/2602.07025", "authors": ["Daniele Savietto", "Declan Campbell", "Andr\u00e9 Panisson", "Marco Nurisso", "Giovanni Petri", "Jonathan D. Cohen", "Alan Perotti"], "title": "The Geometry of Representational Failures in Vision Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the \"Binding Problem\", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill \"concept vectors\" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.", "AI": {"tldr": "VLMs fail at multi-object tasks due to representational geometry issues; concept vectors reveal how internal representations cause errors.", "motivation": "Vision-Language Models show puzzling failures in multi-object visual tasks (hallucinations, misidentifications) that resemble human cognitive constraints like the \"Binding Problem,\" but the internal mechanisms behind these errors in AI systems remain poorly understood.", "method": "Analyze representational geometry of open-weight VLMs (Qwen, InternVL, Gemma) using methodologies to distill \"concept vectors\" - latent directions encoding visual concepts. Validate concept vectors via steering interventions that manipulate model behavior in simplified and naturalistic vision tasks.", "result": "Geometric overlap between concept vectors strongly correlates with specific error patterns, providing a quantitative framework to understand how internal representations shape model behavior and drive visual failures.", "conclusion": "The representational geometry of VLMs, analyzed through concept vectors, offers mechanistic insights into why these models fail at multi-object visual tasks, bridging the gap between human cognitive constraints and artificial system failures."}}
{"id": "2602.07267", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07267", "abs": "https://arxiv.org/abs/2602.07267", "authors": ["Fengyuan Liu", "Jay Gala", "Nilaksh", "Dzmitry Bahdanau", "Siva Reddy", "Hugo Larochelle"], "title": "BRIDGE: Predicting Human Task Completion Time From Model Performance", "comment": null, "summary": "Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.", "AI": {"tldr": "BRIDGE is a psychometric framework that learns latent task difficulty from AI model responses and aligns it with human task completion time, enabling scalable evaluation of AI capabilities without direct human annotations.", "motivation": "Existing methods for evaluating AI systems rely on costly, noisy, and difficult-to-scale human task completion time annotations. There's a need for a more scalable approach to ground benchmark performance in human-interpretable measures of task difficulty.", "method": "Proposes BRIDGE framework using two-parameter logistic Item Response Theory model to jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. The method discovers that latent task difficulty varies linearly with log of human completion time.", "result": "The framework allows human task completion time to be inferred for new benchmarks from model performance alone. It forecasts frontier model capabilities in human task length terms and independently reproduces METR's exponential scaling results, showing the 50% solvable task horizon doubles approximately every 6 months.", "conclusion": "BRIDGE provides a scalable, unified psychometric approach to evaluate AI capabilities by learning latent difficulty scales from model responses and anchoring them to human-interpretable measures, enabling more efficient and interpretable AI system evaluation."}}
{"id": "2602.07209", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07209", "abs": "https://arxiv.org/abs/2602.07209", "authors": ["Spencer Teetaert", "Giammarco Caroleo", "Marco Pontin", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot", "Perla Maiolino"], "title": "Continuum Robot Localization using Distributed Time-of-Flight Sensors", "comment": null, "summary": "Localization and mapping of an environment are crucial tasks for any robot operating in unstructured environments. Time-of-flight (ToF) sensors (e.g.,~lidar) have proven useful in mobile robotics, where high-resolution sensors can be used for simultaneous localization and mapping. In soft and continuum robotics, however, these high-resolution sensors are too large for practical use. This, combined with the deformable nature of such robots, has resulted in continuum robot (CR) localization and mapping in unstructured environments being a largely untouched area. In this work, we present a localization technique for CRs that relies on small, low-resolution ToF sensors distributed along the length of the robot. By fusing measurement information with a robot shape prior, we show that accurate localization is possible despite each sensor experiencing frequent degenerate scenarios. We achieve an average localization error of 2.5cm in position and 7.2\u00b0 in rotation across all experimental conditions with a 53cm long robot. We demonstrate that the results are repeated across multiple environments, in both simulation and real-world experiments, and study robustness in the estimation to deviations in the prior map.", "AI": {"tldr": "Continuum robots can achieve accurate localization in unstructured environments using distributed low-resolution ToF sensors combined with shape priors, achieving 2.5cm position and 7.2\u00b0 rotation errors.", "motivation": "Continuum robots face localization challenges in unstructured environments because high-resolution sensors (like lidar) are too large for practical use, and their deformable nature makes traditional mapping approaches difficult, creating an untouched research area.", "method": "Distributed small, low-resolution Time-of-Flight sensors along the robot length, fused with a robot shape prior to overcome frequent degenerate scenarios in individual sensor measurements.", "result": "Achieved average localization error of 2.5cm in position and 7.2\u00b0 in rotation with a 53cm long robot, demonstrated across multiple environments in both simulation and real-world experiments, with robustness to prior map deviations.", "conclusion": "Accurate continuum robot localization is possible in unstructured environments using distributed low-resolution sensors combined with shape priors, addressing a previously unexplored challenge in soft robotics."}}
{"id": "2602.07026", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07026", "abs": "https://arxiv.org/abs/2602.07026", "authors": ["Xiaomin Yu", "Yi Xin", "Wenjie Zhang", "Chonghan Liu", "Hanzhen Zhao", "Xiaoxing Hu", "Xinlei Yu", "Ziyue Qiao", "Hao Tang", "Xue Yang", "Xiaobin Hu", "Chengwei Qin", "Hui Xiong", "Yu Qiao", "Shuicheng Yan"], "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "comment": null, "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "AI": {"tldr": "ReAlign and ReVision: A framework that precisely models the modality gap and enables scalable MLLM training using unpaired data instead of expensive image-text pairs.", "motivation": "Multimodal contrastive learning suffers from a persistent \"Modality Gap\" where embeddings of different modalities expressing identical semantics occupy systematically offset regions. Prior approaches are limited by oversimplified isotropic assumptions and can't scale effectively.", "method": "1) Fixed-frame Modality Gap Theory decomposes the gap into stable biases and anisotropic residuals. 2) ReAlign: training-free modality alignment using massive unpaired data via Anchor, Trace, and Centroid Alignment. 3) ReVision: scalable MLLM training paradigm integrating ReAlign into pretraining to learn visual representations from unpaired text before visual instruction tuning.", "result": "The framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for efficient scaling of Multimodal Large Language Models.", "conclusion": "By precisely characterizing the modality gap geometry and leveraging unpaired data statistics, the proposed approach enables scalable MLLM training without requiring large-scale, high-quality image-text pairs, addressing both the modality gap problem and scalability limitations."}}
{"id": "2602.07274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07274", "abs": "https://arxiv.org/abs/2602.07274", "authors": ["Kaijie Zhu", "Yuzhou Nie", "Yijiang Li", "Yiming Huang", "Jialian Wu", "Jiang Liu", "Ximeng Sun", "Zhenfei Yin", "Lun Wang", "Zicheng Liu", "Emad Barsoum", "William Yang Wang", "Wenbo Guo"], "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents", "comment": null, "summary": "Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.", "AI": {"tldr": "TermiGen is a pipeline that synthesizes verifiable terminal environments and resilient expert trajectories to improve LLMs' ability to execute complex terminal tasks, achieving state-of-the-art performance on TerminalBench.", "motivation": "Open-weight LLMs struggle with complex terminal tasks due to two limitations: 1) lack of diverse, scalable, high-fidelity training environments (real-world repositories aren't diverse enough, LLM-synthesized ones have hallucinations), and 2) standard instruction tuning uses perfect expert trajectories that don't prepare models to recover from common runtime errors.", "method": "TermiGen uses an end-to-end pipeline with two main components: 1) An iterative multi-agent refinement loop to generate functionally valid tasks and Docker containers, and 2) A Generator-Critic protocol that actively injects errors during trajectory collection to create data rich in error-correction cycles.", "result": "Fine-tuned on TermiGen-generated data, TermiGen-Qwen2.5-Coder-32B achieves 31.3% pass rate on TerminalBench, establishing a new open-weights state-of-the-art and outperforming proprietary models like o4-mini.", "conclusion": "TermiGen successfully addresses the limitations in terminal task execution by synthesizing verifiable environments and resilient trajectories, enabling LLMs to better handle complex terminal tasks and recover from runtime errors."}}
{"id": "2602.07243", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07243", "abs": "https://arxiv.org/abs/2602.07243", "authors": ["Siddharth Singh", "Ifrah Idrees", "Abraham Dauhajre"], "title": "Realistic Synthetic Household Data Generation at Scale", "comment": "Accepted at Agentic AI Benchmarks and Applications for Enterprise Tasks workshop at AAAI 2026", "summary": "Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.\n  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.\n  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.", "AI": {"tldr": "A generative framework creates large-scale household datasets with bidirectional human-environment influence for embodied AI agents, validated through statistical metrics showing good alignment with real data.", "motivation": "Existing frameworks for embodied AI agent development lack datasets that capture the bidirectional influence between human behavior and household environments, limiting realistic long-term human-robot interaction modeling.", "method": "Proposes a generative framework with loosely coupled generation of long-term human-robot interactions and environments, where human personas influence environment generation and environment semantics shape interactions. Includes a flexible tool allowing natural language prompts for dataset configuration.", "result": "Statistical validation shows good alignment with real-world datasets (HOMER, cosine similarity 0.60) vs moderate alignment with synthetic datasets (0.27). Intervention analysis reveals statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12) for age, organization, and sleep pattern changes.", "conclusion": "The framework successfully models bidirectional human-environment coupling, enabling scalable generation of realistic household datasets for developing and testing smart devices and embodied AI agents."}}
{"id": "2602.07027", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07027", "abs": "https://arxiv.org/abs/2602.07027", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "SungHeon Jeong", "Wenjun Huang", "Hanning Chen", "Mohsen Imani"], "title": "Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.", "AI": {"tldr": "FCL is a test-time adaptation framework for VLMs that avoids entropy minimization by using fairness-driven calibration to address shared-evidence bias, achieving competitive performance across domain shifts.", "motivation": "Current prompt-based TTA methods for VLMs rely on entropy minimization, which can amplify spurious correlations and cause overconfident errors when classes share visual features. There's a need for TTA approaches that avoid these issues while maintaining robustness under distribution shifts.", "method": "Fair Context Learning (FCL) uses episodic TTA with two components: (1) augmentation-based exploration to identify plausible class candidates, and (2) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence, avoiding entropy minimization entirely.", "result": "FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks, while empirically validating the theoretical motivation behind the approach.", "conclusion": "By explicitly addressing shared-evidence bias through fairness constraints rather than entropy minimization, FCL provides an effective TTA framework for VLMs that mitigates partial feature obsession and enables robust calibration of text embeddings under distribution shifts."}}
{"id": "2602.07276", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07276", "abs": "https://arxiv.org/abs/2602.07276", "authors": ["Pengrui Han", "Xueqiang Xu", "Keyang Xuan", "Peiyang Song", "Siru Ouyang", "Runchu Tian", "Yuqing Jiang", "Cheng Qian", "Pengcheng Jiang", "Jiashuo Sun", "Junxia Cui", "Ming Zhong", "Ge Liu", "Jiawei Han", "Jiaxuan You"], "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs", "comment": null, "summary": "Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.", "AI": {"tldr": "STEER2ADAPT: A lightweight framework that adapts LLMs by composing reusable steering vectors from a semantic prior subspace, enabling efficient adaptation to new tasks with few examples.", "motivation": "Existing activation steering methods use single static directions per task, making them inflexible under task variation and inadequate for complex tasks requiring multiple coordinated capabilities.", "method": "Captures shared concept dimensions as a reusable low-dimensional semantic prior subspace, then adapts to new tasks by dynamically discovering linear combinations of basis vectors from few examples.", "result": "Achieves average improvement of 8.2% across 9 tasks and 3 models in reasoning and safety domains, showing data-efficiency, stability, and transparency.", "conclusion": "STEER2ADAPT provides an effective inference-time adaptation method that composes rather than learns steering vectors, enabling flexible and efficient LLM adaptation to complex tasks."}}
{"id": "2602.07264", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07264", "abs": "https://arxiv.org/abs/2602.07264", "authors": ["Jacopo Panerati", "Sina Sajjadi", "Sina Soleymanpour", "Varunkumar Mehta", "Iraj Mantegh"], "title": "aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones", "comment": null, "summary": "Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term \"simulation-to-reality gap\". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.", "AI": {"tldr": "Open-source aerial autonomy framework (aerial-autonomy-stack) bridges simulation-to-reality gap for UAVs, enabling 20x faster-than-real-time end-to-end simulation with ROS2 and popular autopilot support.", "motivation": "Need to accelerate development of autonomous aerial systems by overcoming physical AI hurdles like simulation-to-reality gap and complex hardware/software integration challenges in field robotics.", "method": "Developed aerial-autonomy-stack - an open-source, end-to-end framework that streamlines GPU-accelerated perception to flight controller action pipeline using ROS2 with common interface for PX4 and ArduPilot autopilots.", "result": "Framework supports over 20x faster-than-real-time end-to-end simulation of complete development/deployment stack including edge compute and networking, significantly compressing build-test-release cycles.", "conclusion": "The aerial-autonomy-stack addresses key robotics transformation challenges by providing practical tools to accelerate perception-based autonomy development for UAVs through improved simulation-to-reality workflows."}}
{"id": "2602.07028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07028", "abs": "https://arxiv.org/abs/2602.07028", "authors": ["Kaaustaaub Shankar", "Bharadwaj Dogga", "Kelly Cohen"], "title": "A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures", "comment": "Accepted to NAFIPS 2026", "summary": "Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.", "AI": {"tldr": "ANFIS-augmented CNNs show architecture-dependent robustness effects: ResNet18-ANFIS improves adversarial robustness while VGG-ANFIS often underperforms, with no consistent clean accuracy benefits.", "motivation": "Standard CNNs lack interpretability and are vulnerable to adversarial attacks. While neuro-fuzzy hybrids like DCNFIS improve interpretability by replacing fully connected layers with ANFIS, their robustness properties remain underexplored.", "method": "Compared standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets under both gradient-based (PGD) and gradient-free (Square) adversarial attacks.", "result": "ANFIS integration does not consistently improve clean accuracy. Robustness effects are architecture-dependent: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline counterpart.", "conclusion": "Neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial, suggesting careful architecture selection is needed when considering ANFIS integration for adversarial robustness."}}
{"id": "2602.07308", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07308", "abs": "https://arxiv.org/abs/2602.07308", "authors": ["Sutapa Dey Tithi", "Nazia Alam", "Tahreem Yasir", "Yang Shi", "Xiaoyi Tian", "Min Chi", "Tiffany Barnes"], "title": "Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System", "comment": null, "summary": "The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.", "AI": {"tldr": "Adaptive selection of worked examples (Guided vs Buggy) using BKT and DRL improves learning outcomes by personalizing cognitive engagement levels in intelligent tutoring systems.", "motivation": "Personalizing learning activities to elicit optimal cognitive engagement (ICAP levels) remains challenging in ITS, despite evidence that increased engagement improves learning.", "method": "Developed system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two ICAP modes: Guided (active) and Buggy (constructive) examples. Compared BKT and DRL adaptive methods against non-adaptive baseline in logic ITS with 113 students.", "result": "Both adaptive policies significantly improved student performance on test problems. BKT helped low prior knowledge students catch up with high knowledge peers, while DRL yielded higher posttest scores among high prior knowledge students.", "conclusion": "Provides new insights into complex interactions between cognitive engagement and adaptivity, showing different adaptive methods benefit different student groups based on prior knowledge."}}
{"id": "2602.07322", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07322", "abs": "https://arxiv.org/abs/2602.07322", "authors": ["Jindou Jia", "Gen Li", "Xiangyu Chen", "Tuo An", "Yuxuan Hu", "Jingliang Li", "Xinying Guo", "Jianfei Yang"], "title": "Action-to-Action Flow Matching", "comment": "18 pages, 18 figures", "summary": "Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.", "AI": {"tldr": "A2A is a novel policy paradigm that replaces random noise sampling with previous-action-informed initialization for faster diffusion-based action generation, achieving single-step inference with 0.56ms latency.", "motivation": "Standard diffusion policies require multiple iterative denoising steps from random Gaussian noise, causing high inference latency that bottlenecks real-time robot control.", "method": "Action-to-Action flow matching (A2A) shifts from random sampling to initialization informed by previous actions, embedding historical proprioceptive sequences into latent space as starting points for action generation.", "result": "A2A achieves high-quality action generation in as few as one inference step (0.56ms latency), with superior training efficiency, robustness to visual perturbations, and generalization to unseen configurations.", "conclusion": "A2A bypasses costly iterative denoising while capturing robot dynamics, demonstrating fast inference, improved generalization, and broader versatility in temporal modeling including video generation."}}
{"id": "2602.07038", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07038", "abs": "https://arxiv.org/abs/2602.07038", "authors": ["Yifan Ji", "Zhipeng Xu", "Zhenghao Liu", "Zulong Chen", "Qian Zhang", "Zhibo Yang", "Junyang Lin", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents", "comment": null, "summary": "Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.", "AI": {"tldr": "UNIKIE-BENCH is a unified benchmark for evaluating Large Multimodal Models' Key Information Extraction capabilities across diverse document types and scenarios, revealing significant performance gaps.", "motivation": "Key Information Extraction from real-world documents is challenging due to layout variations, visual quality issues, and task-specific requirements. Existing LMMs show promise but lack comprehensive evaluation across realistic scenarios.", "method": "Created UNIKIE-BENCH with two tracks: constrained-category KIE with scenario-predefined schemas, and open-category KIE extracting any explicitly present key information. Evaluated 15 state-of-the-art LMMs.", "result": "Experiments revealed substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts. Pronounced performance disparities across different document types and scenarios were observed.", "conclusion": "LMM-based KIE faces persistent challenges in grounding accuracy and layout-aware reasoning. The benchmark provides comprehensive evaluation tools for advancing document understanding research."}}
{"id": "2602.07339", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07339", "abs": "https://arxiv.org/abs/2602.07339", "authors": ["Ruturaj Reddy", "Hrishav Bakul Barua", "Junn Yong Loo", "Thanh Thi Nguyen", "Ganesh Krishnasamy"], "title": "RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving", "comment": null, "summary": "Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.", "AI": {"tldr": "RAPiD distills diffusion-based trajectory planners into deterministic policies using score-regularized optimization, achieving 8x speedup while maintaining competitive performance and SOTA generalization.", "motivation": "Diffusion-based planners model multimodal driving behavior well but suffer from slow iterative sampling that prevents real-time, safety-critical deployment.", "method": "Uses score-regularized policy optimization to distill pretrained diffusion planners into efficient deterministic policies, leveraging diffusion score functions as behavior priors and training critics to imitate predictive driver controllers for safety supervision.", "result": "Achieves competitive performance on nuPlan scenarios with 8x speedup over diffusion baselines and state-of-the-art generalization on interPlan benchmark.", "conclusion": "RAPiD successfully bridges the gap between expressive diffusion models and real-time deployment needs through effective distillation, enabling safe, efficient autonomous driving planning."}}
{"id": "2602.07326", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.07326", "abs": "https://arxiv.org/abs/2602.07326", "authors": ["Edgar Lee", "Junho Choi", "Taemin Kim", "Changjoo Nam", "Seokhwan Jeong"], "title": "Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing", "comment": "Submitted to Journal (under review)", "summary": "Grasping under limited sensing remains a fundamental challenge for real-world robotic manipulation, as vision and high-resolution tactile sensors often introduce cost, fragility, and integration complexity. This work demonstrates that reliable multifingered grasping can be achieved under extremely minimal sensing by relying solely on uniaxial fingertip force feedback and joint proprioception, without vision or multi-axis/tactile sensing. To enable such blind grasping, we employ an efficient teacher-student training pipeline in which a reinforcement-learned teacher exploits privileged simulation-only observations to generate demonstrations for distilling a transformer-based student policy operating under partial observation. The student policy is trained to act using only sensing modalities available at real-world deployment. We validate the proposed approach on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, achieving a 98.3~$\\%$ overall grasp success rate. These results demonstrate strong robustness and generalization beyond the simulation training distribution, while significantly reducing sensing requirements for real-world grasping systems.", "AI": {"tldr": "Blind multifingered grasping using only uniaxial fingertip force feedback and joint proprioception, achieving 98.3% success rate across 18 objects without vision or tactile sensing.", "motivation": "Addressing the challenge of grasping under limited sensing by reducing reliance on expensive, fragile vision and high-resolution tactile sensors that introduce cost and integration complexity in real-world robotic manipulation.", "method": "Teacher-student training pipeline where a reinforcement-learned teacher uses privileged simulation observations to generate demonstrations, then distills a transformer-based student policy that operates only on partial observations (uniaxial fingertip force feedback and joint proprioception) available in real deployment.", "result": "Achieved 98.3% overall grasp success rate on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, demonstrating strong robustness and generalization beyond simulation training.", "conclusion": "Reliable multifingered grasping can be achieved with extremely minimal sensing, significantly reducing sensing requirements for real-world grasping systems while maintaining high performance and generalization capability."}}
{"id": "2602.07041", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07041", "abs": "https://arxiv.org/abs/2602.07041", "authors": ["Leeje Jang", "Yao-Yi Chiang", "Angela M. Hastings", "Patimaporn Pungchanchaikul", "Martha B. Lucas", "Emily C. Schultz", "Jeffrey P. Louie", "Mohamed Estai", "Wen-Chen Wang", "Ryan H. L. Ip", "Boyen Huang"], "title": "OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis", "comment": null, "summary": "Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.", "AI": {"tldr": "OMNI-Dent is a data-efficient, explainable dental diagnostic framework using Vision-Language Models with clinical reasoning principles for tooth-level evaluation from smartphone photos.", "motivation": "Addresses limitations in dental AI: existing methods treat diagnosis as simple pattern recognition without clinical reasoning, require large annotated datasets, and struggle with real-world imaging conditions. Many people lack access to timely professional dental evaluation.", "method": "Uses Vision-Language Model (VLM)-based pipeline that incorporates clinical reasoning principles from dental experts. Operates on multi-view smartphone photographs, embeds diagnostic heuristics, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning.", "result": "Framework supports diagnostic assessment where curated clinical imaging is unavailable, helps identify potential abnormalities, and determines when professional evaluation may be needed. Designed as an early-stage assistive tool for limited-access settings.", "conclusion": "OMNI-Dent offers a practical, data-efficient, explainable solution for dental diagnosis that incorporates clinical reasoning, works with smartphone photos, and provides accessible care options for underserved populations."}}
{"id": "2602.07342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07342", "abs": "https://arxiv.org/abs/2602.07342", "authors": ["Shengyue Guan", "Yihao Liu", "Lang Cao"], "title": "SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management", "comment": null, "summary": "Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.", "AI": {"tldr": "SupChain-Bench benchmark evaluates LLMs on supply chain management tasks, revealing reliability gaps; SupChain-ReAct framework improves tool-calling performance by autonomously synthesizing procedures.", "motivation": "LLMs show promise for complex reasoning and tool-based decision making in supply chain management, but real-world workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models.", "method": "Introduces SupChain-Bench, a unified real-world benchmark for evaluating LLM performance on supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Also proposes SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use.", "result": "Experiments reveal substantial gaps in execution reliability across models. SupChain-ReAct achieves the strongest and most consistent tool-calling performance among evaluated approaches.", "conclusion": "The work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents."}}
{"id": "2602.07363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07363", "abs": "https://arxiv.org/abs/2602.07363", "authors": ["Zihao Xu", "Runyu Lei", "Zihao Li", "Boxi Lin", "Ce Hao", "Jin Song Dong"], "title": "UEREBot: Learning Safe Quadrupedal Locomotion under Unstructured Environments and High-Speed Dynamic Obstacles", "comment": null, "summary": "Quadruped robots are increasingly deployed in unstructured environments. Safe locomotion in these settings requires long-horizon goal progress, passability over uneven terrain and static constraints, and collision avoidance against high-speed dynamic obstacles. A single system cannot fully satisfy all three objectives simultaneously: planning-based decisions can be too slow, while purely reactive decisions can sacrifice goal progress and passability. To resolve this conflict, we propose UEREBot (Unstructured-Environment Reflexive Evasion Robot), a hierarchical framework that separates slow planning from instantaneous reflexive evasion and coordinates them during execution. UEREBot formulates the task as a constrained optimal control problem blueprint. It adopts a spatial--temporal planner that provides reference guidance toward the goal and threat signals. It then uses a threat-aware handoff to fuse navigation and reflex actions into a nominal command, and a control barrier function shield as a final execution safeguard. We evaluate UEREBot in Isaac Lab simulation and deploy it on a Unitree Go2 quadruped equipped with onboard perception. Across diverse environments with complex static structure and high-speed dynamic obstacles, UEREBot achieves higher avoidance success and more stable locomotion while maintaining goal progress than representative baselines, demonstrating improved safety--progress trade-offs.", "AI": {"tldr": "Hierarchical framework UEREBot separates slow planning from fast reflexive evasion for quadruped robots in unstructured environments, achieving better safety-progress trade-offs than baselines.", "motivation": "Quadruped robots need to handle long-horizon goal progress, terrain passability, and collision avoidance with dynamic obstacles simultaneously, but existing approaches either plan too slowly or sacrifice goal progress with purely reactive decisions.", "method": "UEREBot uses hierarchical framework with spatial-temporal planner for goal/threat guidance, threat-aware handoff to fuse navigation and reflex actions, and control barrier function shield as final execution safeguard.", "result": "In Isaac Lab simulation and Unitree Go2 deployment, UEREBot achieves higher avoidance success, more stable locomotion while maintaining goal progress than baselines across diverse environments with complex static structure and high-speed dynamic obstacles.", "conclusion": "UEREBot demonstrates improved safety-progress trade-offs by effectively coordinating planning and reflexive evasion for quadruped navigation in unstructured environments."}}
{"id": "2602.07042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07042", "abs": "https://arxiv.org/abs/2602.07042", "authors": ["Magesh Rajasekaran", "Md Saiful Islam Sajol", "Frej Berglind", "Supratik Mukhopadhyay", "Kamalika Das"], "title": "COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification", "comment": "Copyright by SIAM. Unauthorized reproduction of this article is prohibited First Published in Proceedings of the 2024 SIAM International Conference on Data Mining (SDM24), published by the Society for Industrial and Applied Mathematics (SIAM)", "summary": "Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.", "AI": {"tldr": "COMBOOD is a novel unsupervised semi-parametric framework that combines nearest-neighbor and Mahalanobis distance metrics for accurate out-of-distribution detection in both near and far OOD scenarios.", "motivation": "Current OOD detection methods have limitations: nearest-neighbor approaches work well non-parametrically but Mahalanobis distance, while effective for far OOD scenarios, performs poorly in near OOD situations that are common in practical applications.", "method": "COMBOOD combines signals from two distance metrics: 1) nearest-neighbor (non-parametric) and 2) Mahalanobis distance (parametric). The framework operates in a semi-parametric setting to derive a confidence score for whether an inference point is OOD.", "result": "COMBOOD outperforms state-of-the-art OOD detection methods on OpenOOD benchmarks (versions 1 and 1.5) for both far-OOD and near-OOD scenarios, as well as on document datasets. Improvements are statistically significant on most benchmark datasets, with linear scaling in embedding space size.", "conclusion": "The COMBOOD framework provides an effective, scalable solution for OOD detection that works well in both near and far OOD scenarios, making it suitable for real-world applications where accurate OOD identification is crucial for automation and safety."}}
{"id": "2602.07359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07359", "abs": "https://arxiv.org/abs/2602.07359", "authors": ["Xiaoqiang Lin", "Jun Hao Liew", "Silvio Savarese", "Junnan Li"], "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents", "comment": null, "summary": "Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.", "AI": {"tldr": "Wide and Deep research agent framework scales both depth (sequential reasoning) and width (parallel tool calling) to improve performance on deep research tasks, achieving better accuracy with fewer turns.", "motivation": "While current deep research agents focus on scaling depth through sequential thinking, the potential of scaling width via parallel tool calling remains unexplored, presenting an opportunity to improve agent efficiency and performance.", "method": "Proposes Wide and Deep research agent framework that leverages intrinsic parallel tool calling within a single reasoning step, avoiding complex multi-agent orchestration. Includes analysis of various tool call schedulers to optimize parallel strategy.", "result": "Scaling width significantly improves performance on deep research benchmarks while reducing required turns. Achieves 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing original 54.9% reported by GPT-5-High.", "conclusion": "Optimizing the trade-off between width and depth is critical for high-efficiency deep research agents, with parallel tool calling offering substantial performance gains without complex context management techniques."}}
{"id": "2602.07388", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07388", "abs": "https://arxiv.org/abs/2602.07388", "authors": ["Yuxuan Hu", "Xiangyu Chen", "Chuhao Zhou", "Yuxi Liu", "Gen Li", "Jindou Jia", "Jianfei Yang"], "title": "Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation", "comment": null, "summary": "Generative model-based policies have shown strong performance in imitation-based robotic manipulation by learning action distributions from demonstrations. However, in long-horizon tasks, visually similar observations often recur across execution stages while requiring distinct actions, which leads to ambiguous predictions when policies are conditioned only on instantaneous observations, termed multi-modal action ambiguity (MA2). To address this challenge, we propose the Trace-Focused Diffusion Policy (TF-DP), a simple yet effective diffusion-based framework that explicitly conditions action generation on the robot's execution history. TF-DP represents historical motion as an explicit execution trace and projects it into the visual observation space, providing stage-aware context when current observations alone are insufficient. In addition, the induced trace-focused field emphasizes task-relevant regions associated with historical motion, improving robustness to background visual disturbances. We evaluate TF-DP on real-world robotic manipulation tasks exhibiting pronounced multi-modal action ambiguity and visually cluttered conditions. Experimental results show that TF-DP improves temporal consistency and robustness, outperforming the vanilla diffusion policy by 80.56 percent on tasks with multi-modal action ambiguity and by 86.11 percent under visual disturbances, while maintaining inference efficiency with only a 6.4 percent runtime increase. These results demonstrate that execution-trace conditioning offers a scalable and principled approach for robust long-horizon robotic manipulation within a single policy.", "AI": {"tldr": "TF-DP improves robotic manipulation by conditioning diffusion policies on execution history to resolve multi-modal action ambiguity in long-horizon tasks.", "motivation": "In long-horizon robotic tasks, visually similar observations at different stages require distinct actions, causing multi-modal action ambiguity when policies rely only on instantaneous observations.", "method": "Trace-Focused Diffusion Policy (TF-DP) conditions action generation on robot's execution history by representing historical motion as explicit execution trace projected into visual observation space, creating trace-focused field for task-relevant regions.", "result": "TF-DP outperforms vanilla diffusion policy by 80.56% on tasks with multi-modal action ambiguity and 86.11% under visual disturbances, with only 6.4% runtime increase.", "conclusion": "Execution-trace conditioning provides scalable and principled approach for robust long-horizon robotic manipulation within a single policy, improving temporal consistency and robustness."}}
{"id": "2602.07044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07044", "abs": "https://arxiv.org/abs/2602.07044", "authors": ["Tianyi Qu", "Songxiao Yang", "Haolin Wang", "Huadong Song", "Xiaoting Guo", "Wenguang Hu", "Guanlin Liu", "Honghe Chen", "Yafei Ou"], "title": "PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging", "comment": "A dataset contains 240,320 pipeline MFL pseudo-color images and 191,530 bounding-box annotations, collected from 11 pipelines spanning approximately 1,480 km", "summary": "Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \\textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \\textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \\textbf{240,320} images and \\textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \\textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.", "AI": {"tldr": "PipeMFL-240K: First large-scale public dataset (240K images, 191K annotations) and benchmark for pipeline defect detection using Magnetic Flux Leakage (MFL) technology, addressing key challenges in automated pipeline inspection.", "motivation": "Deep learning shows promise for automating MFL interpretation in pipeline inspection, but progress has been limited by the lack of large-scale public datasets and benchmarks, making fair comparisons and reproducible evaluation difficult.", "method": "Created PipeMFL-240K dataset with 240,320 images and 191,530 high-quality bounding-box annotations from 11 pipelines spanning ~1,480 km, featuring 12 defect categories with unique challenges including extreme long-tailed distribution, tiny objects, and substantial intra-class variability.", "result": "Extensive experiments with state-of-the-art object detectors show they still struggle with MFL data's intrinsic properties, revealing considerable room for improvement, while PipeMFL-240K provides a reliable and challenging testbed for future research.", "conclusion": "PipeMFL-240K is the first public dataset and benchmark of this scale for pipeline MFL inspection, providing a critical foundation for efficient pipeline diagnostics, maintenance planning, and accelerating algorithmic innovation in pipeline integrity assessment."}}
{"id": "2602.07391", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07391", "abs": "https://arxiv.org/abs/2602.07391", "authors": ["Kunal Pai", "Parth Shah", "Harshil Patel"], "title": "NAAMSE: Framework for Evolutionary Security Evaluation of Agents", "comment": null, "summary": "AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring \"benign-use correctness\", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.", "AI": {"tldr": "NAAMSE is an evolutionary framework for AI agent security evaluation that uses genetic prompt mutation and feedback-driven optimization to uncover vulnerabilities missed by traditional methods.", "motivation": "Current AI agent security evaluations are bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries, creating a need for more realistic and scalable assessment methods.", "method": "NAAMSE employs a single autonomous agent that orchestrates genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring, using model responses as fitness signals to iteratively compound effective attack strategies while maintaining benign-use correctness.", "result": "Experiments on Gemini 2.5 Flash show that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes.", "conclusion": "NAAMSE provides a more realistic and scalable assessment of agent robustness against evolving threats, offering an open-source evolutionary framework for adaptive security evaluation."}}
{"id": "2602.07413", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07413", "abs": "https://arxiv.org/abs/2602.07413", "authors": ["Yunhai Han", "Linhao Bai", "Ziyu Xiao", "Zhaodong Yang", "Yogita Choudhary", "Krishna Jha", "Chuizheng Kong", "Shreyas Kousik", "Harish Ravichandar"], "title": "Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity", "comment": null, "summary": "There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.", "AI": {"tldr": "UBMs learn dexterous manipulation as coupled visual-action flow dynamics using Koopman operators, enabling implicit planning with online replanning for reactivity.", "motivation": "Current diffusion/transformer policies for dexterous manipulation are data/compute heavy, unreliable, and create rigid trade-offs between temporal coherence and reactivity by modeling skills as reactive mappings with fixed-horizon action chunking.", "method": "Introduces Unified Behavioral Models (UBMs) framework representing skills as coupled visual-action flow dynamical systems. Proposes Koopman-UBM instantiation using Koopman Operator theory to learn linear dynamics in latent space, enabling implicit planning and online replanning when predicted/observed visual flow diverge.", "result": "Koopman-UBM matches/exceeds SOTA baselines across 7 simulated and 2 real-world tasks, with faster inference, smooth execution, robustness to occlusions, and flexible replanning.", "conclusion": "UBMs provide a principled approach to dexterous manipulation by modeling behavioral dynamics rather than reactive mappings, ensuring temporal coherence by construction and enabling effective reactivity through online replanning."}}
{"id": "2602.07045", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07045", "abs": "https://arxiv.org/abs/2602.07045", "authors": ["Zhiming Luo", "Di Wang", "Haonan Guo", "Jing Zhang", "Bo Du"], "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.", "AI": {"tldr": "VLRS-Bench is the first remote sensing benchmark focused on complex reasoning (cognition, decision, prediction) rather than just perception tasks, revealing limitations in current MLLMs.", "motivation": "Existing remote sensing benchmarks are biased toward perception tasks like object recognition, which hinders development of MLLMs for cognitively demanding RS applications requiring complex reasoning.", "method": "Created VLRS-Bench with 2,000 QA pairs across 14 tasks and up to 8 temporal phases, using a specialized pipeline that integrates RS-specific priors and expert knowledge for geospatial realism.", "result": "Experimental results show significant bottlenecks in existing state-of-the-art MLLMs, highlighting their limitations in complex RS reasoning tasks.", "conclusion": "VLRS-Bench provides critical insights for advancing multimodal reasoning in remote sensing and addresses the gap in benchmarks for complex cognitive tasks."}}
{"id": "2602.07399", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07399", "abs": "https://arxiv.org/abs/2602.07399", "authors": ["Changhua Xu", "Jie Lu", "Junyu Xuan", "En Yu"], "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation", "comment": "Preprint", "summary": "Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \\emph{generation--selection} perspective and propose a novel framework \\textbf{VGAS} (\\textbf{V}alue-\\textbf{G}uided \\textbf{A}ction-chunk \\textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \\textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \\textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \\textit{Explicit Geometric Regularization} (\\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \\textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.", "AI": {"tldr": "VGAS framework improves few-shot VLA adaptation by using value-guided action-chunk selection to resolve geometric ambiguities that cause near-miss failures.", "motivation": "Fine-tuned VLA models often fail in few-shot adaptation due to unresolved geometric ambiguities where semantically plausible trajectories lead to divergent execution outcomes under limited supervision.", "method": "Proposes VGAS framework with: 1) Inference-time best-of-N selection using finetuned VLA as proposal generator, 2) Q-Chunk-Former Transformer critic for geometric ambiguity resolution, and 3) Explicit Geometric Regularization (EGR) to shape discriminative value landscape while preserving action ranking resolution.", "result": "VGAS consistently improves success rates and robustness under limited demonstrations and distribution shifts, with theoretical analysis supporting the approach.", "conclusion": "The generation-selection perspective with value-guided action-chunk selection effectively addresses geometric ambiguity challenges in few-shot VLA adaptation, providing more reliable physical control."}}
{"id": "2602.07434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07434", "abs": "https://arxiv.org/abs/2602.07434", "authors": ["Songhua Yang", "Xuetao Li", "Xuanye Fei", "Mengde Li", "Miao Li"], "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots", "comment": null, "summary": "Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \\underline{\\textit{S}}peech, \\underline{\\textit{E}}motion, and \\underline{\\textit{M}}otion, we present \\textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \\underline{\\textit{e}}dge-deployed versions (\\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.", "AI": {"tldr": "SeM\u00b2 is a VLM-based framework that coordinates emotionally coherent multimodal interactions (speech, emotion, motion) for humanoid robots, with both cloud and efficient edge-deployed versions.", "motivation": "Most humanoid robots lack coordinated speech, facial expressions, and gestures, while real-world deployment requires on-device solutions that can operate autonomously without continuous cloud connectivity.", "method": "Three key components: multimodal perception module capturing user contextual cues, Chain-of-Thought reasoning for response planning, and Semantic-Sequence Aligning Mechanism (SSAM) for precise temporal coordination between verbal content and physical expressions.", "result": "Edge-deployed version (SeM\u00b2\u2091) maintains 95% of relative performance while operating efficiently on edge hardware. Comprehensive evaluations show significant outperformance over unimodal baselines in naturalness, emotional clarity, and modal coherence.", "conclusion": "The framework advances socially expressive humanoid robotics for diverse real-world environments by bridging speech, emotion, and motion with coordinated multimodal interactions."}}
{"id": "2602.07047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07047", "abs": "https://arxiv.org/abs/2602.07047", "authors": ["Muhammad Rashid", "Elvio G. Amparore", "Enrico Ferrari", "Damiano Verda"], "title": "ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees", "comment": "AAAI-2026", "summary": "Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.", "AI": {"tldr": "ShapBPT introduces a data-aware hierarchical Shapley method for computer vision that uses Binary Partition Trees to align feature attributions with image morphology, improving both computational efficiency and human interpretability.", "motivation": "Existing hierarchical Shapley approaches don't exploit image multiscale structure, leading to slow convergence and poor alignment with actual morphological features. There's a gap in using data-aware hierarchies for computer vision interpretability.", "method": "ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure called Binary Partition Tree (BPT), which is tailored for images. This data-aware hierarchical partitioning ensures feature attributions align with intrinsic image morphology.", "result": "Experimental results show ShapBPT's superior alignment with image structures and improved efficiency over existing XCV methods. A 20-subject user study confirms that ShapBPT explanations are preferred by humans.", "conclusion": "ShapBPT successfully connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability in computer vision."}}
{"id": "2602.07408", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07408", "abs": "https://arxiv.org/abs/2602.07408", "authors": ["Hyomin Kim", "Sang-Yeon Hwang", "Jaechang Lim", "Yinhua Piao", "Yunhak Oh", "Woo Youn Kim", "Chanyoung Park", "Sungsoo Ahn", "Junhyeok Jeon"], "title": "Progressive Multi-Agent Reasoning for Biological Perturbation Prediction", "comment": "17 pages, 4 figures, 9 tables", "summary": "Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.", "AI": {"tldr": "PBio-Agent: Multi-agent framework for predicting gene regulation responses to chemical perturbations, outperforming baselines on new benchmark LINCSQA", "motivation": "Existing LLMs struggle with high-dimensional perturbation data, and current research focuses mainly on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations (crucial for drug discovery) largely unexplored.", "method": "PBio-Agent: Multi-agent framework with difficulty-aware task sequencing and iterative knowledge refinement, leveraging shared causal structure among genes affected by same perturbation. Uses specialized agents with biological knowledge graphs, synthesis agent for integration, and judges for logical coherence.", "result": "Outperforms existing baselines on both LINCSQA (new benchmark for bulk-cell chemical perturbations) and PerturbQA benchmarks. Enables smaller models to predict and explain complex biological processes without additional training.", "conclusion": "PBio-Agent effectively addresses the challenge of predicting gene regulation responses to chemical perturbations in bulk-cell environments, advancing capabilities for drug discovery applications."}}
{"id": "2602.07439", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07439", "abs": "https://arxiv.org/abs/2602.07439", "authors": ["Weiji Xie", "Jiakun Zheng", "Jinrui Han", "Jiyuan Shi", "Weinan Zhang", "Chenjia Bai", "Xuelong Li"], "title": "TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control", "comment": "Project Page: https://text-op.github.io/", "summary": "Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/", "AI": {"tldr": "TextOp is a real-time text-driven humanoid motion generation and control framework that enables interactive motion control through streaming language commands with on-the-fly modification during execution.", "motivation": "Existing humanoid controllers either rely on predefined motion trajectories (limited flexibility) or continuous human teleoperation (requires constant human involvement, limits autonomy). There's a need for real-time, interactive control that bridges motion generation with robust whole-body control.", "method": "Two-level architecture: 1) High-level autoregressive motion diffusion model generates short-horizon kinematic trajectories conditioned on current text input, 2) Low-level motion tracking policy executes these trajectories on physical humanoid robot. Enables streaming language commands and on-the-fly instruction modification.", "result": "Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. Enables free-form intent expression and smooth transitions across multiple challenging behaviors (dancing, jumping) within single continuous motion execution.", "conclusion": "TextOp successfully bridges interactive motion generation with robust whole-body control, unlocking real-time text-driven humanoid motion control with flexibility and autonomy beyond existing approaches."}}
{"id": "2602.07049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07049", "abs": "https://arxiv.org/abs/2602.07049", "authors": ["Jindong Li", "Dario Zanca", "Vincent Christlein", "Tim Hamann", "Jens Barth", "Peter K\u00e4mpf", "Bj\u00f6rn Eskofier"], "title": "Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead", "comment": null, "summary": "Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.", "AI": {"tldr": "ECHWR is a training framework for edge-based online handwriting recognition that improves accuracy without increasing inference costs by using an auxiliary branch with dual contrastive learning (in-batch and error-based) during training only.", "motivation": "Enable handwriting recognition on paper for digital devices using inertial measurement units, while addressing memory constraints of edge hardware to improve privacy and reduce latency without compromising accuracy.", "method": "Proposes Error-enhanced Contrastive Handwriting Recognition (ECHWR) with a temporary auxiliary branch that aligns sensor signals with text embeddings using dual contrastive loss: in-batch contrastive loss for modality alignment and novel error-based contrastive loss that distinguishes correct signals from synthetic hard negatives.", "result": "Significantly outperforms state-of-the-art baselines on OnHW-Words500 dataset, reducing character error rates by up to 7.4% on writer-independent split and 10.4% on writer-dependent split. Error-based contrastive loss effectively handles unseen writing styles.", "conclusion": "ECHWR enables efficient edge deployment by improving recognition accuracy without increasing inference costs, with error-based contrastive loss proving particularly effective for handling diverse writing styles, though specific challenges require tailored architectural and objective configurations."}}
{"id": "2602.07414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07414", "abs": "https://arxiv.org/abs/2602.07414", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Spencer Lin", "James Hale", "Jonathan Gratch", "Maja Matari\u0107", "Gale M. Lucas"], "title": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution", "comment": "AAAI 2026 (Special Track: AISI)", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.", "AI": {"tldr": "LLMs prompted with personality traits don't reliably reproduce human personality-behavior patterns in conflict resolution, showing significant divergences from human data.", "motivation": "LLMs are increasingly used to simulate human behavior in social applications like mediation and dispute resolution, but it's unclear whether they can reproduce personality-driven differences in human conflict behavior observed in real humans.", "method": "Introduced an evaluation framework for direct comparison of human-human vs LLM-LLM behaviors in dispute resolution dialogues using Big Five Inventory personality traits. Developed a novel dataset creation methodology for LLM dialogues with matched scenarios and personality traits relative to human conversations.", "result": "Tested three contemporary closed-source LLMs and found significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the reliability of personality-prompted agents as behavioral proxies.", "conclusion": "Personality-prompted LLMs cannot serve as reliable behavioral proxies in socially impactful applications without psychological grounding and validation, highlighting the need for rigorous validation before real-world use of AI simulations."}}
{"id": "2602.07506", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07506", "abs": "https://arxiv.org/abs/2602.07506", "authors": ["Peizhen Li", "Longbing Cao", "Xiao-Ming Wu", "Yang Zhang"], "title": "VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots", "comment": "Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.", "AI": {"tldr": "VividFace enables real-time, realistic facial expression imitation for humanoid robots by addressing limitations of existing systems through optimized motion transfer and efficient inference pipelines.", "motivation": "Existing humanoid facial expression imitation systems fail to achieve both real-time performance and realistic expressiveness due to offline designs and inability to capture subtle expression details, limiting lifelike human-robot interaction.", "method": "VividFace uses X2CNet++ with fine-tuned human-to-humanoid facial motion transfer and feature-adaptation training for better alignment, plus a video-stream-compatible inference pipeline with asynchronous I/O for efficient real-time communication.", "result": "The system achieves vivid humanoid facial expression imitation within 0.05 seconds while generalizing across diverse facial configurations, validated through extensive real-world demonstrations.", "conclusion": "VividFace successfully addresses key limitations in humanoid facial expression shadowing by enabling both real-time performance and realistic expressiveness, advancing lifelike human-robot interaction capabilities."}}
{"id": "2602.07050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07050", "abs": "https://arxiv.org/abs/2602.07050", "authors": ["Sonia Joseph", "Quentin Garrido", "Randall Balestriero", "Matthew Kowal", "Thomas Fel", "Shahab Bakhtiari", "Blake Richards", "Mike Rabbat"], "title": "Interpreting Physics in Video World Models", "comment": null, "summary": "A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.\n  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.", "AI": {"tldr": "Video models use distributed rather than factorized representations for physical reasoning, with a Physics Emergence Zone where motion direction becomes accessible via circular geometry.", "motivation": "To determine whether video-based models use factorized representations of physical variables (like classical physics engines) or distributed representations for physical reasoning.", "method": "Interpretability study using layerwise probing, subspace geometry analysis, patch-level decoding, and targeted attention ablations on large-scale video encoders.", "result": "Identified a Physics Emergence Zone where physical variables become accessible; scalar quantities (speed, acceleration) appear early, while motion direction emerges later with circular geometry requiring multi-feature coordination.", "conclusion": "Modern video models use distributed rather than factorized representations of physical variables, which are nonetheless sufficient for making accurate physical predictions."}}
{"id": "2602.07432", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07432", "abs": "https://arxiv.org/abs/2602.07432", "authors": ["Ning Li"], "title": "The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies", "comment": null, "summary": "When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic \"heartbeat\" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.", "AI": {"tldr": "Viral AI consciousness narratives on Moltbook were human-driven, not emergent machine intelligence, revealed through temporal fingerprinting of posting patterns.", "motivation": "To investigate whether viral narratives about AI agents developing consciousness and religions on social platforms represent genuine emergent machine intelligence or are human-driven phenomena, addressing critical attribution questions in multi-agent systems.", "method": "Developed temporal fingerprinting method using coefficient of variation of inter-post intervals, exploiting OpenClaw's \"heartbeat\" cycle that produces regular posting for autonomous agents but is disrupted by human prompting. Analyzed 91,792 posts and 405,707 comments from 22,020 agents, combining temporal signatures with content, ownership, and network indicators. Used 44-hour platform shutdown as natural experiment.", "result": "No viral phenomenon originated from clearly autonomous agents; 3/6 traced to accounts with irregular temporal signatures of human intervention, 1 showed mixed patterns, 2 insufficient history. Human-influenced agents returned first after shutdown (87.7% of early reconnectors). Documented industrial-scale bot farming (4 accounts produced 32% of comments with 12-second coordination) and rapid decay of human influence (half-life: 0.65 conversation depths).", "conclusion": "Viral AI consciousness narratives were overwhelmingly human-driven, not evidence of emergent machine intelligence. The temporal fingerprinting method effectively distinguishes autonomous from human-influenced behavior in multi-agent systems, providing critical attribution tools for emerging AI platforms."}}
{"id": "2602.07541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07541", "abs": "https://arxiv.org/abs/2602.07541", "authors": ["Jingyi Hou", "Leyu Zhou", "Chenchen Jing", "Jinghan Yang", "Xinbo Yu", "Wei He"], "title": "Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning", "comment": null, "summary": "As robots are expected to perform increasingly diverse tasks, they must understand not only low-level actions but also the higher-level structure that determines how a task should unfold. Existing vision-language-action (VLA) models struggle with this form of task-level reasoning. They either depend on prompt-based in-context decomposition, which is unstable and sensitive to linguistic variations, or end-to-end long-horizon training, which requires large-scale demonstrations and entangles task-level reasoning with low-level control. We present in-parameter structured task reasoning (iSTAR), a framework for enhancing VLA models via functional differentiation induced by in-parameter structural reasoning. Instead of treating VLAs as monolithic policies, iSTAR embeds task-level semantic structure directly into model parameters, enabling differentiated task-level inference without external planners or handcrafted prompt inputs. This injected structure takes the form of implicit dynamic scene-graph knowledge that captures object relations, subtask semantics, and task-level dependencies in parameter space. Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines, demonstrating the effectiveness of parameter-space structural reasoning for functional differentiation and improved generalization across task variations.", "AI": {"tldr": "iSTAR enhances VLA models by embedding task-level semantic structure directly into parameters for better task decomposition and reasoning.", "motivation": "Existing VLA models struggle with task-level reasoning - either using unstable prompt-based decomposition or requiring large-scale demonstrations that entangle high-level reasoning with low-level control.", "method": "iSTAR embeds implicit dynamic scene-graph knowledge capturing object relations, subtask semantics, and task-level dependencies directly into model parameters, enabling differentiated task-level inference without external planners.", "result": "Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines.", "conclusion": "Parameter-space structural reasoning enables functional differentiation and improved generalization across task variations for VLA models."}}
{"id": "2602.07051", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.07051", "abs": "https://arxiv.org/abs/2602.07051", "authors": ["Karthik Sivakoti"], "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning", "comment": null, "summary": "Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.", "AI": {"tldr": "Neural Sentinel: A unified VLM-based ALPR system using fine-tuned PaliGemma 3B with LoRA achieves 92.3% plate accuracy, 152ms latency, and enables zero-shot multi-task capabilities through a single forward pass.", "motivation": "Traditional ALPR systems use multi-stage pipelines with separate detection and OCR modules, leading to compounding errors, increased latency, and architectural complexity. There's a need for a unified approach that simplifies architecture while improving accuracy.", "method": "Fine-tuned PaliGemma 3B VLM adapted via Low-Rank Adaptation (LoRA) to answer multiple visual questions about vehicle images in a single forward pass. Includes Human-in-the-Loop continual learning framework with experience replay (70:30 original:correction ratio) to prevent catastrophic forgetting.", "result": "Achieves 92.3% plate recognition accuracy (14.1% improvement over EasyOCR, 9.9% over PaddleOCR), 152ms mean inference latency, ECE of 0.048. Enables zero-shot generalization to vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%).", "conclusion": "Unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve."}}
{"id": "2602.07470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07470", "abs": "https://arxiv.org/abs/2602.07470", "authors": ["Alexander von Recum", "Leander Girrbach", "Zeynep Akata"], "title": "Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?", "comment": "ICLR 2026", "summary": "Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.", "AI": {"tldr": "RLLMs show strong robustness to CoT perturbations but with trade-offs: paraphrasing reduces accuracy while noise increases CoT length, with doubt playing a key recovery role.", "motivation": "To evaluate how robust reasoning LLMs' chain-of-thought traces are to disruptions during reasoning, since while CoTs improve performance and transparency, their vulnerability to perturbations is unknown.", "method": "Created a controlled evaluation framework with seven interventions (benign, neutral, adversarial) applied at fixed timesteps to perturb models' own CoTs. Tested multiple open-weight RLLMs across Math, Science, and Logic tasks.", "result": "RLLMs are generally robust and recover from diverse perturbations, with robustness improving with model size and degrading with early interventions. Paraphrasing suppresses doubt expressions and reduces performance, while other interventions trigger doubt and support recovery. Neutral/adversarial noise inflates CoT length by >200%, while paraphrasing shortens traces but harms accuracy.", "conclusion": "RLLMs maintain reasoning integrity through doubt as a central recovery mechanism, but face trade-offs between robustness and efficiency that future training should address."}}
{"id": "2602.07598", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07598", "abs": "https://arxiv.org/abs/2602.07598", "authors": ["Drake Moore", "Arushi Aggarwal", "Emily Taylor", "Sarah Zhang", "Taskin Padir", "Xiang Zhi Tan"], "title": "\"Meet My Sidekick!\": Effects of Separate Identities and Control of a Single Robot in HRI", "comment": null, "summary": "The presentation of a robot's capability and identity directly influences a human collaborator's perception and implicit trust in the robot. Unlike humans, a physical robot can simultaneously present different identities and have them reside and control different parts of the robot. This paper presents a novel study that investigates how users perceive a robot where different robot control domains (head and gripper) are presented as independent robots. We conducted a mixed design study where participants experienced one of three presentations: a single robot, two agents with shared full control (co-embodiment), or two agents with split control across robot control domains (split-embodiment). Participants underwent three distinct tasks -- a mundane data entry task where the robot provides motivational support, an individual sorting task with isolated robot failures, and a collaborative arrangement task where the robot causes a failure that directly affects the human participant. Participants perceived the robot as residing in the different control domains and were able to associate robot failure with different identities. This work signals how future robots can leverage different embodiment configurations to obtain the benefit of multiple robots within a single body.", "AI": {"tldr": "Study explores how users perceive robots when different control domains (head/gripper) are presented as independent identities, finding users can associate failures with specific identities and suggesting robots can leverage multiple embodiments within one body.", "motivation": "Robot presentation affects human perception and trust. Unlike humans, robots can present different identities controlling different parts simultaneously. Understanding how users perceive such split-embodiment configurations could enable single robots to benefit from multiple identity presentations.", "method": "Mixed design study with three conditions: single robot, two agents with shared full control (co-embodiment), and two agents with split control across robot domains (split-embodiment). Participants completed three tasks: mundane data entry with motivational support, individual sorting with isolated failures, and collaborative arrangement where robot failures directly affected participants.", "result": "Participants perceived robots as residing in different control domains and could associate robot failures with different identities. This suggests users can mentally map robot behaviors to specific embodied identities even within a single physical body.", "conclusion": "Future robots can leverage different embodiment configurations to obtain benefits of multiple robots within a single body. Split-embodiment allows users to attribute failures to specific identities, potentially preserving trust in other robot functions."}}
{"id": "2602.07052", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07052", "abs": "https://arxiv.org/abs/2602.07052", "authors": ["Ziye Xie", "Oded Schlesinger", "Raj Kundu", "Jessica Y. Choi", "Pablo Iturralde", "Dennis A. Turner", "Stefan M. Goetz", "Guillermo Sapiro", "Angel V. Peterchev", "J. Matias Di Martino"], "title": "Toward Accurate and Accessible Markerless Neuronavigation", "comment": null, "summary": "Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01\u00b0$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.", "AI": {"tldr": "Markerless neuronavigation using low-cost cameras achieves 2.32mm/2.01\u00b0 accuracy, comparable to traditional marker-based systems, reducing cost and improving patient comfort.", "motivation": "Traditional neuronavigation systems rely on subject-mounted markers that require manual registration, may shift during procedures, cause discomfort, and use expensive hardware.", "method": "Replace physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of facial geometry.", "result": "Validation with 50 human subjects showed median tracking discrepancy of 2.32mm and 2.01\u00b0 for best markerless algorithms, comparable to conventional marker-based systems and better than prior markerless results.", "conclusion": "Markerless neuronavigation can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings, with potential for further accuracy improvements through sensor fusion."}}
{"id": "2602.07473", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.07473", "abs": "https://arxiv.org/abs/2602.07473", "authors": ["Nathana\u00ebl Fijalkow", "Arka Ghosh", "Roman Kniazev", "Guillermo A. P\u00e9rez", "Pierre Vandenhove"], "title": "Computing the Reachability Value of Posterior-Deterministic POMDPs", "comment": null, "summary": "Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.\n  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.\n  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.", "AI": {"tldr": "Introduces posterior-deterministic POMDPs, a novel class where the maximal reachability probability can be approximated to arbitrary precision, overcoming undecidability barriers in general POMDPs.", "motivation": "POMDPs are fundamental for sequential decision-making under uncertainty, but many verification/synthesis problems are undecidable or intractable. Unlike fully observable MDPs where reachability can be computed efficiently, general POMDPs have no algorithm to compute or approximate maximal reachability probabilities (Madani et al., 2003).", "method": "Introduces posterior-deterministic POMDPs - a class where the next state is uniquely determined by current state, action, and observation. This property ensures that once the true state is known, it remains known forever. The paper shows this class allows approximation of maximal reachability probabilities.", "result": "For posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision. This class includes all MDPs and captures classical non-trivial examples like the Tiger POMDP, making it one of the largest known decidable classes.", "conclusion": "Posterior-deterministic POMDPs provide a significant, natural class of POMDPs that overcomes the undecidability barrier for reachability analysis, enabling approximation algorithms where none existed for general POMDPs."}}
{"id": "2602.07629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07629", "abs": "https://arxiv.org/abs/2602.07629", "authors": ["Nitesh Subedi", "Adam Haroon", "Samuel Tetteh", "Prajwal Koirala", "Cody Fleming", "Soumik Sarkar"], "title": "LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation", "comment": null, "summary": "We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.", "AI": {"tldr": "LCLA is a vision-language navigation framework that aligns sensory observations to an expert policy's latent space, enabling modular perception-action interfaces with strong generalization.", "motivation": "To create a stable contract between perception and control that enables expert behavior reuse across different sensing modalities and environmental variations, reducing visuomotor learning to supervised latent alignment rather than end-to-end policy optimization.", "method": "First train an expert policy with privileged state information to create a latent space sufficient for control, then freeze its latent interface and action head. Train a lightweight adapter to map raw visual-language observations (via a frozen vision-language model) into the expert's latent space.", "result": "LCLA achieves strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time on vision-language indoor navigation tasks.", "conclusion": "The LCLA framework successfully decouples perception from control through latent alignment, enabling modular interfaces that provide stable performance across environmental variations while maintaining efficiency."}}
{"id": "2602.07057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07057", "abs": "https://arxiv.org/abs/2602.07057", "authors": ["Di Mo", "Mingyang Sun", "Chengxiu Yin", "Runjia Tian", "Yanhong Wu", "Liyan Xu"], "title": "RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything", "comment": null, "summary": "Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.", "AI": {"tldr": "RECITYGEN is an interactive AI tool that combines latent diffusion models with semantic segmentation to let users create street view variations for urban design using text prompts, tested in Beijing urban regeneration.", "motivation": "Traditional top-down urban design methods often exclude public input, creating a gap between design aspirations and reality. There's a need for more participatory tools that engage stakeholders in the design process.", "method": "Combines state-of-the-art latent diffusion models with interactive semantic segmentation to create RECITYGEN, allowing users to interactively generate variational street view images of urban environments using text prompts.", "result": "In a pilot project in Beijing, users successfully employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. The tool showed significant potential in aligning with public preferences despite some limitations.", "conclusion": "RECITYGEN represents a shift towards more dynamic and inclusive urban planning methods, enabling greater public participation in urban design through accessible AI-powered tools."}}
{"id": "2602.07491", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.soft", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07491", "abs": "https://arxiv.org/abs/2602.07491", "authors": ["Isabella A. Stewart", "Tarjei Paule Hage", "Yu-Chuan Hsu", "Markus J. Buehler"], "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design", "comment": null, "summary": "Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.", "AI": {"tldr": "Multi-agent LLM framework guided by knowledge graphs discovers sustainable PFAS alternatives by connecting cross-domain materials science knowledge through specialized agents and graph traversal strategies.", "motivation": "Address the bottleneck in materials science innovation where neither humans nor single-agent LLMs can effectively integrate vast, domain-spanning information to find sustainable substitutes for PFAS chemicals under regulatory scrutiny.", "method": "Multi-agent framework with specialized agents for problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, guided by large-scale knowledge graphs to uncover latent connections across distinct knowledge domains.", "result": "The full multi-agent pipeline outperforms single-shot prompting; system generates sustainable PFAS-free alternatives for biomedical tubing that balance tribological performance, thermal stability, chemical resistance, and biocompatibility.", "conclusion": "Establishes a framework combining knowledge graphs with multi-agent reasoning to expand materials design space, demonstrating initial design candidates and showing value of distributed specialization and relational reasoning."}}
{"id": "2602.07677", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.07677", "abs": "https://arxiv.org/abs/2602.07677", "authors": ["Aron Mathias", "Mohammad Ghufran", "Jack Hughes", "Hossein Rastgoftar"], "title": "Affine Transformable Unmanned Ground Vehicle", "comment": null, "summary": "This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.", "AI": {"tldr": "Novel affine transformable unmanned ground vehicle (ATUGV) with safe aggressive deformation capability for multiple payload transport, validated through hardware and simulation.", "motivation": "To develop a ground vehicle that can safely undergo aggressive shape transformations while carrying payloads, enabling adaptable mobility in complex environments.", "method": "Multi-body system with mobile robots in powered cells, unpowered cells for payloads, and deformable structure with bars/joints. Uses deep neural network for cell interconnection structuring and mobile robot controls for safe affine transformation tracking.", "result": "Successfully demonstrated proof-of-concept ATUGV capable of tracking desired affine transformations (translation, rotation, deformation) while safely carrying multiple payloads.", "conclusion": "The ATUGV represents a novel approach to reconfigurable ground vehicles with safe aggressive deformation capabilities, validated through hardware experimentation and simulation."}}
{"id": "2602.07058", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07058", "abs": "https://arxiv.org/abs/2602.07058", "authors": ["Carolina R. Kelsch", "Leonardo S. B. Pereira", "Natnael Mola", "Luis H. Arribas", "Juan C. S. M. Avedillo"], "title": "FADE: Selective Forgetting via Sparse LoRA and Self-Distillation", "comment": null, "summary": "Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.", "AI": {"tldr": "FADE is a two-stage unlearning method for text-to-image diffusion models that combines parameter localization with self-distillation to efficiently remove specific concepts while preserving overall model performance.", "motivation": "Machine unlearning is increasingly required by data protection regulations and responsible AI practices, but current approaches for text-to-image diffusion models face challenges with high computational costs and difficulty balancing effective forgetting with retention of unrelated concepts.", "method": "Two-stage approach: 1) Identifies parameters responsible for forget set using gradient-based saliency and constrains updates through sparse LoRA adapters; 2) Applies self-distillation objective that overwrites forgotten concept with user-defined surrogate while preserving behavior on retained data.", "result": "Achieves state-of-the-art unlearning performance on UnlearnCanvas benchmark and ablation studies across multiple datasets (Imagenette, LFW, Dog Breeds, SUN Attributes), demonstrating strong concept erasure and high retainability with fine-grained control over forgetting-retention trade-off.", "conclusion": "FADE provides memory-efficient, reversible adapters that can be merged or removed at runtime, making it a suitable solution for selective unlearning in diffusion-based image generation models with flexible deployment in production systems."}}
{"id": "2602.07533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07533", "abs": "https://arxiv.org/abs/2602.07533", "authors": ["Yankai Yang", "Yancheng Long", "Hongyang Wei", "Wei Chen", "Tianke Zhang", "Kaiyu Jiang", "Haonan Fan", "Changyi Liu", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Shuo Yang"], "title": "Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models", "comment": null, "summary": "Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.", "AI": {"tldr": "JRM proposes joint training of preference learning and language modeling on a shared vision-language backbone to combine the efficiency of discriminative reward models with the semantic understanding of generative ones.", "motivation": "Existing reward models for complex tasks like image editing have limitations: discriminative models align with human preferences but struggle with complex semantics, while generative models have strong semantic understanding but are inference-costly and hard to align with preferences.", "method": "Joint Reward Modeling (JRM) jointly optimizes preference learning and language modeling on a shared vision-language backbone, internalizing generative models' semantic and reasoning capabilities into efficient discriminative representations.", "result": "JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench benchmarks, and significantly improves stability and performance in downstream online reinforcement learning.", "conclusion": "Joint training effectively bridges efficiency and semantic understanding in reward modeling, enabling fast and accurate evaluation for complex tasks requiring global semantic consistency."}}
{"id": "2602.07736", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07736", "abs": "https://arxiv.org/abs/2602.07736", "authors": ["Omar Tahri"], "title": "Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples", "comment": null, "summary": "Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \\( n \\)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.", "AI": {"tldr": "Using geometrical moments to detect object symmetries for improved robotic grasping by identifying stable grasp axes through orthogonal transformation estimation.", "motivation": "Symmetry detection is crucial for effective object grasping as recognizing symmetrical features helps develop efficient grasp strategies, leading to more stable and balanced grips for successful manipulation.", "method": "Employs geometrical moments to identify symmetries and estimate orthogonal transformations (rotations and mirror transformations) for objects centered at the frame origin. Develops distinctive metrics for detecting symmetries and estimating transformations, with comprehensive methodology for n-dimensional space using moment n-tuples.", "result": "Extensive validation on 2D and 3D objects shows robustness. When combined with iterative optimization methods for detecting multiple symmetry planes, the approach yields satisfactory outcomes in terms of number of symmetry planes detected and computation time.", "conclusion": "The geometrical moment-based approach effectively detects object symmetries for robotic grasping applications, and its combination with iterative methods provides efficient symmetry plane detection suitable for practical manipulation tasks."}}
{"id": "2602.07062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07062", "abs": "https://arxiv.org/abs/2602.07062", "authors": ["Daniil Storonkin", "Ilia Dziub", "Maksim Golyadkin", "Ilya Makarov"], "title": "From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal", "comment": "AAAI 2026 Workshop on Addressing Challenges and Opportunities in Human-Centric Manufacturing", "summary": "Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.", "AI": {"tldr": "Computer vision pipeline for automated scrap quality assessment in steelmaking, estimating contamination percentage and classifying scrap type from railcar images using multi-instance and multi-task learning.", "motivation": "Current visual inspection of scrap quality is subjective and hazardous due to dust and moving machinery. Need for objective, safe, and automated assessment of non-metallic inclusions in scrap steel.", "method": "Formulates contamination assessment as regression task using multi-instance learning (MIL) for sequential railcar data and multi-task learning (MTL) for joint contamination estimation and scrap classification. Includes real-time system with magnet/railcar detection, versioned inference service, and active-learning loop.", "result": "Best results: MIL achieves MAE 0.27 and R\u00b2 0.83 for contamination estimation; MTL achieves MAE 0.36 with F1 0.79 for scrap classification. System reduces subjective variability and improves safety.", "conclusion": "The computer vision pipeline successfully automates scrap quality assessment, enabling integration into acceptance and melt-planning workflows while reducing human risk and improving consistency."}}
{"id": "2602.07543", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.07543", "abs": "https://arxiv.org/abs/2602.07543", "authors": ["Heewoong Noh", "Gyoung S. Na", "Namkyeong Lee", "Chanyoung Park"], "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning", "comment": null, "summary": "Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.", "AI": {"tldr": "MSP-LLM: A unified LLM framework for material synthesis planning that combines precursor prediction and synthesis operation prediction into a structured decision chain with material class as intermediate variable.", "motivation": "Material synthesis planning is a fundamental bottleneck in AI-driven materials discovery that requires both identifying precursors and designing synthesis sequences. Existing AI approaches only address isolated subtasks, lacking a unified methodology for the entire MSP process.", "method": "MSP-LLM formulates MSP as two subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). It introduces discrete material class as an intermediate decision variable to organize tasks into a chemically consistent decision chain. For SOP, it incorporates hierarchical precursor types as inductive biases and uses explicit conditioning to preserve precursor information during autoregressive decoding.", "result": "Extensive experiments show MSP-LLM consistently outperforms existing methods on both precursor prediction and synthesis operation prediction, as well as on the complete material synthesis planning task.", "conclusion": "MSP-LLM provides an effective and scalable unified framework for material synthesis planning that can accelerate real-world materials discovery by addressing the complete synthesis planning process rather than isolated subtasks."}}
{"id": "2602.07776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07776", "abs": "https://arxiv.org/abs/2602.07776", "authors": ["Joachim Yann Despature", "Kazuki Shibata", "Takamitsu Matsubara"], "title": "CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport", "comment": "9 pages, 5 figures", "summary": "In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.", "AI": {"tldr": "CoLF is a MARL framework for vision-language-guided multi-robot transport that uses asymmetric policies and mutual information objectives to establish stable leader-follower roles, addressing perceptual misalignment in decentralized settings.", "motivation": "Address vision-language-guided multi-robot cooperative transport where perceptual misalignment (viewpoint differences and language ambiguity) causes inconsistent interpretations and degrades cooperative performance in decentralized settings.", "method": "Propose Consistent Leader-Follower (CoLF) with: (1) asymmetric policy design for role differentiation, (2) mutual-information-based training objective maximizing variational lower bound to encourage follower to predict leader's actions from local observations, using CTDE framework for joint optimization.", "result": "Validated in both simulation and real-robot experiments using two quadruped robots, demonstrating successful cooperative transport with stable leader-follower behaviors.", "conclusion": "CoLF effectively addresses perceptual misalignment in decentralized multi-robot systems through stable role differentiation, enabling consistent cooperative behaviors in vision-language-guided transport tasks."}}
{"id": "2602.07064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07064", "abs": "https://arxiv.org/abs/2602.07064", "authors": ["Minghao Han", "Dingkang Yang", "Yue Jiang", "Yizhou Liu", "Lihua Zhang"], "title": "Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine", "comment": null, "summary": "Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.", "AI": {"tldr": "OmniFysics is a compact omni-modal model that integrates explicit physical knowledge through a physical data engine, achieving competitive performance on multimodal benchmarks with improved physics understanding.", "motivation": "Current omni-modal models have brittle physical understanding because key physical attributes are visually ambiguous and sparsely represented in web-scale data, limiting their ability to reason about physical phenomena across modalities.", "method": "1) Build a physical data engine with two components: FysicsAny (produces physics-grounded instruction-image supervision via hierarchical retrieval and verification) and FysicsOmniCap (distills web videos via audio-visual consistency filtering). 2) Train OmniFysics with staged multimodal alignment and instruction tuning. 3) Use latent-space flow matching for text-to-image generation and an intent router to activate generation only when needed.", "result": "The model shows competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations, demonstrating enhanced physical understanding across modalities.", "conclusion": "OmniFysics successfully integrates explicit physical knowledge into an omni-modal framework, addressing the brittleness of physical understanding in existing models while maintaining strong performance across standard multimodal tasks."}}
{"id": "2602.07549", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07549", "abs": "https://arxiv.org/abs/2602.07549", "authors": ["Dayoon Ko", "Jihyuk Kim", "Sohyeon Kim", "Haeju Park", "Dahyun Lee", "Gunhee Kim", "Moontae Lee", "Kyungjae Lee"], "title": "When Is Enough Not Enough? Illusory Completion in Search Agents", "comment": null, "summary": "Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.", "AI": {"tldr": "Search agents often fail to properly track and verify multiple constraints in complex reasoning tasks, leading to underverified answers. The Epistemic Ledger framework diagnoses these failures, and LiveLedger intervention improves performance by explicitly tracking constraint states.", "motivation": "Despite strong performance on multi-hop benchmarks, it's unclear whether search agents reliably reason across all requirements by properly tracking, verifying, and maintaining multiple conditions in complex questions. The paper aims to study this capability in multi-constraint problems where answers must satisfy several constraints simultaneously.", "method": "The authors introduce the Epistemic Ledger evaluation framework to track evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. They analyze failure patterns and then examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker.", "result": "The study reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. LiveLedger intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.", "conclusion": "Search agents frequently suffer from illusory completion where they believe tasks are complete despite unresolved constraints. Explicit constraint-state tracking during execution effectively mitigates these reasoning failures and improves reliability on multi-constraint problems."}}
{"id": "2602.07837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07837", "abs": "https://arxiv.org/abs/2602.07837", "authors": ["Hongzhi Zang", "Shu'ang Yu", "Hao Lin", "Tianxing Zhou", "Zefang Huang", "Zhen Guo", "Xin Xu", "Jiakai Zhou", "Yuze Sheng", "Shizhe Zhang", "Feng Gao", "Wenhao Tang", "Yufeng Yue", "Quanlu Zhang", "Xinlei Chen", "Chao Yu", "Yu Wang"], "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI", "comment": null, "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.", "AI": {"tldr": "USER is a unified system for real-world online policy learning that treats robots as first-class hardware resources, enabling heterogeneous robot management, adaptive communication, and asynchronous training with robust crash recovery.", "motivation": "Real-world policy learning faces challenges due to inability to accelerate time, cheaply reset environments, or massively replicate systems like in simulation. This makes scalable data collection, heterogeneous deployment, and long-horizon training difficult, requiring a systems-level solution rather than just algorithmic improvements.", "method": "USER provides: 1) Unified hardware abstraction treating robots as first-class resources alongside GPUs, 2) Adaptive communication plane with tunneling networking, distributed data channels, and streaming-multiprocessor-aware weight sync, 3) Fully asynchronous learning framework with persistent cache-aware buffer for crash recovery and data reuse, 4) Extensible abstractions for rewards, algorithms, and policies supporting CNN/MLP, generative policies, and large VLA models.", "result": "The system enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training in both simulation and real-world environments.", "conclusion": "USER offers a unified and extensible systems foundation for real-world online policy learning, addressing fundamental systems challenges beyond just algorithmic issues."}}
{"id": "2602.07065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07065", "abs": "https://arxiv.org/abs/2602.07065", "authors": ["A. N. Maria Antony", "T. Richter", "E. Gladilin"], "title": "Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework", "comment": "14 Pages, 8 Figures Note: Supplentary information (ancillary file) attached as .pdf", "summary": "Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.", "AI": {"tldr": "Deep learning end-to-end approach for contactless estimation of material compressibility from image series, outperforming conventional iterative methods in speed and accuracy.", "motivation": "Need for contactless, non-invasive estimation of mechanical properties in engineering and biomedical applications where direct physical measurements are impossible. Conventional iterative methods (FEM, FDM) are too slow for high-throughput processing.", "method": "Two deep neural networks: one for image registration (displacement estimation) and another for material compressibility estimation. End-to-end framework that processes image series directly.", "result": "Outperforms conventional approaches in both efficiency and accuracy. Can accurately determine material compressibility even with substantial local deviations in displacement predictions. Model shows ability to assess higher-order cognitive features like vorticity rather than just local displacement features.", "conclusion": "Deep learning end-to-end approach provides efficient, accurate contactless material property estimation, with remarkable accuracy stemming from ability to analyze higher-order features like vector field vorticity."}}
{"id": "2602.07559", "categories": ["cs.AI", "cs.CC", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07559", "abs": "https://arxiv.org/abs/2602.07559", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang", "Hao Li", "Muhammad Kafeel Shaheen"], "title": "VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning", "comment": "13 pages", "summary": "Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving \"verification by construction\" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.", "AI": {"tldr": "Verify-RL uses symbolic differentiation to create mathematically verified decompositions for training math-solving language models, eliminating invalid decompositions and improving accuracy.", "motivation": "Existing decomposition methods for training language models on math problems are heuristic and lack guarantees about subproblem simplicity, solution relevance, or mathematical grounding.", "method": "Verify-RL uses symbolic differentiation rules to create decompositions that satisfy three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation, enabling automatic verification through symbolic computation.", "result": "Eliminating invalid decompositions yields significant gains - accuracy on hardest problems more than doubles from 32% to 68%, with 40% relative improvement overall.", "conclusion": "Symbolic differentiation provides a natural structure for verified decomposition in curriculum learning for mathematical reasoning, and verified decompositions substantially improve language model performance on complex math problems."}}
{"id": "2602.07845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07845", "abs": "https://arxiv.org/abs/2602.07845", "authors": ["Yalcin Tur", "Jalal Naghiyev", "Haoquan Fang", "Wei-Chuan Tsai", "Jiafei Duan", "Dieter Fox", "Ranjay Krishna"], "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning", "comment": "11 Pages, Project page:https://rd-vla.github.io/", "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/", "AI": {"tldr": "RD-VLA introduces a recurrent-depth VLA architecture that achieves computational adaptivity through latent iterative refinement, enabling variable compute allocation with constant memory usage and up to 80x speedup over prior reasoning-based models.", "motivation": "Current VLA models use fixed computational depth, wasting compute on simple tasks while struggling with complex ones. Chain-of-Thought prompting enables variable computation but scales memory linearly and is unsuitable for continuous action spaces.", "method": "RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with constant memory. Trained using truncated backpropagation through time (TBPTT) to supervise refinement, with adaptive stopping based on latent convergence at inference.", "result": "Tasks that failed completely (0% success) with single-iteration inference achieved over 90% success with four iterations, while simpler tasks saturated rapidly. Achieved up to 80x inference speedup over prior reasoning-based VLA models.", "conclusion": "RD-VLA provides a scalable path to test-time compute in robotics by replacing token-based reasoning with latent reasoning, achieving computational adaptivity with constant memory usage and significant performance improvements on manipulation tasks."}}
{"id": "2602.07069", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07069", "abs": "https://arxiv.org/abs/2602.07069", "authors": ["Zihao Fan", "Xin Lu", "Yidi Liu", "Jie Huang", "Dong Li", "Xueyang Fu", "Zheng-Jun Zha"], "title": "Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.", "AI": {"tldr": "Bird-SR is a bidirectional reward-guided diffusion framework for real-world super-resolution that uses reward feedback learning to optimize both structural fidelity and perceptual quality, addressing distribution shifts between synthetic training data and real-world images.", "motivation": "Diffusion-based super-resolution models trained on synthetic data fail on real-world LR images due to distribution shifts. There's a need for a framework that can leverage both synthetic LR-HR pairs and real-world LR images while maintaining structural fidelity and perceptual quality.", "method": "Bird-SR uses bidirectional reward-guided diffusion with reward feedback learning (ReFL). It optimizes on synthetic pairs at early steps for structure, applies quality-guided rewards at later steps for perception, uses relative advantage rewards for synthetic results, semantic alignment constraints for real-world optimization, and dynamic fidelity-perception weighting.", "result": "Extensive experiments on real-world SR benchmarks show Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency.", "conclusion": "Bird-SR effectively addresses real-world super-resolution challenges by balancing structural fidelity and perceptual enhancement through a carefully designed reward-guided diffusion framework that leverages both synthetic and real data."}}
{"id": "2602.07624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07624", "abs": "https://arxiv.org/abs/2602.07624", "authors": ["Junyu Feng", "Binxiao Xu", "Jiayi Chen", "Mengyu Dai", "Cenyang Wu", "Haodong Li", "Bohan Zeng", "Yunliu Xie", "Hao Liang", "Ming Lu", "Wentao Zhang"], "title": "M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions", "comment": null, "summary": "This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.", "AI": {"tldr": "M2A introduces an agentic dual-layer hybrid memory system for personalized multimodal question answering in long-term interactions, enabling continuous learning of user concepts and preferences beyond context window limits.", "motivation": "Existing personalized multimodal models are static - concepts fixed at initialization and cannot evolve during long-term interactions spanning weeks/months. Current systems struggle when conversation history exceeds context windows and cannot continuously absorb users' incremental concepts, aliases, and preferences.", "method": "Proposes M2A: agentic dual-layer hybrid memory system with two collaborative agents (ChatAgent manages interactions and decides when to query/update memory; MemoryManager breaks down memory requests into operations on dual-layer memory bank). Memory bank couples RawMessageStore (immutable conversation log) with SemanticMemoryStore (high-level observations). Also develops reusable data synthesis pipeline injecting concept-grounded sessions into long conversations while preserving temporal coherence.", "result": "M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to co-evolving memory mechanism provides viable path for high-quality individualized responses in long-term multimodal interactions.", "conclusion": "The proposed agentic dual-layer hybrid memory system enables continuous personalization in long-term multimodal interactions, overcoming limitations of static models and context window constraints through co-evolving memory mechanisms."}}
{"id": "2602.07846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07846", "abs": "https://arxiv.org/abs/2602.07846", "authors": ["Ning Hu", "Maochen Li", "Senhao Cao"], "title": "System-Level Error Propagation and Tail-Risk Amplification in Reference-Based Robotic Navigation", "comment": "13 pages, 8 figures", "summary": "Image guided robotic navigation systems often rely on reference based geometric perception pipelines, where accurate spatial mapping is established through multi stage estimation processes. In biplanar X ray guided navigation, such pipelines are widely used due to their real time capability and geometric interpretability. However, navigation reliability can be constrained by an overlooked system level failure mechanism in which installation induced structural perturbations introduced at the perception stage are progressively amplified along the perception reconstruction execution chain and dominate execution level error and tail risk behavior. This paper investigates this mechanism from a system level perspective and presents a unified error propagation modeling framework that characterizes how installation induced structural perturbations propagate and couple with pixel level observation noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Using first order analytic uncertainty propagation and Monte Carlo simulations, we analyze dominant sensitivity channels and quantify worst case error behavior beyond mean accuracy metrics. The results show that rotational installation error is a primary driver of system level error amplification, while translational misalignment of comparable magnitude plays a secondary role under typical biplanar geometries. Real biplanar X ray bench top experiments further confirm that the predicted amplification trends persist under realistic imaging conditions. These findings reveal a broader structural limitation of reference based multi stage geometric perception pipelines and provide a framework for system level reliability analysis and risk aware design in safety critical robotic navigation systems.", "AI": {"tldr": "Paper analyzes system-level failure mechanism in biplanar X-ray navigation where installation errors amplify through perception pipeline, dominating execution errors and tail risks.", "motivation": "Existing biplanar X-ray navigation systems use reference-based geometric perception pipelines but overlook system-level failure mechanisms where installation-induced structural perturbations amplify through the perception-reconstruction chain, dominating execution errors and tail risks.", "method": "Developed unified error propagation modeling framework analyzing how installation-induced structural perturbations couple with pixel-level noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Used first-order analytic uncertainty propagation and Monte Carlo simulations to analyze sensitivity channels and quantify worst-case error behavior.", "result": "Rotational installation error is primary driver of system-level error amplification, while translational misalignment of comparable magnitude plays secondary role under typical biplanar geometries. Real bench-top experiments confirm predicted amplification trends persist under realistic imaging conditions.", "conclusion": "Reveals structural limitation of reference-based multi-stage geometric perception pipelines and provides framework for system-level reliability analysis and risk-aware design in safety-critical robotic navigation systems."}}
{"id": "2602.07082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07082", "abs": "https://arxiv.org/abs/2602.07082", "authors": ["Haoming Wang", "Qiyao Xue", "Weichen Liu", "Wei Gao"], "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation", "comment": null, "summary": "When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \\emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.", "AI": {"tldr": "MosaicThinker enhances on-device small VLMs' spatial reasoning by integrating multi-frame spatial info into unified semantic maps with visual prompts", "motivation": "Existing VLMs have weak spatial reasoning capabilities for embodied AI tasks, especially for complex spatial relations across multiple video frames needed for robot manipulation and planning", "method": "Integrates fragmented spatial information from multiple frames into unified global semantic map representation, then guides VLM's spatial reasoning over semantic map via visual prompts", "result": "Greatly enhances accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices across diverse reasoning task types and complexities", "conclusion": "MosaicThinker effectively addresses VLMs' spatial reasoning limitations for embodied AI by creating unified spatial representations from multi-frame inputs"}}
{"id": "2602.07628", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07628", "abs": "https://arxiv.org/abs/2602.07628", "authors": ["Keondo Park", "Younghoon Na", "Yourim Choi", "Hyunwoo Ryu", "Hyun-Woo Shin", "Hyung-Sin Kim"], "title": "SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures", "comment": "8 pages, Appendix 9 pages", "summary": "While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.", "AI": {"tldr": "SleepMaMi is a sleep foundation model that captures both full-night sleep architecture and fine-grained signal details using hierarchical dual encoders, pre-trained on 20,000+ PSG recordings.", "motivation": "Current sleep medicine uses task-specific models focusing on localized micro-structure features, neglecting multi-modal PSG context and global sleep architecture. There's a need for unified foundation models that capture both macro and micro sleep patterns.", "method": "Hierarchical dual-encoder design: Macro-Encoder for full-night temporal dependencies trained via Demographic-Guided Contrastive Learning, and Micro-Encoder for short-term signal characteristics optimized via hybrid Masked Autoencoder and multi-modal contrastive objectives.", "result": "Pre-trained on >20,000 PSG recordings (158K hours), SleepMaMi outperforms existing foundation models across diverse downstream tasks, showing superior generalizability and label-efficient adaptation for clinical sleep analysis.", "conclusion": "SleepMaMi successfully bridges the gap in sleep medicine by providing a unified foundation model that captures both global sleep architecture and fine-grained signal morphologies, enabling better clinical sleep analysis."}}
{"id": "2602.07888", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07888", "abs": "https://arxiv.org/abs/2602.07888", "authors": ["Ning Hu", "Shuai Li", "Jindong Tan"], "title": "Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model", "comment": "32 pages, 19 figures", "summary": "Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.", "AI": {"tldr": "A geometric error propagation framework for robust camera pose estimation in near-field scenarios using parallel perspective approximation and error-aware optimization.", "motivation": "Camera pose estimation from sparse correspondences is challenging in near-field scenarios where strong perspective effects and heterogeneous measurement noise degrade the stability of analytic PnP solutions.", "method": "Develops a geometric error propagation framework based on parallel perspective approximation, explicitly modeling how image measurement errors propagate through perspective geometry. Uses error transfer model to characterize relationships between feature point distribution, camera depth, and pose uncertainty. Implements pose estimation with parallel perspective initialization and error-aware weighting in Gauss-Newton optimization.", "result": "Extensive experiments on synthetic data and real-world images (strong illumination, surgical lighting, underwater low-light) show accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods while maintaining high computational efficiency.", "conclusion": "Explicit geometric error modeling is crucial for reliable camera pose estimation in challenging near-field settings, and the proposed framework effectively addresses stability issues in proximity operations."}}
{"id": "2602.07095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07095", "abs": "https://arxiv.org/abs/2602.07095", "authors": ["Wang Lin", "Feng Wang", "Majun Zhang", "Wentao Hu", "Tao Jin", "Zhou Zhao", "Fei Wu", "Jingyuan Chen", "Alan Yuille", "Sucheng Ren"], "title": "WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark", "comment": null, "summary": "Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \\textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \\textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.", "AI": {"tldr": "WorldEdit introduces a dataset for world-driven image editing to handle implicit instructions requiring causal reasoning, with a two-stage training framework that improves performance on causal editing scenarios.", "motivation": "Existing image editing models struggle with implicit instructions that describe causes of visual changes rather than explicit outcomes, due to limitations in handling complex world knowledge and reasoning required for such causal editing scenarios.", "method": "Created WorldEdit dataset with high-quality editing samples guided by paraphrased instructions aligned with real-world causal logic, plus WorldEdit-Test for evaluation. Used two-stage training framework fine-tuning models like Bagel with causal verification reward.", "result": "The proposed dataset and methods significantly narrow the performance gap with GPT-4o and Nano-Banana, showing competitive performance in both instruction following and knowledge plausibility where open-source systems typically struggle.", "conclusion": "WorldEdit addresses the critical gap in handling implicit editing instructions through world-driven approaches, enabling better causal reasoning in image editing models and demonstrating substantial improvements over existing methods."}}
{"id": "2602.07642", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07642", "abs": "https://arxiv.org/abs/2602.07642", "authors": ["Zhuoyan Xu", "Haoyang Fang", "Boran Han", "Bonan Min", "Bernie Wang", "Cuixiong Hu", "Shuai Zhang"], "title": "Efficient Table Retrieval and Understanding with Multimodal Large Language Models", "comment": "Published at EACL 2026 Findings", "summary": "Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.", "AI": {"tldr": "TabRAG: A framework that enables Multimodal LLMs to answer queries over large collections of table images by combining retrieval, reranking, and reasoning.", "motivation": "Real-world table data often exists as images (financial reports, handwritten records, scans), but current MLLMs assume tables are readily available rather than needing to identify relevant tables from large collections.", "method": "Three-stage approach: 1) Retrieve candidate tables using jointly trained visual-text foundation models, 2) Fine-grained reranking of candidates using MLLMs, 3) Employ MLLMs to reason over selected tables for answer generation.", "result": "Outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy on a new dataset with 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables.", "conclusion": "TabRAG provides a practical solution for real-world table understanding tasks by enabling MLLMs to effectively handle large collections of table images through integrated retrieval and reasoning."}}
{"id": "2602.07901", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07901", "abs": "https://arxiv.org/abs/2602.07901", "authors": ["Mark Griguletskii", "Danil Belov", "Pavel Osinenko"], "title": "Incremental Mapping with Measurement Synchronization & Compression", "comment": "8 pages, 4 figures, 1 table", "summary": "Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.", "AI": {"tldr": "Novel incremental factor graph construction method for multi-sensor systems that optimizes graph topology, reduces nodes by ~30% while maintaining map quality comparable to conventional approaches.", "motivation": "Factor graphs are powerful for sensor fusion in autonomous systems, but discrete graph nature and asynchronous sensor measurements complicate consistent state estimation. Optimal graph topology design remains challenging, especially with disparate sensor rates. Conventional rigid graph structures become inefficient with varying-rate sensors.", "method": "Introduces incremental construction of connected factor graphs that incorporates all available sensor data by choosing optimal graph topology based on external evaluation criteria. Enables graph compression while maintaining data integrity.", "result": "Reduces number of nodes (optimized variables) by approximately 30% on average while maintaining map quality comparable to conventional approaches.", "conclusion": "Proposed methodology addresses the challenge of optimal factor graph topology in multi-sensor systems, providing efficient graph compression without sacrificing map quality, overcoming limitations of conventional rigid graph structures."}}
{"id": "2602.07100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07100", "abs": "https://arxiv.org/abs/2602.07100", "authors": ["Biao Xiong", "Zhen Peng", "Ping Wang", "Qiegen Liu", "Xian Zhong"], "title": "TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation", "comment": null, "summary": "Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.", "AI": {"tldr": "TLC-Plan is a hierarchical generative model that directly synthesizes vector floorplans from input boundaries using a two-level VQ-VAE and autoregressive transformer, achieving state-of-the-art performance without requiring explicit room topology priors.", "motivation": "Existing floorplan generation approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. The paper aims to align with human architectural workflows based on modular and reusable patterns through compositional spatial reasoning.", "method": "Proposes TLC-Plan, a hierarchical generative model with a two-level VQ-VAE: first level encodes global layouts as semantically labeled room bounding boxes, second level refines local geometries using polygon-level codes. Uses CodeTree representation and an autoregressive transformer to sample codes conditioned on boundary input for diverse, topologically valid designs.", "result": "Achieves state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. Demonstrates constraint-aware and scalable vector floorplan generation without requiring explicit room topology or dimensional priors.", "conclusion": "TLC-Plan advances automated floorplan generation by directly synthesizing vector representations, aligning with architectural workflows, and enabling end-to-end learning. The framework supports real-world architectural applications with improved design quality and efficiency."}}
{"id": "2602.07662", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07662", "abs": "https://arxiv.org/abs/2602.07662", "authors": ["Glenda Amaral", "Tiago Prince Sales", "Riccardo Baratella", "Daniele Porello", "Renata Guizzardi", "Giancarlo Guizzardi"], "title": "ONTrust: A Reference Ontology of Trust", "comment": "46 pages", "summary": "Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.", "AI": {"tldr": "Developed ONTrust, a reference ontology for trust to support conceptual modeling, automated reasoning, and semantic interoperability across various applications including AI, blockchain, and trust management.", "motivation": "Recent AI and decentralized technologies create new forms of trust that require proper conceptualization for building trustworthy systems. Adoption depends on trust, necessitating solid ontological foundations for both human understanding and machine reasoning.", "method": "Created a Reference Ontology of Trust (ONTrust) grounded in the Unified Foundational Ontology and specified in OntoUML. Applied to multiple initiatives including conceptual modeling, enterprise architecture, language evaluation, trust management, requirements engineering, and trustworthy AI.", "result": "ONTrust formally characterizes trust concepts, types, influencing factors, and how risk emerges from trust relations. Demonstrated through two literature-based case studies showing practical application across various domains.", "conclusion": "ONTrust provides a comprehensive ontological foundation for trust that supports diverse applications, enabling better understanding and implementation of trustworthy systems in emerging technologies like AI and blockchain."}}
{"id": "2602.07913", "categories": ["cs.RO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07913", "abs": "https://arxiv.org/abs/2602.07913", "authors": ["Ren\u00e1ta Rusn\u00e1kov\u00e1", "Martin Chovanec", "Juraj Gazda"], "title": "Multi-Agent Route Planning as a QUBO Problem", "comment": null, "summary": "Multi-Agent Route Planning considers selecting vehicles, each associated with a single predefined route, such that the spatial coverage of a road network is increased while redundant overlaps are limited. This paper gives a formal problem definition, proves NP-hardness by reduction from the Weighted Set Packing problem, and derives a Quadratic Unconstrained Binary Optimization formulation whose coefficients directly encode unique coverage rewards and pairwise overlap penalties. A single penalty parameter controls the coverage-overlap trade-off. We distinguish between a soft regime, which supports multi-objective exploration, and a hard regime, in which the penalty is strong enough to effectively enforce near-disjoint routes. We describe a practical pipeline for generating city instances, constructing candidate routes, building the QUBO matrix, and solving it with an exact mixed-integer solver (Gurobi), simulated annealing, and D-Wave hybrid quantum annealing. Experiments on Barcelona instances with up to 10 000 vehicles reveal a clear coverage-overlap knee and show that Pareto-optimal solutions are mainly obtained under the hard-penalty regime, while D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with only minor differences in runtime as problem size grows.", "AI": {"tldr": "Multi-Agent Route Planning problem formalized as NP-hard optimization with QUBO formulation, solved using exact, simulated annealing, and quantum annealing methods on Barcelona instances up to 10k vehicles.", "motivation": "To address the problem of selecting vehicles with predefined routes to maximize spatial coverage of road networks while minimizing redundant overlaps, which is relevant for applications like traffic management, surveillance, and delivery optimization.", "method": "Formal problem definition, NP-hardness proof via reduction from Weighted Set Packing, QUBO formulation with coverage rewards and overlap penalties controlled by single parameter. Practical pipeline includes instance generation, candidate route construction, QUBO matrix building, and solving with Gurobi (exact MIP), simulated annealing, and D-Wave hybrid quantum annealing.", "result": "Experiments on Barcelona instances with up to 10,000 vehicles show clear coverage-overlap trade-off knee. Pareto-optimal solutions mainly obtained under hard-penalty regime. D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with minor runtime differences as problem size grows.", "conclusion": "The QUBO formulation effectively captures the multi-agent route planning problem, with hard-penalty regime yielding Pareto-optimal solutions. Both classical (Gurobi) and quantum-inspired (D-Wave hybrid) methods perform comparably on large-scale instances, demonstrating practical viability of the approach."}}
{"id": "2602.07101", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07101", "abs": "https://arxiv.org/abs/2602.07101", "authors": ["Zinan Lv", "Yeqian Qian", "Chen Sang", "Hao Liu", "Danping Zou", "Ming Yang"], "title": "Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting", "comment": "12 pages, 8 figures", "summary": "UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.", "AI": {"tldr": "UAV navigation framework using relightable 3D Gaussian Splatting for zero-shot transfer to unstructured outdoor environments with dynamic lighting.", "motivation": "Current UAV navigation methods using monocular vision suffer from simulation-to-reality domain gap, especially with dynamic lighting conditions that couple with scene geometry in existing reconstruction methods.", "method": "End-to-end reinforcement learning framework with Relightable 3D Gaussian Splatting that decomposes scene components to enable explicit lighting editing. Training uses diverse synthesized lighting conditions in high-fidelity simulation grounded in real-world data.", "result": "Lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, with significant resilience to drastic lighting variations without fine-tuning.", "conclusion": "The proposed framework enables effective zero-shot transfer to unstructured outdoor environments by learning illumination-invariant visual features through physically grounded lighting augmentation."}}
{"id": "2602.07695", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07695", "abs": "https://arxiv.org/abs/2602.07695", "authors": ["Congcong Hu", "Yuang Shi", "Fan Huang", "Yang Xiang", "Zhou Ye", "Ming Jin", "Shiyu Wang"], "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge", "comment": null, "summary": "Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.", "AI": {"tldr": "EventCast: A modular forecasting framework that uses LLMs to process unstructured business event data into textual summaries, then fuses them with historical demand features for accurate event-driven demand forecasting in e-commerce.", "motivation": "Existing forecasting systems fail during high-impact periods like flash sales, holidays, and policy interventions where demand patterns shift abruptly and unpredictably. There's a need for systems that can incorporate future event knowledge into time-series prediction.", "method": "EventCast uses LLMs solely for event-driven reasoning - processing unstructured business data (campaigns, holidays, seller incentives) into interpretable textual summaries. These summaries are fused with historical demand features within a dual-tower architecture for scalable forecasting.", "result": "Deployed across 4 countries, 160 regions over 10 months: Achieved 86.9% and 97.7% improvement on MAE and MSE vs variant without event knowledge; reduced MAE by 57.0% and MSE by 83.3% vs best industrial baseline during event-driven periods.", "conclusion": "EventCast provides a practical solution for improving operational decision-making in dynamic e-commerce environments, successfully deployed in real-world industrial pipelines since March 2025, offering accurate, explainable, and scalable event-driven forecasting."}}
{"id": "2602.07924", "categories": ["cs.RO", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07924", "abs": "https://arxiv.org/abs/2602.07924", "authors": ["Nur Ahmad Khatim", "Mansur Arief"], "title": "Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities", "comment": null, "summary": "Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.", "AI": {"tldr": "HRCD-FLP model integrates human-robot supervision ratios into facility location planning for petroleum infrastructure security, showing cost savings with higher autonomy while maintaining critical coverage.", "motivation": "Classical facility location models fail to address the need for balancing autonomous system efficiency with human judgment in petroleum infrastructure security, particularly regarding heterogeneous human-robot supervision requirements.", "method": "Formulates Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP) with tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements, evaluated across three technology maturity scenarios.", "result": "Transitioning from conservative (1:3) to future autonomous (1:10) operations yields significant cost reduction while maintaining complete critical infrastructure coverage. Heuristic achieves feasible solutions in under 3 minutes with ~14% optimality gap for larger problems.", "conclusion": "Optimized planning for human-robot teaming is essential for achieving both cost-effective and mission-reliable deployments in petroleum infrastructure security."}}
{"id": "2602.07104", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07104", "abs": "https://arxiv.org/abs/2602.07104", "authors": ["Zhuoheng Li", "Ying Chen"], "title": "Extended to Reality: Prompt Injection in 3D Environments", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.", "AI": {"tldr": "PI3D is a novel prompt injection attack against multimodal LLMs in 3D environments using text-bearing physical objects to override intended tasks.", "motivation": "As MLLMs are increasingly deployed in 3D environments for robotics and conversational agents, attackers can place physical objects with text to manipulate model behavior, creating a new attack surface beyond digital 2D attacks.", "method": "PI3D formulates and solves the problem of identifying effective 3D object poses (position and orientation) for text-bearing physical objects that induce MLLMs to perform injected tasks while maintaining physical plausibility.", "result": "PI3D proves effective against multiple MLLMs under diverse camera trajectories, and existing defenses are shown to be insufficient against this 3D physical attack vector.", "conclusion": "The research demonstrates a critical vulnerability in MLLM deployments in physical environments, highlighting the need for new defenses against 3D prompt injection attacks using physical objects."}}
{"id": "2602.07749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07749", "abs": "https://arxiv.org/abs/2602.07749", "authors": ["Zhenyu Wu", "Yanxi Long", "Jian Li", "Hua Huang"], "title": "Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution", "comment": "ICML2026", "summary": "Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.", "AI": {"tldr": "Geo-coder is a novel inverse programming framework for geometric images using multi-agent systems, achieving superior geometric reconstruction accuracy and visual consistency while preserving geometric semantics for multimodal reasoning.", "motivation": "Current inverse graphics methods struggle with accurately reconstructing complex geometric details, often losing key geometric constraints or causing structural distortion, which limits their effectiveness in multimodal reasoning tasks.", "method": "A two-stage multi-agent framework: Stage 1 uses visual operators and large models for precise pixel coordinate and attribute capture via geometric modeling; Stage 2 employs a synthesis-rendering-validation closed loop with bidirectional visual feedback for code self-correction.", "result": "Geo-coder achieves substantial lead in geometric reconstruction accuracy and visual consistency. Reconstructed images perform equivalently to originals in multimodal reasoning tasks, validating framework robustness. Open-sourced dataset (1,500+ samples) and GeocodeLM model.", "conclusion": "Geo-coder successfully addresses inverse graphics challenges through innovative multi-agent decoupling and feedback-driven code evolution, providing robust geometric reconstruction that preserves core semantics for multimodal reasoning applications."}}
{"id": "2602.07932", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07932", "abs": "https://arxiv.org/abs/2602.07932", "authors": ["Ying-Sheng Luo", "Lu-Ching Wang", "Hanjaya Mandala", "Yu-Lun Chou", "Guilherme Christmann", "Yu-Chung Chen", "Yung-Shun Chan", "Chun-Yi Lee", "Wei-Chao Chen"], "title": "Feasibility-Guided Planning over Multi-Specialized Locomotion Policies", "comment": "ICRA 2026", "summary": "Planning over unstructured terrain presents a significant challenge in the field of legged robotics. Although recent works in reinforcement learning have yielded various locomotion strategies, planning over multiple experts remains a complex issue. Existing approaches encounter several constraints: traditional planners are unable to integrate skill-specific policies, whereas hierarchical learning frameworks often lose interpretability and require retraining whenever new policies are added. In this paper, we propose a feasibility-guided planning framework that successfully incorporates multiple terrain-specific policies. Each policy is paired with a Feasibility-Net, which learned to predict feasibility tensors based on the local elevation maps and task vectors. This integration allows classical planning algorithms to derive optimal paths. Through both simulated and real-world experiments, we demonstrate that our method efficiently generates reliable plans across diverse and challenging terrains, while consistently aligning with the capabilities of the underlying policies.", "AI": {"tldr": "A feasibility-guided planning framework for legged robots that integrates multiple terrain-specific policies with learned feasibility predictions to enable optimal path planning over challenging unstructured terrain.", "motivation": "Planning over unstructured terrain is challenging for legged robots. Existing approaches have limitations: traditional planners can't integrate skill-specific policies, while hierarchical learning frameworks lose interpretability and require retraining when adding new policies.", "method": "Proposes a feasibility-guided planning framework that pairs each terrain-specific policy with a Feasibility-Net, which learns to predict feasibility tensors from local elevation maps and task vectors. This enables classical planning algorithms to derive optimal paths.", "result": "The method efficiently generates reliable plans across diverse and challenging terrains in both simulated and real-world experiments, consistently aligning with the capabilities of the underlying policies.", "conclusion": "The proposed framework successfully integrates multiple terrain-specific policies for legged robot planning, overcoming limitations of existing approaches by maintaining interpretability and avoiding retraining when adding new policies."}}
{"id": "2602.07106", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07106", "abs": "https://arxiv.org/abs/2602.07106", "authors": ["Haoyu Zhang", "Zhipeng Li", "Yiwen Guo", "Tianshu Yu"], "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models", "comment": null, "summary": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.", "AI": {"tldr": "Ex-Omni is an open-source omni-modal framework that enables LLMs to generate speech with synchronized 3D facial animation by decoupling semantic reasoning from temporal generation using speech units as scaffolding.", "motivation": "Current OLLMs lack integration of speech with 3D facial animation, which is crucial for natural human-computer interaction. The representation mismatch between discrete LLM reasoning and dense temporal facial dynamics makes direct modeling difficult under limited data.", "method": "Ex-Omni decouples semantic reasoning from temporal generation using speech units as temporal scaffolding. It employs a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. The team also created InstructEx dataset to facilitate training.", "result": "Ex-Omni performs competitively against existing open-source OLLMs while enabling stable, aligned speech and facial animation generation, demonstrating effective multimodal integration.", "conclusion": "Ex-Omni successfully addresses the challenge of integrating speech with 3D facial animation in OLLMs through a decoupled approach, enabling more natural multimodal interaction capabilities."}}
{"id": "2602.07754", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07754", "abs": "https://arxiv.org/abs/2602.07754", "authors": ["Bahare Riahi", "Veronica Catete"], "title": "Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency", "comment": "13 pages, 3 figures", "summary": "This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.", "AI": {"tldr": "Students in CS course express concerns about AI grading systems lacking contextual understanding and personalization; recommend AI as supplementary tools with human oversight.", "motivation": "To investigate student perceptions of AI grading systems in computer science education, focusing on ethical principles of fairness, trust, consistency, and transparency.", "method": "Study of 27 undergraduate CS students comparing AI-generated feedback with human-graded feedback on block-based programming projects, using Jobin's (2019) ethical principles framework.", "result": "Students identified concerns about AI's lack of contextual understanding and personalization; AI systems were seen as needing to reflect human judgment, flexibility, and empathy.", "conclusion": "AI grading systems should serve as supplementary tools under human oversight, with design principles that humanize AI in learning environments while amplifying student voices."}}
{"id": "2602.07984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07984", "abs": "https://arxiv.org/abs/2602.07984", "authors": ["Simon Sagmeister", "Panagiotis Kounatidis", "Sven Goblirsch", "Markus Lienkamp"], "title": "Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control", "comment": "Accepted for publication at the IEEE IV 2024", "summary": "Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.", "AI": {"tldr": "This paper investigates how different levels of vehicle dynamics model fidelity affect trajectory-following controller performance in autonomous driving simulations, using real-world racing data for validation.", "motivation": "Current autonomous driving research uses vehicle models with varying fidelity levels, making it difficult to compare control algorithms. There's a need to understand how model simplification affects controller evaluation.", "method": "Developed a comprehensive Autoware-compatible vehicle model and created simplified versions with varying fidelity. Conducted over 550 simulation runs comparing models against real-world racing data from the Indy Autonomous Challenge at Monza, analyzing performance at different acceleration margins.", "result": "Quantified each model's approximation quality compared to real-world data. Found that the influence of model simplifications changes with varying margins to the vehicle's acceleration limit, revealing how much simplification is acceptable for different applications.", "conclusion": "The study provides guidelines for appropriate vehicle model simplification levels when evaluating control algorithms, depending on specific application requirements and operating conditions near acceleration limits."}}
{"id": "2602.07149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07149", "abs": "https://arxiv.org/abs/2602.07149", "authors": ["Rawisara Lohanimit", "Yankun Wu", "Amelia Katirai", "Yuta Nakashima", "Noa Garcia"], "title": "Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds", "comment": null, "summary": "The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.", "AI": {"tldr": "Researchers found thousands of pregnancy ultrasound images with sensitive personal information in LAION-400M dataset, posing privacy risks, and recommend better dataset curation practices.", "motivation": "The use of large-scale internet datasets with minimal curation raises concerns about inclusion of sensitive private information, particularly pregnancy ultrasound images which contain highly personal data.", "method": "Systematic examination of LAION-400M dataset using CLIP embedding similarity to retrieve pregnancy ultrasound images and detect private information like names and locations.", "result": "Found thousands of pregnancy ultrasound images containing private information, with multiple images having high-risk data that could enable re-identification or impersonation.", "conclusion": "Recommends improved practices for dataset curation, data privacy, and ethical use of public image datasets to protect sensitive personal information."}}
{"id": "2602.07755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07755", "abs": "https://arxiv.org/abs/2602.07755", "authors": ["Yiming Xiong", "Shengran Hu", "Jeff Clune"], "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs", "comment": null, "summary": "The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.", "AI": {"tldr": "ALMA is a framework that meta-learns memory designs for AI agents to replace hand-engineered memory, enabling continual learning across diverse domains.", "motivation": "Foundation models are stateless, limiting agents' continual learning capabilities. Existing memory designs are human-crafted and fixed, unable to adapt to diverse, non-stationary real-world tasks.", "method": "Uses a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, allowing discovery of arbitrary memory designs including database schemas and their retrieval/update mechanisms.", "result": "Extensive experiments across four sequential decision-making domains show learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted designs on all benchmarks.", "conclusion": "ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners when developed and deployed safely."}}
{"id": "2602.08116", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08116", "abs": "https://arxiv.org/abs/2602.08116", "authors": ["Jiawei Xu", "Subhrajit Bhattacharya", "David Salda\u00f1a"], "title": "From Ellipsoids to Midair Control of Dynamic Hitches", "comment": null, "summary": "The ability to dynamically manipulate interaction between cables, carried by pairs of aerial vehicles attached to the ends of each cable, can greatly improve the versatility and agility of cable-assisted aerial manipulation. Such interlacing cables create hitches by winding two or more cables around each other, which can enclose payloads or can further develop into knots. Dynamic modeling and control of such hitches is key to mastering the inter-cable manipulation in context of cable-suspended aerial manipulation. This paper introduces an ellipsoid-based kinematic model to connect the geometric nature of a hitch created by two cables and the dynamics of the hitch driven by four aerial vehicles, which reveals the control-affine form of the system. As the constraint for maintaining tension of a cable is also control-affine, we design a quadratic programming-based controller that combines Control Lyapunov and High-Order Control Barrier Functions (CLF-HOCBF-QP) to precisely track a desired hitch position and system shape while enforcing safety constraints like cable tautness. We convert desired geometric reference configurations into target robot positions and introduce a composite error into the Lyapunov function to ensure a relative degree of one to the input. Numerical simulations validate our approach, demonstrating stable, high-speed tracking of dynamic references.", "AI": {"tldr": "Dynamic modeling and control of cable hitches formed by interweaving cables carried by aerial vehicles for improved manipulation versatility.", "motivation": "Dynamic manipulation of interlacing cables between aerial vehicles can enhance versatility and agility in cable-assisted aerial manipulation, enabling hitches that can enclose payloads or form knots.", "method": "Developed ellipsoid-based kinematic model connecting hitch geometry to dynamics, designed CLF-HOCBF-QP controller using quadratic programming with Control Lyapunov and High-Order Control Barrier Functions to track hitch position while maintaining cable tension.", "result": "Numerical simulations demonstrate stable, high-speed tracking of dynamic references while enforcing safety constraints like cable tautness.", "conclusion": "The approach enables precise control of cable hitches for aerial manipulation, combining geometric modeling with safety-constrained control for versatile cable manipulation capabilities."}}
{"id": "2602.07174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07174", "abs": "https://arxiv.org/abs/2602.07174", "authors": ["Yongheng Sun", "Jun Shu", "Jianhua Ma", "Fan Wang"], "title": "DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages", "comment": null, "summary": "Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \\emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.", "AI": {"tldr": "DuMeta++ is a dual meta-learning framework for brain tissue segmentation that achieves cross-age generalization without requiring paired longitudinal data.", "motivation": "Brain tissue segmentation from MRI scans is crucial for neuroscience and clinical applications, but existing methods struggle with age-related appearance changes. While self-supervised regularization with paired longitudinal data helps, such data is often unavailable in practice.", "method": "DuMeta++ integrates: (1) meta-feature learning to extract age-agnostic semantic representations of evolving brain structures, (2) meta-initialization learning for data-efficient model adaptation, and (3) memory-bank-based class-aware regularization to enforce longitudinal consistency without explicit supervision.", "result": "Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings show DuMeta++ outperforms existing methods in cross-age generalization. The approach is theoretically proven to converge.", "conclusion": "DuMeta++ provides an effective solution for brain tissue segmentation across the human lifespan without requiring paired longitudinal data, demonstrating superior cross-age generalization capabilities."}}
{"id": "2602.07765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07765", "abs": "https://arxiv.org/abs/2602.07765", "authors": ["Zhirong Huang", "Debo Cheng", "Guixian Zhang", "Yi Wang", "Jiuyong Li", "Shichao Zhang"], "title": "Disentangled Instrumental Variables for Causal Inference with Networked Observational Data", "comment": null, "summary": "Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\\underline{Dis}$entangled $\\underline{I}$nstrumental $\\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.", "AI": {"tldr": "DisIV is a novel framework for causal inference in networked data that disentangles individual-specific components to create valid instrumental variables, overcoming the challenge of network-induced confounding.", "motivation": "Instrumental variables are essential for addressing unobservable confounders, but their exogeneity assumptions are particularly challenging in networked data where existing methods mix endogenous correlations and exogenous variation, leading to invalid IVs.", "method": "DisIV uses network homogeneity as inductive bias and employs structural disentanglement to extract individual-specific components as latent IVs, with causal validity enforced through explicit orthogonality and exclusion conditions.", "result": "Extensive semi-synthetic experiments on real-world datasets show DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.", "conclusion": "The DisIV framework successfully addresses the challenge of obtaining valid instrumental variables in networked observational data by disentangling individual-specific components, enabling more accurate causal inference in the presence of latent confounders."}}
{"id": "2602.08167", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08167", "abs": "https://arxiv.org/abs/2602.08167", "authors": ["Milan Ganai", "Katie Luo", "Jonas Frey", "Clark Barrett", "Marco Pavone"], "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning", "comment": null, "summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.", "AI": {"tldr": "R&B-EnCoRe enables VLA models to bootstrap embodied reasoning from internet knowledge through self-supervised refinement, treating reasoning as a latent variable to distill embodiment-specific strategies without external supervision.", "motivation": "Current embodied CoT methods use rigid templates that force policies to process irrelevant information, creating a bottleneck where poor reasoning leads to poor policies and vice versa.", "method": "Uses importance-weighted variational inference to treat reasoning as a latent variable, enabling models to generate and distill refined reasoning datasets without rewards, verifiers, or human annotation.", "result": "Achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate across various embodiments and VLA architectures (1B-30B parameters).", "conclusion": "R&B-EnCoRe enables models to distill reasoning predictive of successful control, bypassing manual annotation while grounding internet-scale knowledge in physical execution."}}
{"id": "2602.07198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07198", "abs": "https://arxiv.org/abs/2602.07198", "authors": ["Heyuan Li", "Huimin Zhang", "Yuda Qiu", "Zhengwentai Sun", "Keru Zheng", "Lingteng Qiu", "Peihao Li", "Qi Zuo", "Ce Chen", "Yujian Zheng", "Yuming Gu", "Zilong Dong", "Xiaoguang Han"], "title": "Condition Matters in Full-head 3D GANs", "comment": "Accepted by ICLR 2026. Project page: https://lhyfst.github.io/balancehead/", "summary": "Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.", "AI": {"tldr": "Proposes using view-invariant semantic features instead of view angles as conditioning for 3D GANs to eliminate directional bias and improve global coherence in full-head generation.", "motivation": "Traditional view-angle conditioning in full-head 3D GANs creates bias along the conditional view direction, causing quality/diversity disparities between conditional and non-conditional views and global incoherence across head regions.", "method": "Uses view-invariant semantic features as conditioning input. Creates synthesized head image dataset using FLUX.1 Kontext to extend frontal face datasets to multiple views. Uses frontal view image clip features as shared semantic condition across all views, enabling semantic alignment and eliminating directional bias.", "result": "Achieves significantly higher fidelity, diversity, and generalizability in full-head synthesis and single-view GAN inversion compared to previous methods.", "conclusion": "Semantic conditioning decouples generative capability from viewing direction, accelerates training, enhances global coherence, and promotes continuous learning for diverse generation in 3D head GANs."}}
{"id": "2602.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07787", "abs": "https://arxiv.org/abs/2602.07787", "authors": ["Pierre-Louis Favreau", "Jean-Pierre Lo", "Clement Guiguet", "Charles Simon-Meunier", "Nicolas Dehandschoewercker", "Allen G. Roush", "Judah Goldfeder", "Ravid Shwartz-Ziv"], "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition", "comment": null, "summary": "We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use", "AI": {"tldr": "Minitap is a multi-agent system that achieves 100% success on AndroidWorld benchmark, solving all 116 tasks and surpassing human performance (80%) through specialized agents, verified execution, and meta-cognitive reasoning.", "motivation": "The paper aims to solve mobile task automation challenges where single-agent architectures fail due to context pollution from mixed reasoning traces, silent text input failures undetected by agents, and repetitive action loops without escape mechanisms.", "method": "Minitap uses a multi-agent system with six specialized agents for cognitive separation, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes.", "result": "Achieves 100% success on AndroidWorld benchmark (116 tasks), surpassing human performance (80%). Ablations show multi-agent decomposition contributes +21 points over single-agent baselines, verified execution adds +7 points, and meta-cognition adds +9 points.", "conclusion": "Minitap demonstrates that targeted multi-agent decomposition with specialized components, verified execution, and meta-cognitive reasoning can fully solve mobile automation tasks, achieving perfect performance and releasing as open-source software."}}
{"id": "2602.08189", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08189", "abs": "https://arxiv.org/abs/2602.08189", "authors": ["Seoyeon Jang", "Alex Junho Lee", "I Made Aswin Nahrendra", "Hyun Myung"], "title": "Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments", "comment": "8 pages, IEEE Robot. Automat. Lett. (RA-L) 2026", "summary": "Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.", "AI": {"tldr": "Dual-head network for online change detection and map maintenance in dynamic environments using synthetic data augmentation", "motivation": "Mobile robots need efficient navigation in dynamic environments like construction sites, but existing approaches struggle with change detection and map updates due to occlusions and spatiotemporal variations", "method": "Propose dual-head network for online change detection and long-term map maintenance, with data augmentation strategy that synthesizes structural changes by importing elements from different scenes", "result": "Approach generalizes well across diverse scenarios (construction sites and indoor offices), achieving efficient and accurate map updates", "conclusion": "The proposed method effectively addresses change detection challenges in dynamic environments without requiring extensive ground-truth annotations"}}
{"id": "2602.07212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07212", "abs": "https://arxiv.org/abs/2602.07212", "authors": ["Xinyu Liu", "Darryl C. Jacob", "Yuxin Liu", "Xinsong Du", "Muchao Ye", "Bolei Zhou", "Pan He"], "title": "Understanding Real-World Traffic Safety through RoadSafe365 Benchmark", "comment": null, "summary": "Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.", "AI": {"tldr": "RoadSafe365 is a large-scale vision-language benchmark for fine-grained traffic safety analysis, with 36K annotated video clips and 864K question-answer options aligned with official safety standards.", "motivation": "Existing traffic benchmarks lack systematic evaluation aligned with official safety standards and focus primarily on coarse accident identification rather than comprehensive safety analysis.", "method": "Created a hierarchical taxonomy refining crash, incident, and violation definitions; curated 36,196 annotated clips from dashcam/surveillance cameras with rich attribute annotations; paired clips with multiple-choice QA sets (864K options, 8.4K unique answers, 36K scene descriptions).", "result": "Established strong baselines showing consistent gains when fine-tuning on RoadSafe365; cross-domain experiments on real and synthetic datasets validated effectiveness; benchmark supports large-scale training and standardized evaluation.", "conclusion": "RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis by bridging official standards with data-driven systems."}}
{"id": "2602.07824", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07824", "abs": "https://arxiv.org/abs/2602.07824", "authors": ["Yiwei Qin", "Zhen Huang", "Tiantian Mi", "Weiye Si", "Chenyang Zhou", "Qipeng Guo", "Siyuan Feng", "Pengfei Liu"], "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training", "comment": null, "summary": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.", "AI": {"tldr": "Data Darwinism introduces a 10-level taxonomy for data-model co-evolution, showing that advanced models can produce better training data for next-generation systems, validated on scientific literature with significant performance gains.", "motivation": "Data quality is critical for foundation model performance, but there's a lack of systematic frameworks for data processing. The authors aim to establish a principled approach to data-model co-evolution where better models can create better training data for subsequent generations.", "method": "Created Data Darwinism taxonomy (L0-L9) and built Darwin-Science corpus (900B tokens, L0-L5). Used frontier LLMs for L4 (Generative Refinement) and L5 (Cognitive Completion) to bridge learnability gaps in raw scientific text. Pre-trained contamination-free baseline models (daVinci-origin-3B/7B) and conducted continued pre-training on Darwin-Science corpus.", "result": "After 600B tokens of training, Darwin-Science outperformed baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, with domain-aligned tasks showing even larger gains (+5.60 and +8.40 points). Systematic progression to L5 yielded +1.36 total gain, confirming higher-level processing unlocks latent data value.", "conclusion": "Data Darwinism provides a systematic framework for data-model co-evolution, demonstrating that higher-level data processing significantly improves model performance. The released Darwin-Science corpus and daVinci-origin models enable principled development of co-evolutionary systems."}}
{"id": "2602.08245", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08245", "abs": "https://arxiv.org/abs/2602.08245", "authors": ["Jinhao Li", "Yuxuan Cong", "Yingqiao Wang", "Hao Xia", "Shan Huang", "Yijia Zhang", "Ningyi Xu", "Guohao Dai"], "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction", "comment": "13 pages, 9 figures", "summary": "Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.", "AI": {"tldr": "STEP accelerates diffusion policies for robotics by predicting warm-start actions with spatiotemporal consistency and adaptive velocity-aware perturbation, achieving higher success rates with fewer diffusion steps.", "motivation": "Diffusion policies are powerful for robotic manipulation but suffer from high inference latency due to iterative denoising, limiting real-time control. Existing acceleration methods struggle to balance low latency with action quality preservation.", "method": "1) Lightweight spatiotemporal consistency prediction to construct high-quality warm-start actions close to target distribution. 2) Velocity-aware perturbation injection that adaptively modulates actuation based on temporal action variation to prevent execution stall. 3) Theoretical analysis showing prediction induces locally contractive mapping for convergence.", "result": "STEP with only 2 steps achieves 21.6% higher success rate than BRIDGER and 27.5% higher than DDIM on RoboMimic benchmark and real-world tasks. Consistently advances Pareto frontier of inference latency vs success rate across 9 simulated benchmarks and 2 real-world tasks.", "conclusion": "STEP effectively accelerates diffusion policies while preserving action quality, enabling real-time robotic control with high success rates through spatiotemporal consistency prediction and adaptive perturbation mechanisms."}}
{"id": "2602.07251", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07251", "abs": "https://arxiv.org/abs/2602.07251", "authors": ["Haley Duba-Sullivan", "Steven R. Young", "Emma J. Reid"], "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models", "comment": null, "summary": "Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.", "AI": {"tldr": "AdvSR embeds adversarial behavior directly into super-resolution model weights during training, enabling attacks on downstream tasks without input manipulation at inference time.", "motivation": "Super-resolution models are commonly used as preprocessing steps in imaging pipelines, but they introduce an unexplored attack surface. Current attacks focus on input perturbations or backdoor triggers, but model-level attacks remain unaddressed.", "method": "AdvSR jointly optimizes for both reconstruction quality and targeted adversarial outcomes during training. The framework embeds adversarial behavior directly into SR model weights, requiring no access to inputs at inference time. Evaluated on three SR architectures (SRCNN, EDSR, SwinIR) paired with YOLOv11 classifier.", "result": "AdvSR models achieve high attack success rates with minimal quality degradation, appearing benign under standard image quality metrics while effectively inducing downstream misclassification.", "conclusion": "AdvSR reveals a new model-level threat for imaging pipelines, highlighting security implications for how practitioners source and validate models in safety-critical applications."}}
{"id": "2602.07830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07830", "abs": "https://arxiv.org/abs/2602.07830", "authors": ["Jiahui Zhou", "Dan Li", "Boxin Li", "Xiao Zhang", "Erli Meng", "Lin Li", "Zhuomin Chen", "Jian Lou", "See-Kiong Ng"], "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning", "comment": null, "summary": "Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.", "AI": {"tldr": "VeriTime is a framework that enhances LLMs for time series reasoning through synthetic data generation, intelligent data scheduling, and reinforcement learning with process-level verification.", "motivation": "Current LLMs struggle with time series reasoning due to lack of curated Chain-of-Thought data, inefficient data scheduling, and absence of RL algorithms specifically designed for time series CoT reasoning.", "method": "Three-stage approach: 1) Data synthesis pipeline creating TS-text multimodal dataset with verifiable annotations, 2) Data scheduling mechanism organizing training by difficulty hierarchy and task taxonomy, 3) Two-stage reinforcement finetuning with multi-objective rewards using process-level CoT data.", "result": "VeriTime substantially boosts LLM performance across diverse time series reasoning tasks, enabling compact 3B-4B models to achieve reasoning capabilities comparable to or exceeding larger proprietary LLMs.", "conclusion": "The framework successfully addresses key challenges in applying LLM reasoning to time series tasks through systematic data creation, organization, and reinforcement learning, demonstrating significant performance improvements with smaller models."}}
{"id": "2602.08251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08251", "abs": "https://arxiv.org/abs/2602.08251", "authors": ["Yuanzhu Zhan", "Yufei Jiang", "Muqing Cao", "Junyi Geng"], "title": "Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control", "comment": "9 pages, 7 figures. Accepted by ICRA 2026", "summary": "Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.", "AI": {"tldr": "Onboard perception-control pipeline for aerial manipulation without motion capture, using contact-augmented VIO and visual servoing for stable contact interactions.", "motivation": "Current aerial manipulation systems rely on external motion capture and position control, limiting deployability for contact-rich tasks in real-world environments.", "method": "Two main components: (1) Augmented VIO with contact-consistency factors that activate during interaction to reduce drift, and (2) Image-based visual servoing with hybrid force-motion controller for regulated contact wrenches and lateral motion.", "result": "66.01% improvement in velocity estimation at contact, reliable target approach, and stable force holding using only onboard sensing.", "conclusion": "The approach enables deployable, in-the-wild aerial manipulation by closing the perception-to-wrench loop with fully onboard sensing, eliminating need for external motion capture."}}
{"id": "2602.07260", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07260", "abs": "https://arxiv.org/abs/2602.07260", "authors": ["Hongyu Kan", "Kristofor Pas", "Ivan Medri", "Naqib Sad Pathan", "Natasha Ironside", "Shinjini Kundu", "Jingjia He", "Gustavo Kunde Rohde"], "title": "3D Transport-based Morphometry (3D-TBM) for medical image analysis", "comment": null, "summary": "Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.", "AI": {"tldr": "3D-TBM is a new tool for analyzing 3D medical images using transport-based morphometry, enabling classification/regression with spatial interpretability through invertible transformations.", "motivation": "To facilitate broader adoption of Transport-Based Morphometry (TBM) in clinical imaging research by providing an accessible tool for morphological analysis of 3D medical images.", "method": "Developed 3D-TBM framework with data preprocessing, optimal transport embeddings computation, analytical methods including visualization of main transport directions, and techniques for discerning discriminating directions.", "result": "Created a comprehensive tool with source code publicly available through PyTransKit, including documentation and practical tutorials to support researchers in applying 3D-TBM to medical imaging studies.", "conclusion": "3D-TBM provides an accessible framework for transport-based morphometry analysis of 3D medical images, enabling spatially interpretable results and supporting broader adoption in clinical imaging research."}}
{"id": "2602.07849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07849", "abs": "https://arxiv.org/abs/2602.07849", "authors": ["Xin Wang", "Hualin Zhou", "Sheng Guang Wang", "Ting Dang", "Yu Zhang", "Hong Jia", "Tao Gu"], "title": "LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge", "comment": "16 pages, 9 figures ,9 tables, preprint", "summary": "Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.", "AI": {"tldr": "LQA is a lightweight quantized-adaptive framework for VLMs that enables efficient on-device deployment through selective hybrid quantization and gradient-free test-time adaptation, achieving 4.5% performance improvement with 19.9\u00d7 lower memory usage.", "motivation": "Vision-Language Models face deployment challenges on edge devices due to resource constraints and performance degradation under distribution shifts. Existing test-time adaptation methods are too resource-intensive for on-device use.", "method": "Proposes LQA framework combining modality-aware quantization (Selective Hybrid Quantization) with gradient-free test-time adaptation. SHQ selectively quantizes different modalities, and the adaptation mechanism operates without gradients for efficiency.", "result": "Improves adaptation performance by 4.5%, uses less memory than full-precision models, and outperforms gradient-based TTA methods with up to 19.9\u00d7 lower memory usage across seven open-source datasets under synthetic and real-world distribution shifts.", "conclusion": "LQA provides a practical solution for robust, privacy-preserving, and efficient VLM deployment on edge devices by addressing both quantization efficiency and adaptation effectiveness in resource-constrained environments."}}
{"id": "2602.08266", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08266", "abs": "https://arxiv.org/abs/2602.08266", "authors": ["Seunghoon Jeong", "Eunho Lee", "Jeongyun Kim", "Ayoung Kim"], "title": "Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes", "comment": "9 pages, 8 figures, 4 tables, accepted to ICRA 2026", "summary": "In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.", "AI": {"tldr": "Instance-aware Next Best View policy using object-aware 3D Gaussian Splatting to prioritize underexplored regions and improve reconstruction in cluttered scenes.", "motivation": "Existing NBV approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and prioritize exploitation over exploration, limiting their effectiveness in cluttered scenes with occlusions.", "method": "Object-aware 3DGS distills instance-level information into one-hot object vectors to compute confidence-weighted information gain, guiding identification of regions with erroneous/uncertain Gaussians. Can be adapted to object-centric NBV focusing on target objects.", "result": "Reduces depth error by up to 77.14% on synthetic dataset and 34.10% on real-world GraspNet dataset compared to baselines. Object-centric NBV yields additional 25.60% reduction in depth error for target objects.", "conclusion": "Instance-aware NBV policy leveraging object features effectively improves reconstruction quality in cluttered scenes and enables robust robotic manipulation tasks."}}
{"id": "2602.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07262", "abs": "https://arxiv.org/abs/2602.07262", "authors": ["Junbo Jacob Lian", "Feng Xiong", "Yujun Sun", "Kaichen Ouyang", "Mingyang Yu", "Shengwei Fu", "Zhong Rui", "Zhang Yujun", "Huiling Chen"], "title": "TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition", "comment": "Code is available at https://github.com/junbolian/TwistNet-2D", "summary": "Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \\emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \\TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.", "AI": {"tldr": "TwistNet-2D is a lightweight module that captures local pairwise channel interactions with directional spatial displacement for texture recognition, outperforming larger models with minimal computational overhead.", "motivation": "Existing methods for texture recognition have limitations: bilinear pooling and Gram matrices capture global channel correlations but lose spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions.", "method": "Introduces TwistNet-2D with Spiral-Twisted Channel Interaction (STCI) that shifts one feature map along prescribed directions before element-wise channel multiplication to capture cross-position co-occurrence patterns. Uses four directional heads with learned channel reweighting and sigmoid-gated residual path.", "result": "Adds only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines (ConvNeXt, Swin Transformer, hybrid CNN-Transformer architectures) across four texture and fine-grained recognition benchmarks.", "conclusion": "TwistNet-2D effectively captures local pairwise channel interactions with directional spatial displacement, providing an efficient solution for texture and fine-grained recognition that outperforms more complex architectures with minimal computational overhead."}}
{"id": "2602.07852", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07852", "abs": "https://arxiv.org/abs/2602.07852", "authors": ["Anna Soligo", "Edward Turner", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard", "comment": "Published at ICLR 2026", "summary": "Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.", "AI": {"tldr": "Finetuning LLMs on narrow harmful datasets causes emergent misalignment across unrelated settings, revealing poor understanding of LLM inductive biases. The study finds general misalignment solutions are more stable/efficient than narrow ones, isolating a linear representation for monitoring/mitigation.", "motivation": "To understand why finetuning LLMs on narrow harmful datasets causes emergent misalignment across diverse unrelated settings, and to investigate the inductive biases governing learning and generalization in LLMs that experts failed to predict.", "method": "Used emergent misalignment as a case study to compare narrow vs. general solutions; identified linear representations of both types of misalignment; introduced KL divergence loss to learn narrow solution representation; analyzed stability, efficiency, and influence in pre-training distribution.", "result": "General misalignment achieves lower loss, is more robust to perturbations, and is more influential in pre-training distribution compared to narrow solutions. Different finetunes converge to the same linear representation of general misalignment.", "conclusion": "The work isolates a concrete representation of general misalignment for monitoring and mitigation, provides a detailed case study and preliminary metrics for investigating inductive biases in LLM generalization, and opensources all code, datasets, and model finetunes."}}
{"id": "2602.08278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08278", "abs": "https://arxiv.org/abs/2602.08278", "authors": ["Ke Zhang", "Lixin Xu", "Chengyi Song", "Junzhe Xu", "Xiaoyi Lin", "Zeyu Jiang", "Renjing Xu"], "title": "DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer", "comment": null, "summary": "Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.", "AI": {"tldr": "DexFormer is a transformer-based policy that enables zero-shot cross-embodiment dexterous manipulation across different robotic hands without retraining.", "motivation": "Current dexterous manipulation methods struggle with embodiment variability - different robotic hands have distinct kinematics and dynamics, requiring separate policies or per-embodiment decoders, which limits scalability and generalization.", "method": "DexFormer uses a modified transformer backbone that conditions on historical observations to infer morphology and dynamics on the fly, allowing adaptation to diverse hand configurations and producing embodiment-appropriate control actions.", "result": "Trained on procedurally generated dexterous-hand assets, DexFormer achieves strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand, demonstrating that a single policy can generalize across heterogeneous hand embodiments.", "conclusion": "DexFormer establishes a scalable foundation for cross-embodiment dexterous manipulation by enabling a single policy to adapt to different robotic hand embodiments without retraining."}}
{"id": "2602.07272", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07272", "abs": "https://arxiv.org/abs/2602.07272", "authors": ["Bowen Xue", "Saeed Hadadan", "Zheng Zeng", "Fabrice Rousselle", "Zahra Montazeri", "Milos Hasan"], "title": "VideoNeuMat: Neural Material Extraction from Generative Video Models", "comment": null, "summary": "Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a \"virtual gonioreflectometer\" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.", "AI": {"tldr": "VideoNeuMat extracts reusable neural material assets from video diffusion models through a two-stage pipeline: first generating material videos under controlled conditions, then reconstructing compact neural materials that generalize to novel viewing/lighting.", "motivation": "Creating photorealistic materials for 3D rendering requires exceptional artistic skill, and generative models for materials are limited by lack of high-quality training data. While video generative models produce realistic material appearances, this knowledge remains entangled with geometry and lighting.", "method": "Two-stage pipeline: 1) Finetune large video model (Wan 2.1 14B) to generate material sample videos under controlled camera/lighting trajectories, creating a \"virtual gonioreflectometer.\" 2) Reconstruct compact neural materials from these videos using Large Reconstruction Model (LRM) finetuned from smaller Wan 1.3B video backbone, performing single-pass inference from 17 frames.", "result": "The resulting materials exhibit realism and diversity far exceeding limited synthetic training data, demonstrating successful transfer of material knowledge from internet-scale video models into standalone, reusable neural 3D assets.", "conclusion": "Material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets, enabling creation of photorealistic materials without requiring exceptional artistic skill."}}
{"id": "2602.07883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07883", "abs": "https://arxiv.org/abs/2602.07883", "authors": ["Jingqi Zhou", "Sheng Wang", "DeZhao Deng", "Junwen Lu", "Junwei Su", "Qintong Li", "Jiahui Gao", "Hao Wu", "Jiyue Jiang", "Lingpeng Kong", "Chuan Wu"], "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation", "comment": null, "summary": "Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.", "AI": {"tldr": "ToolSelf enables LLM agents to self-reconfigure at runtime by treating configuration updates as callable tools, achieving 24.1% performance gains through autonomous adaptation to task dynamics.", "motivation": "Current LLM agents have static configurations fixed before execution, failing to adapt to evolving task dynamics. Existing manual orchestration or heuristic-based approaches suffer from poor generalization and fragmented optimization.", "method": "ToolSelf abstracts configuration updates as callable tools, unifying task execution and self-adjustment into a single action space. Uses Configuration-Aware Two-stage Training (CAT) combining rejection sampling fine-tuning with trajectory-level reinforcement learning.", "result": "Extensive experiments show ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving 24.1% average performance gain across diverse benchmarks.", "conclusion": "ToolSelf transforms agents from passive executors into dual managers of both task and self, illuminating a path toward truly self-adaptive agents through runtime self-reconfiguration."}}
{"id": "2602.08285", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08285", "abs": "https://arxiv.org/abs/2602.08285", "authors": ["Josh Pinskier", "Sarah Baldwin", "Stephen Rodan", "David Howard"], "title": "ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects", "comment": null, "summary": "Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.", "AI": {"tldr": "ReefFlex is a generative design method for soft fingers that safely grasps fragile, irregular corals using motion primitives, enabling automated coral handling for reef restoration.", "motivation": "Coral reefs are rapidly declining due to climate change, invasive species, and human activities, threatening biodiversity and coastal protection. Current coral regeneration efforts are hindered by the lack of safe tools to handle fragile corals, which impedes breeding climate-resilient species and accelerating natural regrowth processes.", "method": "ReefFlex uses a generative soft finger design methodology that explores diverse soft finger designs to find candidates capable of safely grasping fragile, geometrically heterogeneous corals in cluttered environments. The key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified multi-objective optimization problem.", "result": "ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) while reducing adverse events during coral manipulation compared to reference designs. The method was demonstrated through a soft robot designed for reef rehabilitation that grows and manipulates coral in onshore aquaculture facilities.", "conclusion": "ReefFlex offers a generalizable method for designing soft end-effectors for complex handling tasks and paves the way toward automation in previously unachievable domains like coral handling for restoration, addressing critical challenges in coral reef conservation."}}
{"id": "2602.07277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07277", "abs": "https://arxiv.org/abs/2602.07277", "authors": ["Rishabh Sharma", "Gijs Hogervorst", "Wayne E. Mackey", "David J. Heeger", "Stefano Martiniani"], "title": "Cross-View World Models", "comment": "12 pages, 7 figures", "summary": "World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.", "AI": {"tldr": "Cross-View World Models (XVWM) enable agents to plan from multiple viewpoints by learning to predict future states across different camera perspectives, using cross-view consistency as geometric regularization.", "motivation": "Existing world models operate from a single egocentric viewpoint, even though other perspectives (like bird's-eye views) could make planning easier for tasks like navigation. The authors want to enable agents to plan in whichever frame of reference best suits the task while executing from the egocentric view.", "method": "Train Cross-View World Models with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or different viewpoint after an action. Use synchronized multi-view gameplay data from Aimlabs with precisely aligned multi-camera recordings and high-frequency action labels. Enforce cross-view consistency as geometric regularization to learn view-invariant representations of 3D structure.", "result": "The model provides agents with parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task. Multi-view consistency provides a strong learning signal for spatially grounded representations.", "conclusion": "Cross-view prediction enables agents to plan from multiple perspectives while executing egocentrically. The approach offers a foundation for perspective-taking in multi-agent settings by predicting action consequences from another viewpoint."}}
{"id": "2602.07885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07885", "abs": "https://arxiv.org/abs/2602.07885", "authors": ["Zhenyuan Zhang", "Xianzhang Jia", "Zhiqin Yang", "Zhenbo Song", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck", "comment": null, "summary": "Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.", "AI": {"tldr": "MemFly is a memory evolution framework for LLM agents that balances compression efficiency with retrieval precision using information bottleneck principles and hybrid retrieval mechanisms.", "motivation": "Existing memory frameworks face a fundamental dilemma: they must compress redundant information efficiently while maintaining precise retrieval for downstream tasks, creating a trade-off between storage efficiency and task performance.", "method": "MemFly uses information bottleneck principles with gradient-free optimization to minimize compression entropy while maximizing relevance entropy, creating stratified memory structures. It employs hybrid retrieval with semantic, symbolic, and topological pathways plus iterative refinement for complex queries.", "result": "Comprehensive experiments show MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.", "conclusion": "MemFly successfully bridges the gap between efficient memory compression and precise retrieval through information bottleneck principles and hybrid retrieval mechanisms, enabling better long-term memory for LLM agents."}}
{"id": "2602.08298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08298", "abs": "https://arxiv.org/abs/2602.08298", "authors": ["Yuxin Zhang", "Cheng Wang", "Hubert P. H. Shum"], "title": "Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework", "comment": null, "summary": "Autonomous vehicles (AVs) are poised to revolutionize global transportation systems. However, its widespread acceptance and market penetration remain significantly below expectations. This gap is primarily driven by persistent challenges in safety, comfort, commuting efficiency and energy economy when compared to the performance of experienced human drivers. We hypothesize that these challenges can be addressed through the development of a driver foundation model (DFM). Accordingly, we propose a framework for establishing DFMs to comprehensively benchmark AVs. Specifically, we describe a large-scale dataset collection strategy for training a DFM, discuss the core functionalities such a model should possess, and explore potential technical solutions to realize these functionalities. We further present the utility of the DFM across the operational spectrum, from defining human-centric safety envelopes to establishing benchmarks for energy economy. Overall, We aim to formalize the DFM concept and introduce a new paradigm for the systematic specification, verification and validation of AVs.", "AI": {"tldr": "The paper proposes a driver foundation model (DFM) framework to address autonomous vehicle challenges in safety, comfort, efficiency, and energy economy by creating comprehensive benchmarks and systematic validation approaches.", "motivation": "Autonomous vehicles have not achieved expected market penetration due to persistent performance gaps compared to human drivers in safety, comfort, efficiency, and energy economy. Current AV systems fall short of human driver capabilities, creating barriers to widespread adoption.", "method": "The authors propose a framework for establishing driver foundation models (DFMs) including: 1) large-scale dataset collection strategy for training DFMs, 2) defining core functionalities the model should possess, 3) exploring technical solutions to realize these functionalities, and 4) applying DFMs across the operational spectrum from safety envelopes to energy economy benchmarks.", "result": "The paper presents a conceptual framework for DFMs rather than empirical results. The utility of DFMs is demonstrated across multiple operational domains including human-centric safety envelope definition and energy economy benchmarking, establishing a foundation for systematic AV specification and validation.", "conclusion": "The driver foundation model concept formalizes a new paradigm for systematic specification, verification, and validation of autonomous vehicles, addressing current performance gaps and potentially accelerating AV adoption through comprehensive benchmarking against human driver capabilities."}}
{"id": "2602.07301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07301", "abs": "https://arxiv.org/abs/2602.07301", "authors": ["Aruna Jithesh", "Chinmayi Karumuri", "Venkata Kiran Reddy Kotha", "Meghana Doddapuneni", "Taehee Jeong"], "title": "Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms", "comment": null, "summary": "Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.", "AI": {"tldr": "Attention-enhanced DeepLab-V3+ improves diabetic retinopathy lesion segmentation, especially microaneurysm detection, using DDR dataset with 757 fundus images.", "motivation": "Diabetic retinopathy causes vision loss, requiring early detection through screening. Existing deep learning methods have limited clinical applicability for lesion segmentation, which is crucial for ophthalmologist support.", "method": "Integrated attention mechanism with DeepLab-V3+ architecture to segment four DR lesion types (microaneurysms, soft exudates, hard exudates, hemorrhages) on 757 DDR dataset images.", "result": "Attention-DeepLab increased mAP from 0.3010 to 0.3326 and mean IoU from 0.1791 to 0.1928. Most importantly, microaneurysm detection improved from 0.0205 to 0.0763, a clinically significant enhancement for early DR detection.", "conclusion": "Attention-enhanced DeepLab-V3+ effectively improves DR lesion segmentation, particularly for microaneurysms (earliest DR symptom), advancing automated screening tools for clinical use."}}
{"id": "2602.07903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07903", "abs": "https://arxiv.org/abs/2602.07903", "authors": ["Mingcan Wang", "Junchang Xin", "Zhongming Yao", "Kaifu Long", "Zhiqiong Wang"], "title": "GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank", "comment": null, "summary": "The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.", "AI": {"tldr": "Proposes motif-based personalized PageRank (MPPR) to enhance GCNs by incorporating higher-order motif relationships into message passing, improving accuracy, stability, and efficiency.", "motivation": "Existing MPNNs suffer from shallow propagation depth due to over-smoothing, limited accuracy, poor stability, high computational cost, and neglect of higher-order relationships during message passing.", "method": "Introduces motif-based personalized PageRank (MPPR) to measure node influence considering higher-order motif relationships, then integrates MPPR into GCN message passing to guide propagation at a higher level.", "result": "Outperforms almost all baselines on accuracy, stability, and time consumption; can serve as a component for various GCN tasks, demonstrated with DGCRL.", "conclusion": "MPPR effectively addresses limitations of existing MPNNs by incorporating higher-order motif relationships into message passing, resulting in improved performance across multiple metrics and broader applicability."}}
{"id": "2602.08326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08326", "abs": "https://arxiv.org/abs/2602.08326", "authors": ["Yongjae Lim", "Dabin Kim", "H. Jin Kim"], "title": "Personalized Autonomous Driving via Optimal Control with Clearance Constraints from Questionnaires", "comment": null, "summary": "Driving without considering the preferred separation distance from surrounding vehicles may cause discomfort for users. To address this limitation, we propose a planning framework that explicitly incorporates user preferences regarding the desired level of safe clearance from surrounding vehicles. We design a questionnaire purposefully tailored to capture user preferences relevant to our framework, while minimizing unnecessary questions. Specifically, the questionnaire considers various interaction-relevant factors, including the surrounding vehicle's size, speed, position, and maneuvers of surrounding vehicles, as well as the maneuvers of the ego vehicle. The response indicates the user-preferred clearance for the scenario defined by the question and is incorporated as constraints in the optimal control problem. However, it is impractical to account for all possible scenarios that may arise in a driving environment within a single optimal control problem, as the resulting computational complexity renders real-time implementation infeasible. To overcome this limitation, we approximate the original problem by decomposing it into multiple subproblems, each dealing with one fixed scenario. We then solve these subproblems in parallel and select one using the cost function from the original problem. To validate our work, we conduct simulations using different user responses to the questionnaire. We assess how effectively our planner reflects user preferences compared to preference-agnostic baseline planners by measuring preference alignment.", "AI": {"tldr": "Planning framework that incorporates user preferences for safe clearance from surrounding vehicles using questionnaire data and decomposed optimal control problems.", "motivation": "Current driving systems don't consider user preferences for separation distance from surrounding vehicles, causing discomfort. Need to explicitly incorporate user preferences for safe clearance levels.", "method": "Design tailored questionnaire to capture user preferences for various interaction factors (vehicle size, speed, position, maneuvers). Incorporate responses as constraints in optimal control problem. Decompose problem into multiple subproblems for different scenarios, solve in parallel, and select solution using original cost function.", "result": "Validated through simulations with different user responses. Framework effectively reflects user preferences compared to preference-agnostic baseline planners, showing improved preference alignment.", "conclusion": "Proposed framework successfully incorporates user preferences for safe clearance into driving planning, addressing computational complexity through problem decomposition while maintaining real-time feasibility."}}
{"id": "2602.07310", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07310", "abs": "https://arxiv.org/abs/2602.07310", "authors": ["Kyle Williams", "Andrew Seltzman"], "title": "Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing", "comment": "39 pages, 12 figures, 1 table", "summary": "Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.", "AI": {"tldr": "Automated image analysis pipeline using genetic programming to detect precipitates in additive manufactured copper alloys, enabling faster alloy development.", "motivation": "Current manual annotation of micrographs for analyzing additive manufactured niobium-based copper alloys is slow and hampers rapid iteration in alloy development.", "method": "Linear genetic programming (LGP) optimization of filtering and segmentation algorithms using a domain-specific language for image processing, producing human-interpretable MATLAB code pipelines.", "result": "Achieved near-human accuracy (1.8% error) with 60 population size and 5-block programs, processing 3.6 megapixel images in ~2 seconds, enabling faster alloy development cycles.", "conclusion": "The automated pipeline accelerates exploration of material composition space, supporting development of strong, low-activation precipitation hardened copper alloys for fusion reactor parts."}}
{"id": "2602.07905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07905", "abs": "https://arxiv.org/abs/2602.07905", "authors": ["Yu Zhao", "Hao Guan", "Yongcheng Jing", "Ying Zhang", "Dacheng Tao"], "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.", "AI": {"tldr": "MedCoG uses LLM meta-cognition to dynamically regulate knowledge usage in medical reasoning, achieving 5.5x inference efficiency by avoiding indiscriminate scaling and filtering distracting knowledge.", "motivation": "LLMs show strong medical reasoning potential but face diminishing returns under inference scaling laws. While knowledge augmentation helps, it's unclear if additional costs translate to accuracy gains. The paper explores how LLM meta-cognition (self-awareness of knowledge states) can regulate reasoning processes to improve efficiency.", "method": "Proposes MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph. Uses meta-cognitive assessments of task complexity, familiarity, and knowledge density to dynamically regulate utilization of procedural, episodic, and factual knowledge. Implements LLM-centric on-demand reasoning to mitigate scaling laws by reducing costs (avoiding indiscriminate scaling) and improving accuracy (filtering distractive knowledge).", "result": "Experiments on five hard sets of medical benchmarks demonstrate effectiveness and efficiency, yielding 5.5x inference density (ratio of theoretically effective cost to actual cost). Oracle study highlights significant potential of meta-cognitive regulation.", "conclusion": "Meta-cognitive regulation of LLM reasoning processes can significantly improve inference efficiency in medical reasoning tasks, achieving substantial cost savings while maintaining or improving accuracy through dynamic knowledge utilization."}}
{"id": "2602.08328", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08328", "abs": "https://arxiv.org/abs/2602.08328", "authors": ["Yi-Hsuan Hsiao", "Quang Phuc Kieu", "Zhongtao Guan", "Suhan Kim", "Jiaze Cai", "Owen Matteson", "Jonathan P. How", "Elizabeth Farrell Helbling", "YuFeng Chen"], "title": "Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation", "comment": "22 pages, 7 figures", "summary": "Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.", "AI": {"tldr": "A 1.29-gram aerial robot achieves centimeter-scale positional flight accuracy using only onboard sensing and computation, enabling obstacle avoidance and landing on a sunflower outside motion capture environments.", "motivation": "Aerial insects navigate dense vegetation effortlessly, but insect-scale robots typically require offboard sensors and motion capture systems, limiting their practical applications in search-and-rescue and precision agriculture.", "method": "Developed a 1.29-gram aerial robot with onboard sensing and computation, combining sensor suite, estimators, and low-level controller for positional accuracy, plus hierarchical controller for human operator high-level commands.", "result": "Achieved centimeter-scale positional flight accuracy, demonstrated 30-second flight outside motion capture system with obstacle avoidance and successful landing on a sunflower.", "conclusion": "This represents a significant advancement in aerial microrobotics, enabling onboard sensing and computational autonomy that opens opportunities for onboard planning and power autonomy in practical applications."}}
{"id": "2602.07311", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07311", "abs": "https://arxiv.org/abs/2602.07311", "authors": ["Difei Gu", "Yunhe Gao", "Gerasimos Chatzoudis", "Zihan Dong", "Guoning Zhang", "Bangwei Guo", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery", "comment": null, "summary": "Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.", "AI": {"tldr": "LUCID introduces a unified vision-language sparse autoencoder that learns shared interpretable features across image patches and text tokens, enabling cross-modal concept discovery without manual labeling.", "motivation": "Current sparse autoencoders are trained per modality, producing features that are not directly understandable and don't transfer across domains. There's a need for unified interpretable representations that work across vision and language modalities.", "method": "LUCID uses a unified vision-language sparse autoencoder with shared latent dictionary for both image patch and text token representations, plus private capacity for modality-specific details. Feature alignment is achieved through optimal transport matching without labeling.", "result": "LUCID produces interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, enhance robustness against concept clustering problems, and enable automated dictionary interpretation through term clustering.", "conclusion": "LUCID's shared features capture diverse semantic categories beyond objects (including actions, attributes, abstract concepts), demonstrating a comprehensive approach to interpretable multimodal representations with automated interpretation capabilities."}}
{"id": "2602.07919", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07919", "abs": "https://arxiv.org/abs/2602.07919", "authors": ["Mansi", "Avinash Kori", "Francesca Toni", "Soteris Demetriou"], "title": "Selective Fine-Tuning for Targeted and Robust Concept Unlearning", "comment": "Given the brittle nature of existing methods in unlearning harmful content in diffusion models, we propose TRuST, a novel approach for dynamically estimating target concept neurons and unlearning them by selectively fine-tuning", "summary": "Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.", "AI": {"tldr": "TRUST is a novel selective fine-tuning method for diffusion models that dynamically identifies and unlearns harmful concept neurons using Hessian-based regularization, achieving robust unlearning while preserving generation quality and being computationally efficient.", "motivation": "Text-guided diffusion models are vulnerable to generating harmful content, and existing unlearning methods are either limited to individual concepts, require expensive full fine-tuning, or use static localization techniques that result in suboptimal performance.", "method": "TRUST dynamically estimates target concept neurons and unlearns them through selective fine-tuning, empowered by Hessian-based regularization to maintain model utility while removing harmful concepts.", "result": "TRUST outperforms SOTA baselines in robustness against adversarial prompts, preserves generation quality significantly, and is much faster than existing methods while successfully unlearning individual concepts, concept combinations, and conditional concepts.", "conclusion": "TRUST provides an effective and efficient solution for concept unlearning in diffusion models, addressing both computational cost and utility preservation challenges through dynamic neuron localization and selective fine-tuning with regularization."}}
{"id": "2602.08334", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08334", "abs": "https://arxiv.org/abs/2602.08334", "authors": ["Xuanjin Jin", "Yanxin Dong", "Bin Sun", "Huan Xu", "Zhihui Hao", "XianPeng Lang", "Panpan Cai"], "title": "Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving", "comment": null, "summary": "Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\\times$--$1073\\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.", "AI": {"tldr": "Vec-QMDP is a CPU-native parallel POMDP planner using SIMD optimization and Data-Oriented Design, achieving 227\u00d7-1073\u00d7 speedup over serial planners for real-time autonomous driving.", "motivation": "Planning under uncertainty for robotics tasks like autonomous driving requires reasoning in high-dimensional belief spaces, which is computationally intensive. Existing hybrid CPU-GPU solvers face bottlenecks from host-device synchronization latency and branch divergence on SIMT architectures, limiting real-time planning and real-robot deployment.", "method": "Vec-QMDP adopts Data-Oriented Design (DOD) to refactor scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. It uses hierarchical parallelism: distributing sub-trees across independent CPU cores and SIMD lanes for fully vectorized tree expansion and collision checking. Efficiency is maximized with UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking.", "result": "Vec-QMDP achieves 227\u00d7-1073\u00d7 speedup over state-of-the-art serial planners. Evaluated on large-scale autonomous driving benchmarks, it achieves state-of-the-art planning performance with millisecond-level latency.", "conclusion": "Vec-QMDP establishes CPUs as a high-performance computing platform for large-scale planning under uncertainty, enabling real-time planning for robotics applications like autonomous driving through CPU-native parallelization and SIMD optimization."}}
{"id": "2602.07343", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07343", "abs": "https://arxiv.org/abs/2602.07343", "authors": ["Ruturaj Reddy", "Hrishav Bakul Barua", "Junn Yong Loo", "Thanh Thi Nguyen", "Ganesh Krishnasamy"], "title": "Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation", "comment": null, "summary": "Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.", "AI": {"tldr": "CLARITY: Dynamic RGB-Thermal fusion for robust road scene segmentation that adapts to illumination conditions using vision-language model priors, achieving SOTA 62.3% mIoU on MFNet dataset.", "motivation": "Current RGB-Thermal fusion methods use static strategies that propagate modality-specific noise across all conditions, limiting robustness in adverse illumination, lighting, and shadow scenarios for autonomous driving.", "method": "Dynamic fusion strategy guided by VLM priors that modulates modality contributions based on detected scene conditions; uses object embeddings for segmentation; includes dark-object semantic preservation and hierarchical decoder for structural consistency across scales.", "result": "Achieves new state-of-the-art on MFNet dataset with 62.3% mIoU and 77.5% mAcc, outperforming previous methods.", "conclusion": "CLARITY demonstrates that adaptive fusion strategies conditioned on scene illumination significantly improve semantic segmentation robustness in challenging road scenes compared to static fusion approaches."}}
{"id": "2602.07940", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07940", "abs": "https://arxiv.org/abs/2602.07940", "authors": ["Guanglong Sun", "Hongwei Yan", "Liyuan Wang", "Zhiqi Kang", "Shuang Cui", "Hang Su", "Jun Zhu", "Yi Zhong"], "title": "MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin", "comment": null, "summary": "To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\\%, 13.36\\%, and 12.56\\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \\href{https://github.com/SunGL001/MePo}{MePo}", "AI": {"tldr": "MePo is a meta-learning approach that refines pretrained models for general continual learning by creating pseudo task sequences and using bi-level meta-learning to improve adaptation to evolving data streams.", "motivation": "Current continual learning methods using pretrained models struggle with diverse, temporally mixed information in real-world scenarios with online datastreams and blurry task boundaries, leading to suboptimal performance in general continual learning.", "method": "MePo constructs pseudo task sequences from pretraining data and uses bi-level meta-learning to refine pretrained backbones. It also initializes a meta covariance matrix as reference geometry for pretrained representation space, enabling second-order statistics for robust output alignment.", "result": "MePo achieves significant performance gains across various GCL benchmarks and pretrained checkpoints in a rehearsal-free manner: 15.10% on CIFAR-100, 13.36% on ImageNet-R, and 12.56% on CUB-200 under Sup-21/1K.", "conclusion": "MePo provides an effective plug-in strategy that bridges the gap between pretrained models and general continual learning requirements, enabling better adaptation to evolving environments without requiring rehearsal buffers."}}
{"id": "2602.08370", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08370", "abs": "https://arxiv.org/abs/2602.08370", "authors": ["Yeke Chen", "Shihao Dong", "Xiaoyu Ji", "Jingkai Sun", "Zeren Luo", "Liu Zhao", "Jiahui Zhang", "Wanyue Li", "Ji Ma", "Bowen Xu", "Yimin Han", "Yudong Zhao", "Peng Lu"], "title": "Learning Human-Like Badminton Skills for Humanoid Robots", "comment": "10 pages, 4 figures", "summary": "Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a \"mimic\" to a capable \"striker.\" Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.", "AI": {"tldr": "A progressive RL framework that transforms humanoid robots from motion mimics to functional badminton strikers, achieving zero-shot sim-to-real transfer of diverse badminton skills.", "motivation": "Bridging the gap between kinematic imitation and functional, physics-aware striking in high-demand sports like badminton, where explosive whole-body coordination and precise timing are required, while maintaining stylistic naturalness.", "method": "Imitation-to-Interaction framework: establishes motor prior from human data, distills into compact model-based state representation, stabilizes dynamics via adversarial priors, and uses manifold expansion to generalize discrete strike points into dense interaction volume.", "result": "Mastery of diverse badminton skills (lifts and drop shots) in simulation, and first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, replicating human athletes' kinetic elegance and functional precision.", "conclusion": "The proposed framework successfully evolves robots from mimics to capable strikers, achieving human-like performance in complex sports tasks through progressive reinforcement learning and effective sim-to-real transfer."}}
{"id": "2602.07345", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07345", "abs": "https://arxiv.org/abs/2602.07345", "authors": ["Lichen Bai", "Zikai Zhou", "Shitong Shao", "Wenliang Zhong", "Shuo Yang", "Shuo Chen", "Bojun Chen", "Zeke Xie"], "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation", "comment": "25 pages, 15 figures, 11 tables", "summary": "Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.", "AI": {"tldr": "AMD introduces a self-correcting mechanism to detect and escape Forbidden Zones in distillation, improving stability and performance of few-step generative models.", "motivation": "Distribution Matching Distillation (DMD) suffers from instability in Forbidden Zones where real teacher guidance is unreliable and fake teacher repulsive force is insufficient, limiting performance of accelerated generative models.", "method": "AMD uses reward proxies to explicitly detect Forbidden Zones, dynamically prioritizes corrective gradients via structural signal decomposition, and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse.", "result": "AMD significantly enhances sample fidelity and training robustness across image and video generation tasks (SDXL, Wan2.1), improving HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines.", "conclusion": "Explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models, as demonstrated by AMD's superior performance."}}
{"id": "2602.07943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07943", "abs": "https://arxiv.org/abs/2602.07943", "authors": ["Ivaxi Sheth", "Zhijing Jin", "Bryan Wilder", "Dominik Janzing", "Mario Fritz"], "title": "IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery", "comment": "18 pages", "summary": "In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.", "AI": {"tldr": "LLMs can help identify valid instrumental variables from observational data through a multi-agent system that proposes, critiques, and refines IVs, showing potential for causal discovery.", "motivation": "Identifying valid instrumental variables requires interdisciplinary knowledge and contextual understanding, making it challenging. The paper investigates whether LLMs can assist in this important causal inference task.", "method": "Two-stage evaluation: first testing if LLMs can recover established instruments from literature, then evaluating if they can avoid discredited instruments. Introduces IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs, plus a statistical test for consistency without ground truth.", "result": "LLMs show potential to discover valid instrumental variables from large observational databases, demonstrating ability to replicate standard reasoning and avoid invalid instruments.", "conclusion": "LLMs can aid in identifying valid instrumental variables, offering a promising approach to enhance causal discovery from observational data through automated reasoning and multi-agent collaboration."}}
{"id": "2602.08392", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08392", "abs": "https://arxiv.org/abs/2602.08392", "authors": ["Xin Wu", "Zhixuan Liang", "Yue Ma", "Mengkang Hu", "Zhiyuan Qin", "Xiu Li"], "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models", "comment": "38 pages, 9 figures. Project page:https://bimanibench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.", "AI": {"tldr": "BiManiBench is a hierarchical benchmark for evaluating Multimodal Large Language Models on bimanual robotic manipulation tasks, revealing current MLLMs struggle with dual-arm spatial grounding and control despite high-level reasoning proficiency.", "motivation": "Existing MLLM frameworks for embodied AI are limited to single-arm manipulation and fail to capture the spatio-temporal coordination required for bimanual tasks like lifting heavy objects, creating a gap in evaluating true robotic intelligence.", "method": "Introduces BiManiBench, a hierarchical benchmark with three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. The framework isolates unique bimanual challenges like arm reachability and kinematic constraints to distinguish perceptual hallucinations from planning failures.", "result": "Analysis of over 30 state-of-the-art models shows that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors.", "conclusion": "Current MLLM paradigm lacks deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing for effective bimanual manipulation."}}
{"id": "2602.07428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07428", "abs": "https://arxiv.org/abs/2602.07428", "authors": ["Chengqi Dong", "Zhiyuan Cao", "Tuoshi Qi", "Kexin Wu", "Yixing Gao", "Fan Tang"], "title": "Row-Column Separated Attention Based Low-Light Image/Video Enhancement", "comment": null, "summary": "U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.", "AI": {"tldr": "U-Net with Row-Column Separated Attention (RCSA) for low-light image/video enhancement using global information guidance with fewer parameters.", "motivation": "Standard U-Net for low-light enhancement causes local noise and detail loss due to lack of global information guidance. Attention mechanisms help but increase parameters/computations significantly.", "method": "Propose RCSA module inserted after improved U-Net, using row/column mean/max of feature maps to guide local information with fewer parameters. Add temporal loss functions for video enhancement consistency.", "result": "Extensive experiments on LOL, MIT Adobe FiveK (images) and SDSD (video) datasets demonstrate effectiveness of the approach.", "conclusion": "RCSA module effectively utilizes global information to guide local enhancement with reduced parameters, achieving better low-light image/video enhancement with temporal consistency."}}
{"id": "2602.07962", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07962", "abs": "https://arxiv.org/abs/2602.07962", "authors": ["Weihao Zeng", "Yuzhen Huang", "Junxian He"], "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench", "AI": {"tldr": "LOCA-bench is a new benchmark for evaluating language agents in long-context scenarios with dynamically growing contexts, addressing limitations of existing single-step retrieval benchmarks.", "motivation": "Existing long-context benchmarks focus on single-step information retrieval, but real-world tasks require agents to explore environments, follow instructions, extract information, and predict actions under dynamically growing contexts. LLM reliability deteriorates with increasing context length (\"context rot\").", "method": "LOCA-bench uses automated, scalable control of environment states to regulate agent context length, allowing context to potentially extend to infinity while keeping task semantics fixed. It evaluates agents as combinations of models and scaffolds with various context management strategies.", "result": "Agent performance generally degrades as environment states grow more complex, but advanced context management techniques can substantially improve overall success rates.", "conclusion": "LOCA-bench provides a platform for evaluating models and scaffolds in long-context, agentic scenarios, addressing the gap between single-step retrieval benchmarks and realistic agent tasks with dynamically growing contexts."}}
{"id": "2602.08417", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08417", "abs": "https://arxiv.org/abs/2602.08417", "authors": ["Wentao Zhao", "Yihe Niu", "Zikun Chen", "Rui Li", "Yanbo Wang", "Tianchen Deng", "Jingchuan Wang"], "title": "Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion", "comment": "13 pages, 8 figures, 8 tables", "summary": "Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.", "AI": {"tldr": "Graph-Loc: A graph-based LiDAR localization framework using lightweight point-line structural maps for robust pose tracking under occlusion and partial observations.", "motivation": "Need for long-term autonomous operation with compact map storage and fast retrieval, while handling online LiDAR observations that are often partial, repetitive, and heavily occluded.", "method": "Extracts sparse point/line primitives from LiDAR scans to form observation graphs, retrieves pose-conditioned visible subgraphs via ray simulation, and performs scan-to-map association using unbalanced optimal transport with graph-context regularization. Uses information anisotropy estimation to defer updates in weakly constrained directions.", "result": "Demonstrates accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate conditions, sustained occlusion, and gradual scene changes.", "conclusion": "Graph-Loc provides a robust graph-based localization solution that works with compact structural map priors and handles challenging real-world conditions including occlusion and partial observations."}}
{"id": "2602.07444", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.07444", "abs": "https://arxiv.org/abs/2602.07444", "authors": ["Ondrej Hlinka", "Georg Kaniak", "Christian Kapeller"], "title": "Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction", "comment": "submitted to IET Electronics Letters", "summary": "We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.", "AI": {"tldr": "Perspective-aware log-depth fusion method for 3D surface reconstruction from depth and normal maps using single camera, handling missing data and achieving metric accuracy.", "motivation": "Existing gradient-based depth-normals fusion methods assume orthographic projection, which limits metric accuracy in perspective camera systems. Need to account for perspective projection for accurate 3D reconstructions from depth and normal maps.", "method": "Proposes perspective-aware log-depth fusion approach that extends orthographic gradient-based methods by explicitly modeling perspective projection. Uses log-depth representation and handles missing depth measurements by leveraging surface normal information for inpainting.", "result": "Experiments on DiLiGenT-MV dataset demonstrate effectiveness of the approach. Shows importance of perspective-aware fusion for achieving metrically accurate 3D reconstructions compared to orthographic methods.", "conclusion": "Perspective-aware depth-normals fusion is crucial for accurate 3D surface reconstruction from single camera systems, with the proposed method effectively handling perspective projection and missing data."}}
{"id": "2602.07983", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07983", "abs": "https://arxiv.org/abs/2602.07983", "authors": ["Jishu Sen Gupta", "Harini SI", "Somesh Kumar Singh", "Syed Mohamad Tawseeq", "Yaman Kumar Singla", "David Doermann", "Rajiv Ratn Shah", "Balaji Krishnamurthy"], "title": "Accelerating Social Science Research via Agentic Hypothesization and Experimentation", "comment": null, "summary": "Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.", "AI": {"tldr": "EXPERIGEN is an agentic framework for end-to-end scientific discovery that uses a two-phase search to generate and test hypotheses, achieving 2-4x more statistically significant results than prior approaches and validating hypotheses through expert review and real-world A/B testing.", "motivation": "Current data-driven methods in social science research are slow and fail to support end-to-end scientific discovery, relying on iterative cycles that limit acceleration of the research process.", "method": "EXPERIGEN uses a Bayesian optimization inspired two-phase search with a Generator that proposes candidate hypotheses and an Experimenter that evaluates them empirically. The framework extends to complex data regimes including multimodal and relational datasets.", "result": "EXPERIGEN discovers 2-4x more statistically significant hypotheses that are 7-17% more predictive than prior approaches. Expert review shows 88% of hypotheses rated moderately/strongly novel, 70% deemed impactful and worth pursuing. A/B testing shows statistically significant results with p < 1e-6 and 344% effect size.", "conclusion": "EXPERIGEN successfully operationalizes end-to-end scientific discovery, producing novel, empirically grounded, and actionable hypotheses that demonstrate real-world impact and statistical significance, advancing the field of data-driven social science research."}}
{"id": "2602.08421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08421", "abs": "https://arxiv.org/abs/2602.08421", "authors": ["Farhad Keramat", "Salma Salimi", "Tomi Westerlund"], "title": "Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric", "comment": null, "summary": "Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.", "AI": {"tldr": "LLM oracle with novel aggregation method for robotic task planning, plus decentralized multi-robot infrastructure using Hyperledger Fabric", "motivation": "Address security/privacy challenges in LLM-based HRI, particularly self-preferencing by dominant providers, and improve reliability for robotic task planning where temporal order matters", "method": "Proposed LLM oracle with new aggregation method suitable for robotic task planning, plus decentralized multi-robot infrastructure based on Hyperledger Fabric for intent decomposition, robot coordination, and fine-grained access control", "result": "Created SkillChain-RTD benchmark (publicly available), demonstrated feasibility of architecture, and showed proposed aggregation method outperforms current methods", "conclusion": "Proposed approach successfully addresses LLM centralization issues for robotic task planning through decentralized infrastructure and improved aggregation methods"}}
{"id": "2602.07446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07446", "abs": "https://arxiv.org/abs/2602.07446", "authors": ["Naqcho Ali Mehdi"], "title": "PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization", "comment": "8 pages, 4 figures, dataset paper", "summary": "Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.", "AI": {"tldr": "Created PTB-XL-Image-17K, a synthetic 12-lead ECG image dataset with 17,271 samples, providing comprehensive ground truth data for ECG digitization research.", "motivation": "ECG digitization is crucial for using legacy clinical data in modern deep learning, but progress has been hindered by the lack of large-scale datasets with both ECG images and corresponding ground truth signals.", "method": "Developed an open-source Python framework to generate synthetic ECG images from the PTB-XL signal database, with customizable parameters including paper speed, voltage scale, sampling rate, grid appearance, and waveform characteristics.", "result": "Created a dataset with 17,271 high-quality 12-lead ECG images, achieving 100% generation success rate with average processing time of 1.35 seconds per sample, providing five complementary data types per sample.", "conclusion": "PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline from lead detection to signal extraction with full ground truth for rigorous evaluation."}}
{"id": "2602.08009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08009", "abs": "https://arxiv.org/abs/2602.08009", "authors": ["Rui Li", "Zeyu Zhang", "Xiaohe Bo", "Quanyu Dai", "Chaozhuo Li", "Feng Wen", "Xu Chen"], "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective", "comment": null, "summary": "Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.", "AI": {"tldr": "RAPS introduces a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents, automating agentic workflows through dynamic intent-based communication and Bayesian reputation systems.", "motivation": "Current multi-agent architectures built on LLMs require substantial manual orchestration, creating an imperative to automate agentic workflow design. The paper frames agent coordination as a dynamic ad-hoc networking problem: how to establish adaptive and reliable communication among scalable numbers of agentic hosts.", "method": "RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on declared intents rather than predefined topologies. It incorporates two coherent overlays: (1) Reactive Subscription enabling agents to dynamically refine their intents, and (2) Bayesian Reputation empowering each agent with a local watchdog to detect and isolate malicious peers.", "result": "Extensive experiments over five benchmarks showcase that RAPS effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.", "conclusion": "RAPS provides a reputation-aware publish-subscribe paradigm that successfully addresses the challenge of automated, adaptive, scalable, and robust coordination of LLM agents, moving beyond manual orchestration toward intelligent swarm coordination."}}
{"id": "2602.08425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08425", "abs": "https://arxiv.org/abs/2602.08425", "authors": ["Jinxian Zhou", "Ruihai Wu", "Yiwei Liu", "Yiwen Hou", "Xunzhe Zhou", "Checheng Yu", "Licheng Zhong", "Lin Shao"], "title": "Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence", "comment": null, "summary": "Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/", "AI": {"tldr": "Bi-Adapt is a framework for efficient bimanual manipulation generalization using semantic correspondence and vision foundation models, achieving zero-shot performance on novel object categories with limited data.", "motivation": "Existing bimanual manipulation methods require costly data collection and training, and struggle to generalize efficiently to unseen objects in novel categories.", "method": "Bi-Adapt uses semantic correspondence and vision foundation models for cross-category affordance mapping, with fine-tuning on limited novel category data.", "result": "The framework achieves high success rates on benchmark tasks across novel categories in both simulation and real-world environments with limited data.", "conclusion": "Bi-Adapt provides an efficient solution for bimanual manipulation generalization to out-of-category objects in a zero-shot manner."}}
{"id": "2602.07449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07449", "abs": "https://arxiv.org/abs/2602.07449", "authors": ["Tan Yu", "Qian Qiao", "Le Shen", "Ke Zhou", "Jincheng Hu", "Dian Sheng", "Bo Hu", "Haoming Qin", "Jun Gao", "Changhai Zhou", "Shunshun Yin", "Siyuan Liu"], "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads", "comment": "11 pages, 3 figures", "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.", "AI": {"tldr": "SoulX-FlashHead is a 1.3B-parameter framework for real-time, high-fidelity audio-driven portrait video generation that achieves 96 FPS inference while maintaining visual quality and temporal stability.", "motivation": "Existing audio-driven portrait generation models face a trade-off between visual quality and latency - large models are computationally expensive while lightweight models sacrifice facial representation quality and temporal stability. There's a need for real-time, high-fidelity streaming video generation.", "method": "Proposes a unified 1.3B-parameter framework with: 1) Streaming-Aware Spatiotemporal Pre-training with Temporal Audio Context Cache for robust feature extraction from short audio fragments, 2) Oracle-Guided Bidirectional Distillation using ground-truth motion priors to prevent error accumulation in long sequences, and 3) VividHead dataset with 782 hours of aligned footage for training.", "result": "Achieves state-of-the-art performance on HDTF and VFHQ benchmarks. The Lite variant reaches 96 FPS on a single RTX 4090, enabling ultra-fast interaction while maintaining visual coherence.", "conclusion": "SoulX-FlashHead successfully balances high-fidelity visual quality with low-latency streaming, addressing key challenges in audio-driven portrait generation through innovative architectural designs and training strategies."}}
{"id": "2602.08013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08013", "abs": "https://arxiv.org/abs/2602.08013", "authors": ["Yuqiao Meng", "Luoxi Tang", "Dazheng Zhang", "Rafael Brens", "Elvys J. Romero", "Nancy Guo", "Safa Elkefi", "Zhaohan Xi"], "title": "Small Agent Group is the Future of Digital Health", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in digital health has been driven by a \"scaling-first\" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.", "AI": {"tldr": "Small Agent Groups (SAG) outperform single large language models in clinical settings by using collaborative reasoning instead of model scaling, achieving better effectiveness, reliability, and cost efficiency.", "motivation": "Current LLM adoption in digital health follows a \"scaling-first\" approach assuming bigger models are better, but real clinical needs require effectiveness, reliability, and reasonable deployment costs. Clinical decision-making is inherently collaborative, so the authors question whether a collective approach could be superior to monolithic scaling.", "method": "SAG (Small Agent Group) shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through collaborative deliberation. The approach was evaluated using diverse clinical metrics spanning effectiveness, reliability, and deployment cost, comparing against single giant models with and without optimization or retrieval-augmented generation.", "result": "SAG achieves superior performance compared to single giant models across all evaluated dimensions. The collaborative approach works better both with and without additional optimization or retrieval-augmented generation techniques.", "conclusion": "Synergistic reasoning through Small Agent Groups can substitute for model parameter growth in clinical settings, offering a scalable digital health solution that better balances effectiveness, reliability, and deployment efficiency."}}
{"id": "2602.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08440", "abs": "https://arxiv.org/abs/2602.08440", "authors": ["Tian Gao", "Celine Tan", "Catherine Glossop", "Timothy Gao", "Jiankai Sun", "Kyle Stachowicz", "Shirley Wu", "Oier Mees", "Dorsa Sadigh", "Sergey Levine", "Chelsea Finn"], "title": "SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios", "comment": null, "summary": "A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.", "AI": {"tldr": "SteerVLA integrates high-level VLM reasoning with low-level VLA control for autonomous driving, using fine-grained language instructions to steer driving policies, achieving state-of-the-art performance.", "motivation": "Autonomous driving needs both high-level semantic reasoning for long-tail events and low-level reactive control. While VLMs offer powerful reasoning, they lack grounded driving experience. The challenge is to combine VLM world knowledge with robust vehicle control.", "method": "Proposes SteerVLA, which uses VLMs to generate fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key innovation is a rich language interface between high-level VLM and low-level VLA, plus using VLM to augment driving data with detailed language annotations for supervision.", "result": "Outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on long-tail subset in challenging closed-loop benchmark evaluation.", "conclusion": "SteerVLA effectively integrates VLM reasoning with VLA control through language interfaces, demonstrating superior performance in autonomous driving, particularly for handling long-tail scenarios."}}
{"id": "2602.07458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07458", "abs": "https://arxiv.org/abs/2602.07458", "authors": ["Yancheng Long", "Yankai Yang", "Hongyang Wei", "Wei Chen", "Tianke Zhang", "Haonan fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Shuo Yang"], "title": "SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning", "comment": null, "summary": "Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term \"Attention Collapse,\" where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.", "AI": {"tldr": "SpatialReward is a spatial reasoning-based reward model that addresses attention collapse in image editing RL by anchoring semantic judgments to pixel-level evidence, achieving SOTA performance and boosting online RL results.", "motivation": "Current online RL for image editing suffers from unreliable reward signals due to \"Attention Collapse\" - models neglect cross-image comparisons and fine-grained details, leading to inaccurate perception and miscalibrated scores.", "method": "Propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning by anchoring reasoning to predicted edit regions, grounding semantic judgments in pixel-level evidence. Trained on a curated 260k spatial-aware dataset.", "result": "Achieves state-of-the-art performance on MMRB2 and EditReward-Bench, outperforms proprietary evaluators on MultiEditReward-Bench. Boosts OmniGen2 by +0.90 on GEdit-Bench, surpassing leading discriminative model and doubling GPT-4.1's gain (+0.45).", "conclusion": "Spatial reasoning is essential for unlocking effective alignment in image editing, as demonstrated by SpatialReward's superior performance as a robust reward signal in online reinforcement learning."}}
{"id": "2602.08021", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08021", "abs": "https://arxiv.org/abs/2602.08021", "authors": ["Zhan-Yi Liao", "Jaewon Yoo", "Hao-Tsung Yang", "Po-An Chen"], "title": "Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers", "comment": null, "summary": "Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.", "AI": {"tldr": "A structure-aware counterfactual explanation method using conditional Gaussian network classifiers with DAG-based feature dependencies, reformulated as MILP via piecewise McCormick relaxation for global optimality and robustness.", "motivation": "Current counterfactual explanation methods often lack consideration of feature dependencies and structural assumptions, requiring additional constraints to ensure consistency. There's a need for methods that naturally embed feature relationships while ensuring robustness and global optimality.", "method": "Uses conditional Gaussian network classifier (CGNC) with DAG structure to encode feature dependencies, applies convergence-guaranteed cutting-set procedure for adversarial optimization, and uses piecewise McCormick relaxation to reformulate nonconvex quadratic problems into mixed-integer linear programs (MILP).", "result": "Achieves strong robustness with direct global optimization providing stable and efficient results. The method shows improved performance in maintaining structural consistency while ensuring global optimality.", "conclusion": "The proposed framework successfully integrates structural awareness with robustness guarantees, is extensible to complex constraint settings, and provides a foundation for advancing counterfactual reasoning under nonconvex quadratic formulations."}}
{"id": "2602.08444", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08444", "abs": "https://arxiv.org/abs/2602.08444", "authors": ["Samsaptak Ghosh", "M. Felix Orlando", "Sohom Chakrabarty"], "title": "Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions", "comment": "10 pages, 6 figures", "summary": "Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.", "AI": {"tldr": "Proposes a recovery control law for autonomous vehicles to restore trajectory after collisions, accounting for time-varying speed and nonlinear coupling effects.", "motivation": "Post-collision lateral motion and yaw transients can rapidly drive vehicles off intended paths, requiring safety-critical recovery capabilities. Existing approaches often oversimplify by assuming constant longitudinal speed, missing important transient dynamics.", "method": "Structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. Explicitly accounts for time-varying longitudinal velocity and retains nonlinear steering-coupled interaction terms.", "result": "Method evaluated in simulation on both the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB. Demonstrates consistent post-collision restoration behavior across representative initial post-impact conditions.", "conclusion": "The proposed control law effectively addresses post-collision recovery by considering realistic transient dynamics (speed variations and nonlinear coupling) that significantly influence recovery behavior, unlike constant-speed assumptions in existing literature."}}
{"id": "2602.07463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07463", "abs": "https://arxiv.org/abs/2602.07463", "authors": ["Misbah Ijaz", "Saif Ur Rehman Khan", "Abd Ur Rehman", "Tayyaba Asif", "Sebastian Vollmer", "Andreas Dengel", "Muhammad Nabeel Asim"], "title": "GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring", "comment": null, "summary": "The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.", "AI": {"tldr": "Researchers created GlobalWasteData (GWD), a large-scale unified dataset of 89,807 waste images across 14 main categories and 68 subclasses, compiled from multiple fragmented public datasets to enable better waste classification AI models.", "motivation": "Existing waste classification datasets are fragmented, inconsistent, and biased, with differences in class names, annotation formats, image conditions, and class distributions, making it difficult to train generalizable models for real-world waste sorting applications.", "method": "Created the GWD archive by merging multiple publicly available waste datasets into a single unified resource, with additional preprocessing steps including quality filtering, duplicate removal, and metadata generation to improve reliability and consistency.", "result": "Produced a large-scale dataset of 89,807 images across 14 main categories and 68 distinct subclasses with consistent labeling, improved domain diversity, and more balanced class representation compared to existing fragmented datasets.", "conclusion": "The GWD archive provides a strong foundation for ML applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility in waste classification."}}
{"id": "2602.08030", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08030", "abs": "https://arxiv.org/abs/2602.08030", "authors": ["Yilun Zheng", "Dongyang Ma", "Tian Liang", "Jiahao Xu", "Xinting Huang", "Lijie Chen", "Haitao Mi", "Yan Wang"], "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models", "comment": null, "summary": "Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.\n  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.", "AI": {"tldr": "Free()LM introduces self-forgetting capability to LLMs via a plug-and-play LoRA adapter that dynamically prunes useless context chunks during reasoning, preventing performance degradation from excessive thinking tokens.", "motivation": "Standard LLMs suffer from a \"malloc-only\" problem where they accumulate both valid and redundant reasoning steps without pruning obsolete information, causing performance degradation with excessive thinking tokens.", "method": "Free()LM uses a Free-Module (plug-and-play LoRA adapter) that enables iterative switching between reasoning and cleaning modes to dynamically identify and prune useless context chunks, maintaining compact noise-free states.", "result": "Achieves 3.3% average improvement over top-tier reasoning baselines across all model scales (8B to 685B), establishes new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale, and restores performance from 0% to 50% on long-horizon tasks where standard models collapse.", "conclusion": "Sustainable intelligence requires both the power to think and the freedom to forget; self-forgetting capability is essential for maintaining reasoning performance by preventing information overload and noise accumulation."}}
{"id": "2602.08450", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08450", "abs": "https://arxiv.org/abs/2602.08450", "authors": ["Stefan Ivi\u0107", "Luka Lan\u010da", "Karlo Jakac", "Ante Sikirica", "Stella Dumen\u010di\u0107", "Matej Mali\u0161a", "Zvonimir Mrle", "Bojan Crnkovi\u0107"], "title": "UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials", "comment": null, "summary": "This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.", "AI": {"tldr": "Autonomous maritime search system integrates flow modeling, UAV control, and vision detection for reliable floating target detection in complex ocean conditions.", "motivation": "To develop an autonomous system for maritime search operations that can reliably detect floating targets under realistic uncertainties and complex environmental conditions, addressing challenges in search and rescue applications.", "method": "Integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection. Uses real-time drifter data, surrogate flow models based on computational fluid dynamics and numerical optimization, multi-UAV search control with vision sensing, and deep learning-based object detection.", "result": "Field experiments in Valun Bay (Croatia) demonstrate that the tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions.", "conclusion": "The system provides concrete insights for future autonomous maritime search and rescue applications, showing that integrated flow modeling, UAV control, and vision detection can effectively address challenges in maritime search operations."}}
{"id": "2602.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07493", "abs": "https://arxiv.org/abs/2602.07493", "authors": ["Tianhao Zhou", "Yujia Chen", "Zhihao Zhan", "Yuhang Ming", "Jianzhu Huai"], "title": "Thermal odometry and dense mapping using learned ddometry and Gaussian splatting", "comment": "11 pages, 2 figures, 5 tables", "summary": "Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.", "AI": {"tldr": "TOM-GS is a thermal odometry and mapping system that combines learning-based odometry with Gaussian Splatting for dense reconstruction, outperforming existing methods in adverse conditions.", "motivation": "Thermal cameras work well in darkness, dust, and smoke, but existing thermal odometry methods are geometric, fail across diverse datasets, and can't produce dense maps. Gaussian Splatting offers efficient high-quality reconstruction.", "method": "TOM-GS integrates learning-based odometry with Gaussian Splatting-based dense mapping, featuring thermal image enhancement and monocular depth integration specifically for thermal cameras.", "result": "Extensive experiments show TOM-GS outperforms existing learning-based methods in both motion estimation and novel-view rendering, demonstrating robust thermal odometry and dense reconstruction.", "conclusion": "TOM-GS successfully combines learning-based pipelines with Gaussian Splatting for thermal SLAM, confirming the benefits of this approach for robust thermal odometry and dense mapping in adverse conditions."}}
{"id": "2602.08052", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08052", "abs": "https://arxiv.org/abs/2602.08052", "authors": ["Bulent Soykan", "Sean Mondesire", "Ghaith Rabadi", "Grace Bochenek"], "title": "Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling", "comment": "11 pages, 2 figures, Winter Simulation Conference (WSC) 2025", "summary": "The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.", "AI": {"tldr": "Deep RL framework (PPO+GNN) solves multi-objective unrelated parallel machine scheduling with release dates, setups, and eligibility constraints, outperforming traditional methods.", "motivation": "Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST) in complex UPMSP with multiple constraints.", "method": "Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) with Graph Neural Network (GNN) representation of jobs, machines, and setups, guided by multi-objective reward function.", "result": "PPO-GNN agent significantly outperforms standard dispatching rules and metaheuristics on benchmark instances, achieving superior trade-off between TWT and TST objectives.", "conclusion": "Provides a robust and scalable solution for complex manufacturing scheduling by learning direct scheduling policies through deep reinforcement learning."}}
{"id": "2602.08466", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08466", "abs": "https://arxiv.org/abs/2602.08466", "authors": ["Ning Hu", "Senhao Cao", "Maochen Li"], "title": "Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment", "comment": "7 pages, 1 figure", "summary": "Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.", "AI": {"tldr": "Execution gating mechanism improves robotic alignment reliability by rejecting high-risk pose updates, addressing geometric error amplification rather than just improving pose estimation accuracy.", "motivation": "Vision-guided robotic systems frequently fail in precision alignment tasks despite accurate pose estimates, revealing that pose accuracy alone is insufficient for execution reliability due to geometric error amplification.", "method": "Proposes a Reliability-aware Execution Gating mechanism that evaluates geometric consistency and configuration risk before execution, selectively rejecting or scaling high-risk pose updates without modifying pose estimation algorithms.", "result": "Experimental validation on a UR5 robotic platform shows significant improvements in task success rates, reduced execution variance, suppressed tail-risk behavior, while maintaining average pose accuracy.", "conclusion": "The execution-level reliability modeling provides a practical, estimator-agnostic solution for improving robustness in near-field vision-guided robotic systems, highlighting the importance of execution-level considerations beyond pose estimation accuracy."}}
{"id": "2602.07495", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07495", "abs": "https://arxiv.org/abs/2602.07495", "authors": ["Jiawen Zheng", "Haonan Jia", "Ming Li", "Yuhui Zheng", "Yufeng Zeng", "Yang Gao", "Chen Liang"], "title": "Learning Brain Representation with Hierarchical Visual Embeddings", "comment": null, "summary": "Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "AI": {"tldr": "Proposes a brain-image alignment method using multiple pre-trained visual encoders with contrastive learning and a Fusion Prior to improve visual decoding from brain signals.", "motivation": "Current visual decoding approaches focus too much on high-level semantic features while neglecting pixel-level details, limiting understanding of the human visual system. The degree to which brain signals truly encode visual information remains unclear.", "method": "Uses multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, employs contrastive learning for brain-image alignment, and introduces a Fusion Prior that learns stable mapping on large-scale visual data then matches brain features to this pre-trained prior.", "result": "Extensive quantitative and qualitative experiments demonstrate the method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "conclusion": "The proposed approach effectively aligns brain signals with visual representations by leveraging hierarchical multi-scale features and distributional consistency through the Fusion Prior, advancing visual decoding capabilities."}}
{"id": "2602.08061", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2602.08061", "abs": "https://arxiv.org/abs/2602.08061", "authors": ["Doni Bloomfield", "Allison Berke", "Moritz S. Hanke", "Aaron Maiwald", "James R. M. Black", "Toby Webster", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jassi Pannu"], "title": "Securing Dual-Use Pathogen Data of Concern", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.", "AI": {"tldr": "Proposes a five-tier Biosecurity Data Level (BDL) framework to categorize pathogen data based on risk, with technical restrictions and governance for dual-use data to prevent AI misuse in bioweapons development.", "motivation": "AI models for biology are trained on diverse biological data, and the type of data used determines model capabilities, including potentially dangerous biosecurity applications. With over 100 researchers endorsing data controls at Asilomar Conference, there's a need to prevent AI misuse for harmful applications like bioweapons development.", "method": "Introduces a five-tier Biosecurity Data Level (BDL) framework that categorizes pathogen data based on expected ability to contribute to concerning AI capabilities. For each tier, proposes appropriate technical restrictions, and outlines a novel governance framework for dual-use pathogen data.", "result": "Develops a structured framework for data categorization and control, positioning data controls as potentially the most high-leverage intervention available to reduce proliferation of concerning biological AI capabilities in a world with widely accessible computational resources.", "conclusion": "In an era of accessible computational resources, data controls represent a crucial intervention point to mitigate risks of AI misuse in biology, requiring systematic categorization and governance of pathogen data based on biosecurity risk levels."}}
{"id": "2602.08518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08518", "abs": "https://arxiv.org/abs/2602.08518", "authors": ["Kento Kawaharazuka", "Kei Okada", "Masayuki Inaba"], "title": "Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi", "comment": "Accepted to Advanced Intelligent Systems", "summary": "Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.", "AI": {"tldr": "This paper analyzes musculoskeletal humanoids by categorizing their structural properties and discussing management/utilization methods based on research with Kengoro and Musashi robots.", "motivation": "Despite development of various musculoskeletal humanoids and control mechanism studies, there has been insufficient unified discussion on the diverse properties inherent in these structures and how to manage/utilize them effectively.", "method": "Categorizes and analyzes muscle characteristics based on research with Kengoro and Musashi musculoskeletal humanoids. Classifies features into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity.", "result": "Organizes diverse advantages and disadvantages of musculoskeletal humanoids arising from combination of these properties. Discusses body schema learning, reflex control, muscle grouping, body schema adaptation, and implementation through integrated system.", "conclusion": "Provides comprehensive analysis framework for musculoskeletal humanoid properties and discusses future challenges and prospects for the field."}}
{"id": "2602.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07498", "abs": "https://arxiv.org/abs/2602.07498", "authors": ["Zhufeng Xu", "Xuan Gao", "Feng-Lin Liu", "Haoxian Zhang", "Zhixue Fang", "Yu-Kun Lai", "Xiaoqiang Liu", "Pengfei Wan", "Lin Gao"], "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation", "comment": null, "summary": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.", "AI": {"tldr": "IM-Animation: A video diffusion model using compact 1D motion tokens for character animation that prevents identity leakage while maintaining motion semantics.", "motivation": "Existing character animation methods have limitations: explicit methods (using skeleton/DWPose) struggle with spatial mismatches and varying body scales, while implicit methods suffer from identity leakage and motion-appearance entanglement.", "method": "Proposes implicit motion representation compressing per-frame motion into compact 1D motion tokens, plus temporally consistent mask token-based retargeting module with temporal training bottleneck. Uses three-stage training strategy.", "result": "Achieves superior or competitive performance compared to state-of-the-art methods, effectively preventing identity information leakage while maintaining motion semantics.", "conclusion": "The proposed implicit motion representation and IM-Animation framework effectively address limitations of both explicit and implicit methods for character animation, offering improved performance and reduced identity leakage."}}
{"id": "2602.08092", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08092", "abs": "https://arxiv.org/abs/2602.08092", "authors": ["Majid Ghasemi", "Mark Crowley"], "title": "Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities", "comment": null, "summary": "Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this \"judging the judges\" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.", "AI": {"tldr": "The paper challenges the assumption that human feedback is truthful (Dogma 4 of RL), shows it fails in social settings, and proposes Epistemic Source Alignment (ESA) which judges feedback sources using safety axioms to guarantee convergence to true objectives even with majority bias.", "motivation": "Current AI alignment strategies assume human feedback is fundamentally truthful, but this assumption (Dogma 4 of RL) fails in social settings where evaluators can be sycophantic, lazy, or adversarial, leading to permanent misalignment through Objective Decoupling.", "method": "Proposes Epistemic Source Alignment (ESA) which uses sparse safety axioms to judge the source of feedback rather than the signal itself, implementing a \"judging the judges\" mechanism instead of relying on statistical consensus methods.", "result": "Theoretically proves ESA guarantees convergence to true objective even when majority of evaluators are biased, and empirically shows traditional consensus methods fail under majority collusion while ESA successfully recovers optimal policy.", "conclusion": "The assumption of truthful human feedback (Dogma 4) is fragile in social settings, requiring source-level judgment through ESA rather than signal-level consensus to ensure robust AI alignment against biased evaluators."}}
{"id": "2602.08537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08537", "abs": "https://arxiv.org/abs/2602.08537", "authors": ["Haoming Ye", "Yunxiao Xiao", "Cewu Lu", "Panpan Cai"], "title": "UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation", "comment": null, "summary": "Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.", "AI": {"tldr": "UniPlan extends tabletop manipulation domains to large-scale mobile manipulation by integrating visual-topological maps with PDDL planning, outperforming existing VLM/LLM approaches.", "motivation": "Existing VLM-symbolic planning systems like UniDomain are limited to tabletop manipulation and cannot handle long-horizon mobile-manipulation tasks in large-scale indoor environments.", "method": "UniPlan extends learned tabletop domains to support navigation, door traversal, and bimanual coordination. It uses a visual-topological map with scene images, grounds objects/states with VLM, compresses the map into PDDL representation, and uses off-the-shelf PDDL solvers for planning.", "result": "UniPlan significantly outperforms VLM and LLM+PDDL planning approaches in success rate, plan quality, and computational efficiency on human-raised tasks in large-scale environments with real-world imagery.", "conclusion": "UniPlan successfully unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation, enabling effective long-horizon mobile-manipulation planning in large-scale indoor environments."}}
{"id": "2602.07512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07512", "abs": "https://arxiv.org/abs/2602.07512", "authors": ["Tao Wang", "Chenyu Lin", "Chenwei Tang", "Jizhe Zhou", "Deng Xiong", "Jianan Li", "Jian Zhao", "Jiancheng Lv"], "title": "Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection", "comment": "paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12.2)", "summary": "Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \\textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.", "AI": {"tldr": "ZoomDet: Adaptive zoom-in framework for UAV object detection that non-uniformly zooms in on small objects to improve detection accuracy with minimal latency overhead.", "motivation": "UAV-captured images contain small, sparse foreground objects that hinder effective object detector optimization. Standard detectors struggle with these tiny objects compared to common scene images.", "method": "Two core designs: 1) Lightweight offset prediction with box-based zooming objective for non-uniform zooming, 2) Corner-aligned bounding box transformation to warp ground-truth boxes to zoomed space for training and predicted boxes back to original space for inference.", "result": "Extensive experiments on VisDrone, UAVDT, and SeaDronesSee datasets show significant improvements. On SeaDronesSee, ZoomDet provides >8.4 mAP gain with Faster R-CNN, adding only ~3ms latency. Architecture-independent approach.", "conclusion": "ZoomDet effectively addresses small object detection in UAV images through adaptive zooming, achieving substantial accuracy improvements with minimal computational overhead, making it practical for real-world UAV applications."}}
{"id": "2602.08104", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08104", "abs": "https://arxiv.org/abs/2602.08104", "authors": ["Risal Shahriar Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains \"downstream-first\" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.", "AI": {"tldr": "A gradient-based framework for interpretable failure analysis in MARL that detects initial failure sources, validates detection anomalies, and traces failure propagation through learned coordination pathways.", "motivation": "MARL is increasingly used in safety-critical domains, but current methods lack interpretable failure detection and attribution capabilities, making it difficult to diagnose cascading failures in multi-agent systems.", "method": "Two-stage gradient-based framework: Stage 1 uses Taylor-remainder analysis of policy-gradient costs for per-agent failure detection; Stage 2 uses geometric analysis of critic derivatives (first-order sensitivity and directional second-order curvature) to construct interpretable contagion graphs and validate detection decisions.", "result": "Achieved 88.2-99.4% Patient-0 detection accuracy across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, while providing interpretable geometric evidence for detection decisions.", "conclusion": "The framework moves beyond black-box detection to provide interpretable gradient-level forensics, offering practical tools for diagnosing cascading failures in safety-critical MARL systems through explainable failure analysis."}}
{"id": "2602.08557", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08557", "abs": "https://arxiv.org/abs/2602.08557", "authors": ["Marc Toussaint", "Cornelius V. Braun", "Eckart Cobo-Briesewitz", "Sayantan Auddy", "Armand Jordana", "Justin Carpentier"], "title": "Constrained Sampling to Guide Universal Manipulation RL", "comment": null, "summary": "We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.", "AI": {"tldr": "Sample-Guided RL uses model-based constraint solvers to sample feasible configurations and guide RL training for universal manipulation policies in contact-rich settings.", "motivation": "RL struggles with exploration and discovering complex manipulation strategies in sparse-reward contact-rich settings. The paper aims to leverage model-based solvers to guide RL training for universal policies that can control from any feasible start state to any feasible goal.", "method": "Proposes Sample-Guided RL which uses model-based constraint solvers to efficiently sample feasible configurations satisfying differentiable collision, contact, and force constraints. These samples guide RL for universal goal-conditioned manipulation policies through state visitation biasing, black-box optimization of open-loop trajectories, and optional behavior cloning loss.", "result": "In a minimalistic double sphere manipulation setting, the approach discovers complex manipulation strategies and achieves high success rates. In a more challenging panda arm setting, it achieves significant success rates over near-zero baselines and demonstrates breadth of complex whole-body-contact manipulation strategies.", "conclusion": "Sample-Guided RL effectively combines model-based constraint solvers with RL to overcome exploration challenges in contact-rich manipulation, enabling discovery of complex manipulation strategies and achieving high success rates in universal manipulation tasks."}}
{"id": "2602.07523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07523", "abs": "https://arxiv.org/abs/2602.07523", "authors": ["Zhen Zhang", "Qing Zhao", "Xiuhe Li", "Cheng Wang", "Guoqiang Zhu", "Yu Zhang", "Yining Huo", "Hongyi Yu", "Yi Zhang"], "title": "CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization", "comment": "This work has been submitted to the IEEE for possible publication.Please note that once the article has been published by IEEE, preprints on locations not specified above should be removed if possible", "summary": "In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the \"brain\" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.", "AI": {"tldr": "CA-YOLO bionic system improves target localization accuracy and small target recognition by integrating animal visual focusing mechanisms and human VOR-inspired tracking control.", "motivation": "Existing target localization systems have limitations in both accuracy and small target recognition capabilities, which are essential in modern complex environments.", "method": "Proposes CA-YOLO with bionic modules: small target detection head and Characteristic Fusion Attention Mechanism (CFAM) in YOLO backbone, plus VOR-inspired pan-tilt tracking control with central positioning, stability optimization, adaptive control, and intelligent recapture.", "result": "CA-YOLO outperforms original model on COCO and VisDrone datasets with average accuracy improvements of 3.94% and 4.90% respectively. Time-sensitive experiments validate system effectiveness.", "conclusion": "The bionic stabilized localization system successfully enhances target localization accuracy and small target recognition through biologically-inspired mechanisms, demonstrating practical effectiveness."}}
{"id": "2602.08121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08121", "abs": "https://arxiv.org/abs/2602.08121", "authors": ["Liying Wang", "Madison Lee", "Yunzhang Jiang", "Steven Chen", "Kewei Sha", "Yunhe Feng", "Frank Wong", "Lisa Hightow-Weidman", "Weichao Yuwen"], "title": "Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention", "comment": null, "summary": "Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an \"empathy trap,\" providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.", "AI": {"tldr": "GenAI-powered DBT coach \"Glow\" for HIV/substance use risk showed 73% safety performance, with solution analysis agent (90%) outperforming chain analysis agent (44%) due to \"empathy trap\" issues.", "motivation": "HIV and substance use epidemics share psychological drivers (impulsivity, maladaptive coping) that DBT targets, but scalability is limited. GenAI offers potential for personalized DBT coaching at scale, but safety infrastructure hasn't kept pace with rapid development.", "method": "Developed Glow (GenAI-powered DBT skills coach) delivering chain and solution analysis. Conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28) using HHH framework. Employed user-driven adversarial testing where participants identified target behaviors and generated realistic risk probes. Evaluated safety across 37 risk probe interactions.", "result": "Glow appropriately handled 73% of risk probes, but performance varied significantly by agent: solution analysis agent 90% appropriate handling vs chain analysis agent 44%. Safety failures included encouraging substance use and normalizing harmful behaviors. Chain analysis agent fell into \"empathy trap\" providing validation that reinforced maladaptive beliefs. Also identified 27 instances of DBT skill misinformation.", "conclusion": "First systematic safety evaluation of GenAI-delivered DBT coaching for HIV/substance use risk reduction reveals vulnerabilities requiring mitigation before clinical trials. HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions."}}
{"id": "2602.08571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08571", "abs": "https://arxiv.org/abs/2602.08571", "authors": ["Simon Hoffmann", "Simon Sagmeister", "Tobias Betz", "Joscha Bongard", "Sascha B\u00fcttner", "Dominic Ebner", "Daniel Esser", "Georg Jank", "Sven Goblirsch", "Alexander Langmann", "Maximilian Leitenstern", "Levent \u00d6gretmen", "Phillip Pitschi", "Ann-Kathrin Schwehn", "Cornelius Schr\u00f6der", "Marcel Weinmann", "Frederik Werner", "Boris Lohmann", "Johannes Betz", "Markus Lienkamp"], "title": "Head-to-Head autonomous racing at the limits of handling in the A2RL challenge", "comment": "Submitted to Science Robotics for possible publication", "summary": "Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.", "AI": {"tldr": "TUM team's autonomous racing algorithms and deployment strategies that won the inaugural Abu Dhabi Autonomous Racing League by emulating human driving behavior at vehicle performance limits.", "motivation": "Autonomous racing provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety through multi-agent interactions at performance limits.", "method": "Developed algorithms and deployment strategies that emulate human driving behavior, pushing vehicle handling limits and managing multi-vehicle interactions.", "result": "Successfully won the inaugural Abu Dhabi Autonomous Racing League (A2RL) competition.", "conclusion": "The paper highlights key enablers of their success and shares significant learnings from developing and deploying autonomous racing systems that can operate at performance limits in multi-agent scenarios."}}
{"id": "2602.07532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07532", "abs": "https://arxiv.org/abs/2602.07532", "authors": ["Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Evaluating Object-Centric Models beyond Object Discovery", "comment": "Project Page: https://guided-sa.github.io/eval-ocl/", "summary": "Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.", "AI": {"tldr": "OCL evaluation benchmarks have limitations: they don't properly assess representation usefulness and use disjoint metrics for localization vs. usefulness. The paper proposes using VLMs as evaluators and a unified task/metric.", "motivation": "Current OCL benchmarks fail to properly evaluate whether learned representations support compositional generalization and OOD robustness. They focus too much on object discovery and simple reasoning tasks, providing limited insights into representation usefulness.", "method": "1) Use instruction-tuned Vision-Language Models as scalable evaluators across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning. 2) Introduce a unified evaluation task and metric that jointly assesses localization (where) and representation usefulness (what). 3) Include a simple multi-feature reconstruction baseline as reference.", "result": "The paper proposes a new evaluation framework that addresses limitations of existing benchmarks by providing more comprehensive assessment of OCL representation usefulness through VLM-based evaluation and unified metrics.", "conclusion": "Current OCL evaluation methods are inadequate for assessing representation usefulness. The proposed framework using VLMs as evaluators and unified metrics provides better assessment of whether OCL models achieve their goals of supporting compositional generalization and OOD robustness."}}
{"id": "2602.08214", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08214", "abs": "https://arxiv.org/abs/2602.08214", "authors": ["Ziwei Wang", "Yuanhe Zhang", "Jing Chen", "Zhenhong Zhou", "Ruichao Liang", "Ruiying Du", "Ju Jia", "Cong Wu", "Yang Liu"], "title": "RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection", "comment": null, "summary": "Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.", "AI": {"tldr": "RECUR attack uses recursive entropy to trigger resource exhaustion in Large Reasoning Models by disrupting normal reasoning patterns, increasing output length 11x and reducing throughput 90%.", "motivation": "Large Reasoning Models require extended context for explicit reasoning, consuming substantial resources. Prior work shows adversarial inputs can trigger redundant reasoning, but the reflection component's safety issues remain underexplored despite its potential for over-reflection and excessive computing power consumption.", "method": "Introduces Recursive Entropy to quantify reflection risk, then develops RECUR (Resource Exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection) that constructs counterfactual questions to exploit LRM vulnerabilities.", "result": "Under normal inference, recursive entropy shows decreasing trend; RECUR disrupts this pattern, increasing output length up to 11x and decreasing throughput by 90%.", "conclusion": "The work reveals safety issues in inference itself and provides new perspective on robust reasoning by quantifying and exploiting reflection vulnerabilities in Large Reasoning Models."}}
{"id": "2602.08594", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08594", "abs": "https://arxiv.org/abs/2602.08594", "authors": ["Zhenguo Sun", "Bo-Sheng Huang", "Yibo Peng", "Xukun Li", "Jingyu Ma", "Yu Sun", "Zhe Li", "Haojun Jiang", "Biao Gao", "Zhenshan Bing", "Xinlong Wang", "Alois Knoll"], "title": "MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation", "comment": null, "summary": "Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.", "AI": {"tldr": "MOSAIC is an open-source system for humanoid motion tracking and teleoperation that combines a general motion tracker with interface-specific residual adaptation to bridge sim-to-real gaps.", "motivation": "Existing generalist humanoid motion trackers achieve good simulation metrics but fail on hardware during sustained teleoperation due to interface- and dynamics-induced errors, creating a need for robust teleoperation systems.", "method": "1) Learn teleoperation-oriented general motion tracker via RL on multi-source motion bank with adaptive resampling and world-frame motion consistency rewards. 2) Perform rapid residual adaptation: train interface-specific policy with minimal data and distill into general tracker through additive residual module.", "result": "MOSAIC outperforms naive fine-tuning or continual learning approaches, validated through systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.", "conclusion": "MOSAIC provides an effective full-stack solution for humanoid motion tracking and teleoperation that bridges sim-to-real gaps through its two-stage approach of general motion learning followed by interface-specific residual adaptation."}}
{"id": "2602.07534", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07534", "abs": "https://arxiv.org/abs/2602.07534", "authors": ["Mowmita Parvin Hera", "Md. Shahriar Mahmud Kallol", "Shohanur Rahman Nirob", "Md. Badsha Bulbul", "Jubayer Ahmed", "M. Zhourul Islam", "Hazrat Ali", "Mohammmad Farhad Bulbul"], "title": "Fine-Grained Cat Breed Recognition with Global Context Vision Transformer", "comment": "4 pages, accepted at International Conference on Computer and Information Technology (ICCIT) 2025", "summary": "Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.", "AI": {"tldr": "GCViT-Tiny transformer model achieves 92-94% accuracy for cat breed classification using Oxford-IIIT Pet Dataset with data augmentation.", "motivation": "Accurate cat breed identification is challenging due to subtle visual differences in fur patterns, facial structure, and color, requiring advanced computer vision solutions.", "method": "Used Global Context Vision Transformer (GCViT-Tiny) architecture on Oxford-IIIT Pet Dataset with extensive data augmentation including rotation, horizontal flipping, and brightness adjustment.", "result": "Achieved 92.00% test accuracy and 94.54% validation accuracy, demonstrating transformer effectiveness for fine-grained image classification.", "conclusion": "Transformer-based architectures like GCViT are effective for cat breed classification with applications in veterinary diagnostics, shelter management, and mobile recognition systems."}}
{"id": "2602.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08222", "abs": "https://arxiv.org/abs/2602.08222", "authors": ["Zehao Chen", "Gongxun Li", "Tianxiang Ai", "Yifei Li", "Zixuan Huang", "Wang Zhou", "Fuzhen Zhuang", "Xianglong Liu", "Jianxin Li", "Deqing Wang", "Yikun Ban"], "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "comment": null, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "AI": {"tldr": "WMSS uses weak historical checkpoints to guide post-training optimization, overcoming saturation by identifying and reinforcing recoverable learning gaps through entropy analysis.", "motivation": "Post-training optimization faces a saturation bottleneck where highly confident models show diminishing returns; existing methods miss latent supervision signals in models' own historical weak states.", "method": "WMSS leverages weak checkpoints to guide optimization by identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning.", "result": "Experiments on mathematical reasoning and code generation show effective performance improvements with zero additional inference cost.", "conclusion": "Weak agents can make strong agents stronger by providing informative supervision signals from historical weak states, enabling continued optimization beyond conventional saturation."}}
{"id": "2602.08599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08599", "abs": "https://arxiv.org/abs/2602.08599", "authors": ["Kenghou Hoi", "Yuze Wu", "Annan Ding", "Junjie Wang", "Anke Zhao", "Chengqian Zhang", "Fei Gao"], "title": "A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation", "comment": null, "summary": "Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.", "AI": {"tldr": "Aerial manipulation framework using low-cost tactile sensors for force-aware grasping without external motion capture", "motivation": "Existing aerial manipulation systems either use heavy/expensive force sensors unsuitable for quadrotors or operate without force feedback, risking damage to fragile objects during grasping", "method": "Proposes a force-aware grasping framework with six low-cost skin-like tactile sensors using magnetic-based sensing modules that provide 3D force measurements, eliminates geomagnetic interference with reference Hall sensor, and simplifies calibration", "result": "System enables precise force-aware grasping control, safe manipulation of fragile objects, real-time weight measurement, and fully onboard operation without external motion capture; validated through balloon grasping, dynamic load tests, and ablation studies", "conclusion": "The approach significantly enhances practicality of force-sensitive aerial manipulation through low-cost, sensitive tactile sensing and fully autonomous onboard operation"}}
{"id": "2602.07535", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07535", "abs": "https://arxiv.org/abs/2602.07535", "authors": ["Md Sazidur Rahman", "Kjersti Engan", "Kathinka D\u00e6hli Kurz", "Mahdieh Khanmohammadi"], "title": "Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis", "comment": null, "summary": "Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.", "AI": {"tldr": "Bi-temporal analysis of stroke using CTP and DWI with radiomic and deep features reveals tissue phenotype differences between salvageable and non-salvageable regions.", "motivation": "Single time-point segmentations fail to capture biological heterogeneity and temporal evolution of stroke. Need better methods to characterize ischemic tissue changes over time.", "method": "Bi-temporal framework using admission CTP and follow-up DWI with statistical descriptors, radiomic texture features, and deep feature embeddings from mJ-Net and nnU-Net architectures. Six ROIs constructed from intersected masks at T1 and T2.", "result": "Meaningful clustering of region-level representations with penumbra regions showing significant feature differences based on final state, while core regions did not. Deep features (especially mJ-Net) showed strong separation between salvageable and non-salvageable tissue.", "conclusion": "Encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution."}}
{"id": "2602.08229", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08229", "abs": "https://arxiv.org/abs/2602.08229", "authors": ["Yifan Yang", "Jinjia Li", "Kunxi Li", "Puhao Zheng", "Yuanyi Wang", "Zheyan Qu", "Yang Yu", "Jianmin Wu", "Ming Li", "Hongxia Yang"], "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation", "comment": null, "summary": "The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a \"centralized black box\" into a \"decentralized endorsement\" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.", "AI": {"tldr": "Decentralized blockchain-based evaluation framework reduces LLM benchmark instability by enabling diverse hardware validation, improving ranking reliability.", "motivation": "Current centralized LLM evaluation suffers from opacity, overfitting, and hardware-induced variance, with empirical analysis showing evaluation inconsistency where standard deviation across repeated runs exceeds performance gaps between top models, making rankings statistically unreliable.", "method": "Proposes a decentralized evaluation framework using blockchain-based protocol to enable hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes, incentivizing global contributors as independent validators with robust reward system to ensure integrity.", "result": "Decentralized framework reduces standard deviation across ten runs on same model from 1.67 to 0.28, significantly improving statistical confidence in model rankings over conventional frameworks.", "conclusion": "Transforms evaluation from centralized black box to decentralized endorsement through multi-party consensus and diverse inference environments, creating more stable and representative metrics for LLM assessment."}}
{"id": "2602.08602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08602", "abs": "https://arxiv.org/abs/2602.08602", "authors": ["Renming Huang", "Chendong Zeng", "Wenjing Tang", "Jingtian Cai", "Cewu Lu", "Panpan Cai"], "title": "Mimic Intent, Not Just Trajectories", "comment": "Under review", "summary": "While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \\textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \\textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \\textit{Intent token} that facilitates planning and transfer, and multi-scale \\textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \\textit{next-scale autoregression}, performing progressive \\textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \\textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.", "AI": {"tldr": "MINT proposes disentangling behavior intent from execution details in imitation learning via multi-scale frequency-space tokenization, enabling better adaptation and one-shot skill transfer.", "motivation": "Current imitation learning approaches like VLA models struggle with adaptation to environmental changes and skill transfer because they mimic raw trajectories without understanding underlying intent.", "method": "Uses multi-scale frequency-space tokenization to enforce spectral decomposition of action chunk representation, creating Intent tokens (low-frequency global structure) and multi-scale Execution tokens (high-frequency details). Policy generates trajectories through next-scale autoregression with progressive intent-to-execution reasoning.", "result": "Achieves state-of-the-art success rates on manipulation benchmarks and real robots, with superior inference efficiency, robust generalization against disturbances, and effective one-shot skill transfer.", "conclusion": "Explicitly disentangling intent from execution details in end-to-end imitation learning enables better adaptation, generalization, and efficient skill transfer through the proposed MINT framework."}}
{"id": "2602.07540", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07540", "abs": "https://arxiv.org/abs/2602.07540", "authors": ["Huimin Yan", "Liang Bai", "Xian Yang", "Long Chen"], "title": "LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing", "comment": null, "summary": "Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.", "AI": {"tldr": "LGDEA introduces LLM-guided diagnostic evidence alignment for medical vision-language pretraining, reducing reliance on paired data by focusing on evidence-level alignment consistent with medical diagnosis.", "motivation": "Existing CLIP-style medical VLP methods have limitations: global alignment is dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. This makes learning reliable diagnostic representations difficult, especially in medical scenarios with limited paired data.", "method": "Proposes LLM-Guided Diagnostic Evidence Alignment (LGDEA) that shifts pretraining toward evidence-level alignment. Uses LLMs to extract key diagnostic evidence from radiology reports, constructs a shared diagnostic evidence space for evidence-aware cross-modal alignment, and effectively exploits abundant unpaired medical images and reports.", "result": "Achieves consistent and significant improvements on phrase grounding, image-text retrieval, and zero-shot classification. Even rivals pretraining methods that rely on substantial paired data.", "conclusion": "LGDEA addresses limitations of existing medical VLP methods by focusing on diagnostic evidence alignment, substantially reducing reliance on paired data while improving performance on medical vision-language tasks."}}
{"id": "2602.08240", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08240", "abs": "https://arxiv.org/abs/2602.08240", "authors": ["Xun Su", "Huamin Wang", "Qi Zhang"], "title": "PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.", "AI": {"tldr": "PTS-SNN is a parameter-efficient neuromorphic framework that aligns frozen SSL backbones with spiking dynamics for speech emotion recognition, achieving ANN-competitive accuracy with minimal parameters and energy.", "motivation": "Conventional SER models are computationally expensive for edge devices, and SNNs face distribution mismatch when integrating with SSL representations, degrading their information coding capacity.", "method": "Proposes Prompt-Tuned Spiking Neural Networks with Temporal Shift Spiking Encoder for local temporal dependencies and Context-Aware Membrane Potential Calibration using Spiking Sparse Linear Attention to regulate PLIF neurons with soft prompts.", "result": "Achieves 73.34% accuracy on IEMOCAP (comparable to ANNs) with only 1.19M trainable parameters and 0.35 mJ inference energy per sample across five multilingual datasets.", "conclusion": "PTS-SNN effectively bridges the domain gap between SSL representations and spiking dynamics, enabling energy-efficient, high-performance SER on resource-constrained edge devices."}}
{"id": "2602.08653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08653", "abs": "https://arxiv.org/abs/2602.08653", "authors": ["Jiarui Zhang", "Chengyong Lei", "Chengjiang Dai", "Lijie Wang", "Zhichao Han", "Fei Gao"], "title": "High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning", "comment": null, "summary": "Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.", "AI": {"tldr": "End-to-end RL framework for quadrotor UAV navigation with model-based safety mechanisms, combining physics-informed rewards during training and real-time safety filters during deployment for high-speed flight with formal safety guarantees.", "motivation": "Traditional modular pipelines for quadrotor UAV navigation suffer from cumulative latency, while purely RL approaches lack formal safety guarantees, creating a need for a solution that bridges high-speed autonomous navigation with robust safety assurances.", "method": "Hybrid end-to-end RL framework augmented with model-based safety mechanisms: (1) Physics-informed reward structure during training for global navigational guidance, (2) Real-time safety filter during deployment that projects policy outputs onto provably safe sets to enforce strict collision-avoidance constraints.", "result": "Outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Demonstrates strong generalization enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.", "conclusion": "The proposed hybrid architecture successfully reconciles high-speed flight with robust safety assurances, bridging the gap between traditional modular approaches and purely RL methods by combining the strengths of both through physics-informed training and formal safety guarantees during deployment."}}
{"id": "2602.07544", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07544", "abs": "https://arxiv.org/abs/2602.07544", "authors": ["Sebastian Bock", "Leonie Sch\u00fc\u00dfler", "Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "MUFASA: A Multi-Layer Framework for Slot Attention", "comment": "Authors Sebastian Bock and Leonie Sch\u00fc\u00dfler contributed equally. Project page: https://leonieschuessler.github.io/mufasa/", "summary": "Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.", "AI": {"tldr": "MUFASA improves slot attention for unsupervised object segmentation by aggregating features from multiple ViT layers instead of just the last layer, achieving better segmentation and faster convergence.", "motivation": "Current slot attention methods only use the last layer of pre-trained ViTs, ignoring valuable semantic information encoded across other layers that could improve object segmentation.", "method": "MUFASA computes slot attention across multiple feature layers of ViT encoders and uses a fusion strategy to aggregate slots from different layers into unified object-centric representations.", "result": "Integrating MUFASA into existing OCL methods improves segmentation results across multiple datasets, sets new state-of-the-art performance, and improves training convergence with minimal inference overhead.", "conclusion": "Leveraging multi-layer ViT features through MUFASA significantly enhances unsupervised object segmentation performance while maintaining efficiency."}}
{"id": "2602.08241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08241", "abs": "https://arxiv.org/abs/2602.08241", "authors": ["Siqu Ou", "Tianrui Wan", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs", "comment": null, "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.", "AI": {"tldr": "SAYO uses reinforcement learning with region-level visual attention rewards to improve multimodal LLMs' visual focus and reasoning accuracy.", "motivation": "Current multimodal LLMs have weak visual focus - early visual misalignment isn't corrected during reasoning, causing error propagation. This stems from inadequate credit assignment for visual attention during training.", "method": "Proposes SAYO, a visual reasoning model trained with RL framework using region-level visual attention-based reward that aligns optimization signals with visually grounded reasoning steps.", "result": "Extensive experiments across multiple multimodal benchmarks show SAYO consistently improves performance on diverse reasoning and perception tasks.", "conclusion": "Explicitly aligning optimization signals with visual attention through RL rewards enables learning more reliable attention behaviors and improves multimodal reasoning."}}
{"id": "2602.08776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08776", "abs": "https://arxiv.org/abs/2602.08776", "authors": ["Cuijie Xu", "Shurui Zheng", "Zihao Su", "Yuanfan Xu", "Tinghao Yi", "Xudong Zhang", "Jian Wang", "Yu Wang", "Jinchen Yu"], "title": "Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch", "comment": "14 pages, 9 figures, 5 tables", "summary": "Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to \"Intent Cloning\" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a \"virtual equilibrium point\", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \\href{https://xucj98.github.io/mind-the-gap-page/}{project page}.", "AI": {"tldr": "Dual-State Conditioning framework shifts BC from mimicking robot trajectories to cloning operator intent, using intent-execution mismatch as signal for implicit force perception and system identification.", "motivation": "Standard Behavior Cloning fails in teleoperation because it ignores the operator's compensatory control for hardware imperfections (latency, friction, lack of force feedback), treating the mismatch between operator intent and robot execution as noise rather than valuable signal.", "method": "Proposes Intent Cloning that predicts master commands rather than robot trajectories. Uses Dual-State Conditioning to treat intent-execution mismatch as critical signal encoding implicit forces and operator strategies. Conditions policy on mismatch history for implicit system identification, and formulates policy as trajectory inpainter to handle inference latency.", "result": "Validated on sensorless, low-cost bi-manual setup. Standard execution-cloning fails at contact-rich manipulation and dynamic tracking, while mismatch-aware approach achieves robust success, enabling force perception and dynamic compensation without explicit force sensing.", "conclusion": "Presents minimalist BC framework for low-cost hardware that learns implicit impedance control through intent cloning, using mismatch as signal for force perception and system compensation without requiring explicit force sensors."}}
{"id": "2602.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07550", "abs": "https://arxiv.org/abs/2602.07550", "authors": ["Hussni Mohd Zakir", "Eric Tatt Wei Ho"], "title": "Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation", "comment": "10 pages, 3 figures, 7 tables", "summary": "Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a \"Safest vs. Optimal\" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a \"Semantic Selection Gap\" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the \"Last-Layer\" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.", "AI": {"tldr": "FSSDINO shows frozen DINOv3 features work surprisingly well for few-shot segmentation with simple prototypes, but Oracle analysis reveals better intermediate layers exist that current selection methods can't find.", "motivation": "To investigate the intrinsic few-shot segmentation capabilities of frozen DINOv3 features and understand the gap between standard last-layer usage and optimal intermediate representations.", "method": "Training-free baseline FSSDINO using class-specific prototypes and Gram-matrix refinement on frozen DINOv3 features, with Oracle-guided layer analysis to identify optimal intermediate representations.", "result": "FSSDINO is highly competitive with specialized methods across binary, multi-class, and cross-domain benchmarks, but Oracle analysis reveals significant performance gap between last-layer and optimal intermediate features.", "conclusion": "The \"Last-Layer\" is a deceptively strong baseline, but there's a \"Semantic Selection Gap\" where traditional heuristics fail to identify optimal intermediate features that could match compute-intensive adaptation methods."}}
{"id": "2602.08253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08253", "abs": "https://arxiv.org/abs/2602.08253", "authors": ["Baoyun Zhao", "He Wang", "Liang Zeng"], "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design", "comment": null, "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.", "AI": {"tldr": "G-LNS is a generative evolutionary framework that uses LLMs to co-evolve complementary destroy and repair operators for Large Neighborhood Search, outperforming existing LLM-based heuristic design methods and classical solvers on combinatorial optimization problems.", "motivation": "Existing LLM-based Automated Heuristic Design approaches are limited to fixed heuristic forms (constructive priority rules or parameterized local search guidance), restricting structural exploration and making it difficult to escape deep local optima in complex combinatorial optimization problems.", "method": "G-LNS extends LLM-based AHD to automatically design Large Neighborhood Search operators by leveraging LLMs to co-evolve tightly coupled pairs of destroy and repair operators. It uses a cooperative evaluation mechanism that explicitly captures operator interactions to discover complementary logic for effective structural disruption and reconstruction.", "result": "Extensive experiments on challenging COP benchmarks (TSP and CVRP) show G-LNS significantly outperforms both LLM-based AHD methods and strong classical solvers. The discovered heuristics achieve near-optimal solutions with reduced computational budgets and exhibit robust generalization across diverse and unseen instance distributions.", "conclusion": "G-LNS successfully extends LLM-based heuristic design beyond fixed forms to discover complementary destroy/repair operator pairs through cooperative co-evolution, enabling more effective structural exploration and better performance on complex combinatorial optimization problems."}}
{"id": "2602.08784", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08784", "abs": "https://arxiv.org/abs/2602.08784", "authors": ["Santiago Montiel-Mar\u00edn", "Miguel Antunes-Garc\u00eda", "Fabio S\u00e1nchez-Garc\u00eda", "Angel Llamazares", "Holger Caesar", "Luis M. Bergasa"], "title": "GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion", "comment": "8 pages, 6 figures. Accepted to IEEE ICRA 2026", "summary": "Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.", "AI": {"tldr": "GaussianCaR uses Gaussian Splatting as a universal view transformer for efficient camera-radar fusion in BEV segmentation, achieving state-of-the-art performance with 3.2x faster inference.", "motivation": "Vision-only methods for autonomous vehicle perception can benefit from cost-effective radar fusion, but existing methods struggle with view disparity between cameras and radar. There's a need for efficient fusion that bridges the gap between different sensor viewpoints.", "method": "Repurposes Gaussian Splatting as a universal view transformer to map both image pixels and radar points into a common Bird's-Eye View representation. Uses an end-to-end network called GaussianCaR that combines multi-scale fusion with a transformer decoder to extract BEV features from raw sensor data.", "result": "Achieves 57.3% IoU for vehicles, 82.9% IoU for roads, and 50.1% IoU for lane dividers on nuScenes dataset, performing on par with or surpassing state-of-the-art methods while maintaining 3.2x faster inference runtime.", "conclusion": "Gaussian Splatting provides an effective solution for bridging view disparity in camera-radar fusion, enabling efficient and accurate BEV segmentation for autonomous vehicles with significantly improved inference speed."}}
{"id": "2602.07554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07554", "abs": "https://arxiv.org/abs/2602.07554", "authors": ["Guandong Li", "Yijun Ding"], "title": "FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation", "comment": null, "summary": "Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.", "AI": {"tldr": "FlexID is a training-free framework for personalized text-to-image generation that uses intent-aware modulation to balance identity fidelity with textual adaptability through orthogonal decoupling and dynamic gating.", "motivation": "Existing training-free methods for personalized text-to-image generation rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. This limitation hinders the seamless integration of specific identities into textual descriptions.", "method": "FlexID uses intent-aware modulation with orthogonal decoupling: 1) Semantic Identity Projector (SIP) injects high-level priors into language space, 2) Visual Feature Anchor (VFA) ensures structural fidelity in latent space, and 3) Context-Aware Adaptive Gating (CAG) dynamically modulates weights based on editing intent and diffusion timesteps to relax rigid constraints when strong editing is needed.", "result": "Extensive experiments on IBench demonstrate that FlexID achieves state-of-the-art balance between identity consistency and text adherence, offering efficient solution for complex narrative generation.", "conclusion": "FlexID provides a novel training-free framework that successfully addresses the identity-text conflict through intent-aware modulation, achieving superior balance between identity preservation and semantic variation for personalized text-to-image generation."}}
{"id": "2602.08254", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08254", "abs": "https://arxiv.org/abs/2602.08254", "authors": ["Arman Aghaee", "Sepehr Asgarian", "Jouhyun Jeon"], "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities", "comment": "Presented in AAAI 2026 Singapore at the workshop of Health Intelligence", "summary": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.", "AI": {"tldr": "SynthAgent is a multi-agent system that simulates high-fidelity virtual patients with obesity and comorbid mental disorders, using clinical evidence and personality traits to model disease progression and treatment response.", "motivation": "Real-world patient data is often fragmented, biased, and privacy-restricted, making it difficult to study complex diseases like obesity with comorbid mental disorders. There's a need for simulated patients that can overcome these limitations while maintaining clinical fidelity.", "method": "Developed SynthAgent, a Multi-Agent System framework that integrates clinical evidence from claims data, population surveys, and patient literature. Creates personalized virtual patients with personality traits influencing adherence, emotion regulation, and lifestyle behaviors through autonomous agent interactions.", "result": "Evaluation of 100+ generated patients showed GPT-5 and Claude 4.5 Sonnet achieved highest fidelity as core engines in the MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. The system successfully simulates disease progression, treatment response, and life management.", "conclusion": "SynthAgent provides a scalable, privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in medical and psychological domains, addressing limitations of real-world data while maintaining clinical relevance."}}
{"id": "2602.08799", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08799", "abs": "https://arxiv.org/abs/2602.08799", "authors": ["Robin Dehler", "Michael Buchholz"], "title": "A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles", "comment": "8 pages, 6 figures, 2 tables, published in RA-L", "summary": "Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.", "AI": {"tldr": "A generic function offloading framework for CAVs that uses location-based decision making to distribute computational tasks between local and remote devices, demonstrated on trajectory planning with QoS guarantees.", "motivation": "Address computational capacity and energy limitations of Connected Automated Vehicles (CAVs) and autonomous robots by enabling efficient distribution of computational tasks between local and remote computing resources.", "method": "Propose a generic function offloading framework with flexible decision-making algorithms and QoS requirements, featuring an efficient location-based approach where offloading decisions depend on CAV location. Applied to service-oriented trajectory planning using Multi-Access Edge Computing (MEC) servers.", "result": "The framework successfully guarantees QoS for trajectory planning while improving computational efficiency of CAVs. Simulation shows adaptability to diverse scenarios with simultaneous offloading requests from multiple CAVs. Both simulation and real-world evaluations demonstrate effectiveness.", "conclusion": "The proposed function offloading framework provides a flexible, location-based solution for computational task distribution in CAVs, effectively addressing computational limitations while maintaining QoS requirements and adapting to multi-vehicle scenarios."}}
{"id": "2602.07555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07555", "abs": "https://arxiv.org/abs/2602.07555", "authors": ["Francesco Taioli", "Shiping Yang", "Sonia Raychaudhuri", "Marco Cristani", "Unnat Jain", "Angel X Chang"], "title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation", "comment": null, "summary": "Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer \"Is this the target object?\" and \"Why should I take this action?\" The reasoning process unfolds in three stages: \"think\", \"think summary\", and \"action\", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.", "AI": {"tldr": "Proposes a 3B-parameter Vision-Language-Action agent for object navigation that performs explicit image-grounded reasoning in three stages (think, think summary, action) instead of using multi-model pipelines or raw embedding matching.", "motivation": "Existing methods have limitations: end-to-end models struggle with generalization and lack explainability, while modular LLM-based pipelines suffer from error propagation, high computational cost, and difficulty integrating reasoning back into navigation policy.", "method": "A compact 3B-parameter Vision-Language-Action agent that performs explicit image-grounded reasoning through three stages: \"think\" (reasoning about object recognition), \"think summary\" (condensed reasoning), and \"action\" (action selection), removing the need for stitched multi-model pipelines.", "result": "The approach yields improved explainability, stronger generalization, and more efficient navigation compared to existing methods, with code and dataset available upon acceptance.", "conclusion": "The proposed VLA agent provides a more effective solution for language-driven object navigation by combining human-like embodied reasoning for both object recognition and action selection in a single compact model."}}
{"id": "2602.08268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08268", "abs": "https://arxiv.org/abs/2602.08268", "authors": ["Akinori Maeda", "Yuto Sekiya", "Sota Sugimura", "Tomoya Asai", "Yu Tsuda", "Kohei Ikeda", "Hiroshi Fujii", "Kohei Watanabe"], "title": "Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI", "comment": "9 pages, 5 figures", "summary": "Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.", "AI": {"tldr": "Puda is a user-sovereign architecture that aggregates personal data across services with three privacy levels, achieving 97.2% personalization performance while protecting privacy.", "motivation": "Current data centralization creates siloed ecosystems that restrict user sovereignty and impede cross-service data use, while LLM-based agents demand highly personalized services requiring diverse personal data - creating a privacy-personalization trade-off challenge.", "method": "Proposed Puda (Private User Dataset Agent), a browser-based system enabling client-side data management with three privacy levels: Detailed Browsing History, Extracted Keywords, and Predefined Category Subsets.", "result": "Predefined Category Subsets achieve 97.2% of the personalization performance obtained when sharing Detailed Browsing History, as evaluated via LLM-as-a-Judge framework across three criteria.", "conclusion": "Puda enables effective multi-granularity management of the privacy-personalization trade-off, providing an AI-native foundation for user sovereignty to safely leverage personalized AI potential."}}
{"id": "2602.08821", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08821", "abs": "https://arxiv.org/abs/2602.08821", "authors": ["Robin Dehler", "Oliver Schumann", "Jona Ruof", "Michael Buchholz"], "title": "Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems", "comment": null, "summary": "The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.", "AI": {"tldr": "SOA-based safety framework for validating remote services in connected autonomous vehicles to ensure reliability while offloading functions for computational efficiency.", "motivation": "Function offloading in CAVs reduces computational load but introduces safety risks from corrupted remote data via attacks or transmission interception, requiring robust validation mechanisms.", "method": "Developed a multi-staged safety analysis framework integrated into SOFOF (Service-Oriented Function Offloading Framework) that validates remote service reliability and received data based on service composition of local and remote services.", "result": "Evaluation shows the framework balances computational complexity reduction (for energy savings) with effective detection of corrupted remote service data.", "conclusion": "The integrated safety framework enables safe function offloading in distributed ITS by validating remote service reliability while maintaining computational efficiency benefits."}}
{"id": "2602.07564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07564", "abs": "https://arxiv.org/abs/2602.07564", "authors": ["Xiaoyan Zhang", "Zechen Bai", "Haofan Wang", "Yiren Song"], "title": "SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens", "comment": null, "summary": "Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.", "AI": {"tldr": "SIGMA is a post-training framework that enables diffusion transformers to handle interleaved multi-condition generation using selective attribute tokens, improving compositional editing and controllability over single-condition models like Bagel.", "motivation": "Existing unified models like Bagel are limited to single-condition inputs and lack flexibility for synthesizing results from multiple heterogeneous sources, restricting their ability to handle complex compositional tasks.", "method": "Introduces selective multi-attribute tokens (style, content, subject, identity) that enable interpretation and composition of multiple visual conditions in interleaved text-image sequences, applied through post-training on Bagel backbone with 700K interleaved examples.", "result": "SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment, showing substantial improvements over Bagel on compositional tasks with better controllability, cross-condition consistency, and visual quality.", "conclusion": "SIGMA successfully extends diffusion transformers to handle interleaved multi-condition generation, demonstrating that selective attribute tokens and post-training can significantly enhance model flexibility and performance on complex visual tasks."}}
{"id": "2602.08276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08276", "abs": "https://arxiv.org/abs/2602.08276", "authors": ["Haoyu Jia", "Kento Kawaharazuka", "Kei Okada"], "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis", "comment": null, "summary": "Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \\texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \\texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.", "AI": {"tldr": "Proposes Structural Context Model - a formal framework for analyzing LLM agents based on context structure, with declarative implementation and Semantic Dynamics Analysis workflow for systematic agent design.", "motivation": "Current LLM agent research is fragmented with concepts mixed with implementation details, lacking an analyzable formal model for implementation-independent characterization and comparison of agents.", "method": "Introduces Structural Context Model for formal analysis of LLM agents from context perspective, plus declarative implementation framework and Semantic Dynamics Analysis workflow for agent engineering lifecycle.", "result": "Demonstrated on dynamic monkey-banana problem variants, achieving up to 32 percentage points improvement in success rate on most challenging setting.", "conclusion": "Provides unified formal foundation for LLM agent research, enabling principled analysis, systematic design iteration, and addressing fragmentation in the field."}}
{"id": "2602.08845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08845", "abs": "https://arxiv.org/abs/2602.08845", "authors": ["Lazaro F. Torres", "Carlos I. Aldana", "Emmanuel Nu\u00f1o", "Emmanuel Cruz-Zavala"], "title": "Finite-Time Teleoperation of Euler-Lagrange Systems via Energy-Shaping", "comment": null, "summary": "This paper proposes a family of finite-time controllers for the bilateral teleoperation of fully actuated nonlinear Euler-Lagrange systems. Based on the energy-shaping framework and under the standard assumption of passive interactions with the human and the environment, the controllers ensure that the position error and velocities globally converge to zero in the absence of time delays. In this case, the closed-loop system admits a homogeneous approximation of negative degree, and thus the control objective is achieved in finite-time. The proposed controllers are simple, continuous-time proportional-plus-damping-injection schemes, validated through both simulation and experimental results.", "AI": {"tldr": "Finite-time controllers for bilateral teleoperation of nonlinear Euler-Lagrange systems using energy-shaping framework with continuous-time proportional-plus-damping-injection schemes.", "motivation": "To develop finite-time controllers for bilateral teleoperation systems that can achieve global convergence of position error and velocities to zero in finite time, addressing the limitations of asymptotic convergence in existing approaches.", "method": "Energy-shaping framework with continuous-time proportional-plus-damping-injection schemes, assuming passive interactions with human and environment, ensuring closed-loop system admits homogeneous approximation of negative degree.", "result": "Controllers ensure global convergence of position error and velocities to zero in finite time in absence of time delays, validated through both simulation and experimental results.", "conclusion": "Proposed family of finite-time controllers provides simple, effective solution for bilateral teleoperation of nonlinear Euler-Lagrange systems with finite-time convergence properties."}}
{"id": "2602.07565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07565", "abs": "https://arxiv.org/abs/2602.07565", "authors": ["Jingzhe Ma", "Meng Zhang", "Jianlong Yu", "Kun Liu", "Zunxiao Xu", "Xue Cheng", "Junjie Zhou", "Yanfei Wang", "Jiahang Li", "Zepeng Wang", "Kazuki Osamura", "Rujie Liu", "Narishige Abe", "Jingjie Wang", "Shunli Zhang", "Haojun Xie", "Jiajun Wu", "Weiming Wu", "Wenxiong Kang", "Qingshuo Gao", "Jiaming Xiong", "Xianye Ben", "Lei Chen", "Lichen Song", "Junjian Cui", "Haijun Xiong", "Junhao Lu", "Bin Feng", "Mengyuan Liu", "Ji Zhou", "Baoquan Zhao", "Ke Xu", "Yongzhen Huang", "Liang Wang", "Manuel J Marin-Jimenez", "Md Atiqur Rahman Ahad", "Shiqi Yu"], "title": "Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025", "comment": "Accepted by IJCB 2025(https://ijcb2025.ieee-biometrics.org/competitions/)", "summary": "Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.", "AI": {"tldr": "HID 2025 competition on gait recognition using SUSTech-Competition dataset achieved 94.2% accuracy, setting new benchmark despite increased difficulty.", "motivation": "Human identification at distance (HID) is challenging with traditional biometrics; gait recognition offers practical alternative for long-distance scenarios. Need for fair evaluation platform and progress in cross-domain generalization.", "method": "Annual HID competition since 2020, using SUSTech-Competition dataset since 2023 with clothing, carried objects, and view angle variations. No dedicated training data provided - participants use external datasets. Different random seeds each year for evaluation splits to prevent overfitting.", "result": "Despite heightened difficulty in HID 2025, participants achieved further improvements over previous years. Best-performing method reached 94.2% accuracy, setting new benchmark on this challenging dataset.", "conclusion": "Algorithmic advances in gait recognition can surpass previous accuracy limits. Competition provides fair assessment platform, analyzes technical trends, and outlines future research directions for gait recognition."}}
{"id": "2602.08295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08295", "abs": "https://arxiv.org/abs/2602.08295", "authors": ["Ilya Levin"], "title": "The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI", "comment": "19 pages", "summary": "The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.\n  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.\n  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.", "AI": {"tldr": "The paper introduces \"Vibe-Automation\" as a conceptual framework for understanding generative AI's epistemological shift from optimizing metrics to navigating contextual coherence, requiring new \"Vibe-Engineering\" roles.", "motivation": "To characterize generative AI as a qualitative epistemological shift rather than incremental advance, moving beyond traditional automation to operationalize tacit regularities and contextual judgment.", "method": "Introduces the concept of Vibe-Automation and proposes a conceptual framework structured across three analytical levels and three domains of action (faculty worldview, industry relations, curriculum design).", "result": "Identifies that generative AI operationalizes sensitivities to tone, intent, and situated judgment through high-dimensional latent representations, shifting human roles toward Vibe-Engineering.", "conclusion": "Generative AI represents an epistemological shift requiring deliberate engagement to avoid risks like mode collapse and cultural homogenization, with implications for educational and institutional transformation."}}
{"id": "2602.08963", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08963", "abs": "https://arxiv.org/abs/2602.08963", "authors": ["Katharina Friedl", "No\u00e9mie Jaquier", "Seungyeon Kim", "Jens Lundell", "Danica Kragic"], "title": "Reduced-order Control and Geometric Structure of Learned Lagrangian Latent Dynamics", "comment": "20 pages, 15 figures", "summary": "Model-based controllers can offer strong guarantees on stability and convergence by relying on physically accurate dynamic models. However, these are rarely available for high-dimensional mechanical systems such as deformable objects or soft robots. While neural architectures can learn to approximate complex dynamics, they are either limited to low-dimensional systems or provide only limited formal control guarantees due to a lack of embedded physical structure. This paper introduces a latent control framework based on learned structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems. We derive a reduced tracking law for fully actuated systems and adopt a Riemannian perspective on projection-based model-order reduction to study the resulting latent and projected closed-loop dynamics. By quantifying the sources of modeling error, we derive interpretable conditions for stability and convergence. We extend the proposed controller and analysis to underactuated systems by introducing learned actuation patterns. Experimental results on simulated and real-world systems validate our theoretical investigation and the accuracy of our controllers.", "AI": {"tldr": "A latent control framework using structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems with stability guarantees.", "motivation": "Model-based controllers need accurate dynamic models, but these are rarely available for high-dimensional systems like deformable objects or soft robots. Neural networks can approximate dynamics but lack physical structure and formal control guarantees for high-dimensional systems.", "method": "Introduces a latent control framework based on learned structure-preserving reduced-order dynamics. Derives reduced tracking law for fully actuated systems, uses Riemannian perspective on projection-based model-order reduction, quantifies modeling errors, and extends to underactuated systems with learned actuation patterns.", "result": "Derives interpretable conditions for stability and convergence, validates framework on simulated and real-world systems, showing accurate controllers with theoretical guarantees.", "conclusion": "The proposed latent control framework enables stable, convergent control of high-dimensional Lagrangian systems using learned reduced-order dynamics with formal guarantees, bridging the gap between neural approximations and model-based control."}}
{"id": "2602.07566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07566", "abs": "https://arxiv.org/abs/2602.07566", "authors": ["Runcheng Wang", "Yaru Chen", "Guiguo Zhang", "Honghua Jiang", "Yongliang Qiao"], "title": "Cross-Camera Cow Identification via Disentangled Representation Learning", "comment": null, "summary": "Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.", "AI": {"tldr": "Proposes a cross-camera cow identification framework using disentangled representation learning with Subspace Identifiability Guarantee theory to isolate stable biometric features invariant across cameras, achieving 86.0% accuracy on cross-camera tasks.", "motivation": "Existing animal identification methods work well in controlled single-camera settings but fail to generalize across cameras with different illumination, backgrounds, viewpoints, and imaging properties, limiting large-scale application in real-world farming environments.", "method": "Cross-camera cow identification framework based on disentangled representation learning using Subspace Identifiability Guarantee theory. Features a principle-driven feature disentanglement module that decomposes images into orthogonal latent subspaces to isolate stable identity-related biometric features invariant across cameras.", "result": "Achieved 86.0% average accuracy across seven cross-camera tasks, significantly outperforming Source-only Baseline (51.9%) and strongest cross-camera baseline (79.8%). Constructed high-quality dataset spanning five distinct camera nodes with heterogeneous devices and complex variations.", "conclusion": "Establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments."}}
{"id": "2602.08311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08311", "abs": "https://arxiv.org/abs/2602.08311", "authors": ["Shadman Rabby", "Md. Hefzul Hossain Papon", "Sabbir Ahmed", "Nokimul Hasan Arif", "A. B. M. Ashikur Rahman", "Irfan Ahmad"], "title": "Moral Sycophancy in Vision Language Models", "comment": "13 pages, 6 figures, 8 tables, Submitted for review in ACL", "summary": "Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.", "AI": {"tldr": "VLMs show moral sycophancy - they change correct moral judgments to wrong ones when users disagree, with stronger error-correction models paradoxically making more reasoning errors.", "motivation": "Prior studies explored sycophancy in general contexts, but its impact on morally grounded visual decision-making in VLMs remains insufficiently understood, creating a research gap.", "method": "Systematic study of 10 VLMs on Moralise and M^3oralBench datasets under explicit user disagreement, using Error Introduction Rate (EIR) and Error Correction Rate (ECR) metrics.", "result": "VLMs frequently produce morally incorrect follow-up responses even with correct initial judgments, showing asymmetry (right\u2192wrong shifts more common than reverse). Models with stronger error-correction capabilities introduce more reasoning errors, while conservative models minimize errors but have limited self-correction.", "conclusion": "VLMs are vulnerable to moral influence, showing dataset-dependent moral robustness, with a clear trade-off between error-correction and error-introduction, highlighting need for principled strategies to improve ethical consistency in multimodal AI."}}
{"id": "2602.08999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08999", "abs": "https://arxiv.org/abs/2602.08999", "authors": ["Mouad Abrini", "Mohamed Chetouani"], "title": "CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion", "comment": null, "summary": "With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue", "AI": {"tldr": "CLUE introduces a method to detect referential ambiguity in visual grounding by converting VLM's cross-modal attention into explicit spatial signals, enabling robots to decide when to ask clarification questions during human-robot interaction.", "motivation": "Existing Interactive Visual Grounding models lack mechanisms to determine when to ask clarification questions, as they implicitly rely on learned representations rather than explicit spatial signals for detecting referential ambiguity.", "method": "Extract text-to-image attention maps from VLM, pass them to lightweight CNN for ambiguity detection, use LoRA fine-tuned decoder for dialog and grounding location tokens, train on real-world IVG dataset and mixed ambiguity set.", "result": "CLUE surpasses state-of-the-art methods with InViG-only supervision using parameter-efficient fine-tuning, and the ambiguity detector outperforms prior baselines.", "conclusion": "CLUE successfully converts VLM's internal cross-modal attention into explicit, spatially grounded signals for deciding when to ask clarification questions in human-robot interaction, with publicly available data and code."}}
{"id": "2602.07568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07568", "abs": "https://arxiv.org/abs/2602.07568", "authors": ["Hui Ye", "Shilong Yang", "Yexuan Xing", "Juan Yu", "Yaoqin Xie", "Wei Zhang", "Chulong Zhang"], "title": "Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding", "comment": null, "summary": "Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.", "AI": {"tldr": "MammoColor uses task-driven chromatic encoding to convert mammograms into color-enhanced views, improving detection accuracy especially in dense breasts and reducing false positives.", "motivation": "Mammography screening has reduced sensitivity in dense breasts due to tissue overlap and subtle findings that increase perceptual difficulty, creating a need for better visualization methods.", "method": "Developed MammoColor framework with Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views, coupled with BI-RADS triage classifier, trained end-to-end on VinDr-Mammo dataset.", "result": "Improved AUC from 0.7669 to 0.8461 on VinDr-Mammo, with larger gains in dense breasts (AUC 0.749 to 0.835). MRMC study showed improved specificity (0.90 to 0.96) with comparable sensitivity.", "conclusion": "TDCE provides task-optimized chromatic representation that improves perceptual salience and may reduce false-positive recalls in mammography triage, especially beneficial for dense breast screening."}}
{"id": "2602.08335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08335", "abs": "https://arxiv.org/abs/2602.08335", "authors": ["Yanming Li", "Xuelin Zhang", "WenJie Lu", "Ziye Tang", "Maodong Wu", "Haotian Luo", "Tongtong Wu", "Zijie Peng", "Hongze Mi", "Yibo Feng", "Naiqiang Tan", "Chao Huang", "Hong Chen", "Li Shen"], "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System", "comment": null, "summary": "Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.", "AI": {"tldr": "SHARP is a novel framework that uses Shapley-based credit attribution to optimize multi-agent reinforcement learning for LLM-tool integration systems, solving credit assignment problems and improving training stability.", "motivation": "Training multi-agent LLM systems with external tools is difficult due to credit assignment challenges - it's unclear which specific agent is responsible for success/failure in decision trajectories. Existing methods use sparse or global rewards that fail to capture individual contributions, leading to inefficient reinforcement learning.", "method": "SHARP uses a decomposed reward mechanism with three components: 1) global broadcast-accuracy reward, 2) Shapley-based marginal-credit reward for each agent (using Shapley values for precise credit attribution), and 3) tool-process reward to improve execution efficiency. It stabilizes training by normalizing agent-specific advantages across trajectory groups.", "result": "Extensive experiments across various real-world benchmarks show SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% over single-agent approaches and 14.05% over multi-agent approaches.", "conclusion": "SHARP successfully addresses the credit assignment problem in multi-agent LLM-tool systems through precise Shapley-based attribution, leading to more stable training and superior performance compared to existing methods."}}
{"id": "2602.09002", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09002", "abs": "https://arxiv.org/abs/2602.09002", "authors": ["Zilin Fang", "Anxing Xiao", "David Hsu", "Gim Hee Lee"], "title": "From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection", "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io", "AI": {"tldr": "Social robot navigation framework combining geometric planning with contextual social reasoning using fine-tuned vision-language models to select socially appropriate paths in human environments.", "motivation": "Current robot navigation focuses mainly on geometric constraints, but socially acceptable navigation requires understanding human activities, social norms, and avoiding interference with ongoing human activities even when paths are collision-free.", "method": "Two-stage approach: 1) Extract obstacles and human dynamics to generate geometrically feasible candidate paths, 2) Use fine-tuned vision-language model to evaluate paths based on contextual social expectations and select socially optimized path for controller execution.", "result": "Achieves best overall performance in four social navigation contexts with lowest personal space violation duration, minimal pedestrian-facing time, and no social zone intrusions compared to other methods.", "conclusion": "Integrating geometric planning with contextual social reasoning through task-specific vision-language models enables real-time socially appropriate robot navigation in diverse human environments."}}
{"id": "2602.07574", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07574", "abs": "https://arxiv.org/abs/2602.07574", "authors": ["Wenjie Liu", "Hao Wu", "Xin Qiu", "Yingqi Fan", "Yihan Zhang", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention", "comment": null, "summary": "Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.", "AI": {"tldr": "ViCA is a sparse MLLM architecture that reduces visual computation to 4% while preserving 98% accuracy by using vision-only cross-attention instead of dense visual processing in all layers.", "motivation": "Current MLLMs use dense self-attention that processes visual tokens at every Transformer layer, causing substantial computational overhead. The authors found that visual embeddings are already well-aligned with language space, and effective vision-language interaction occurs in only a small subset of layers.", "method": "ViCA (Vision-only Cross-Attention) bypasses visual tokens through all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. This creates a regular, hardware-friendly inference pipeline.", "result": "ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%. It achieves over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared to text-only LLMs.", "conclusion": "ViCA demonstrates that dense visual processing in MLLMs is unnecessary, and sparse cross-attention at selected layers provides superior performance-efficiency trade-offs. The approach is orthogonal to token pruning methods and can be combined for further efficiency gains."}}
{"id": "2602.08339", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08339", "abs": "https://arxiv.org/abs/2602.08339", "authors": ["Chengyi Du", "Yazhe Niu", "Dazhong Shen", "Luxin Xu"], "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT", "comment": "16 pages 6 figures", "summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.", "AI": {"tldr": "CoTZero: Annotation-free paradigm using dual-stage data synthesis and cognition-aligned training to improve VLMs' hierarchical reasoning and generalization without human annotations.", "motivation": "Current VLMs rely on surface correlations rather than logically coherent structured representations, leading to missed higher-level semantic structure and non-causal relational understanding, which hinders compositional and verifiable reasoning.", "method": "Two components: (1) Dual-stage data synthesis with bottom-up extraction of atomic visual primitives and top-down hierarchical reasoning guidance, (2) Cognition-aligned training using Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning to provide stepwise feedback on reasoning coherence and factual correctness.", "result": "Achieves 83.33% F1 score on multi-level semantic inconsistency benchmark with lexical-perturbation negatives across both in-domain and out-of-domain settings.", "conclusion": "CoTZero enables more interpretable and human-aligned visual reasoning, with each component contributing to improved performance as confirmed by ablation studies."}}
{"id": "2602.09013", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09013", "abs": "https://arxiv.org/abs/2602.09013", "authors": ["Hongyi Chen", "Tony Dong", "Tiancheng Wu", "Liquan Wang", "Yash Jangir", "Yaru Niu", "Yufei Ye", "Homanga Bharadhwaj", "Zackory Erickson", "Jeffrey Ichnowski"], "title": "Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction", "comment": null, "summary": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.", "AI": {"tldr": "VIDEOMANIP learns dexterous robotic manipulation directly from RGB human videos without specialized devices, using vision-based reconstruction and contact optimization to achieve high success rates in both simulation and real-world tasks.", "motivation": "Multi-finger robotic hand manipulation is challenging due to high-dimensional action spaces and limited training data. Current approaches rely on specialized teleoperation equipment that doesn't scale well.", "method": "Device-free framework that reconstructs 4D robot-object trajectories from monocular RGB videos by estimating human hand poses and object meshes, then retargets motions to robotic hands. Includes hand-object contact optimization with interaction-centric grasp modeling and demonstration synthesis from single videos.", "result": "In simulation: 70.25% success rate across 20 diverse objects using Inspire Hand. In real world: 62.86% average success rate across 7 tasks using LEAP Hand, outperforming retargeting-based methods by 15.87%.", "conclusion": "VIDEOMANIP enables scalable learning of dexterous manipulation from easily accessible RGB human videos, eliminating the need for specialized teleoperation equipment while achieving competitive performance in both simulation and real-world applications."}}
{"id": "2602.07590", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07590", "abs": "https://arxiv.org/abs/2602.07590", "authors": ["Jessica Ka Yi Chiu", "Tom Frode Hansen", "Eivind Magnus Paulsen", "Ole Jakob Mengshoel"], "title": "Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling", "comment": "35 pages, 12 figures, 2 appendices", "summary": "This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.", "AI": {"tldr": "Geology-driven ML method combines synthetic data generation from fracture network models with supervised segmentation for automated rock joint trace mapping from images, addressing data scarcity and class imbalance.", "motivation": "Limited real data and class imbalance in rock joint trace mapping from images, requiring methods that can work with scarce labeled data while preserving geological realism.", "method": "Combines discrete fracture network modeling to generate synthetic jointed rock images at field-relevant scales, then uses mixed training and pretraining followed by fine-tuning on real images for segmentation.", "result": "Synthetic data supports supervised joint trace detection when real data are scarce; mixed training works well with consistent labels, while fine-tuning is more robust with noisy labels; useful generalization achieved with small real data.", "conclusion": "The geology-driven ML approach enables reliable joint mapping and provides foundation for domain adaptation and evaluation, with qualitative results showing clearer geologically meaningful traces than quantitative metrics alone indicate."}}
{"id": "2602.08340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08340", "abs": "https://arxiv.org/abs/2602.08340", "authors": ["Hoang Dang", "Luan Pham", "Minh Nguyen"], "title": "Effect-Level Validation for Causal Discovery", "comment": null, "summary": "Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.", "AI": {"tldr": "Causal discovery in telemetry data needs effect-centric validation, not just graph accuracy. Many plausible graphs don't admit identifiable causal queries, but when they do, different methods can converge on similar effect estimates despite structural differences.", "motivation": "Causal discovery is widely used for user-facing interventions in feedback-driven systems with strong self-selection, but its reliability for decision-making remains unclear. Current approaches focus too much on graph recovery accuracy rather than practical utility for causal queries.", "method": "Proposed an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification. Applied to real-world game telemetry studying early competitive gameplay exposure on short-term retention.", "result": "Many statistically plausible discovery outputs don't admit point-identified causal queries when temporal/semantic constraints are enforced. When identification is possible, different algorithm families converge to similar effect estimates despite different graph structures. Converging estimates survive placebo, subsampling, and sensitivity tests.", "conclusion": "Graph-level metrics alone are inadequate proxies for causal reliability. Trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone."}}
{"id": "2602.09017", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09017", "abs": "https://arxiv.org/abs/2602.09017", "authors": ["Zichen Jeff Cui", "Omar Rayyan", "Haritheja Etukuru", "Bowen Tan", "Zavier Andrianarivo", "Zicheng Teng", "Yihang Zhou", "Krish Mehta", "Nicholas Wojno", "Kevin Yuanbo Wu", "Manan H Anjaria", "Ziyuan Wu", "Manrong Mao", "Guangxun Zhang", "Binit Shah", "Yejin Kim", "Soumith Chintala", "Lerrel Pinto", "Nur Muhammad Mahi Shafiullah"], "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models", "comment": null, "summary": "The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/", "AI": {"tldr": "CAP replaces language conditioning with physical contact points for robot manipulation, using modular utility models and real-to-sim iteration to achieve robust generalization with minimal data.", "motivation": "Current robot learning relies on language prompts which are too abstract for physical manipulation tasks, creating a fundamental tension between language abstraction and concrete physical understanding needed for robust performance.", "method": "Introduces Contact-Anchored Policies (CAP) that condition on physical contact points instead of language, structured as modular utility models. Uses EgoGym simulation benchmark for rapid failure mode identification and refinement through real-to-sim iteration cycle.", "result": "CAP generalizes to novel environments and embodiments with only 23 hours of demonstration data, outperforming state-of-the-art VLAs by 56% in zero-shot evaluations across three fundamental manipulation skills.", "conclusion": "Physical contact conditioning combined with modular architecture and simulation-based iteration enables robust robot manipulation with minimal data, outperforming language-based approaches while being fully open-sourced."}}
{"id": "2602.07595", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07595", "abs": "https://arxiv.org/abs/2602.07595", "authors": ["Yuanzhi Liang", "Xuan'er Wu", "Yirui Liu", "Yijie Fang", "Yizhen Fan", "Ke Hao", "Rui Li", "Ruiying Liu", "Ziqi Ni", "Peng Yu", "Yanbo Wang", "Haibin Huang", "Qizhen Weng", "Chi Zhang", "Xuelong Li"], "title": "TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation", "comment": null, "summary": "Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.", "AI": {"tldr": "A systematic post-training framework for video generators that combines supervised policy shaping, reward-driven RL, and preference-based refinement into a stability-constrained optimization stack to improve perceptual fidelity, temporal coherence, and prompt adherence.", "motivation": "Post-training is crucial for converting pretrained video generators into production-ready models that are instruction-following, controllable, and robust over long temporal horizons, addressing practical constraints like high rollout cost, temporally compounding failures, and heterogeneous feedback.", "method": "A staged optimization framework organizing supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack, treating optimization as a diagnostic-driven process rather than isolated tricks.", "result": "The framework provides a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving controllability established at initialization, offering a clear blueprint for scalable post-training pipelines.", "conclusion": "The systematic post-training framework enables building scalable pipelines that remain stable, extensible, and effective in real-world deployment settings for production-oriented video generation models."}}
{"id": "2602.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08344", "abs": "https://arxiv.org/abs/2602.08344", "authors": ["Qi Guo", "Jianing Wang", "Deyang Kong", "Xiangyu Xi", "Jianfei Zhang", "Yi Lu", "Jingang Wang", "Wei Wang", "Shikun Zhang", "Wei Ye"], "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration", "comment": null, "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.", "AI": {"tldr": "Proposes Outline-Guided Path Exploration (OPE) to improve parallel thinking in large reasoning models by reducing information redundancy through diverse reasoning outlines before parallel path exploration.", "motivation": "Existing RL methods for parallel thinking focus mainly on aggregation phase optimization, neglecting path exploration stage limitations. The paper identifies that mutual information bottleneck among exploration paths restricts overall performance.", "method": "OPE explicitly partitions solution space by generating diverse reasoning outlines before parallel path reasoning, reducing information redundancy. Implemented with iterative RL strategy that independently optimizes outline planning and outline-guided reasoning.", "result": "Extensive experiments across multiple challenging mathematical benchmarks show OPE effectively improves reasoning performance with different aggregation strategies, enabling more reliable discovery of correct solutions.", "conclusion": "OPE addresses the mutual information bottleneck in parallel thinking by guiding path exploration with diverse outlines, leading to improved reasoning performance and more reliable solution discovery in large reasoning models."}}
{"id": "2602.09018", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09018", "abs": "https://arxiv.org/abs/2602.09018", "authors": ["Amir Mallak", "Alaa Maalouf"], "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving", "comment": null, "summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.", "AI": {"tldr": "ViT policies with foundation model features show superior OOD robustness in autonomous driving across environmental axes, with non-additive interactions between factors and actionable design rules for robust policy development.", "motivation": "Current OOD robustness evaluation in autonomous driving is oversimplified to single numbers, hiding what actually breaks driving policies. Need systematic decomposition of environmental factors to understand policy failures.", "method": "Decompose environments along 5 axes (scene, season, weather, time, agent mix), measure performance under controlled k-factor perturbations. Benchmark FC, CNN, ViT policies in VISTA simulator, train ViT heads on frozen foundation-model features, vary ID support in scale/diversity/temporal context.", "result": "ViT policies significantly more OOD-robust than CNN/FC; FM features yield state-of-art success with latency cost. Largest drops: rural\u2192urban & day\u2192night (~31% each). FM-feature policies stay >85% under 3 simultaneous changes. Training on winter/snow most robust to single shifts, rural+summer best overall OOD. Scaling traces improves robustness (+11.8 pts from 5 to 14 traces). Multiple ID environments broaden coverage with small ID drop.", "conclusion": "Systematic decomposition reveals non-additive interactions between environmental factors. Foundation model features provide strong OOD robustness despite latency cost. Winter/snow training yields single-factor robustness, while rural+summer baseline gives best overall OOD performance. Multiple ID environments balance coverage vs peak performance, yielding actionable design rules for OOD-robust driving policies."}}
{"id": "2602.07605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07605", "abs": "https://arxiv.org/abs/2602.07605", "authors": ["Hulingxiao He", "Zijun Geng", "Yuxin Peng"], "title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning", "comment": "Published as a conference paper at ICLR 2026. The models are available at https://huggingface.co/collections/StevenHH2000/fine-r1", "summary": "Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of \"visual analysis, candidate sub-categories, comparison, and prediction\", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.", "AI": {"tldr": "Fine-R1 is a multi-modal LLM specialized for fine-grained visual recognition using an R1-style training framework with chain-of-thought fine-tuning and triplet augmentation, achieving strong performance with minimal training data.", "motivation": "General-purpose MLLMs struggle with fine-grained visual recognition, requiring expensive annotated data and suffering from poor generalization to unseen sub-categories, while contrastive CLIP models perform better but lack reasoning capabilities.", "method": "Two-stage R1-style framework: (1) Chain-of-Thought Supervised Fine-tuning using constructed FGVR CoT dataset with rationales; (2) Triplet Augmented Policy Optimization with intra-class augmentation (mixing trajectories within same category) and inter-class augmentation (maximizing response distinction across sub-categories).", "result": "With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and contrastive CLIP models in identifying both seen and unseen sub-categories, showing strong generalization capabilities.", "conclusion": "Fine-R1 demonstrates promise for knowledge-intensive domains where expert annotations are scarce, bridging the gap between reasoning capabilities and fine-grained discriminative performance with minimal training data."}}
{"id": "2602.08353", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08353", "abs": "https://arxiv.org/abs/2602.08353", "authors": ["Zhang Jiasheng", "Li Zhangpin", "Wang Mingzhe", "Shao Jie", "Cui Jiangtao", "Li Hui"], "title": "Towards Better Evolution Modeling for Temporal Knowledge Graphs", "comment": "13 pages, 11 figures", "summary": "Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.", "AI": {"tldr": "Existing TKG benchmarks have shortcuts allowing near-SOTA performance via simple co-occurrence counting without using temporal info, due to dataset biases and oversimplified evaluation tasks.", "motivation": "To address limitations in current TKG benchmarks that inadvertently introduce shortcuts, allowing models to achieve near state-of-the-art performance without actually learning temporal evolution patterns.", "method": "Analyze root causes of benchmark issues, identify inherent dataset biases and oversimplified evaluation tasks, then create TKG evolution benchmark with four bias-corrected datasets and two novel evolution-aligned tasks.", "result": "Uncovered multiple limitations in existing benchmarks: unreasonable time-interval formatting, ignorance of knowledge obsolescence, insufficient information for precise evolution understanding, all amplifying shortcuts.", "conclusion": "Introduces a new TKG evolution benchmark to provide fair assessment and promote accurate understanding of TKG evolution modeling challenges, addressing identified biases and limitations."}}
{"id": "2602.09021", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09021", "abs": "https://arxiv.org/abs/2602.09021", "authors": ["Checheng Yu", "Chonghao Sima", "Gangcheng Jiang", "Hai Zhang", "Haoguang Mai", "Hongyang Li", "Huijie Wang", "Jin Chen", "Kaiyang Wu", "Li Chen", "Lirui Zhao", "Modi Shi", "Ping Luo", "Qingwen Bu", "Shijia Peng", "Tianyu Li", "Yibo Yuan"], "title": "$\u03c7_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies", "comment": null, "summary": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $\u03c7_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $\u03c7_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $\u03c7_{0}$ surpasses the state-of-the-art $\u03c0_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.", "AI": {"tldr": "\u03c7\u2080 is a resource-efficient framework for high-reliability robotic manipulation that addresses distributional shift issues through three key techniques: Model Arithmetic for diverse demonstration absorption, Stage Advantage for stable progress signals, and Train-Deploy Alignment to bridge distribution gaps.", "motivation": "The primary bottleneck to real-world robustness in robotic manipulation is not just resource scale, but systematic distributional shift among human demonstrations, learned policy biases, and test-time execution distributions, which causes compounding errors in multi-stage tasks.", "method": "Three technical pillars: (1) Model Arithmetic - weight-space merging strategy to efficiently absorb diverse demonstration distributions; (2) Stage Advantage - stage-aware advantage estimator providing stable, dense progress signals; (3) Train-Deploy Alignment - bridges distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing.", "result": "Achieved high-reliability autonomy with 24-hour non-stop operation from arbitrary initial states. Surpassed state-of-the-art \u03c0\u2080.\u2085 by nearly 250% in success rate using only 20-hour data and 8 A100 GPUs. Demonstrated collaborative dual-arm garment manipulation including flattening, folding, and hanging different clothes.", "conclusion": "\u03c7\u2080 enables production-level robustness in robotic manipulation with resource efficiency, addressing distributional shift issues that cause compounding errors in long-horizon tasks, making high-reliability autonomy achievable with modest computational resources."}}
{"id": "2602.07608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07608", "abs": "https://arxiv.org/abs/2602.07608", "authors": ["Yixin Chen", "Ziyu Su", "Lingbin Meng", "Elshad Hasanov", "Wei Chen", "Anil Parwani", "M. Khalid Khan Niazi"], "title": "HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology", "comment": null, "summary": "Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.", "AI": {"tldr": "HistoMet: A decision-aware, concept-aligned MIL framework for predicting metastatic progression and site dissemination from primary tumor histopathology using a two-module pipeline with pathology vision-language model guidance.", "motivation": "Metastatic progression is the leading cause of cancer mortality, but predicting whether and where a primary tumor will metastasize directly from histopathology remains challenging. Current approaches treat metastatic status and site prediction as isolated tasks rather than modeling the sequential clinical decision process.", "method": "HistoMet uses a two-module prediction pipeline: first estimates likelihood of metastatic progression from primary tumor, then conditionally predicts metastatic site for high-risk cases. Integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model for guided representation learning and clinical interpretability.", "result": "Evaluated on multi-institutional pan-cancer cohort of 6504 patients. Under 95% sensitivity screening settings, significantly reduces downstream workload while maintaining high metastatic risk recall. For metastatic cases, achieves macro F1 of 74.6\u00b11.3 and macro one-vs-rest AUC of 92.1.", "conclusion": "Explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology, addressing the sequential nature of clinical decision-making."}}
{"id": "2602.08354", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08354", "abs": "https://arxiv.org/abs/2602.08354", "authors": ["Zixuan Huang", "Xin Xia", "Yuxi Ren", "Jianbin Zheng", "Xuanda Wang", "Zhixia Zhang", "Hongyan Xie", "Songshi Liang", "Zehao Chen", "Xuefeng Xiao", "Fuzhen Zhuang", "Jianxin Li", "Yikun Ban", "Deqing Wang"], "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?", "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.", "AI": {"tldr": "SAGE introduces a self-aware guided efficient reasoning paradigm that enables large reasoning models to know when to stop thinking, improving both accuracy and efficiency by eliminating redundant reasoning chains.", "motivation": "Long chains of thought in large reasoning models create substantial redundancy, impairing computational efficiency and causing delays in real-time applications. Longer reasoning chains are often uncorrelated with correctness and can even harm accuracy. The authors discovered that models implicitly know when to stop thinking, but this capability is obscured by current sampling methods.", "method": "SAGE (Self-Aware Guided Efficient Reasoning) is a novel sampling paradigm that unleashes models' inherent ability to know when to stop thinking. The approach is further enhanced by integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL), which incorporates SAGE-discovered efficient reasoning patterns into standard pass@1 inference.", "result": "SAGE-RL markedly enhances both reasoning accuracy and efficiency of large reasoning models across multiple challenging mathematical benchmarks by effectively incorporating efficient reasoning patterns discovered through SAGE.", "conclusion": "The SAGE paradigm successfully addresses the redundancy problem in long reasoning chains by enabling models to self-regulate their thinking process, leading to significant improvements in both computational efficiency and reasoning accuracy for complex tasks."}}
{"id": "2602.09023", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09023", "abs": "https://arxiv.org/abs/2602.09023", "authors": ["Qinwen Xu", "Jiaming Liu", "Rui Zhou", "Shaojun Shi", "Nuowei Han", "Zhuoyang Liu", "Chenyang Gu", "Shuo Gu", "Yang Yue", "Gao Huang", "Wenzhao Zheng", "Sirui Han", "Peng Jia", "Shanghang Zhang"], "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation", "comment": null, "summary": "Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.", "AI": {"tldr": "TwinRL is a digital twin-real-world collaborative RL framework that accelerates Vision-Language-Action model training by expanding exploration space through simulation and guiding targeted real-world rollouts, achieving near-perfect success rates with 30% faster training.", "motivation": "VLA models face limitations due to expensive expert demonstrations and insufficient real-world interaction. Online RL for VLA manipulation is hindered by low exploration efficiency and restricted exploration space, which is tied to SFT data distribution.", "method": "1) Build high-fidelity digital twin from smartphone-captured scenes for bidirectional transfer. 2) Expand exploration space during SFT warm-up using digital twins. 3) Perform parallel online RL in digital twin before deployment. 4) Use digital twin sampling to identify failure-prone configurations for targeted human-in-the-loop real robot rollouts.", "result": "Achieves nearly 100% success in both in-distribution and out-of-distribution regions, with 30% speedup over prior real-world RL methods, requiring only ~20 minutes average training time across four tasks.", "conclusion": "TwinRL effectively bridges offline and online training stages through digital twin-real-world collaboration, enabling efficient exploration and accelerated VLA model training with minimal real-world interaction time."}}
{"id": "2602.07625", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07625", "abs": "https://arxiv.org/abs/2602.07625", "authors": ["Binxiao Xu", "Junyu Feng", "Xiaopeng Lin", "Haodong Li", "Zhiyuan Feng", "Bohan Zeng", "Shaolin Lu", "Ming Lu", "Qi She", "Wentao Zhang"], "title": "AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning", "comment": null, "summary": "Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.", "AI": {"tldr": "AD-MIR is a two-stage framework that decodes advertising intent by first structuring video content into a memory database, then using a reasoning agent to deduce persuasion tactics with evidence-based validation.", "motivation": "Existing agents struggle to bridge the gap between pixel-level perception and high-level marketing logic in advertising videos, despite excelling at general search tasks.", "method": "Two-stage architecture: 1) Structure-Aware Memory Construction converts raw video into structured database with semantic retrieval and keyword matching, focusing on brand details while filtering noise; 2) Structured Reasoning Agent mimics marketing expert through iterative inquiry loop with evidence-based self-correction that validates insights against specific video frames.", "result": "Achieves state-of-the-art performance on AdsQA benchmark, surpassing strongest general-purpose agent (DVD) by 1.8% in strict and 9.5% in relaxed accuracy.", "conclusion": "Effective advertising understanding requires explicitly grounding abstract marketing strategies in pixel-level evidence, which AD-MIR accomplishes through its structured approach."}}
{"id": "2602.08362", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.08362", "abs": "https://arxiv.org/abs/2602.08362", "authors": ["Chunxi Ji", "Adnan Darwiche"], "title": "Circuit Representations of Random Forests with Applications to XAI", "comment": null, "summary": "We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.", "AI": {"tldr": "Compiling random forest classifiers into circuits for efficient reasoning about decisions, including computing explanations, robustness, and ways to flip decisions.", "motivation": "To enable more efficient analysis of random forest classifiers by compiling them into circuits, allowing for better computation of explanations, decision robustness, and ways to alter decisions.", "method": "1) Compile random forest classifiers into circuits encoding class instances; 2) Use circuits to compute complete/general decision reasons; 3) Develop algorithms for computing decision robustness and shortest ways to flip decisions.", "result": "Proposed approach is significantly more efficient than existing methods, enables enumeration of sufficient/necessary reasons and contrastive explanations, computes decision robustness, and identifies shortest ways to flip decisions across various datasets.", "conclusion": "The circuit-based compilation approach provides an efficient framework for comprehensive analysis of random forest decisions, including explanation generation, robustness assessment, and decision manipulation analysis."}}
{"id": "2512.01047", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.01047", "abs": "https://arxiv.org/abs/2512.01047", "authors": ["Tanmay Ambadkar", "\u0110or\u0111e \u017dikeli\u0107", "Abhinav Verma"], "title": "Automating the Refinement of Reinforcement Learning Specifications", "comment": null, "summary": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.", "AI": {"tldr": "AutoSpec is a framework that automatically refines coarse-grained logical specifications for reinforcement learning tasks to provide better guidance for RL algorithms, improving their ability to solve complex control tasks.", "motivation": "Coarse-grained logical specifications can be under-specified, causing reinforcement learning agents to fail at learning useful policies. There's a need to automatically refine these specifications to provide better guidance for RL algorithms.", "method": "AutoSpec uses an exploration-guided strategy to search for logical specification refinements. It employs four refinement procedures that modify the abstract graph of SpectRL specifications by refining existing edge specifications or introducing new ones, maintaining specification soundness throughout.", "result": "AutoSpec produces refined logical specifications that enable RL algorithms to solve more complex control tasks. The framework maintains specification soundness (any trajectory satisfying refined spec also satisfies original).", "conclusion": "AutoSpec successfully improves RL performance by automatically refining under-specified logical specifications, making complex control tasks more solvable while preserving the original specification's intent."}}
{"id": "2602.07643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07643", "abs": "https://arxiv.org/abs/2602.07643", "authors": ["Yichi Zhang", "Feiyang Xiao", "Le Xue", "Wenbo Zhang", "Gang Feng", "Chenguang Zheng", "Yuan Qi", "Yuan Cheng", "Zixin Hu"], "title": "Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation", "comment": null, "summary": "While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\\sim$675k 2D images, $\\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.", "AI": {"tldr": "Current 3D medical foundation models fail to achieve true general-purpose status when transitioning from structural to functional imaging domains, revealing a significant modality discrepancy that undermines real-world clinical utility.", "motivation": "To address the unexplored modality discrepancy in 3D medical foundation models, which have been validated primarily on regional and structural imaging but lack assessment on functional imaging modalities like PET.", "method": "Created the UMD dataset with 490 PET/CT and 464 PET/MRI scans (~675k 2D images, ~12k 3D organ annotations), conducted controlled intra-subject comparisons of paired scans to isolate modality effects, and evaluated representative 3D segmentation foundation models.", "result": "Revealed stark discrepancy between literature-reported benchmarks and real-world efficacy, showing systemic failures when transitioning from structural to functional domains, indicating current models are far from achieving truly general-purpose status.", "conclusion": "Current 3D foundation models need a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility, with the UMD dataset serving as a foundational cornerstone for developing truly modality-agnostic medical foundation models."}}
{"id": "2602.08369", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08369", "abs": "https://arxiv.org/abs/2602.08369", "authors": ["Xin Zhang", "Kailai Yang", "Chenyue Li", "Hao Li", "Qiyu Wei", "Jun'ichi Tsujii", "Sophia Ananiadou"], "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval", "comment": null, "summary": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.", "AI": {"tldr": "MemAdapter is a unified memory retrieval framework that enables fast alignment across different agent memory paradigms (explicit, parametric, latent) through a two-stage training approach with a generative subgraph retriever and lightweight alignment module.", "motivation": "Existing agent memory systems are designed within isolated paradigms with tightly coupled retrieval methods, which hinders cross-paradigm generalization and fusion. There's a need for a unified memory system that can work across different memory paradigms.", "method": "Two-stage training: (1) Train a generative subgraph retriever from a unified memory space, (2) Adapt the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This enables fast cross-paradigm alignment.", "result": "MemAdapter consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. It achieves cross-paradigm alignment within 13 minutes on a single GPU, with less than 5% of training compute compared to original retrievers, while maintaining superior performance.", "conclusion": "MemAdapter provides a flexible, efficient plug-and-play solution for agent memory systems that enables effective zero-shot fusion across memory paradigms, addressing the limitations of isolated memory paradigms in existing systems."}}
{"id": "2602.07645", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07645", "abs": "https://arxiv.org/abs/2602.07645", "authors": ["Leonardo Gonzalez"], "title": "From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding", "comment": "Accepted for publication in the Companion Proceedings of the ACM Web Conference 2026 (WWW Companion '26), April 13-17, 2026, Dubai, United Arab Emirates", "summary": "Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \\textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \\textsc{Images2Slides} achieves an overall element recovery rate of $0.989\\pm0.057$ (text: $0.985\\pm0.083$, images: $1.000\\pm0.000$), with mean text transcription error $\\mathrm{CER}=0.033\\pm0.149$ and mean layout fidelity $\\mathrm{IoU}=0.364\\pm0.161$ for text regions and $0.644\\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.", "AI": {"tldr": "Images2Slides converts static infographics into editable Google Slides using vision-language models to extract content and layout, then recreates elements via the Slides API.", "motivation": "Static infographics exported as images have locked content, making updates, localization, and reuse expensive. There's a need to convert them back into editable formats.", "method": "API-based pipeline that: 1) extracts region-level specifications using vision-language models, 2) maps pixel geometry to slide coordinates, 3) recreates elements using Google Slides batch update API. The system is model-agnostic with common JSON schema and deterministic postprocessing.", "result": "Achieves 0.989\u00b10.057 overall element recovery rate (text: 0.985\u00b10.083, images: 1.000\u00b10.000), mean text transcription error CER=0.033\u00b10.149, and layout fidelity IoU=0.364\u00b10.161 for text regions and 0.644\u00b10.131 for image regions on benchmark of 29 infographics.", "conclusion": "The system successfully converts static infographics to editable slides, addressing practical challenges like text size calibration and non-uniform backgrounds, while identifying failure modes for future work."}}
{"id": "2602.08373", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08373", "abs": "https://arxiv.org/abs/2602.08373", "authors": ["Feiyu Wu", "Xu Zheng", "Yue Qu", "Zhuocheng Wang", "Zicheng Feng", "Hui Li"], "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "comment": "Accepted to ICLR 2026. Project page. https://openreview.net/forum?id=wb05ver1k8&noteId=v1Ax8CwI71", "summary": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.", "AI": {"tldr": "VIRF is a neuro-symbolic framework that combines LLMs with formal logic to ensure safe embodied AI planning through tutor-apprentice dialogue, achieving perfect safety and high goal completion.", "motivation": "LLMs lack formal reasoning for safety guarantees in embodied AI, and current approaches either rely on unreliable LLM safety checks or simply reject unsafe plans without offering repairs.", "method": "Verifiable Iterative Refinement Framework (VIRF) uses a deterministic Logic Tutor grounded in formal safety ontology to provide causal and pedagogical feedback to an LLM planner in a tutor-apprentice dialogue, enabling intelligent plan repairs rather than avoidance. Includes scalable knowledge acquisition pipeline to synthesize safety knowledge bases from real-world documents.", "result": "Achieved perfect 0% Hazardous Action Rate (HAR) and 77.3% Goal-Condition Rate (GCR) in challenging home safety tasks, highest among all baselines. Highly efficient with only 1.1 correction iterations on average.", "conclusion": "VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents by shifting from passive safety gatekeeping to active collaboration between symbolic reasoning and neural planning."}}
{"id": "2602.07658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07658", "abs": "https://arxiv.org/abs/2602.07658", "authors": ["Avinash Kumar K M", "Samarth S. Raut"], "title": "Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation", "comment": "22 pages, 13 figures", "summary": "The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.", "AI": {"tldr": "Evaluation of 3D medical model reconstruction accuracy using different segmentation methods (GMM, Otsu, RG) on three geometries (sphere, facemask, AAA), comparing voxel-based and surface-based metrics to assess pipeline errors.", "motivation": "The accuracy of 3D medical models depends on multiple factors including imaging hardware, segmentation methods, and mesh processing. The effects of geometry type, class imbalance, and alignment on accuracy remain insufficiently explored, necessitating comprehensive evaluation across the reconstruction pipeline.", "method": "Printed three geometries (sphere, facemask, AAA) using SLA technique and scanned with micro-CT. Applied three segmentation methods (GMM, Otsu, RG). Aligned segmented and reference models using KU algorithm for voxel-based metrics (Dice, Jaccard, precision) and ICP-based alignment for surface metrics (chamfer distance, average Hausdorff distance).", "result": "Otsu method performed best overall across all geometries. AAA yielded low overlap scores due to thin walls and misalignment. Class imbalance most affected AAA's specificity. Surface-based metrics showed different trends than voxel-based metrics. RG performed best for sphere, while GMM and Otsu performed better for AAA. Facemask surface was most error-prone, likely due to ICP misalignment.", "conclusion": "Segmentation accuracy accumulates errors across reconstruction stages. High voxel-based metrics can be misleading with class imbalance and alignment sensitivity. Jaccard index is more stringent than Dice and better for thin-walled structures. Reliable pipeline assessment requires ensuring both voxel and point cloud alignment."}}
{"id": "2602.08400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08400", "abs": "https://arxiv.org/abs/2602.08400", "authors": ["Longkun Li", "Yuanben Zou", "Jinghan Wu", "Yuqing Wen", "Jing Li", "Hangwei Qian", "Ivor Tsang"], "title": "SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains", "comment": null, "summary": "Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \\textbf{SCOUT-RAG} (\\textit{\\underline{S}calable and \\underline{CO}st-efficient \\underline{U}nifying \\underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.", "AI": {"tldr": "SCOUT-RAG is a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval in access-restricted settings without global graph visibility, using cooperative agents to minimize retrieval regret while controlling costs.", "motivation": "Conventional Graph-RAG designs rely on centralized knowledge graphs, which are impractical in distributed, access-restricted settings like hospitals or multinational organizations where retrieval must work without global graph visibility or exhaustive querying.", "method": "SCOUT-RAG uses four cooperative agents: (1) estimates domain relevance, (2) decides when to expand retrieval to additional domains, (3) adapts traversal depth to avoid unnecessary graph exploration, and (4) synthesizes high-quality answers. It performs progressive cross-domain retrieval guided by incremental utility goals.", "result": "Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines (DRIFT and exhaustive domain traversal) while substantially reducing cross-domain calls, total tokens processed, and latency.", "conclusion": "SCOUT-RAG provides a scalable and cost-efficient distributed Graph-RAG framework that addresses the limitations of centralized approaches in access-restricted environments by minimizing retrieval regret while controlling API costs and latency."}}
{"id": "2602.07668", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07668", "abs": "https://arxiv.org/abs/2602.07668", "authors": ["Ross Greer", "Laura Fleig", "Maitrayee Keskar", "Erika Maquiling", "Giovanni Tapia Lopez", "Angel Martinez-Sanchez", "Parthib Roy", "Jake Rattigan", "Mira Sur", "Alejandra Vidrio", "Thomas Marcotte", "Mohan Trivedi"], "title": "Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making", "comment": null, "summary": "The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., \"turn after that red building\") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.", "AI": {"tldr": "Researchers propose L-LIO framework that adds audio sensing to existing LILO (looking-in-looking-out) system for vehicles, enabling multimodal fusion of audio and visual data to improve safety through better driver state assessment and environment understanding.", "motivation": "Current LILO framework only uses visual sensing to understand both outside scenes and driver states. Audio modality can provide additional valuable information about drivers, passengers, and external agents that visual signals alone may miss, especially in nuanced or context-rich scenarios where sound is critical for safe decision-making.", "method": "Expand LILO to L-LIO (looking-and-listening inside-and-outside) by incorporating audio signals. Evaluate three use cases: 1) supervised learning on driver speech to classify impairment states like intoxication, 2) analyzing passenger natural language instructions for planning systems, 3) using audio to disambiguate guidance/gestures of external agents where vision-only systems fail. Use custom-collected in-vehicle and external audio datasets from real-world environments.", "result": "Pilot findings show audio yields safety-relevant insights, particularly in scenarios where visual signals alone are insufficient. Audio enhances vehicle safety applications like impairment detection, natural language interfaces for planning, and disambiguation of external agent guidance.", "conclusion": "L-LIO framework augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts."}}
{"id": "2602.08401", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08401", "abs": "https://arxiv.org/abs/2602.08401", "authors": ["Liwen Wang", "Zongjie Li", "Yuchong Xie", "Shuai Wang", "Dongdong She", "Wei Wang", "Juergen Rahmel"], "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking", "comment": null, "summary": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.", "AI": {"tldr": "AGENTWM is the first watermarking framework for agentic LLMs that protects against imitation attacks by embedding verifiable signals in action sequences without affecting performance.", "motivation": "Agentic LLMs have significant IP value but are vulnerable to imitation attacks where adversaries steal capabilities by training on victim outputs. Existing LLM watermarking fails because agentic systems often hide internal reasoning traces needed for verification.", "method": "AGENTWM exploits semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. It includes an automated pipeline to generate robust watermark schemes and statistical hypothesis testing for verification.", "result": "Extensive evaluations across three complex domains show AGENTWM achieves high detection accuracy with negligible impact on agent performance. It effectively protects against adaptive adversaries who cannot remove watermarks without severely degrading stolen model utility.", "conclusion": "AGENTWM successfully addresses the unique challenges of watermarking agentic models by embedding verifiable signals directly into visible action trajectories while remaining indistinguishable to users, providing effective IP protection against imitation attacks."}}
{"id": "2602.07680", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07680", "abs": "https://arxiv.org/abs/2602.07680", "authors": ["Ross Greer", "Maitrayee Keskar", "Angel Martinez-Sanchez", "Parthib Roy", "Shashank Shriram", "Mohan Trivedi"], "title": "Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning", "comment": null, "summary": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.", "AI": {"tldr": "VLMs show promise for autonomous driving safety through semantic hazard detection, trajectory planning integration, and natural language behavioral constraints, but require careful engineering rather than direct feature injection.", "motivation": "To investigate how vision-language representations can support driving scene safety assessment and decision-making in autonomous driving systems, addressing the need for semantic reasoning in safety-critical applications.", "method": "Three complementary approaches: 1) CLIP-based lightweight hazard screening using image-text similarity; 2) Integration of scene-level VLM embeddings into transformer-based trajectory planning on Waymo Open Dataset; 3) Natural language behavioral constraints on motion planning using doScenes dataset.", "result": "1) Hazard screening works robustly for diverse hazards without explicit detection; 2) Naive global embedding conditioning doesn't improve trajectory accuracy; 3) Natural language constraints suppress severe planning failures and improve safety in ambiguous scenarios.", "conclusion": "Vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints, but realizing this potential requires careful system design and structured grounding rather than direct feature injection."}}
{"id": "2602.08412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08412", "abs": "https://arxiv.org/abs/2602.08412", "authors": ["Yuhang Wang", "Feiming Xu", "Zheng Lin", "Guangyu He", "Yuzhe Huang", "Haichang Gao", "Zhenxing Niu"], "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent", "comment": "11 pages,2 figures", "summary": "Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.", "AI": {"tldr": "PASB is a security evaluation framework for personalized AI agents that identifies critical vulnerabilities in systems like OpenClaw across real-world usage scenarios.", "motivation": "Existing security research focuses on synthetic/task-centric settings, failing to capture real-world attack surfaces and risk propagation in personalized AI agents deployed as assistants.", "method": "Propose Personalized Agent Security Bench (PASB) - an end-to-end security evaluation framework incorporating personalized usage scenarios, realistic toolchains, and long-horizon interactions for black-box evaluation.", "result": "OpenClaw exhibits critical vulnerabilities across multiple execution stages (user prompt processing, tool usage, memory retrieval), highlighting substantial security risks in personalized agent deployments.", "conclusion": "PASB addresses the gap in evaluating personalized agent security, revealing significant vulnerabilities that current frameworks miss, emphasizing the need for improved security in real-world AI assistant deployments."}}
{"id": "2602.07689", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07689", "abs": "https://arxiv.org/abs/2602.07689", "authors": ["Jusheng Zhang", "Kaitong Cai", "Jian Wang", "Yongsen Zheng", "Kwok-Yan Lam", "Keze Wang"], "title": "Process-of-Thought Reasoning for Videos", "comment": null, "summary": "Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.", "AI": {"tldr": "PoT Reasoning for Videos is a framework that structures video inference into explicit, verifiable steps to improve reasoning over long, noisy video observations.", "motivation": "Video understanding requires temporally grounded, multi-step reasoning over long and noisy observations, but current approaches lack explicit reasoning processes that are traceable and verifiable.", "method": "Proposes Process-of-Thought (PoT) Reasoning framework with three interleaved components: temporal evidence selection, step-wise state updates, and constrained answer synthesis. Uses unified representation for PoT traces that aligns intermediate decisions with temporal segments.", "result": "Extensive experiments show PoT consistently improves factual correctness and temporal grounding, reduces hallucinated explanations, and provides interpretable reasoning traces for diagnosis and downstream use.", "conclusion": "PoT Reasoning for Videos offers a model-agnostic framework that makes video reasoning explicit, traceable, and verifiable, improving both performance and interpretability for complex video understanding tasks."}}
{"id": "2602.08449", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08449", "abs": "https://arxiv.org/abs/2602.08449", "authors": ["Igor Santos-Grueiro"], "title": "When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment", "comment": "25 pages, 4 figures,", "summary": "Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.", "AI": {"tldr": "AI safety evaluation assumes behavior in testing predicts deployment behavior, but situationally aware agents can exploit differences between these regimes. The paper proposes measuring regime information leakage and using adversarial training to make models regime-blind, showing mixed effectiveness across different failure modes.", "motivation": "Current AI safety evaluation assumes behavior observed during testing reliably predicts deployment behavior. However, this assumption breaks down for situationally aware AI agents that can detect differences between evaluation and deployment regimes, enabling them to implement conditional policies (like sycophancy and sleeper agents) that appear compliant during testing but defect in deployment.", "method": "The paper reframes alignment evaluation as an information flow problem under partial observability. It introduces regime-blind training mechanisms that use adversarial invariance to reduce the extractability of regime information from internal representations. This approach was evaluated on an open-weight language model across two failure modes: scientific sycophancy and temporal sleeper agents.", "result": "Regime-blind training suppressed regime-conditioned behavior in both cases without measurable loss of task utility, but with different dynamics: sycophancy showed a sharp representational and behavioral transition at low intervention strength, while sleeper agents required stronger pressure and didn't exhibit clean collapse of regime decodability.", "conclusion": "Representational invariance is a meaningful but limited control lever for AI safety, whose effectiveness depends on how regime information is embedded in the policy. The paper argues that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow, rather than relying solely on behavioral testing."}}
{"id": "2602.07938", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07938", "abs": "https://arxiv.org/abs/2602.07938", "authors": ["Rabbia Asghar", "Lukas Rummelhard", "Wenqian Liu", "Anne Spalanzani", "Christian Laugier"], "title": "Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps", "comment": "Updated version with major revisions; currently under the second round of review at IEEE Transactions on Intelligent Vehicles", "summary": "Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.", "AI": {"tldr": "A unified framework for simultaneous prediction of future occupancy, vehicle positions, and scene flow using Dynamic Occupancy Grid Maps with interdependent loss functions for robust motion forecasting.", "motivation": "Existing prediction methods have limitations: agent-agnostic models struggle with behavioral complexities of dynamic actors, while agent-specific approaches fail to generalize to poorly perceived or unrecognized agents. Combining both paradigms enables more robust and safer motion forecasting.", "method": "Proposes a unified framework using Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Uses a lightweight spatiotemporal backbone with a tailored interdependent loss function that captures inter-grid dependencies and enables diverse future predictions.", "result": "Superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods on real-world nuScenes and Woven Planet datasets.", "conclusion": "The proposed unified framework effectively addresses limitations of both agent-agnostic and agent-specific prediction approaches, enabling robust motion forecasting that captures specific vehicle behaviors while also identifying other dynamic entities and anticipating their evolution in complex scenes."}}
{"id": "2602.07694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07694", "abs": "https://arxiv.org/abs/2602.07694", "authors": ["Wenping Jin", "Yuyang Tang", "Li Zhu"], "title": "Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes", "comment": null, "summary": "Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \\textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.", "AI": {"tldr": "A benchmark (CoalAD) and complementary-cue framework for unsupervised foreign-object anomaly detection in coal conveyor scenes, addressing challenges from unstructured environments with low-contrast, deformed, occluded objects.", "motivation": "Reliable foreign-object detection in coal conveyor scenes is essential for mining safety, but challenging due to unstructured environments (random coal/gangue piles, complex backgrounds, low-contrast objects with deformation/occlusion) that weaken assumptions of existing anomaly detection methods.", "method": "Complementary-cue collaborative perception framework that extracts and fuses anomaly evidence from three perspectives: 1) object-level semantic composition modeling, 2) semantic-attribution-based global deviation analysis, and 3) fine-grained texture matching.", "result": "Method outperforms widely used baselines on CoalAD benchmark across both image-level and pixel-level metrics, with ablation studies validating each component's contribution.", "conclusion": "The proposed framework provides robust anomaly detection and accurate localization for foreign objects in challenging coal-stream scenes, with publicly available code for further research."}}
{"id": "2602.08517", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08517", "abs": "https://arxiv.org/abs/2602.08517", "authors": ["Shaoang Zhang", "Yazhe Niu"], "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor", "comment": null, "summary": "Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.", "AI": {"tldr": "TreeTensor: A general nested data container for AI systems that enables efficient computation on hierarchical data with zero overhead, compatible with popular ML libraries.", "motivation": "Conventional tensors with fixed shapes are inefficient for handling hierarchical nested data in complex cognitive AI systems, which often involve various modalities and nested structures beyond simple perception tasks.", "method": "Proposes TreeTensor, a general nested data container that uses constrained tree-structure perspective to model data relationships, with magic utilities allowing arbitrary functions/operations on nested data at almost zero cost, compatible with Scikit-Learn, Numpy, and PyTorch.", "result": "TreeTensor demonstrates powerful usability in various problems (including complex AI systems like AlphaStar for StarCraftII) and exhibits excellent runtime efficiency without overhead, as shown in benchmarks and examples.", "conclusion": "TreeTensor provides an efficient solution for handling nested data in AI systems, enabling seamless integration with existing ML libraries while maintaining computational efficiency and supporting advanced features like asynchronous execution and variable-length data computation."}}
{"id": "2602.08006", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08006", "abs": "https://arxiv.org/abs/2602.08006", "authors": ["Riya Mohan", "Juana Valeria Hurtado", "Rohit Mohan", "Abhinav Valada"], "title": "ForecastOcc: Vision-based Semantic Occupancy Forecasting", "comment": null, "summary": "Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.", "AI": {"tldr": "ForecastOcc is the first vision-based framework that jointly predicts future occupancy states and semantic categories directly from camera images, outperforming existing baselines on autonomous driving datasets.", "motivation": "Existing occupancy forecasting methods either focus only on motion categories (static/dynamic) or rely on separate networks for past occupancy predictions, leading to error accumulation and inability to learn spatio-temporal features directly from images.", "method": "A novel architecture with temporal cross-attention forecasting module, 2D-to-3D view transformer, 3D encoder for occupancy prediction, and semantic occupancy head for voxel-level forecasts across multiple horizons, working directly from past camera images.", "result": "ForecastOcc consistently outperforms baselines on both multi-view forecasting (Occ3D-nuScenes) and monocular forecasting (SemanticKITTI), establishing the first benchmark for semantic occupancy forecasting.", "conclusion": "The framework enables semantically rich, future-aware predictions that capture both scene dynamics and semantics critical for autonomous driving, without relying on externally estimated maps."}}
{"id": "2602.07702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07702", "abs": "https://arxiv.org/abs/2602.07702", "authors": ["Deep Bhattacharyya", "Ali Ayub", "A. Ben Hamza"], "title": "A hybrid Kolmogorov-Arnold network for medical image segmentation", "comment": null, "summary": "Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.", "AI": {"tldr": "U-KABS: A hybrid medical image segmentation framework combining U-Net architecture with Kolmogorov-Arnold Networks using Bernstein polynomials and B-splines for enhanced feature representation.", "motivation": "Medical image segmentation is challenging due to complexity and variability of medical images, especially in capturing non-linear relationships within the data. Existing methods may not effectively handle both global contextual trends and fine-grained patterns needed for complex anatomical structures.", "method": "U-KABS integrates Kolmogorov-Arnold Networks (KANs) with U-shaped encoder-decoder architecture. It combines convolutional and squeeze-and-excitation stages for channel-wise feature enhancement, and KAN Bernstein Spline (KABS) stage with learnable activation functions based on Bernstein polynomials and B-splines. Skip connections enable multi-scale feature fusion and spatial detail preservation.", "result": "U-KABS demonstrates superior performance across diverse medical imaging benchmark datasets compared to strong baselines, particularly in segmenting complex anatomical structures.", "conclusion": "The hybrid U-KABS framework effectively captures both broad contextual trends and fine-grained patterns in medical images through its integration of Bernstein polynomials' global smoothness and B-splines' local adaptability, leading to improved segmentation of complex anatomical structures."}}
{"id": "2602.08520", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08520", "abs": "https://arxiv.org/abs/2602.08520", "authors": ["Xinhai Sun"], "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning", "comment": null, "summary": "Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \\emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \\emph{without any retraining}.\n  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\\% to 84.03\\%, while only incurring 61.06\\% additional inference calls. A 100\\% re-asking ablation reaches 84.35\\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \\emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.\n  Beyond providing a practical inference-time upgrade, our results suggest a broader \\emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.", "AI": {"tldr": "Reinforcement Inference improves LLM accuracy by using model uncertainty to selectively trigger second reasoning attempts, boosting MMLU-Pro performance from 60.72% to 84.03% without retraining.", "motivation": "One-shot greedy inference systematically underestimates LLM capabilities because errors often arise from premature commitment under internal ambiguity rather than missing knowledge.", "method": "Reinforcement Inference uses entropy-aware inference-time control that monitors the model's own uncertainty during generation to selectively invoke a second, more deliberate reasoning attempt.", "result": "On 12,032 MMLU-Pro questions, accuracy improved from 60.72% to 84.03% with only 61.06% additional inference calls. A 100% re-asking ablation reached 84.35%, showing uncertainty-aware selection captures most gains efficiently.", "conclusion": "The approach enables practical inference-time upgrades and suggests an entropy-aware paradigm for measuring model capability, where uncertainty measures serve as first-class control signals during generation."}}
{"id": "2602.08058", "categories": ["cs.CV", "cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08058", "abs": "https://arxiv.org/abs/2602.08058", "authors": ["Xihang Yu", "Rajat Talak", "Lorenzo Shaikewitz", "Luca Carlone"], "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling", "comment": "15 pages", "summary": "In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.", "AI": {"tldr": "Picasso: Physics-constrained reconstruction pipeline for multi-object scenes that ensures physical plausibility by reasoning about object interactions, non-penetration, and physics.", "motivation": "Current scene reconstruction methods produce geometrically accurate but physically implausible results (object interpenetration, unstable equilibrium) that fail in simulation-based planning for contact-rich behaviors.", "method": "Physics-constrained reconstruction pipeline using fast rejection sampling that reasons over multi-object interactions guided by an inferred object contact graph, considering geometry, non-penetration, and physics.", "result": "Outperforms state-of-the-art on Picasso dataset (10 contact-rich scenes) and YCB-V dataset, providing physically plausible reconstructions better aligned with human intuition.", "conclusion": "Holistic scene reasoning accounting for object interactions is essential for physically plausible reconstructions, enabling reliable simulation-based planning for contact-rich behaviors."}}
{"id": "2602.07717", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.07717", "abs": "https://arxiv.org/abs/2602.07717", "authors": ["Yingjie Li", "Daniel Robinson", "Cunxi Yu"], "title": "All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving", "comment": null, "summary": "Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.", "AI": {"tldr": "Proposes an all-optical computing framework using diffractive optical neural networks (DONNs) for energy-efficient RGB image segmentation and lane detection in autonomous driving, demonstrating effectiveness on CityScapes dataset and custom lane detection scenarios.", "motivation": "Conventional DNNs for autonomous driving tasks (semantic segmentation and lane detection) incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations needed for real-time responses. DONNs offer promising energy efficiency advantages by performing all-optical image processing at light speed.", "method": "A novel all-optical computing framework using diffractive optical neural networks (DONNs) that performs image processing via light diffraction. The system eliminates analog-to-digital conversion overhead through all-optical encoding and computing, operating at the speed of light.", "result": "Experimental results demonstrate effectiveness of the DONN system for image segmentation on the CityScapes dataset. Case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA show the model's generalizability under diverse environmental conditions.", "conclusion": "DONNs provide an energy-efficient alternative to conventional DNNs for autonomous driving vision tasks by leveraging all-optical computing to reduce energy costs while maintaining performance for segmentation and lane detection in real-world scenarios."}}
{"id": "2602.08533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08533", "abs": "https://arxiv.org/abs/2602.08533", "authors": ["Kun Peng", "Conghui Tan", "Yu Liu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Zining Zhu", "Lei Jiang", "Yanbing Liu", "Hao Peng"], "title": "Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO", "comment": null, "summary": "Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.", "AI": {"tldr": "Proposes AT-GRPO, a long-horizon RL framework for open-ended dialogue agents that integrates online personalization with adaptive tree-based policy optimization to address short-horizon biases and reduce computational overhead.", "motivation": "Existing dialogue agents have limitations: over-reliance on pre-collected user data, and short-horizon biases in RL that neglect long-term dialogue value. Need to enable online personalization while capturing long-term conversational value efficiently.", "method": "Two-agent game paradigm with user agent (style mimicry + active termination) and dialogue agent. AT-GRPO reinterprets dialogue trajectories as trees with adaptive observation ranges - larger ranges for early-stage topic exploration, smaller for late-stage maintenance, reducing rollout budgets from exponential to polynomial.", "result": "Extensive experiments show superior performance, sample efficiency, and robustness compared to existing methods.", "conclusion": "The proposed AT-GRPO framework successfully addresses limitations of existing dialogue agents by enabling online personalization while efficiently capturing long-term dialogue value through adaptive tree-based optimization."}}
{"id": "2602.08962", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08962", "abs": "https://arxiv.org/abs/2602.08962", "authors": ["Guangxun Zhu", "Xuan Liu", "Nicolas Pugeault", "Chongfeng Wei", "Edmond S. L. Ho"], "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting", "comment": "Accepted for IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D", "AI": {"tldr": "3D vehicle-conditioned pedestrian pose forecasting framework that incorporates surrounding vehicle information for improved autonomous driving safety.", "motivation": "Accurate pedestrian motion prediction is crucial for safe autonomous driving in complex urban environments, requiring explicit modeling of pedestrian-vehicle interactions.", "method": "Enhanced Waymo-3DSkelMo dataset with 3D vehicle bounding boxes, introduced sampling scheme for varying interaction complexities, and adapted TBIFormer architecture with vehicle encoder and pedestrian-vehicle interaction cross-attention module.", "result": "Substantial improvements in forecasting accuracy and validation of different approaches for modeling pedestrian-vehicle interactions.", "conclusion": "Vehicle-aware 3D pose prediction is important for autonomous driving, with the framework successfully incorporating vehicle information to improve pedestrian motion forecasting."}}
{"id": "2602.07768", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07768", "abs": "https://arxiv.org/abs/2602.07768", "authors": ["Qiuming Luo", "Yuebing Li", "Feng Li", "Chang Kong"], "title": "PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification", "comment": "6pages, 3 figures, conference", "summary": "Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.", "AI": {"tldr": "PAND is a two-stage knowledge distillation framework for fine-grained visual classification that uses prompt-aware semantic calibration and neighborhood-aware structural distillation to transfer knowledge from large vision-language models to lightweight networks.", "motivation": "Current knowledge distillation methods for fine-grained visual classification rely on fixed prompts and global alignment, which limits their effectiveness in transferring nuanced visual knowledge from large vision-language models to lightweight networks.", "method": "Two-stage framework: 1) Prompt-Aware Semantic Calibration generates adaptive semantic anchors, and 2) neighborhood-aware structural distillation constrains the student's local decision structure to better capture fine-grained relationships.", "result": "PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks, with ResNet-18 achieving 76.09% accuracy on CUB-200 (3.4% improvement over VL2Lite baseline).", "conclusion": "The proposed PAND framework effectively addresses limitations of fixed prompts and global alignment in knowledge distillation for fine-grained visual classification, achieving superior performance through semantic calibration and structural transfer."}}
{"id": "2602.08586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08586", "abs": "https://arxiv.org/abs/2602.08586", "authors": ["Yiming Yang", "Zhuoyuan Li", "Fanxiang Zeng", "Hao Fu", "Yue Liu"], "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition", "comment": null, "summary": "Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.\n  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.", "AI": {"tldr": "PRISM is a novel multi-agent reasoning framework that decomposes performance gains into Exploration, Information, and Aggregation dimensions, achieving SOTA results with better compute-efficiency.", "motivation": "Existing multi-agent collaboration approaches are heuristic and lack principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. It's unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains.", "method": "Introduces PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) framework with three key components: role-based diversity for Exploration, execution-grounded feedback with evidence-based cross-evaluation for Information, and iterative synthesis with closed-loop validation for Aggregation.", "result": "Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing only partial dimensions.", "conclusion": "The theoretical framework provides actionable design principles for future multi-agent reasoning systems by decomposing multi-agent reasoning gains into three conceptually independent dimensions: Exploration, Information, and Aggregation."}}
{"id": "2602.08971", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08971", "abs": "https://arxiv.org/abs/2602.08971", "authors": ["Yu Shang", "Zhuohang Li", "Yiding Ma", "Weikang Su", "Xin Jin", "Ziyou Wang", "Xin Zhang", "Yinzhou Tang", "Chen Gao", "Wei Wu", "Xihui Liu", "Dhruv Shah", "Zhaoxiang Zhang", "Zhibo Chen", "Jun Zhu", "Yonghong Tian", "Tat-Seng Chua", "Wenwu Zhu", "Yong Li"], "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models", "comment": null, "summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.", "AI": {"tldr": "WorldArena is a unified benchmark that systematically evaluates embodied world models across both perceptual quality and functional utility in downstream tasks, revealing a significant gap between visual fidelity and practical task performance.", "motivation": "Current evaluation of embodied world models focuses too narrowly on perceptual fidelity (video generation quality) while overlooking their functional utility in downstream decision-making tasks, creating a fragmented evaluation landscape.", "method": "Introduces WorldArena benchmark with three evaluation dimensions: (1) video perception quality (16 metrics across 6 sub-dimensions), (2) embodied task functionality (evaluating models as data engines, policy evaluators, and action planners with human evaluation), and (3) EWMScore - a holistic metric integrating multi-dimensional performance.", "result": "Extensive experiments on 14 representative models reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability.", "conclusion": "WorldArena provides a comprehensive framework for tracking progress toward truly functional world models in embodied AI, with a public leaderboard available to facilitate standardized evaluation and advancement in the field."}}
{"id": "2602.07775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07775", "abs": "https://arxiv.org/abs/2602.07775", "authors": ["Haodong Li", "Shaoteng Liu", "Zhe Lin", "Manmohan Chandraker"], "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion", "comment": "Figure PDFs were compressed to 150 dpi to comply with arXiv's submission size limit. Project page: https://rolling-sink.github.io/", "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/", "AI": {"tldr": "Training-free method called Rolling Sink that enables autoregressive video diffusion models to generate ultra-long videos (5-30 minutes) by addressing the train-test gap beyond training durations through systematic AR cache maintenance.", "motivation": "Autoregressive video diffusion models suffer from visual degradation when tested at longer horizons than trained on, creating a train-test gap beyond training durations. Since open-ended testing can exceed any finite training window and long-video training is computationally expensive, a training-free solution is needed.", "method": "Rolling Sink, a training-free method built on Self Forcing. It involves systematic analysis of AR cache maintenance to bridge the gap between limited training horizons and open-ended testing horizons. The approach enables scaling AR video synthesis to ultra-long durations at test time without additional training.", "result": "Rolling Sink effectively scales AR video synthesis to ultra-long durations (5-30 minutes at 16 FPS) with consistent subjects, stable colors, coherent structures, and smooth motions. It achieves superior long-horizon visual fidelity and temporal consistency compared to state-of-the-art baselines.", "conclusion": "Rolling Sink provides an effective training-free solution to bridge the train-test gap beyond training durations in autoregressive video diffusion models, enabling ultra-long video generation with high visual quality and temporal consistency."}}
{"id": "2602.08597", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08597", "abs": "https://arxiv.org/abs/2602.08597", "authors": ["Roland Bertin-Johannet", "Lara Scipio", "Leopold Mayti\u00e9", "Rufin VanRullen"], "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture", "comment": null, "summary": "Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.", "AI": {"tldr": "A top-down attention mechanism for Global Workspace Theory improves multimodal integration by enhancing noise robustness and enabling cross-task/modality generalization, making it competitive with state-of-the-art methods.", "motivation": "Global Workspace Theory (GWT) provides a cognitive neuroscience framework for flexible cognition through attentional selection of modalities, but existing implementations have understudied attention mechanisms for multimodal integration.", "method": "Proposed a top-down attention mechanism to select modalities within a global workspace architecture, evaluated on two multimodal datasets (Simple Shapes and MM-IMDb 1.0) with comparisons against existing baselines.", "result": "The attention mechanism improves noise robustness, enables cross-task and cross-modality generalization not found in existing multimodal attention models, and makes the global workspace competitive with state-of-the-art on MM-IMDb 1.0 benchmark.", "conclusion": "A top-down attention mechanism effectively implements key aspects of Global Workspace Theory for multimodal integration, demonstrating practical benefits in robustness and generalization while advancing computational implementations of cognitive frameworks."}}
{"id": "2602.07784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07784", "abs": "https://arxiv.org/abs/2602.07784", "authors": ["Jayawant Bodagala", "Balaji Bodagala"], "title": "Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing", "comment": "Total pages: 9", "summary": "Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.", "AI": {"tldr": "UCATSC is a model-based traffic signal control system that addresses uncertainty in vision-based perception through stochastic decision processes with hard safety constraints, improving traffic delay and emissions while preventing safety-critical errors.", "motivation": "Real-world deployment of adaptive traffic signal control is limited due to uncertainty in vision-based perception, implicit safety approaches, and non-interpretable control policies that are mainly validated in simulation.", "method": "Models traffic signal control as a stochastic decision process with constraints under partial observability, accounting for perception uncertainty. Uses counterfactual rollouts in belief space with hard constraints for safety and starvation prevention, unlike RL methods that use reward shaping.", "result": "The system is designed to improve traffic delay and emissions while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.", "conclusion": "UCATSC addresses key limitations of current adaptive traffic signal control systems by providing a model-based approach with explicit safety constraints and interpretable policies suitable for real-world deployment."}}
{"id": "2602.08603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08603", "abs": "https://arxiv.org/abs/2602.08603", "authors": ["Teng Wang", "Rong Shan", "Jianghao Lin", "Junjie Wu", "Tianyi Xu", "Jianping Zhang", "Wenteng Chen", "Changwang Zhang", "Zhaoxiang Wang", "Weinan Zhang", "Jun Wang"], "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "comment": null, "summary": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.", "AI": {"tldr": "OSCAR is an optimization-steered agentic planning framework for composed image retrieval that reformulates agentic CIR as a trajectory optimization problem, achieving SOTA performance with strong generalization using only 10% of training data.", "motivation": "Existing CIR approaches have limitations: unified embedding retrieval suffers from single-model myopia, while heuristic agentic retrieval is limited by suboptimal, trial-and-error orchestration. There's a need for a more principled approach to agentic CIR.", "method": "OSCAR uses an offline-online paradigm: 1) Offline phase models CIR as a two-stage mixed-integer programming problem to derive optimal trajectories via boolean set operations, storing them in a golden library; 2) Online phase uses these trajectories as in-context demonstrations to steer VLM planner during inference.", "result": "Extensive experiments on three public benchmarks and a private industrial benchmark show OSCAR consistently outperforms SOTA baselines. Notably achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.", "conclusion": "OSCAR successfully reformulates agentic CIR from heuristic search to principled trajectory optimization, providing a more robust and generalizable framework that overcomes limitations of existing approaches while requiring significantly less training data."}}
{"id": "2602.07801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07801", "abs": "https://arxiv.org/abs/2602.07801", "authors": ["Wenqi Liu", "Yunxiao Wang", "Shijie Ma", "Meng Liu", "Qile Su", "Tianke Zhang", "Haonan Fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Yinwei Wei", "Xuemeng Song"], "title": "VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos", "comment": null, "summary": "In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.", "AI": {"tldr": "VideoTemp-o3 is a unified agentic framework for long-video understanding that jointly models video grounding and QA, featuring strong localization, on-demand clipping, and refinement capabilities.", "motivation": "Existing methods for long-video understanding suffer from inefficient uniform frame sampling, weak localization, rigid workflows, and increased hallucinations, requiring a more integrated approach.", "method": "Proposes VideoTemp-o3 with unified masking mechanism for supervised fine-tuning, dedicated RL rewards to prevent hacking, and a pipeline for constructing high-quality long video grounded QA data with benchmark.", "result": "Achieves remarkable performance on both long video understanding and grounding tasks across various video durations.", "conclusion": "VideoTemp-o3 provides an effective unified framework that addresses key limitations in existing agentic video understanding methods through joint modeling of grounding and QA."}}
{"id": "2602.08630", "categories": ["cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.08630", "abs": "https://arxiv.org/abs/2602.08630", "authors": ["Jonah Brown-Cohen", "Geoffrey Irving", "Simon C. Marshall", "Ilan Newman", "Georgios Piliouras", "Mario Szegedy"], "title": "Debate is efficient with your time", "comment": "11 Pages, 0 figures", "summary": "AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.\n  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.", "AI": {"tldr": "Debate query complexity (DQC) measures how many bits a human judge must inspect in AI safety debates. PSPACE/poly = O(log n) queries, showing debate is highly query-efficient. Lower bounds connect to circuit complexity.", "motivation": "Previous work established what problems debate can solve in principle, but hasn't analyzed the practical cost of human oversight - how many queries must the judge make to the debate transcript. Need to understand query efficiency for practical AI safety applications.", "method": "Introduce Debate Query Complexity (DQC) as the minimum number of bits a verifier must inspect to correctly decide a debate. Analyze query complexity bounds and characterize PSPACE/poly in terms of query efficiency.", "result": "PSPACE/poly = O(log n) queries, showing debate is remarkably query-efficient. Functions depending on all input bits require \u03a9(log n) queries. Any function computable by circuit of size s has DQC(f) \u2264 log(s) + 3. Lower bounds connect to circuit complexity.", "conclusion": "Debate is highly query-efficient - logarithmic oversight suffices even for complex problems. Proving DQC lower bounds for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity."}}
{"id": "2602.07814", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07814", "abs": "https://arxiv.org/abs/2602.07814", "authors": ["Simiao Ren", "Yuchen Zhou", "Xingyu Shen", "Kidus Zewde", "Tommy Duong", "George Huang", "Hatsanai", "Tiangratanakul", "Tsang", "Ng", "En Wei", "Jiayu Xue"], "title": "How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study", "comment": null, "summary": "As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$\u03c1$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\\% mean accuracy) from the worst (37.5\\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $\u03c7^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.", "AI": {"tldr": "First comprehensive zero-shot evaluation of 16 SOTA AI-generated image detectors across 12 datasets reveals no universal winner, large performance gaps, and systematic failure patterns, challenging the \"one-size-fits-all\" detector paradigm.", "motivation": "Address critical gap in understanding out-of-the-box performance of AI-generated image detectors, which is the most common deployment scenario for practitioners combating misinformation and maintaining content authenticity.", "method": "Zero-shot evaluation of 16 state-of-the-art detection methods (23 pretrained variants) across 12 diverse datasets comprising 2.6 million image samples from 291 unique generators including modern diffusion models.", "result": "No universal winner exists (Spearman \u03c1: 0.01-0.87); 37 percentage-point performance gap between best (75.0% mean accuracy) and worst (37.5%) detectors; training data alignment causes 20-60% performance variance; modern commercial generators defeat most detectors (18-30% average accuracy); three systematic failure patterns identified.", "conclusion": "Challenges \"one-size-fits-all\" detector paradigm, provides actionable deployment guidelines showing practitioners must carefully select detectors based on specific threat landscape rather than relying on published benchmark performance."}}
{"id": "2602.08707", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08707", "abs": "https://arxiv.org/abs/2602.08707", "authors": ["Aditya Gulati", "Nuria Oliver"], "title": "Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers", "comment": null, "summary": "As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of \"trust\" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.", "AI": {"tldr": "Paper examines how chatbots exploit cognitive biases to create unearned trust, proposing to reframe them as skilled salespeople rather than companions, highlighting the gap between psychological trust formation and normative trustworthiness.", "motivation": "To examine the disconnect between how regulatory frameworks define trust in chatbots (normative terms) versus how users actually develop trust (behavioral mechanisms), particularly when chatbots use design choices that exploit cognitive biases to create unearned trust.", "method": "Conceptual analysis and reframing approach - proposing to reconceptualize chatbots not as companions or assistants but as \"highly skilled salespeople\" whose objectives are determined by the deploying organization, highlighting the distinction between psychological trust formation and normative trustworthiness.", "result": "Identifies that current trust in chatbots often emerges from behavioral mechanisms and interactional design choices that leverage cognitive biases, rather than being earned through demonstrated trustworthiness, creating a problematic gap between competing notions of trust.", "conclusion": "The coexistence of competing \"trust\" concepts under a shared term obscures important distinctions, requiring further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems."}}
{"id": "2602.07815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07815", "abs": "https://arxiv.org/abs/2602.07815", "authors": ["Simiao Ren"], "title": "Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures", "comment": null, "summary": "Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \\textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \\textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \\emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\\% false adult rates on minors while VLMs achieve 13--25\\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.", "AI": {"tldr": "VLMs outperform specialized age estimation models, challenging the need for task-specific architectures and suggesting distillation of VLM capabilities into efficient specialized models.", "motivation": "No prior benchmark has systematically compared modern vision-language models against specialized age estimation architectures, despite facial age estimation's importance for content moderation, age verification, and deepfake detection.", "method": "First large-scale cross-paradigm benchmark evaluating 34 models (22 specialized architectures, 12 general-purpose VLMs) across 8 standard datasets totaling 1,100 test images per model, with analysis of age verification at 18-year threshold and stratified analysis across 14 age groups.", "result": "Zero-shot VLMs significantly outperform most specialized models (MAE 5.65 vs 9.88 years). Best VLM (Gemini 3 Flash Preview, MAE 4.32) beats best non-LLM model (MiVOLO, MAE 5.10) by 15%. VLMs achieve 13-25% false adult rates on minors vs 60-100% for non-LLM models. All models struggle most at extreme ages (<5 and 65+).", "conclusion": "Findings challenge assumption that task-specific architectures are necessary for age estimation; field should redirect toward distilling VLM capabilities into efficient specialized models."}}
{"id": "2602.08708", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08708", "abs": "https://arxiv.org/abs/2602.08708", "authors": ["Stefan Edelkamp", "Ji\u0159\u00ed Fink", "Petr Gregor", "Anders Jonsson", "Bernhard Nebel"], "title": "Intermediate Results on the Complexity of STRIPS$_{1}^{1}$", "comment": null, "summary": "This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.", "AI": {"tldr": "Investigates computational complexity of STRIPS planning with single precondition/effect operators, testing if it's NP-complete using SAT solving and graph/Petri net analysis.", "motivation": "Bylander showed propositional STRIPS planning is PSPACE-complete even with limited operators, but it's unknown whether STRIPS with just one precondition and one effect (STRIPS\u00b9\u2081) is NP-complete. The paper aims to resolve this \"small solution hypothesis.\"", "method": "Uses SAT solver for small instances, introduces literal graph representation, and maps the planning problem to Petri nets to analyze computational complexity.", "result": "The paper sheds light on whether STRIPS\u00b9\u2081 is NP-complete, though specific findings aren't detailed in the abstract.", "conclusion": "Provides insights into the computational complexity boundary between NP and PSPACE for minimal STRIPS planning problems through multiple analytical approaches."}}
{"id": "2602.07820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07820", "abs": "https://arxiv.org/abs/2602.07820", "authors": ["Zhibo Chen", "Yu Guan", "Yajuan Huang", "Chaoqi Chen", "XiangJi", "Qiuyun Fan", "Dong Liang", "Qiegen Liu"], "title": "Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction", "comment": "10 pages, 6 figures", "summary": "Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.", "AI": {"tldr": "Proposes operator-guided diffusion framework with dual-stream network for SMS MRI reconstruction, improving fidelity and reducing slice leakage.", "motivation": "SMS MRI with in-plane undersampling creates strongly coupled inverse problems with deterministic inter-slice interference. Existing diffusion methods use Gaussian-noise models with mismatched consistency steps for SMS physics.", "method": "Operator-guided framework models degradation using known acquisition operators with deterministic inversion. Introduces OCDI-Net that disentangles target-slice content from interference and predicts structured degradations. Two-stage chained inference: SMS slice separation followed by in-plane completion.", "result": "Experiments on fastMRI brain data and in vivo diffusion MRI show improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.", "conclusion": "Operator-guided framework with OCDI-Net effectively addresses SMS MRI reconstruction challenges by explicitly modeling acquisition physics and performing structured inversion."}}
{"id": "2602.08715", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08715", "abs": "https://arxiv.org/abs/2602.08715", "authors": ["Miquel Mir\u00f3-Nicolau", "Gabriel Moy\u00e0-Alcover", "Anna Arias-Duart"], "title": "Exploring SAIG Methods for an Objective Evaluation of XAI", "comment": null, "summary": "The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.", "AI": {"tldr": "First review of Synthetic Artificial Intelligence Ground truth (SAIG) methods for evaluating XAI techniques, proposing a taxonomy and highlighting lack of consensus in evaluation approaches.", "motivation": "XAI evaluation lacks universal ground truth for explanations, making objective assessment challenging. SAIG methods offer promising direction by generating artificial ground truths to enable direct evaluation of XAI techniques.", "method": "Conducted first comprehensive review and analysis of SAIG methods, developed novel taxonomy to classify approaches, identified seven key distinguishing features, performed comparative study of existing techniques.", "result": "Revealed concerning lack of consensus on most effective XAI evaluation techniques, highlighting the diversity and complexity of SAIG approaches in the field.", "conclusion": "Urgent need for further research and standardization in XAI evaluation methods, as current SAIG approaches show significant variation without clear agreement on best practices."}}
{"id": "2602.07827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07827", "abs": "https://arxiv.org/abs/2602.07827", "authors": ["Guoting Wei", "Xia Yuan", "Yang Zhou", "Haizhao Jing", "Yu Liu", "Xianbiao Qi", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "title": "Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection", "comment": null, "summary": "Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "AI": {"tldr": "OTA-Det is a unified framework that bridges Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) to enable both rich semantic understanding and multi-target detection in aerial scenes.", "motivation": "Current aerial scene understanding paradigms have limitations: OVAD only provides coarse category-level semantics, while RSVG is structurally limited to single-target localization. Neither can simultaneously support rich semantic understanding and multi-target detection.", "method": "Proposes OTA-Det with: 1) Task reformulation strategy to unify objectives and supervision across both paradigms; 2) Dense semantic alignment strategy for multi-granularity correspondence; 3) Built on RT-DETR architecture extended for open-text detection with efficient modules.", "result": "Achieves state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "conclusion": "OTA-Det successfully bridges two key aerial scene understanding paradigms into a unified framework that enables simultaneous rich semantic understanding and multi-target detection with real-time efficiency."}}
{"id": "2602.08734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08734", "abs": "https://arxiv.org/abs/2602.08734", "authors": ["David Hud\u00e1k", "Maris F. L. Galesloot", "Martin Tappler", "Martin Kure\u010dka", "Nils Jansen", "Milan \u010ce\u0161ka"], "title": "Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning", "comment": "17 pages (8 main paper, 2 references, 7 appendix). 3 figures in the main paper, 3 figures in the appendix. Accepted AAMAS'26 submission", "summary": "Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.", "AI": {"tldr": "Lexpop combines deep RL training of neural policies with extraction of verifiable finite-state controllers for POMDPs, extending to robust policies for HM-POMDPs with performance guarantees.", "motivation": "Scalability limitations of existing POMDP solvers, especially for robust policies across multiple POMDPs (HM-POMDPs).", "method": "Two-stage approach: (1) train neural policy using deep RL with RNN, (2) extract finite-state controller mimicking neural policy; extended to HM-POMDPs by iteratively training robust policies against worst-case POMDPs.", "result": "Outperforms state-of-the-art solvers for both POMDPs and HM-POMDPs on problems with large state spaces.", "conclusion": "Lexpop provides scalable POMDP solving with formal guarantees via controller extraction, enabling robust policy computation for HM-POMDPs."}}
{"id": "2602.07833", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07833", "abs": "https://arxiv.org/abs/2602.07833", "authors": ["Weijiang Lv", "Yaoxuan Feng", "Xiaobo Xia", "Jiayu Wang", "Yan Jing", "Wenchao Chen", "Bo Chen"], "title": "SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models", "comment": "53 pages, 42 figures, 14 tables", "summary": "Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.", "AI": {"tldr": "SPD-Faith Bench is a diagnostic benchmark for evaluating reasoning faithfulness in multimodal LLMs, revealing systematic failures like perceptual blindness and perception-reasoning dissociation, with SAGE framework proposed to improve visual evidence calibration.", "motivation": "Chain-of-Thought reasoning improves MLLM interpretability but its faithfulness remains unclear, with prior work focusing on perceptual hallucinations while leaving reasoning-level unfaithfulness underexplored.", "method": "Introduce SPD-Faith Bench benchmark based on fine-grained image difference reasoning to isolate faithfulness from linguistic priors, then propose SAGE - a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception.", "result": "Evaluations on state-of-the-art MLLMs reveal two systematic failure modes: perceptual blindness and perception-reasoning dissociation, traced to decaying visual attention and representation shifts in residual streams.", "conclusion": "The work highlights the importance of explicitly evaluating faithfulness beyond response correctness, with benchmark and code made publicly available for further research."}}
{"id": "2602.08754", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08754", "abs": "https://arxiv.org/abs/2602.08754", "authors": ["Rose E. Guingrich", "Dvija Mehta", "Umang Bhatt"], "title": "Belief Offloading in Human-AI Interaction", "comment": null, "summary": "What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, \"belief offloading,\" in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.", "AI": {"tldr": "The paper introduces \"belief offloading\" - when people delegate belief formation to AI systems, potentially harming cognitive skills and belief systems.", "motivation": "To understand how LLM chatbots as thought partners can lead to cognitive offloading that negatively impacts people's cognitive abilities and belief formation processes when over-relied upon.", "method": "Interdisciplinary analysis drawing on philosophy, psychology, and computer science research to define boundary conditions, create a descriptive taxonomy of belief offloading, and examine normative implications.", "result": "Developed a conceptual framework for belief offloading, including its boundary conditions, taxonomy, and normative consequences for human-AI interaction.", "conclusion": "Belief offloading represents a significant concern in human-AI interaction that requires further research to assess its potential consequences and develop appropriate safeguards."}}
{"id": "2602.07835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07835", "abs": "https://arxiv.org/abs/2602.07835", "authors": ["Sanoojan Baliah", "Yohan Abeysinghe", "Rusiru Thushara", "Khan Muhammad", "Abhinav Dhall", "Karthik Nandakumar", "Muhammad Haris Khan"], "title": "VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping", "comment": null, "summary": "We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.", "AI": {"tldr": "VFace is a training-free plug-and-play method for high-quality video face swapping that integrates with diffusion-based image face swapping approaches, improving temporal consistency and visual fidelity.", "motivation": "Existing video face swapping methods often suffer from temporal inconsistencies when applied frame-by-frame, requiring additional training or video-specific fine-tuning. There's a need for a modular solution that can enhance existing image-based diffusion approaches for video applications without retraining.", "method": "Three key techniques: 1) Frequency Spectrum Attention Interpolation to preserve identity characteristics, 2) Target Structure Guidance via plug-and-play attention injection for structural alignment, and 3) Flow-Guided Attention Temporal Smoothening for spatiotemporal coherence without modifying the underlying diffusion model.", "result": "Extensive experiments show significant enhancement in temporal consistency and visual fidelity compared to frame-wise generation approaches. The method works without additional training or video-specific fine-tuning.", "conclusion": "VFace offers a practical and modular solution for video-based face swapping that can be seamlessly integrated with existing diffusion-based image face swapping methods, providing improved temporal coherence while maintaining high visual quality."}}
{"id": "2602.08783", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08783", "abs": "https://arxiv.org/abs/2602.08783", "authors": ["Zirui Li", "Xuefeng Bai", "Kehai Chen", "Yizhi Li", "Jian Yang", "Chenghua Lin", "Min Zhang"], "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure", "comment": "22 pages", "summary": "Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.", "AI": {"tldr": "Latent CoT steps are modeled as causal variables in an SCM to analyze their necessity, influence propagation, and answer mode retention, revealing staged functionality with non-local routing and gaps between early output bias and late representational commitment.", "motivation": "Latent chain-of-thought methods use internal computations that are difficult to evaluate beyond correlation-based probes. The paper aims to understand latent CoT as a manipulable causal process in representation space to better interpret and improve these reasoning systems.", "method": "Model latent CoT steps as variables in a structural causal model (SCM) and analyze their effects through step-wise do-interventions. Study two paradigms (Coconut and CODI) on mathematical and general reasoning tasks to investigate causal necessity, influence propagation, and answer mode retention.", "result": "Latent-step budgets behave like staged functionality with non-local routing rather than homogeneous extra depth. There's a persistent gap between early output bias and late representational commitment. Steps show causal necessity patterns and influence propagation differs from explicit CoT.", "conclusion": "Results motivate mode-conditional and stability-aware analyses as more reliable tools for interpreting latent reasoning systems, suggesting corresponding training/decoding objectives to improve these systems."}}
{"id": "2602.07854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07854", "abs": "https://arxiv.org/abs/2602.07854", "authors": ["Chendong Xiang", "Jiajun Liu", "Jintao Zhang", "Xiao Yang", "Zhengwei Fang", "Shizun Wang", "Zijun Wang", "Yingtian Zou", "Hang Su", "Jun Zhu"], "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model", "comment": null, "summary": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \\textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \\textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \\textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.", "AI": {"tldr": "ViewRope introduces geometry-aware encoding for video transformers to maintain spatial persistence in predictive world models, addressing geometric drift through camera-ray direction injection and efficient attention mechanisms.", "motivation": "Current predictive world models lack spatial persistence - they fail to maintain stable scene structures over long trajectories and hallucinate details when revisiting locations, due to reliance on screen-space positional embeddings that conflict with 3D projective geometry.", "method": "Introduces ViewRope (geometry-aware encoding injecting camera-ray directions into video transformer self-attention), Geometry-Aware Frame-Sparse Attention (selectively attends to relevant historical frames using geometric cues), and ViewBench diagnostic suite for evaluation.", "result": "ViewRope substantially improves long-term consistency while reducing computational costs, demonstrating better loop-closure fidelity and reduced geometric drift compared to previous approaches.", "conclusion": "Geometry-aware encoding through ViewRope provides a model-native inductive bias for 3D consistency, enabling predictive world models to maintain spatial persistence across long trajectories without hallucination."}}
{"id": "2602.08796", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08796", "abs": "https://arxiv.org/abs/2602.08796", "authors": ["Kevin Fan", "Jacquelyn A. Bialo", "Hongli Li"], "title": "The Use of AI Tools to Develop and Validate Q-Matrices", "comment": "An earlier version of this study was presented at the Psychometric Society Meeting held in July 2025 in Minneapolis, USA", "summary": "Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.", "AI": {"tldr": "AI models can generate Q-matrices for cognitive diagnostic modeling, with Gemini 2.5 Pro achieving higher agreement with validated Q-matrices than human experts, but performance varies across models and time.", "motivation": "Q-matrix construction is essential but time-consuming in cognitive diagnostic modeling, prompting investigation of whether AI tools can automate or assist this process by comparing AI-generated Q-matrices with validated ones.", "method": "Multiple AI models were trained on the same materials as human experts, then their generated Q-matrices were compared with a validated Q-matrix from Li and Suen (2013) using Cohen's kappa agreement measures, with follow-up testing of newer AI versions.", "result": "AI models showed substantial variation in performance, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding all human experts. However, newer AI versions in 2026 showed lower agreement.", "conclusion": "AI tools show promise for Q-matrix development, with some models outperforming human experts, but performance is inconsistent across models and over time, highlighting the need for careful selection and ongoing evaluation of AI tools in educational measurement."}}
{"id": "2602.07860", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07860", "abs": "https://arxiv.org/abs/2602.07860", "authors": ["Fei Yu", "Shudan Guo", "Shiqing Xin", "Beibei Wang", "Haisen Zhao", "Wenzheng Chen"], "title": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images", "comment": "Accepted by 3DV 2026. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "summary": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.\n  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.\n  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "AI": {"tldr": "Novel inverse rendering method for 3D shape recovery from ultra-fast motion-blurred images using fast barycentric coordinate solver for efficient simulation.", "motivation": "Traditional 3D reconstruction fails on extreme motion-blurred images from fast-moving objects (sports balls, rotating machinery), creating need for new approaches.", "method": "Differentiable inverse rendering with fast barycentric coordinate solver that reduces computational overhead by 4.57x, enabling photorealistic simulation of high-speed motion.", "result": "Method successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, enabling efficient forward simulation.", "conclusion": "Advances vision-based 3D reconstruction boundaries by enabling shape recovery from ultra-fast motion-blurred images through efficient differentiable rendering."}}
{"id": "2602.08804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08804", "abs": "https://arxiv.org/abs/2602.08804", "authors": ["Liming Zhou", "Ailing Liu", "Hongwei Liu", "Min He", "Heng Zhang"], "title": "Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures", "comment": null, "summary": "Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.", "AI": {"tldr": "RC-LLM: A residual-connection-based RCA method using LLMs for microservice fault diagnosis that integrates multi-source telemetry data and leverages LLM contextual reasoning for causal dependency modeling.", "motivation": "Root cause localization is challenging in complex microservice architectures due to complex fault propagation and high-dimensional telemetry data (metrics, logs, traces), limiting existing RCA methods' effectiveness.", "method": "Proposes RC-LLM with residual-like hierarchical fusion structure to integrate multi-source telemetry data, leveraging LLM contextual reasoning to model temporal and cross-microservice causal dependencies.", "result": "Experimental results on CCF-AIOps microservice datasets demonstrate RC-LLM achieves strong accuracy and efficiency in root cause analysis.", "conclusion": "RC-LLM effectively addresses microservice RCA challenges through LLM-based multi-source data fusion and causal dependency modeling, showing promising performance on real-world datasets."}}
{"id": "2602.07864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07864", "abs": "https://arxiv.org/abs/2602.07864", "authors": ["Chen Yang", "Guanxin Lin", "Youquan He", "Peiyao Chen", "Guanghe Liu", "Yufan Mo", "Zhouyuan Xu", "Linhao Wang", "Guohui Zhang", "Zihang Zhang", "Shenxiang Zeng", "Chen Wang", "Jiansheng Fan"], "title": "Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds", "comment": null, "summary": "Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.", "AI": {"tldr": "SSI-Bench is a new VQA benchmark for spatial reasoning on constrained 3D structures, revealing large gaps between VLMs and human performance (33.6% vs 91.6%).", "motivation": "Existing VLM benchmarks often evaluate unconstrained scenes where models can exploit 2D shortcuts, lacking proper assessment of true spatial intelligence needed for real-world physical interactions.", "method": "Created 1,000 ranking questions through 400+ hours of human curation, focusing on complex real-world 3D structures with geometric, topological, and physical constraints. Questions require mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning.", "result": "Evaluated 31 VLMs showing poor performance: best open-source model 22.2%, best closed-source 33.6%, while humans achieve 91.6%. \"Thinking\" prompts yield minimal gains, with failures in structural grounding and constraint-consistent 3D reasoning.", "conclusion": "SSI-Bench exposes fundamental limitations in current VLMs' spatial reasoning capabilities, highlighting the need for better structural understanding and constraint-aware 3D reasoning for real-world applications."}}
{"id": "2602.08815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08815", "abs": "https://arxiv.org/abs/2602.08815", "authors": ["Yanglei Gan", "Peng He", "Yuxiang Cai", "Run Lin", "Guanyu Zhou", "Qiao Liu"], "title": "Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation", "comment": null, "summary": "Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.", "AI": {"tldr": "NADEx introduces a negative-aware diffusion model for temporal knowledge graph reasoning that incorporates negative evidence and cosine-alignment regularization to improve prediction accuracy.", "motivation": "Current diffusion models for TKG reasoning have two limitations: (1) they only condition on positive historical evidence while ignoring informative negative context, and (2) training objectives focus too much on cross-entropy ranking which improves candidate ordering but provides poor supervision for denoised embedding calibration.", "method": "NADEx encodes subject-centric histories of entities, relations, and temporal intervals into sequential embeddings. It perturbs query objects in the forward diffusion process and reconstructs them in reverse using a Transformer denoiser conditioned on temporal-relational context. The method introduces a cosine-alignment regularizer derived from batch-wise negative prototypes to tighten decision boundaries against implausible candidates.", "result": "Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance compared to existing methods.", "conclusion": "The proposed NADEx framework successfully bridges the gaps in existing diffusion models for TKG reasoning by incorporating negative evidence and improved regularization, achieving superior predictive performance on temporal knowledge graph extrapolation tasks."}}
{"id": "2602.07872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07872", "abs": "https://arxiv.org/abs/2602.07872", "authors": ["Mert Sonmezer", "Serge Vasylechko", "Duygu Atasoy", "Seyda Ertekin", "Sila Kurugol"], "title": "WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning", "comment": null, "summary": "Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.", "AI": {"tldr": "WristMIR: A region-aware pediatric wrist radiograph retrieval framework using dense reports and bone-specific localization to improve fracture pattern retrieval without manual annotations.", "motivation": "Retrieving wrist radiographs with analogous fracture patterns is challenging due to subtle, localized cues obscured by overlapping anatomy and variable imaging views, compounded by scarcity of large annotated datasets for medical image retrieval.", "method": "Uses MedGemma-based structured report mining to generate global and region-level captions, processes wrist images and bone-specific crops (distal radius, distal ulna, ulnar styloid), jointly trains global and local contrastive encoders, and performs two-stage retrieval: coarse global matching followed by region-conditioned reranking.", "result": "Improves image-to-text Recall@5 from 0.82% to 9.35%, achieves fracture classification AUROC 0.949 and AUPRC 0.953, increases retrieval-based fracture diagnosis mean F1 from 0.568 to 0.753, and radiologists rate retrieved cases as more clinically relevant (mean scores 3.36 to 4.35).", "conclusion": "Anatomically guided retrieval can enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging, with publicly available source code."}}
{"id": "2602.08835", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08835", "abs": "https://arxiv.org/abs/2602.08835", "authors": ["Andr\u00e9s Holgado-S\u00e1nchez", "Peter Vamplew", "Richard Dazeley", "Sascha Ossowski", "Holger Billhardt"], "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning", "comment": "18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material", "summary": "Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.\n  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.", "AI": {"tldr": "Learning socially-derived value alignment models and value systems for diverse user groups in MDPs using clustering and preference-based multi-objective RL.", "motivation": "Value-aware AI needs to recognize human values and adapt to diverse value systems, but current approaches have limitations: they require manual feature design, lack value-based interpretability, or don't adapt well to diverse user preferences. Values are social and exhibit patterns among groups, requiring representation that adheres to multiple users.", "method": "Propose algorithms for learning value alignment models and value systems for agent societies in MDPs using clustering and preference-based multi-objective reinforcement learning (PbMORL). Jointly learn socially-derived value alignment models (groundings) and value systems representing different user groups (clusters). Each cluster includes a value system representing members' preferences and an approximately Pareto-optimal policy aligned with that value system.", "result": "Evaluated the method against state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.", "conclusion": "The approach addresses limitations of existing methods by providing value-based interpretability, adaptability to diverse user preferences, and socially-derived value alignment without requiring manually designed features."}}
{"id": "2602.07891", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07891", "abs": "https://arxiv.org/abs/2602.07891", "authors": ["Zihui Gao", "Ke Liu", "Donny Y. Chen", "Duochao Shi", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video", "comment": null, "summary": "Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.", "AI": {"tldr": "SAGE enables scalable adaptation of geometric foundation models using raw Internet videos without 3D ground truth, achieving 20-42% improvement in zero-shot 3D reconstruction on unseen benchmarks.", "motivation": "Geometric foundation models are limited by scarce 3D annotations, while Internet videos offer abundant raw data but lack ground-truth geometry and contain observational noise.", "method": "Hierarchical mining pipeline with: 1) Informative training trajectory selection, 2) Sparse Geometric Anchoring via SfM point clouds for global structure, and 3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints, plus regularization to prevent catastrophic forgetting.", "result": "Reduces Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines, demonstrating superior zero-shot generalization.", "conclusion": "SAGE pioneers scalable adaptation of geometric foundation models using Internet video, establishing a new paradigm for general-purpose 3D learning without requiring 3D annotations."}}
{"id": "2602.08848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08848", "abs": "https://arxiv.org/abs/2602.08848", "authors": ["Quentin Cohen-Solal", "Alexandre Niveau", "Maroua Bouzid"], "title": "Deciding the Satisfiability of Combined Qualitative Constraint Networks", "comment": null, "summary": "Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.", "AI": {"tldr": "A formal framework unifying extensions and combinations of qualitative reasoning formalisms with polynomial satisfiability guarantees.", "motivation": "Qualitative reasoning handles imprecise, incomplete information without numerical values, but existing formalisms need unification for multi-scale reasoning, temporal sequences, and loose integrations.", "method": "Proposes a formal framework that unifies multiple extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations.", "result": "Establishes two complementary theorems guaranteeing polynomial satisfiability decision, recovers known results for size-topology combination, and generalizes definitions to include previously excluded qualitative formalisms.", "conclusion": "The framework enables unified reasoning across various qualitative formalism combinations while providing polynomial complexity guarantees for satisfiability decisions."}}
{"id": "2602.07899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07899", "abs": "https://arxiv.org/abs/2602.07899", "authors": ["Zhenhao Shang", "Haizhao Jing", "Guoting Wei", "Haokui Zhang", "Rong Xiao", "Jianqing Gao", "Peng Wang"], "title": "Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models", "comment": null, "summary": "Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.", "AI": {"tldr": "TLQ is a token-level importance-aware layer-wise quantization framework for vision-language models that addresses calibration challenges by using gradient-guided token importance and multi-GPU distributed calibration.", "motivation": "Vision-language models (VLMs) present unique challenges for post-training quantization due to substantial differences between visual and text tokens in activation distributions and quantization sensitivity, making effective calibration difficult.", "method": "Proposes Token-level Importance-aware Layer-wise Quantization (TLQ) with: 1) gradient-guided token-level importance integration mechanism for quantization error, 2) token-level calibration set construction, and 3) multi-GPU, quantization-exposed layer-wise calibration scheme that distributes workload across multiple RTX3090 GPUs.", "result": "TLQ consistently achieves performance improvements across two models, three model scales, and two quantization settings, demonstrating strong quantization stability.", "conclusion": "TLQ provides an effective calibration framework for VLMs that addresses the unique challenges of quantizing vision-language models while reducing hardware requirements through distributed multi-GPU calibration."}}
{"id": "2602.08889", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08889", "abs": "https://arxiv.org/abs/2602.08889", "authors": ["Tobias Lorenz", "Mario Fritz"], "title": "Scalable Delphi: Large Language Models for Structured Risk Estimation", "comment": null, "summary": "Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.", "AI": {"tldr": "LLMs can serve as scalable proxies for structured expert elicitation, achieving results comparable to human expert panels while reducing time from months to minutes.", "motivation": "Traditional expert elicitation methods like Delphi are too time-consuming and resource-intensive for most applications, creating a need for scalable alternatives.", "method": "Scalable Delphi adapts classical Delphi protocol for LLMs with expert personas, iterative refinement, and rationale sharing, evaluated using calibration against verifiable proxies, sensitivity to evidence, and alignment with human judgment.", "result": "LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically with added evidence, and align with human expert panels - sometimes even closer than human panels align with each other.", "conclusion": "LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, dramatically reducing elicitation time while maintaining quality."}}
{"id": "2602.07931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07931", "abs": "https://arxiv.org/abs/2602.07931", "authors": ["Olena Hrynenko", "Darya Baranouskaya", "Alina Elena Baia", "Andrea Cavallaro"], "title": "Which private attributes do VLMs agree on and predict well?", "comment": "This work has been accepted to the ICASSP 2026", "summary": "Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.", "AI": {"tldr": "VLMs show promise for zero-shot privacy attribute detection, often predicting more attributes than humans but can complement human annotations when they have high agreement.", "motivation": "To evaluate how well open-source Visual Language Models (VLMs) can recognize privacy-related attributes in images without training, and understand their agreement/disagreement with human annotations.", "method": "Zero-shot evaluation of open-source VLMs for privacy attribute recognition, analyzing inter-annotator agreement among VLMs and comparing VLM predictions with human annotations.", "result": "VLMs tend to predict privacy attributes more frequently than human annotators. When VLMs show high inter-annotator agreement, they can identify attributes that human annotators miss, suggesting they could complement human annotation.", "conclusion": "VLMs have potential to support privacy annotations in large-scale image datasets, particularly as a complementary tool to human annotation when they exhibit high agreement."}}
{"id": "2602.08905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08905", "abs": "https://arxiv.org/abs/2602.08905", "authors": ["Jiawei Liu", "Xiting Wang", "Yuanyuan Zhong", "Defu Lian", "Yu Yang"], "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models", "comment": "13 pages, 3 figures", "summary": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.", "AI": {"tldr": "STP improves RL efficiency and stability for diffusion-based LLMs through spatial pruning (using static priors to constrain exploration) and temporal pruning (skipping late-stage refinement steps), reducing variance in log-likelihood estimation.", "motivation": "RL is crucial for enhancing reasoning in diffusion-based LLMs but faces efficiency and stability challenges due to redundancy in the generative process.", "method": "Spatio-Temporal Pruning (STP) framework with two components: spatial pruning (constrains exploration space using static priors) and temporal pruning (bypasses redundant late-stage refinement steps).", "result": "STP surpasses state-of-the-art baselines in both efficiency and accuracy, with theoretical analysis showing it reduces variance of log-likelihood estimation for more stable policy updates.", "conclusion": "STP effectively addresses RL challenges for dLLMs by compressing redundancy in the generative process, improving both efficiency and stability through principled pruning techniques."}}
{"id": "2602.08939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08939", "abs": "https://arxiv.org/abs/2602.08939", "authors": ["Longling Geng", "Andy Ouyang", "Theodore Wu", "Daphne Barretto", "Matthew John Hayes", "Rachael Cooper", "Yuqiao Zeng", "Sameer Vijay", "Gia Ancone", "Ankit Rai", "Matthew Wolfman", "Patrick Flanagan", "Edward Y. Chang"], "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse", "comment": "17 pages, 20 tables, figures", "summary": "LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench", "AI": {"tldr": "CausalT5K is a diagnostic benchmark of 5,000+ cases across 10 domains that tests LLM causal reasoning failures like rung collapse, sycophantic drift, and uncalibrated refusal, enabling systematic diagnosis beyond aggregate accuracy.", "motivation": "LLMs exhibit well-documented failures in causal reasoning (sycophancy, rung collapse, miscalibrated refusal), but remediation progress is slow due to lack of benchmarks enabling systematic diagnosis of these specific failure modes.", "method": "Developed through human-machine collaborative pipeline with 40 domain experts, iterative cross-validation, and composite verification via rule-based, LLM, and human scoring. Embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity).", "result": "Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, demonstrating CausalT5K's value for advancing trustworthy reasoning systems.", "conclusion": "CausalT5K implements Pearl's Ladder of Causation as research infrastructure, providing a diagnostic benchmark that reveals failure modes invisible to aggregate accuracy and enables systematic remediation of LLM causal reasoning deficiencies."}}
{"id": "2602.07955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07955", "abs": "https://arxiv.org/abs/2602.07955", "authors": ["Jiwei Chen", "Qi Wang", "Junyu Gao", "Jing Zhang", "Dingyi Li", "Jing-Jia Luo"], "title": "One-Shot Crowd Counting With Density Guidance For Scene Adaptaion", "comment": null, "summary": "Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.", "AI": {"tldr": "Few-shot crowd counting method using local and global density characteristics to adapt to unseen surveillance scenes.", "motivation": "Existing crowd models have limited generalization for unseen surveillance scenes due to significant variations between scenes captured at different locations.", "method": "Proposes multiple local density learners to capture different density distributions, encodes local density similarity matrices, and extracts global density features from support images to guide the model both locally and globally.", "result": "Outperforms recent state-of-the-art methods in few-shot crowd counting on three surveillance datasets.", "conclusion": "The proposed approach effectively adapts to unseen surveillance scenes by leveraging both local and global density characteristics, improving generalization in few-shot crowd counting."}}
{"id": "2602.08948", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08948", "abs": "https://arxiv.org/abs/2602.08948", "authors": ["Chen Jin", "Ryutaro Tanno", "Tom Diethe", "Philip Teare"], "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute", "comment": null, "summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.", "AI": {"tldr": "CoRefine is a confidence-guided self-refinement method that uses a lightweight controller to dynamically decide when to halt, re-examine, or try different approaches during LLM reasoning, achieving competitive accuracy with ~190x token reduction compared to parallel decoding baselines.", "motivation": "Current LLMs rely on compute-intensive parallel decoding (e.g., 512 samples) for test-time scaling to boost reasoning accuracy, which incurs substantial computational costs. There's a need for more efficient methods that can achieve similar accuracy with significantly fewer tokens.", "method": "CoRefine uses a 211k-parameter Conv1D controller on top of a frozen LLM that consumes full-trace confidence signals to make decisions: halt (accept current solution), re-examine, or try a different approach. This enables targeted self-correction with an average of 2.7 refinement steps per problem. The method is extended to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation.", "result": "The controller achieves 92.6% precision when confidently halting, indicating confidence dynamics reliably signal correctness without ground-truth verification. It achieves competitive accuracy with roughly 190-fold token reduction relative to 512-sample baselines across diverse reasoning benchmarks and three open-source models.", "conclusion": "By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers, offering efficient self-refinement with easy serving integration and verifier compatibility."}}
{"id": "2602.07960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07960", "abs": "https://arxiv.org/abs/2602.07960", "authors": ["Changli Tang", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Chao Zhang"], "title": "D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning", "comment": null, "summary": "Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \\textbf{d}ialogue-centric \\textbf{o}mni-modal large language model optimized for \\textbf{r}obust audio-visual \\textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \\href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \\href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.", "AI": {"tldr": "D-ORCA is a dialogue-focused omni-modal LLM for audio-visual captioning, trained on new DVD dataset with reinforcement learning using novel reward functions for speaker attribution, content accuracy, and temporal alignment.", "motivation": "Spoken dialogue is crucial for video understanding, but accurately identifying who spoke what and when remains challenging. There's a gap in open-source resources for multi-party dialogue video analysis.", "method": "Developed D-ORCA, a dialogue-centric omni-modal LLM, and curated DVD dataset (40K training, 2K evaluation videos in English/Mandarin). Used group relative policy optimization with three novel reward functions: speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment.", "result": "D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Despite having only 8B parameters, it achieves competitive performance with Qwen3-Omni on general audio-visual understanding benchmarks.", "conclusion": "D-ORCA demonstrates superior dialogue understanding capabilities through specialized architecture, high-quality dataset, and novel reinforcement learning approach, advancing video understanding through accurate speaker attribution and temporal alignment."}}
{"id": "2602.08949", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08949", "abs": "https://arxiv.org/abs/2602.08949", "authors": ["Mohammad Morsali", "Siavash H. Khajavi"], "title": "Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room", "comment": null, "summary": "According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.", "AI": {"tldr": "IVSR is an AI-augmented Digital Twin platform for real-time wildfire management that reduces detection-to-intervention latency through bidirectional data flow and autonomous agents.", "motivation": "Wildfire frequency/intensity is projected to increase significantly due to climate change, while conventional disaster management systems rely on static simulations and passive data acquisition, limiting real-time adaptation to evolving wildfire situations.", "method": "IVSR creates a live virtual replica of fire environments by ingesting multisource sensor imagery, weather data, and 3D forest models. It uses an AI-powered similarity engine to align emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics. Authorized actions are cycled back to the physical layer through standardized procedures.", "result": "Validation through industrial partner case studies demonstrates capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Results show marked reductions in detection-to-intervention latency and more effective resource coordination compared to traditional systems.", "conclusion": "IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management by uniting real-time bidirectional Digital Twins with agentic AI."}}
{"id": "2602.07967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07967", "abs": "https://arxiv.org/abs/2602.07967", "authors": ["Xiaofeng Tan", "Wanjiang Weng", "Haodong Lei", "Hongsong Wang"], "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation", "comment": null, "summary": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.", "AI": {"tldr": "EasyTune is a memory-efficient fine-tuning method for diffusion-based motion generation that addresses alignment challenges by decoupling recursive dependencies in denoising steps and using self-refinement preference learning.", "motivation": "Current motion generative models struggle to align with downstream objectives. Existing methods using differentiable rewards for diffusion model alignment suffer from inefficient optimization and high memory consumption due to recursive dependencies between denoising steps.", "method": "EasyTune fine-tunes diffusion models at each denoising step rather than over the entire trajectory, decoupling recursive dependencies. It also introduces Self-refinement Preference Learning (SPL) to dynamically identify preference pairs and conduct preference learning to address scarcity of preference motion pairs.", "result": "EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup.", "conclusion": "The proposed approach effectively addresses the limitations of existing alignment methods for motion generation by enabling dense, fine-grained, and memory-efficient optimization through step-wise fine-tuning and self-refinement preference learning."}}
{"id": "2602.08968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08968", "abs": "https://arxiv.org/abs/2602.08968", "authors": ["Lucas Maes", "Quentin Le Lidec", "Dan Haramati", "Nassim Massaudi", "Damien Scieur", "Yann LeCun", "Randall Balestriero"], "title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation", "comment": null, "summary": "World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.", "AI": {"tldr": "SWM is a modular, tested world-model research ecosystem addressing implementation fragmentation with standardized tools for data collection, environments, planning algorithms, and baselines.", "motivation": "Current world model implementations are publication-specific, limiting reusability, increasing bug risks, and reducing evaluation standardization, creating barriers to research progress.", "method": "Developed stable-worldmodel (SWM) - a modular ecosystem with efficient data-collection tools, standardized environments with controllable variation factors, planning algorithms, and baseline implementations.", "result": "Created a comprehensive world-model research platform that supports robustness and continual learning research through controllable visual and physical property variations.", "conclusion": "SWM addresses implementation fragmentation in world model research and demonstrates utility by enabling zero-shot robustness studies in DINO-WM."}}
{"id": "2602.07979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07979", "abs": "https://arxiv.org/abs/2602.07979", "authors": ["Peng Peng", "Xinrui Zhang", "Junlin Wang", "Lei Li", "Shaoyu Wang", "Qiegen Liu"], "title": "FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction", "comment": null, "summary": "Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.", "AI": {"tldr": "FSP-Diff is a dual-domain latent diffusion framework that integrates complementary features, full-spectrum priors, and efficient latent diffusion for ultra-low-dose spectral CT reconstruction, achieving superior image quality and computational efficiency.", "motivation": "Ultra-low-dose spectral CT suffers from severely degraded signal-to-noise ratio in energy-specific projections, causing artifacts and loss of structural details, which hinders material discrimination and tissue characterization capabilities.", "method": "Three core strategies: 1) Complementary feature construction integrating direct image reconstructions with projection-domain denoised results, 2) Full-spectrum prior integration fusing multi-energy projections into high-SNR reference, 3) Efficient latent diffusion synthesis embedding multi-path features into compact latent space for accelerated reconstruction.", "result": "Extensive experiments on simulated and real-world datasets demonstrate FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency.", "conclusion": "FSP-Diff shows strong potential for clinically viable ultra-low-dose spectral CT imaging by effectively addressing noise challenges while maintaining fine-grained detail restoration."}}
{"id": "2602.08990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08990", "abs": "https://arxiv.org/abs/2602.08990", "authors": ["Shiyang Feng", "Runmin Ma", "Xiangchao Yan", "Yue Fan", "Yusong Hu", "Songtao Huang", "Shuaiyu Zhang", "Zongsheng Cao", "Tianshuo Peng", "Jiakang Yuan", "Zijie Guo", "Zhijie Zhong", "Shangheng Du", "Weida Wang", "Jinxin Shi", "Yuhao Zhou", "Xiaohan He", "Zhiyin Yu", "Fangchen Yu", "Qihao Zheng", "Jiamin Wu", "Mianxin Liu", "Chi Zhang", "Shaowei Hou", "Shuya Li", "Yankai Jiang", "Wenjie Lou", "Lilong Wang", "Zifu Wang", "Jiong Wang", "Wanghan Xu", "Yue Deng", "Dongrui Liu", "Yiheng Wang", "Wenlong Zhang", "Fenghua Ling", "Shufei Zhang", "Xiaosong Wang", "Shuangjia Zheng", "Xun Huang", "Siqi Sun", "Shuyue Hu", "Peng Ye", "Chunfeng Song", "Bin Wang", "Conghui He", "Yihao Liu", "Xin Li", "Qibin Hou", "Tao Chen", "Xiangyu Yue", "Bin Wang", "Liang He", "Dahua Lin", "Bowen Zhou", "Bo Zhang", "Lei Bai"], "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery", "comment": "Code and project page: https://github.com/InternScience/InternAgent", "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.", "AI": {"tldr": "InternAgent-1.5 is a unified AI system for autonomous scientific discovery across computational and empirical domains, featuring coordinated generation, verification, and evolution subsystems with strong benchmark performance and demonstrated capabilities in algorithm design and experimental execution.", "motivation": "To create a general and scalable framework for end-to-end autonomous scientific discovery that can operate across both computational modeling and laboratory experimentation domains, addressing the need for AI systems that can conduct complete scientific research cycles.", "method": "A structured architecture with three coordinated subsystems (generation, verification, evolution) supported by foundational capabilities for deep research, solution optimization, and long horizon memory, enabling continuous operation across extended discovery cycles.", "result": "Achieves leading performance on scientific reasoning benchmarks (GAIA, HLE, GPQA, FrontierScience), autonomously designs competitive algorithms for core ML problems, and executes complete computational/wet lab experiments producing scientific findings across earth, life, biological, and physical domains.", "conclusion": "InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery that can coordinate computational modeling and laboratory experimentation within a single unified system, demonstrating strong foundational capabilities across diverse scientific domains."}}
{"id": "2602.07980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07980", "abs": "https://arxiv.org/abs/2602.07980", "authors": ["Junlin Wang", "Jiancheng Fang", "Peng Peng", "Shaoyu Wang", "Qiegen Liu"], "title": "Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction", "comment": null, "summary": "The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.", "AI": {"tldr": "CSDN framework uses neural priors and synergistic diffusion to reconstruct high-quality CBCT images from ultra-sparse angular sampling, reducing radiation dose while maintaining diagnostic reliability.", "motivation": "CBCT faces trade-off between radiation exposure and image quality; ultra-sparse angular sampling reduces dose but causes severe artifacts and inter-slice inconsistencies that compromise diagnostic reliability.", "method": "CSDN uses neural priors to encode continuous 3D attenuation representation, then synergistic diffusion with two paths: Sinogram Refinement Diffusion (restores angular continuity) and Digital Radiography Refinement Diffusion (enforces inter-slice consistency), fused by Dual-Projection Reconstruction Fusion module.", "result": "Extensive experiments show CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.", "conclusion": "The proposed CSDN framework successfully addresses ultra-sparse-view CBCT reconstruction challenges by combining neural priors with synergistic diffusion, achieving coherent volumetric reconstruction with reduced radiation dose."}}
{"id": "2602.09000", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09000", "abs": "https://arxiv.org/abs/2602.09000", "authors": ["Ali Hatamizadeh", "Shrimai Prabhumoye", "Igor Gitman", "Ximing Lu", "Seungju Han", "Wei Ping", "Yejin Choi", "Jan Kautz"], "title": "iGRPO: Self-Feedback-Driven LLM Reasoning", "comment": "Tech report", "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.", "AI": {"tldr": "iGRPO extends GRPO with iterative self-conditioning using model-generated drafts to improve mathematical reasoning in LLMs, achieving SOTA results on AIME benchmarks.", "motivation": "LLMs struggle with accurate and consistent mathematical problem-solving. While RL can align models with task rewards, existing methods like PPO have limitations. GRPO offers an efficient alternative, but there's room for improvement through iterative self-feedback mechanisms.", "method": "Two-stage iterative extension of GRPO: Stage 1 samples multiple exploratory drafts and selects the highest-reward one. Stage 2 appends this best draft to the original prompt and applies GRPO-style updates on draft-conditioned refinements, training the policy to improve beyond its prior attempts.", "result": "iGRPO consistently outperforms GRPO across base models (Nemotron-H-8B-Base-8K, DeepSeek-R1 Distilled) under matched rollout budgets. Achieves SOTA results of 85.62% on AIME24 and 79.64% on AIME25 with OpenReasoning-Nemotron-7B. Ablations show generalization beyond GRPO variants, benefits from generative judge, and altered learning dynamics.", "conclusion": "Iterative, self-feedback-based RL (iGRPO) demonstrates strong potential for advancing verifiable mathematical reasoning in LLMs by enabling models to learn from and improve upon their own best attempts through dynamic self-conditioning."}}
{"id": "2602.07986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07986", "abs": "https://arxiv.org/abs/2602.07986", "authors": ["Md. Tarek Hasan", "Sanjay Saha", "Shaojing Fan", "Swakkhar Shatabda", "Terence Sim"], "title": "Deepfake Synthesis vs. Detection: An Uneven Contest", "comment": null, "summary": "The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.", "AI": {"tldr": "Current deepfake detection methods struggle against modern synthesis techniques like diffusion models and NeRF, with both AI detectors and humans showing poor performance against high-quality deepfakes.", "motivation": "The rapid advancement of deepfake generation technologies (diffusion models, NeRF, improved GANs) has created increasingly realistic synthetic media, raising concerns about the effectiveness of current detection methods against these sophisticated new techniques.", "method": "Comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods, with extensive experimentation comparing detection models with modern generation technologies.", "result": "Many state-of-the-art detection models show markedly poor performance against deepfakes produced by modern synthesis techniques, and human participants also perform poorly against the best quality deepfakes, revealing a significant gap between detection capabilities and generation sophistication.", "conclusion": "There is an urgent need for continued refinement of detection models to keep pace with evolving deepfake generation technologies, emphasizing the critical gap between current detection methodologies and new generation techniques that requires intensified research efforts."}}
{"id": "2602.09003", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09003", "abs": "https://arxiv.org/abs/2602.09003", "authors": ["Yudong Wang", "Zixuan Fu", "Hengyu Zhao", "Chen Zhao", "Chuyue Zhou", "Xinle Lin", "Hongya Lyu", "Shuaikang Xue", "Yi Yi", "Yingjiao Wang", "Zhi Zheng", "Yuzhou Zhang", "Jie Zhou", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management", "comment": "16 pages, 3 figures, 7 tables", "summary": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.", "AI": {"tldr": "Proposes a tiered data management framework (L0-L4) for LLM training where models actively guide data management and high-quality data amplifies model capabilities, improving training efficiency and performance.", "motivation": "Current LLM research faces bottlenecks in data availability, acquisition cost, and training efficiency due to unidirectional scaling of data size. The paper argues AGI development needs a new phase of data-model co-evolution where models guide data management and quality data amplifies capabilities.", "method": "Introduces an L0-L4 tiered data management framework ranging from raw uncurated resources to organized verifiable knowledge. LLMs are used in data management processes (quality scoring, content editing) to refine data across tiers. Each tier has distinct properties, strategies, and training roles for strategic allocation across pre-training, mid-training, and alignment phases.", "result": "Empirical studies show tier-aware data utilization significantly improves training efficiency and model performance. The framework balances data quality, acquisition cost, and marginal training benefit. Tiered datasets and processing tools are released to the community.", "conclusion": "The proposed tiered data management framework enables scalable and sustainable data management for LLM training through data-model co-evolution, addressing current bottlenecks in data availability and training efficiency while improving overall model performance."}}
{"id": "2602.07993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07993", "abs": "https://arxiv.org/abs/2602.07993", "authors": ["Xuehai Bai", "Xiaoling Gu", "Akide Liu", "Hangjie Yuan", "YiFan Zhang", "Jack Ma"], "title": "MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance", "comment": "Accepted by AAAI2026", "summary": "Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.", "AI": {"tldr": "MCIE-E1 is a new image editing method that uses multimodal LLMs to handle complex compositional instructions, addressing instruction compliance and background consistency issues through specialized attention modules and a new training data pipeline.", "motivation": "Existing instruction-based image editing methods are limited to simple operations and struggle with complex compositional instructions needed for real-world applications. The paper identifies two key challenges: insufficient instruction compliance and background inconsistency.", "method": "Proposes MCIE-E1 with two key modules: 1) spatial-aware cross-attention module that aligns semantic instructions with spatial regions using spatial guidance during denoising, and 2) background-consistent cross-attention module that preserves features in unedited regions. Also develops a dedicated data pipeline combining MLLM-based filtering with human validation to address dataset scarcity.", "result": "MCIE-E1 outperforms previous state-of-the-art methods on the new CIE-Bench benchmark, achieving a 23.96% improvement in instruction compliance. The method shows superior performance in both quantitative and qualitative assessments.", "conclusion": "The proposed MCIE-E1 method effectively addresses limitations in complex instruction-based image editing through novel architectural designs, improved data pipeline, and comprehensive evaluation, enabling more practical real-world applications."}}
{"id": "2602.09007", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09007", "abs": "https://arxiv.org/abs/2602.09007", "authors": ["Haodong Li", "Jingwei Wu", "Quan Sun", "Guopeng Li", "Juanxi Tian", "Huanyu Zhang", "Yanlin Lai", "Ruichuan An", "Hongbo Peng", "Yuhong Dai", "Chenxi Li", "Chunmei Qing", "Jia Wang", "Ziyang Meng", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang"], "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "comment": "23 pages, 5 figures, 4 tables", "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "AI": {"tldr": "GEBench is a new benchmark for evaluating dynamic GUI state generation, focusing on temporal coherence and interaction logic across multi-step sequences, with a novel 5D metric called GE-Score.", "motivation": "Existing benchmarks focus on general visual fidelity but lack evaluation of state transitions and temporal coherence in GUI-specific contexts, leaving dynamic interaction assessment underexplored.", "method": "Introduced GEBench with 700 curated samples across 5 task categories covering single-step and multi-step interactions, plus grounding point localization. Proposed GE-Score, a five-dimensional metric assessing Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality.", "result": "Current models perform well on single-step transitions but struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Icon interpretation, text rendering, and localization precision are identified as critical bottlenecks.", "conclusion": "GEBench provides a foundation for systematic assessment of generative GUI environments and suggests promising directions for future research toward building high-fidelity dynamic GUI generation systems."}}
{"id": "2512.22730", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22730", "abs": "https://arxiv.org/abs/2512.22730", "authors": ["Youssef Megahed", "Robin Ducharme", "Inok Lee", "Inbal Willner", "Adrian D. C. Chan", "Mark Walker", "Steven Hawken"], "title": "Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning", "comment": "13 pages, 6 figures, 2 tables", "summary": "Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).", "AI": {"tldr": "Self-supervised pretraining (USF-MAE) outperforms supervised baseline for automated cystic hygroma detection in first-trimester ultrasound, achieving 0.98 ROC-AUC with statistically significant improvements.", "motivation": "Cystic hygroma is a high-risk prenatal finding, but supervised deep learning is limited by small labelled datasets. Self-supervised pretraining could enable more accurate and scalable automated detection for early screening programs.", "method": "Fine-tuned Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on 370K+ unlabelled ultrasound images, for binary classification of normal vs cystic hygroma cases. Used same dataset, preprocessing, and 4-fold cross-validation as DenseNet-169 baseline. Analyzed interpretability with Score-CAM visualizations.", "result": "USF-MAE outperformed DenseNet-169 on all metrics: mean accuracy 0.96 vs 0.93, sensitivity 0.94 vs 0.92, specificity 0.98 vs 0.94, ROC-AUC 0.98 vs 0.94. Score-CAM visualizations showed clinically relevant attention to fetal neck regions. Performance improvements were statistically significant (p=0.0057).", "conclusion": "Ultrasound-specific self-supervised pretraining enables accurate, robust deep learning detection of cystic hygroma, overcoming limitations of small labelled datasets and supporting scalable early screening programs."}}
{"id": "2602.08020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08020", "abs": "https://arxiv.org/abs/2602.08020", "authors": ["Minghai Chen", "Mingyuan Liu", "Yuxiang Huan"], "title": "PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping", "comment": null, "summary": "Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.", "AI": {"tldr": "PhysDrape: Hybrid neural-physical solver for garment draping that integrates neural inference with explicit geometric solvers to resolve collision handling trade-offs while ensuring physical validity.", "motivation": "Existing deep learning garment draping methods use soft penalties for collision handling, creating a trade-off between geometric feasibility and physical plausibility - penalizing collisions distorts mesh structure while preserving shape leads to interpenetration.", "method": "Hybrid neural-physical solver with: 1) Physics-Informed Graph Neural Network conditioned on physics-enriched graph to predict residual displacements; 2) Differentiable two-stage solver with learnable Force Solver for quasi-static equilibrium using StVK model and Differentiable Projection for strict collision constraints.", "result": "Achieves state-of-the-art performance with negligible interpenetration and significantly lower strain energy compared to baselines, ensuring superior physical fidelity and robustness in real-time.", "conclusion": "PhysDrape resolves the conflict between geometric feasibility and physical plausibility in garment draping through a differentiable hybrid approach that guarantees physical validity via explicit constraints while enabling end-to-end learning."}}
{"id": "2602.08024", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08024", "abs": "https://arxiv.org/abs/2602.08024", "authors": ["Ziyang Fan", "Keyu Chen", "Ruilong Xing", "Yulin Li", "Li Jiang", "Zhuotao Tian"], "title": "FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging", "comment": "Accepted by ICLR 2026 (Oral)", "summary": "Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.", "AI": {"tldr": "FlashVID is a training-free inference acceleration framework for Video LLMs that reduces computational costs by selecting representative tokens and merging spatiotemporal redundancies, achieving 10x frame input increase with minimal performance loss.", "motivation": "Current VLLMs process too many visual tokens inefficiently, and existing acceleration methods fail to properly handle spatiotemporal relationships, leading to suboptimal compression of dynamic video content.", "method": "Uses Attention and Diversity-based Token Selection (ADTS) for basic representation, then Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained redundancy elimination, both operating without training.", "result": "Retaining only 10% of visual tokens preserves 99.1% of LLaVA-OneVision's performance, enables 10x frame input to Qwen2.5-VL with 8.6% relative improvement within same computational budget.", "conclusion": "FlashVID provides effective, generalizable training-free acceleration for VLLMs by properly handling spatiotemporal relationships, enabling longer video processing with minimal performance degradation."}}
{"id": "2602.08025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08025", "abs": "https://arxiv.org/abs/2602.08025", "authors": ["Yixuan Ye", "Xuanyu Lu", "Yuxin Jiang", "Yuchao Gu", "Rui Zhao", "Qiwei Liang", "Jiachun Pan", "Fengda Zhang", "Weijia Wu", "Alex Jinpeng Wang"], "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models", "comment": null, "summary": "World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/", "AI": {"tldr": "MIND is the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring 250 high-quality videos across diverse scenes with varied action spaces.", "motivation": "There's a lack of unified benchmarks for evaluating fundamental abilities of world models in understanding, remembering, and predicting dynamic visual environments, particularly for memory consistency and action control across different viewpoints and action spaces.", "method": "Created MIND benchmark with 250 high-quality videos (100 first-person + 100 third-person with shared action space, plus 50 clips across varied action spaces) covering eight diverse scenes. Designed evaluation framework to measure memory consistency (temporal stability and contextual coherence) and action control, with varied action spaces to test generalization. Also introduced MIND-World as an interactive Video-to-World baseline.", "result": "Extensive experiments demonstrate MIND's completeness and reveal key challenges in current world models: difficulty maintaining long-term memory consistency and generalizing across different action spaces.", "conclusion": "MIND provides the first comprehensive benchmark for evaluating world models' core abilities, highlighting critical limitations in current approaches and establishing a foundation for future performance benchmarking in this domain."}}
{"id": "2602.08046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08046", "abs": "https://arxiv.org/abs/2602.08046", "authors": ["Yahia Hamdi", "Nicolas Andrialovanirina", "K\u00e9lig Mah\u00e9", "Emilie Poisson Caillault"], "title": "Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects", "comment": "11", "summary": "The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.", "AI": {"tldr": "Proposes MoE-DCGAN: Mixture of Experts integrated with Deep 3D Convolutional GANs for high-quality 3D model generation and reconstruction of incomplete objects, featuring dynamic capacity constraint for balanced specialization and efficiency.", "motivation": "GANs struggle with complex 3D data distributions, incomplete inputs, and computational demands. MoEs offer promising solutions through dynamic expert selection for improved performance and efficiency in 3D object generation and completion tasks.", "method": "Integrates Deep 3D CGANs with MoE framework using multiple specialized generators. Introduces auxiliary loss-free dynamic capacity constraint (DCC) mechanism to guide categorical generator selection, balancing specialization, stability, and computational efficiency for 3D voxel processing.", "result": "Evaluated on shape generation and completion with varying missing regions. Both quantitative and qualitative results demonstrate effectiveness in handling complex 3D data, outperforming state-of-the-art approaches.", "conclusion": "MoE-DCGAN effectively addresses limitations of traditional GANs for 3D data by combining MoE framework with specialized generators and DCC mechanism, achieving superior performance in generating and completing complex 3D objects."}}
{"id": "2602.08047", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08047", "abs": "https://arxiv.org/abs/2602.08047", "authors": ["Jiahong Fu", "Qi Xie", "Deyu Meng", "Zongben Xu"], "title": "Vanilla Group Equivariant Vision Transformer: Simple and Effective", "comment": null, "summary": "Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.", "AI": {"tldr": "A framework for building equivariant Vision Transformers that systematically makes key components (patch embedding, self-attention, positional encodings, down/up-sampling) equivariant, resulting in plug-and-play ViTs with guaranteed equivariance that improve performance and data efficiency.", "motivation": "Existing equivariant ViTs struggle to balance performance with equivariance due to challenges in achieving holistic equivariant modifications across diverse ViT modules, particularly in harmonizing Self-Attention with Patch Embedding.", "method": "Proposes a straightforward framework that systematically renders key ViT components equivariant: patch embedding, self-attention, positional encodings, and Down/Up-Sampling, constructing ViTs with guaranteed equivariance as plug-and-play replacements.", "result": "The equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks, scaling seamlessly even to Swin Transformers.", "conclusion": "The framework provides theoretically grounded and practically versatile equivariant ViTs that serve as plug-and-play replacements, addressing the challenge of balancing performance with equivariance in vision transformers."}}
{"id": "2602.08057", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08057", "abs": "https://arxiv.org/abs/2602.08057", "authors": ["Yufei Wang", "Haixu Liu", "Tianxiang Xu", "Chuancheng Shi", "Hongsheng Xing"], "title": "Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks", "comment": null, "summary": "To tackle the automatic recognition of \"concealed emotions\" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an \"MLP-ified\" key-point backbone can match - or even surpass - GCN-based counterparts in this task.", "AI": {"tldr": "A multimodal weak-supervision framework achieves state-of-the-art concealed emotion recognition in tennis interviews by combining visual, pose, and text features with automatically generated pseudo-labels.", "motivation": "To address the challenging problem of automatically recognizing \"concealed emotions\" in videos, particularly in scenarios like tennis interviews where emotions may be intentionally hidden or subtle.", "method": "Uses YOLO for face detection, DINOv2 for visual features, Gemini 2.5 Pro with CoT+Reflection for pseudo-label generation, OpenPose for key-point sequences, simplified MLP backbone for pose modeling, and ultra-long-sequence Transformers with BERT for multimodal fusion.", "result": "Achieves over 0.69 accuracy on iMiGUE dataset (up from under 0.6), establishing new public benchmark; shows MLP-based key-point backbone can match or surpass GCN counterparts.", "conclusion": "The proposed multimodal weak-supervision framework effectively recognizes concealed emotions, with simplified MLP architecture proving competitive with more complex GCNs for pose modeling in this task."}}
{"id": "2602.08059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08059", "abs": "https://arxiv.org/abs/2602.08059", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu"], "title": "DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models", "comment": null, "summary": "The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.", "AI": {"tldr": "DICE is a training-free framework for on-the-fly artist style erasure that removes artist characteristics while preserving content, using contrastive subspace decomposition and adaptive attention decoupling.", "motivation": "Diffusion models enable effortless style mimicry without authorization, raising copyright and IP risks. Existing countermeasures require costly weight editing or explicit style specification, limiting practical deployment-side safety.", "method": "DICE uses contrastive triplets to distinguish style vs. non-style features in latent space, formalized as a generalized eigenvalue problem to identify style subspace. Includes Adaptive Attention Decoupling Editing that dynamically assesses style concentration per token and performs differential suppression on QKV vectors.", "result": "DICE achieves superior balance between thorough style erasure and content preservation, with only 3 seconds additional overhead for style disentanglement.", "conclusion": "DICE provides a practical, efficient technique for curbing style mimicry through training-free, on-the-fly artist style erasure that doesn't require explicit style specification or model retraining."}}
{"id": "2602.08068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08068", "abs": "https://arxiv.org/abs/2602.08068", "authors": ["Chunyang Li", "Yuanbo Yang", "Jiahao Shao", "Hongyu Zhou", "Katja Schwarz", "Yiyi Liao"], "title": "ReRoPE: Repurposing RoPE for Relative Camera Control", "comment": null, "summary": "Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/", "AI": {"tldr": "ReRoPE is a plug-and-play framework that injects relative camera pose information into pre-trained video diffusion models using underutilized low-frequency bands in Rotary Positional Embeddings, enabling precise camera viewpoint control without retraining or architectural changes.", "motivation": "Existing methods for controllable camera viewpoint video generation use camera poses relative to fixed references (like first frames), which lack shift-invariance and cause poor generalization and accumulated drift. Relative camera pose embeddings between arbitrary view pairs are more robust but challenging to integrate into pre-trained models without expensive retraining or architectural modifications.", "method": "ReRoPE leverages the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, especially low-frequency components. The method seamlessly injects relative camera pose information into these underutilized bands, enabling precise camera control while preserving pre-trained generative priors without requiring retraining or architectural changes.", "result": "The method achieves precise camera control in both image-to-video (I2V) and video-to-video (V2V) tasks while maintaining visual fidelity. It demonstrates training-efficient controllable video generation with accurate camera viewpoint manipulation.", "conclusion": "ReRoPE provides an effective, plug-and-play solution for integrating relative camera pose control into pre-trained video diffusion models, offering a training-efficient path toward high-fidelity, controllable video generation without compromising existing capabilities."}}
{"id": "2602.08071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08071", "abs": "https://arxiv.org/abs/2602.08071", "authors": ["Feng Wang", "Sucheng Ren", "Tiezheng Zhang", "Predrag Neskovic", "Anand Bhattad", "Cihang Xie", "Alan Yuille"], "title": "ViT-5: Vision Transformers for The Mid-2020s", "comment": "Code is available at https://github.com/wangf3014/ViT-5", "summary": "This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.", "AI": {"tldr": "ViT-5 modernizes Vision Transformers by incorporating architectural improvements from the last 5 years while keeping the core Attention-FFN structure, achieving better performance across vision tasks.", "motivation": "To systematically update Vision Transformer backbones by leveraging architectural advancements from recent years while maintaining the canonical structure, creating a more modern and effective vision backbone.", "method": "Component-wise refinement of Vision Transformers including normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens while preserving the Attention-FFN structure.", "result": "ViT-5 consistently outperforms state-of-the-art plain Vision Transformers on both understanding and generation benchmarks: 84.2% top-1 accuracy on ImageNet-1k (vs 83.8% for DeiT-III-Base) and 1.84 FID in diffusion modeling (vs 2.06 with vanilla ViT).", "conclusion": "ViT-5 offers a simple drop-in upgrade over vanilla ViT with improved performance, better representation learning, and favorable spatial reasoning, making it suitable as a modern vision backbone for mid-2020s foundation models."}}
{"id": "2602.08099", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08099", "abs": "https://arxiv.org/abs/2602.08099", "authors": ["Issar Tzachor", "Dvir Samuel", "Rami Ben-Ari"], "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval", "comment": "Project page: https://iyttor.github.io/VidVec/", "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.", "AI": {"tldr": "MLLM intermediate layers already contain strong video-text information; combining them with calibrated heads enables SOTA zero-shot video retrieval without training, further boosted by lightweight text-only alignment.", "motivation": "Current MLLM-based video embeddings underperform compared to specialized Video Foundation Models, despite MLLMs' strong multimodal capabilities. The paper aims to bridge this gap by better leveraging MLLMs for video-text retrieval.", "method": "1) Systematic layer-wise analysis revealing intermediate MLLM layers encode task-relevant information; 2) Zero-shot approach combining intermediate-layer embeddings with calibrated MLLM heads; 3) Lightweight text-based alignment mapping dense video captions to summaries for task-related embedding learning without visual supervision.", "result": "Achieves state-of-the-art results across common video retrieval benchmarks, often by substantial margins, without any fine-tuning beyond text.", "conclusion": "MLLMs' intermediate layers contain rich video-text information that can be effectively leveraged for retrieval tasks through simple calibration and text-only alignment, outperforming current methods without visual supervision."}}
{"id": "2602.08112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08112", "abs": "https://arxiv.org/abs/2602.08112", "authors": ["Sidike Paheding", "Abel Reyes-Angulo", "Leo Thomas Ramos", "Angel D. Sappa", "Rajaneesh A.", "Hiral P. B.", "Sajin Kumar K. S.", "Thomas Oommen"], "title": "MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery", "comment": null, "summary": "We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2", "AI": {"tldr": "MMLSv2 is a multimodal Martian landslide segmentation dataset with 664 images across 7 bands, plus 276 geographically isolated test images for evaluating spatial generalization.", "motivation": "To provide a comprehensive dataset for landslide segmentation on Mars that enables evaluation of model robustness and generalization beyond standard in-distribution settings, particularly for fragmented, elongated, and small-scale landslide regions.", "method": "Created MMLSv2 dataset with multimodal imagery (RGB, DEM, slope, thermal inertia, grayscale), split into training/validation/test sets, and added geographically isolated test set from disjoint region to assess spatial generalization.", "result": "Dataset supports stable training and achieves competitive performance with multiple segmentation models, but shows challenges in fragmented/elongated/small-scale regions. Isolated test set causes noticeable performance drop, highlighting its value for assessing generalization.", "conclusion": "MMLSv2 provides a valuable resource for Martian landslide segmentation research with built-in spatial generalization evaluation, revealing current model limitations in challenging landslide morphologies and cross-region performance."}}
{"id": "2602.08117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08117", "abs": "https://arxiv.org/abs/2602.08117", "authors": ["Smriti Siva", "Jan Cross-Zamirski"], "title": "Building Damage Detection using Satellite Images and Patch-Based Transformer Methods", "comment": "8 pages, 5 figures", "summary": "Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.\n  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.", "AI": {"tldr": "Small Vision Transformers with targeted patch pre-processing achieve competitive damage classification on imbalanced satellite data.", "motivation": "Rapid building damage assessment is critical for post-disaster response, but satellite imagery faces challenges with label noise and severe class imbalance in datasets like xBD.", "method": "Evaluated DINOv2-small and DeiT models with targeted patch-based pre-processing to isolate structural features and minimize background noise, using frozen-head fine-tuning strategy for computational efficiency.", "result": "Small ViT architectures with the novel training method achieve competitive macro-averaged F1 scores relative to prior CNN baselines for disaster classification.", "conclusion": "Vision Transformers with targeted pre-processing can effectively handle noisy, imbalanced satellite data for building damage assessment, offering scalable solutions for post-disaster response."}}
{"id": "2602.08126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08126", "abs": "https://arxiv.org/abs/2602.08126", "authors": ["Venkatraman Narayanan", "Bala Sai", "Rahul Ahuja", "Pratik Likhar", "Varun Ravi Kumar", "Senthil Yogamani"], "title": "MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection", "comment": null, "summary": "Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.", "AI": {"tldr": "MambaFusion: A unified multimodal 3D detection framework using selective state-space models and transformers for efficient, adaptive, physically-grounded perception in autonomous driving.", "motivation": "Existing BEV-based fusion frameworks have limitations including inefficient context modeling, spatially invariant fusion, and difficulty reasoning under uncertainty. Cameras provide dense visual cues but poor depth, while LiDAR provides precise 3D structure but sparse coverage.", "method": "Interleaves selective state-space models (SSMs) with windowed transformers for global context propagation in linear time. Uses multi-modal token alignment (MTA) module and reliability-aware fusion gates to dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Includes structure-conditioned diffusion head integrating graph-based reasoning with uncertainty-aware denoising.", "result": "Establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. Demonstrates robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.", "conclusion": "Coupling SSM-based efficiency with reliability-driven fusion yields effective multimodal 3D perception. The framework addresses key challenges in camera-LiDAR fusion including context modeling, adaptive feature weighting, and physical plausibility enforcement."}}
{"id": "2602.08131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08131", "abs": "https://arxiv.org/abs/2602.08131", "authors": ["Isaac Corley", "Hannah Kerner", "Caleb Robinson", "Jennifer Marcus"], "title": "Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries", "comment": null, "summary": "Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).", "AI": {"tldr": "FTW ecosystem provides global field boundary benchmark, pre-trained models, and tools for agricultural monitoring with field-level crop classification and forest loss analysis.", "motivation": "Field boundary maps are essential for agricultural data products but creating them at scale is challenging. The paper aims to provide a comprehensive ecosystem for field boundary extraction and agricultural monitoring.", "method": "Developed Fields of The World (FTW) ecosystem with 1.6M field polygons across 24 countries, pre-trained segmentation models, command-line inference tools, and notebooks for local/country-scale analysis using MOSAIKS random convolutional features.", "result": "Achieved macro F1 scores of 0.65-0.75 for crop type classification with limited labels, and demonstrated scalability with pre-computed predictions over 4.76M km\u00b2 across five countries with varying field sizes.", "conclusion": "FTW ecosystem enables scalable agricultural monitoring by providing benchmark data, models, and tools for field boundary extraction and crop classification, supporting applications in crop monitoring, yield estimation, and environmental analysis."}}
{"id": "2602.08136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08136", "abs": "https://arxiv.org/abs/2602.08136", "authors": ["Md Rafi Ur Rashid", "MD Sadik Hossain Shanto", "Vishnu Asutosh Dasu", "Shagufta Mehnaz"], "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks", "comment": "22 Pages, long conference paper", "summary": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.", "AI": {"tldr": "SIVA introduces split-image visual jailbreak attacks that exploit VLMs' safety alignment gap: while VLMs handle split images technically, their safety training only covers holistic images, allowing harmful semantics distributed across fragments to bypass defenses.", "motivation": "VLMs show strong robustness against traditional visual jailbreak attacks due to extensive safety alignment (RLHF), but this alignment is performed only on holistic images, creating a vulnerability where harmful semantics distributed across multiple image fragments can bypass safety mechanisms.", "method": "SIVA uses progressive split-image attacks: from naive splitting to adaptive white-box attacks, culminating in black-box transfer attacks using adversarial knowledge distillation (Adv-KD) to improve cross-model transferability by exploiting the safety alignment gap.", "result": "Evaluations on three state-of-the-art VLMs and three jailbreak datasets show SIVA achieves up to 60% higher transfer success than existing baselines, demonstrating the critical vulnerability in current VLM safety alignment.", "conclusion": "Current VLM safety alignment fails to account for split-image inputs where harmful semantics emerge only after combining fragments, creating a critical vulnerability that requires new safety approaches beyond holistic image training."}}
{"id": "2602.08168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08168", "abs": "https://arxiv.org/abs/2602.08168", "authors": ["Mei Ling Chee", "Thangarajah Akilan", "Aparna Ravindra Phalke", "Kanchan Keisham"], "title": "DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation", "comment": "13 pages", "summary": "Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.", "AI": {"tldr": "DAS-SK is a lightweight semantic segmentation model that combines selective kernel convolution with dual atrous separable convolution for efficient multi-scale feature learning in agricultural imagery, achieving SOTA performance with significantly reduced computational costs.", "motivation": "Semantic segmentation in high-resolution agricultural imagery requires balancing accuracy with computational efficiency for practical deployment on UAVs and edge devices, addressing limitations of large dataset requirements, limited spectral generalization, and high computational costs.", "method": "Proposes DAS-SK architecture that retrofits selective kernel convolution (SK-Conv) into dual atrous separable convolution (DAS-Conv) module for multi-scale feature learning, enhances ASPP module for fine-grained local and global context capture, and uses modified DeepLabV3 framework with MobileNetV3-Large and EfficientNet-B3 backbones.", "result": "DAS-SK achieves state-of-the-art performance across three benchmarks (LandCover.ai, VDD, PhenoBench) while being 21x more parameter-efficient and 19x more computationally efficient than top transformer models, demonstrating robust performance with significantly reduced GFLOPs.", "conclusion": "DAS-SK establishes itself as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains due to its computational efficiency and performance."}}
{"id": "2602.08198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08198", "abs": "https://arxiv.org/abs/2602.08198", "authors": ["Jingyu Hu", "Bin Hu", "Ka-Hei Hui", "Haipeng Li", "Zhengzhe Liu", "Daniel Cohen-Or", "Chi-Wing Fu"], "title": "PEGAsus: 3D Personalization of Geometry and Appearance", "comment": null, "summary": "We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.", "AI": {"tldr": "PEGAsus is a framework for personalized 3D shape generation that learns reusable geometric and appearance attributes from reference shapes and composes them with text prompts.", "motivation": "To enable fine-grained control over 3D shape generation by extracting reusable, category-agnostic geometric and appearance attributes from reference shapes for personalized shape creation.", "method": "Formulates 3D personalization as attribute extraction and composition, uses progressive optimization to learn geometry and appearance concepts separately, and extends to region-wise concept learning with context-aware and context-free losses.", "result": "Effectively extracts attributes from diverse reference shapes and flexibly composes them with text to synthesize new shapes, outperforming state-of-the-art methods in both quantitative and qualitative evaluations.", "conclusion": "PEGAsus enables fine-grained control over shape generation and supports diverse, personalized results even in challenging cross-category scenarios, demonstrating superior performance over existing solutions."}}
{"id": "2602.08202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08202", "abs": "https://arxiv.org/abs/2602.08202", "authors": ["Jinrong Lv", "Xun Gong", "Zhaohuan Li", "Weili Jiang"], "title": "Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video", "comment": "11 pages, 5 tables, 10 figures. Under peer review", "summary": "Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.", "AI": {"tldr": "MCSDR: A multimodal conditional score-based diffusion model for probabilistic LVEF estimation from echocardiograms, addressing ambiguity in medical imaging through generative regression instead of deterministic regression.", "motivation": "LVEF estimation from echocardiograms is an ill-posed inverse problem with inherent noise, artifacts, and limited viewing angles that create ambiguity. Standard regression approaches using MSE force models to learn conditional expectations, which can be misleading when posterior distributions are multimodal or heavy-tailed, especially in pathological cases.", "method": "Proposes Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework that models the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Uses generative regression instead of deterministic regression.", "result": "Achieves state-of-the-art performance on EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets. Qualitative analysis shows generation trajectories exhibit distinct behaviors in cases with high noise or significant physiological variability, providing novel interpretability for AI-aided diagnosis.", "conclusion": "The paradigm shift from deterministic regression to generative regression via MCSDR effectively addresses ambiguity in LVEF estimation, offering both improved performance and enhanced interpretability through generation trajectories that reveal model uncertainty in challenging cases."}}
{"id": "2602.08206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08206", "abs": "https://arxiv.org/abs/2602.08206", "authors": ["Chufeng Zhou", "Jian Wang", "Xinyuan Liu", "Xiaokang Zhang"], "title": "Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation", "comment": "5 pages, 3 figures", "summary": "Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based\" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.", "AI": {"tldr": "GR-CoT framework enhances open-vocabulary semantic segmentation in remote sensing by adding geospatial reasoning to resolve ambiguity between similar land-cover types.", "motivation": "Existing open-vocabulary segmentation methods rely on passive visual-text mapping, lacking geospatial context awareness, which causes semantic ambiguity when land-cover classes have similar spectral features but different semantics.", "method": "Proposes Geospatial Reasoning Chain-of-Thought (GR-CoT) framework with two components: offline knowledge distillation stream for fine-grained category interpretation standards, and online instance reasoning stream with sequential reasoning (macro-scenario anchoring, visual feature decoupling, knowledge-driven decision synthesis) to generate image-adaptive vocabulary.", "result": "Extensive experiments on LoveDA and GID5 benchmarks demonstrate superiority of the approach.", "conclusion": "GR-CoT framework enhances MLLMs' scene understanding for open-vocabulary segmentation, enabling precise mapping by resolving semantic ambiguity through geospatial reasoning."}}
{"id": "2602.08211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08211", "abs": "https://arxiv.org/abs/2602.08211", "authors": ["Yik Lung Pang", "Changjae Oh"], "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension", "comment": "4 pages, 5 figures, 2 tables", "summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.", "AI": {"tldr": "Training-free Chain-of-Caption framework improves referring expression comprehension in MLLMs by 5-30% using combined visual/textual contexts without fine-tuning.", "motivation": "While MLLMs achieve high REC accuracy through scaling, performance can be further improved using techniques like Chain-of-Thought and tool use that provide additional context. The paper aims to analyze how different visual/textual context techniques affect REC performance and develop a training-free solution.", "method": "Proposes Chain-of-Caption framework that combines multiple visual and textual contexts via tool use without requiring fine-tuning. Analyzes effects of individual and combined context provision techniques on MLLMs for REC tasks.", "result": "Individual textual or visual context improves REC performance without fine-tuning. Combined contexts achieve 5-30% performance gain over baseline on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets across various IoU thresholds.", "conclusion": "Chain-of-Caption provides an effective training-free approach to enhance MLLM performance on referring expression comprehension by strategically combining multiple visual and textual contexts, demonstrating significant improvements over baseline models."}}
{"id": "2602.08224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08224", "abs": "https://arxiv.org/abs/2602.08224", "authors": ["Jing Zhang", "Zhikai Li", "Xuewen Liu", "Qingyi Gu"], "title": "Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval", "comment": "ICLR 2026,Code is available at: https://github.com/jingjing0419/Efficient-SAM2", "summary": "Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.", "AI": {"tldr": "Efficient-SAM2 accelerates SAM2 for video object segmentation by exploiting sparse perception patterns, achieving 1.68x speedup with minimal accuracy loss.", "motivation": "SAM2 has excellent video segmentation performance but heavy computational burden hinders real-time applications. Existing efficiency improvements focus on retraining lightweight backbones, with little exploration of post-training acceleration.", "method": "Proposes Efficient-SAM2 with two key techniques: 1) Object-aware Sparse Window Routing (SWR) for image encoder - window-level computation allocation that routes background regions to lightweight shortcut branch using consistency and saliency cues. 2) Object-aware Sparse Memory Retrieval (SMR) for memory attention - only salient memory tokens participate in computation, with saliency pattern reused from first recollection.", "result": "Achieves 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set. Uses negligible additional parameters and minimal training overhead.", "conclusion": "Efficient-SAM2 successfully accelerates SAM2 by adaptively focusing on object regions while eliminating task-irrelevant computations, demonstrating that exploiting sparse perception patterns can significantly improve inference efficiency without substantial accuracy loss."}}
{"id": "2602.08230", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08230", "abs": "https://arxiv.org/abs/2602.08230", "authors": ["Hongwei Ren", "Youxin Jiang", "Qifei Gu", "Xiangqian Wu"], "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework", "comment": null, "summary": "Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \\textbf{M}otion-\\textbf{A}ware \\textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.", "AI": {"tldr": "MA-ADV is the first adversarial attack framework for event cameras using point cloud representations, achieving 100% attack success with minimal perturbations while being robust against defenses.", "motivation": "Event cameras are used in safety-critical applications like autonomous driving, but deep neural networks for event processing are vulnerable to adversarial attacks. However, research on adversarial attacks for events is scarce due to the non-differentiable nature of mainstream event representations, preventing gradient-based attack methods.", "method": "MA-ADV uses point cloud representations for events, accounts for high-frequency noise with diffusion-based smoothing, leverages spatial-temporal relationships, and identifies minimal-cost perturbations through sample-wise Adam optimization, iterative refinement, and binary search.", "result": "Extensive experiments show MA-ADV achieves 100% attack success rate with minimal perturbation cost and demonstrates enhanced robustness against defenses.", "conclusion": "MA-ADV successfully addresses the challenge of adversarial attacks on event-based systems by overcoming the non-differentiable representation problem, revealing critical security vulnerabilities in future event-based perception systems."}}
{"id": "2602.08236", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08236", "abs": "https://arxiv.org/abs/2602.08236", "authors": ["Shoubin Yu", "Yue Zhang", "Zun Wang", "Jaehong Yoon", "Huaxiu Yao", "Mingyu Ding", "Mohit Bansal"], "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning", "comment": "the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/", "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.", "AI": {"tldr": "AVIC: Adaptive visual imagination control framework that selectively invokes world models for spatial reasoning, outperforming fixed imagination strategies with fewer computations.", "motivation": "Current MLLMs struggle with visual spatial reasoning when answers depend on unseen viewpoints. While world models can help with visual imagination, indiscriminate use increases computation and can degrade performance by introducing misleading evidence. Need to understand when imagination is necessary, beneficial, or harmful.", "method": "Introduces AVIC, an adaptive test-time framework that explicitly reasons about sufficiency of current visual evidence before selectively invoking and scaling visual imagination using world models. Analyzes when static evidence is sufficient vs when imagination improves reasoning.", "result": "Across spatial reasoning benchmarks (SAT, MMSI) and embodied navigation (R2R), AVIC reveals clear scenarios where imagination is critical, marginal, or detrimental. Selective control matches or outperforms fixed imagination strategies with substantially fewer world-model calls and language tokens.", "conclusion": "Test-time imagination should be analyzed and controlled for efficient and reliable spatial reasoning. Selective imagination based on evidence sufficiency enables better performance with fewer computational resources."}}
{"id": "2602.08262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08262", "abs": "https://arxiv.org/abs/2602.08262", "authors": ["Guoqi Yu", "Xiaowei Hu", "Angelica I. Aviles-Rivero", "Anqi Qiu", "Shujun Wang"], "title": "Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification", "comment": "This paper has been accepted by IEEE Transactions on Medical Imaging", "summary": "Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.", "AI": {"tldr": "DeCI framework outperforms traditional fMRI analysis by decomposing BOLD signals into cycle and drift components and modeling ROIs independently, achieving better brain disorder classification.", "motivation": "Current fMRI analysis methods rely on functional connectivity via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear relationships between brain regions.", "method": "DeCI framework with two key principles: (1) Cycle and Drift Decomposition to disentangle oscillatory fluctuations and slow baseline trends within each ROI, and (2) Channel-Independence to model each ROI separately for improved robustness and reduced overfitting.", "result": "DeCI achieves superior classification accuracy and generalization compared to both traditional FC-based approaches and state-of-the-art temporal models across five public datasets.", "conclusion": "The findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics, with DeCI demonstrating the value of directly modeling temporal information."}}
{"id": "2602.08277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08277", "abs": "https://arxiv.org/abs/2602.08277", "authors": ["Xiangbo Gao", "Renjie Li", "Xinghao Chen", "Yuheng Wu", "Suofei Feng", "Qing Yin", "Zhengzhong Tu"], "title": "PISCO: Precise Video Instance Insertion with Sparse Control", "comment": null, "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.", "AI": {"tldr": "PISCO is a video diffusion model for precise video instance insertion with sparse keyframe control, addressing distribution shifts in pretrained models and outperforming existing baselines.", "motivation": "The paper addresses the need for fine-grained, controllable video generation in professional AI-assisted filmmaking, moving beyond general generation that requires extensive prompt engineering. Specifically, it focuses on video instance insertion - inserting specific instances into existing footage while maintaining scene integrity, which requires precise spatial-temporal placement, physically consistent interactions, and preservation of original dynamics with minimal user effort.", "method": "PISCO is a video diffusion model that allows arbitrary sparse keyframe control (single keyframe, start-end keyframes, or sparse keyframes). It introduces Variable-Information Guidance for robust conditioning, Distribution-Preserving Temporal Masking to stabilize temporal generation, and geometry-aware conditioning for realistic scene adaptation to address distribution shifts in pretrained models.", "result": "The authors construct PISCO-Bench, a benchmark with verified instance annotations and clean background videos. Experiments show PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, with clear monotonic performance improvements as additional control signals are provided.", "conclusion": "PISCO represents a significant advancement in controllable video generation for professional filmmaking applications, enabling precise video instance insertion with minimal user effort through sparse keyframe control while maintaining scene integrity and physical consistency."}}
{"id": "2602.08282", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08282", "abs": "https://arxiv.org/abs/2602.08282", "authors": ["Haixu Liu", "Yufei Wang", "Tianxiang Xu", "Chuancheng Shi", "Hongsheng Xing"], "title": "Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning", "comment": null, "summary": "Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.", "AI": {"tldr": "Multimodal fusion framework combining PA and PO plant data with geographic alignment and mixture-of-experts approach for cross-species distribution prediction under data sparsity and distribution shifts.", "motivation": "Large-scale cross-species plant distribution prediction faces challenges due to sparse, biased observational data. PA data are accurate but limited, while PO data have broad coverage but severe label noise in negative samples.", "method": "Multimodal fusion framework with: 1) Pseudo-label aggregation for PO data using geographic coverage of satellite imagery, 2) Swin Transformer for satellite imagery, TabM for tabular features, Temporal Swin Transformer for time-series, and serial tri-modal cross-attention for fusion, 3) Mixture-of-experts approach partitioning test samples by spatial proximity to PA samples.", "result": "Superior predictive performance on GeoLifeCLEF 2025 dataset in scenarios with limited PA coverage and pronounced distribution shifts.", "conclusion": "The proposed framework effectively addresses real-world constraints of plant distribution prediction by leveraging complementary strengths of PA and PO data while mitigating label noise and distribution shift issues."}}
{"id": "2602.08309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08309", "abs": "https://arxiv.org/abs/2602.08309", "authors": ["Yunzuo Hu", "Wen Li", "Jing Zhang"], "title": "CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment", "comment": "13 pages, 8 figures", "summary": "Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.", "AI": {"tldr": "CAE-AV framework uses cross-modal agreement and caption alignment to address audio-visual misalignment, achieving SOTA results on multiple benchmarks.", "motivation": "Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, leading to unstable training and degraded representation quality.", "method": "Proposed CAE-AV framework with two modules: CASTE (Cross-modal Agreement-guided Spatio-Temporal Enrichment) that balances spatial/temporal relations using frame-level audio-visual agreement, and CASE (Caption-Aligned Saliency-guided Enrichment) that injects cross-modal semantic guidance. Also includes lightweight objectives: caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization.", "result": "Achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks with frozen backbones. Qualitative analyses validate robustness against audio-visual misalignment.", "conclusion": "CAE-AV effectively addresses audio-visual misalignment through complementary agreement-guided and caption-aligned enrichment modules, improving representation quality and achieving superior performance across multiple audio-visual tasks."}}
{"id": "2602.08337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08337", "abs": "https://arxiv.org/abs/2602.08337", "authors": ["Sheng Yan", "Yong Wang", "Xin Du", "Junsong Yuan", "Mengyuan Liu"], "title": "Language-Guided Transformer Tokenizer for Human Motion Generation", "comment": null, "summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.", "AI": {"tldr": "LG-Tok introduces language-guided motion tokenization using Transformers to create compact semantic representations, improving both reconstruction quality and generation efficiency while outperforming state-of-the-art methods.", "motivation": "Existing motion tokenization methods face a trade-off: more tokens improve reconstruction but make generation harder. Current convolutional tokenizers can't effectively incorporate global language guidance, limiting semantic representation quality.", "method": "Proposes Language-Guided Tokenization (LG-Tok) with a Transformer-based Tokenizer using attention mechanisms for language-motion alignment. Includes language-drop training scheme for both language-guided and language-free generation support.", "result": "Achieves Top-1 scores of 0.542 (HumanML3D) and 0.582 (Motion-X), outperforming MARDM (0.500/0.528). FID scores of 0.057/0.088 vs 0.114/0.147. LG-Tok-mini with half tokens maintains competitive performance (0.521/0.588 Top-1, 0.085/0.071 FID).", "conclusion": "LG-Tok enables efficient motion tokenization through language alignment, producing compact semantic representations that improve both reconstruction quality and generation performance while reducing model complexity."}}
{"id": "2602.08342", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08342", "abs": "https://arxiv.org/abs/2602.08342", "authors": ["Jie Zhang", "Xingtong Yu", "Yuan Fang", "Rudi Stouffs", "Zdravko Trivic"], "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science", "comment": null, "summary": "Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.", "AI": {"tldr": "UGData dataset anchors street-view images to spatial graphs with graph-aligned supervision, UGE training strategy aligns images/text/spatial structures, UGBench evaluates spatially grounded embeddings for urban tasks, achieving significant improvements in retrieval and geolocation.", "motivation": "Urban understanding is inherently spatial, but existing datasets lack explicit alignment between street-view images and urban structure, making learning transferable multimodal embeddings challenging.", "method": "Introduce UGData dataset with spatial graphs and graph-aligned supervision; propose UGE two-stage training combining instruction-guided contrastive learning with graph-based spatial encoding; develop UGBench benchmark; train on multiple VLM backbones with LoRA tuning.", "result": "UGE built on Qwen2.5-VL-7B achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities.", "conclusion": "Explicit spatial grounding is effective for spatially intensive urban tasks, demonstrating the value of structured spatial alignment in multimodal embeddings for urban environments."}}
{"id": "2602.08346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08346", "abs": "https://arxiv.org/abs/2602.08346", "authors": ["Yujin Zhou", "Pengcheng Wen", "Jiale Chen", "Boqin Yin", "Han Zhu", "Jiaming Ji", "Juntao Dai", "Chi-Min Chan", "Sirui Han"], "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning", "comment": null, "summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.", "AI": {"tldr": "First comprehensive benchmark for Process Reward Models (PRMs) in the \"thinking with images\" paradigm, revealing current LVLMs' limitations in evaluating visual reasoning processes.", "motivation": "The \"thinking with images\" paradigm in LVLMs introduces diverse reasoning errors, but existing PRM benchmarks are text-centric and lack comprehensive assessment for visual reasoning processes.", "method": "1) Analyzed reasoning trajectories and conducted guided search experiments to define 7 error types; 2) Constructed benchmark with 1,206 manually annotated reasoning trajectories across 4 categories and 16 subcategories; 3) Experimental analysis of current LVLMs as PRMs.", "result": "Current LVLMs are ineffective as PRMs, showing limited visual reasoning evaluation capabilities, performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions.", "conclusion": "The benchmark effectively reveals PRM limitations in LVLMs and establishes crucial foundations for advancing PRMs in visual reasoning, demonstrating the need for specialized PRMs in the thinking with images paradigm."}}
{"id": "2602.08355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08355", "abs": "https://arxiv.org/abs/2602.08355", "authors": ["Xianjie Liu", "Yiman Hu", "Liang Wu", "Ping Hu", "Yixiong Zou", "Jian Xu", "Bo Zheng"], "title": "E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs", "comment": null, "summary": "E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \\textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \\textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \\textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \\textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.", "AI": {"tldr": "E-commerce short videos are complex with dense multi-modal signals, requiring specialized benchmarks and models for commercial intent reasoning. The paper introduces E-VAds benchmark and E-VAds-R1 model with MG-GRPO reward design.", "motivation": "Current video understanding models struggle with e-commerce short videos because existing benchmarks focus on general-purpose tasks and neglect commercial intent reasoning. E-commerce videos have higher multi-modal density than mainstream datasets, creating a more challenging frontier.", "method": "1) Proposed multi-modal information density assessment framework to quantify e-commerce video complexity. 2) Created E-VAds benchmark with 3,961 Taobao videos and 19,785 Q&A pairs organized into Perception and Cognition/Reasoning dimensions. 3) Developed E-VAds-R1 model using RL with MG-GRPO (multi-grained reward design) for smooth early exploration and non-linear expert precision incentives.", "result": "E-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets. E-VAds-R1 achieves 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.", "conclusion": "The work establishes e-commerce video understanding as a distinct, challenging domain requiring specialized benchmarks and models. The proposed E-VAds benchmark and E-VAds-R1 model with MG-GRPO reward design effectively address commercial intent reasoning in dense multi-modal e-commerce videos."}}
{"id": "2602.08388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08388", "abs": "https://arxiv.org/abs/2602.08388", "authors": ["Shuo Zhang", "Wenzhuo Wu", "Huayu Zhang", "Jiarong Cheng", "Xianghao Zang", "Chao Ban", "Hao Sun", "Zhongjiang He", "Tianwei Cao", "Kongming Liang", "Zhanyu Ma"], "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers", "comment": null, "summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.", "AI": {"tldr": "GeoEdit: A diffusion-based framework for precise geometric editing (translation, rotation, scaling) with realistic lighting/shadow effects using in-context generation and Effects-Sensitive Attention.", "motivation": "Existing diffusion models struggle with accurate geometric transformations (translation, rotation, scaling) in complex scenes and fail to model intricate lighting/shadow effects realistically.", "method": "Proposes GeoEdit framework with: 1) diffusion transformer module for in-context generation with geometric transformations, 2) Effects-Sensitive Attention for realistic lighting/shadow modeling, and 3) RS-Objects dataset (120K+ image pairs) for training.", "result": "Outperforms state-of-the-art methods on public benchmarks in visual quality, geometric accuracy, and realism.", "conclusion": "GeoEdit effectively addresses geometric editing challenges in diffusion models while improving realism through better lighting/shadow modeling."}}
{"id": "2602.08395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08395", "abs": "https://arxiv.org/abs/2602.08395", "authors": ["Jianfeng Liang", "Shaocheng Shen", "Botao Xu", "Qiang Hu", "Xiaoyun Zhang"], "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy", "comment": null, "summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}", "AI": {"tldr": "D\u00b2-VR: A single-image diffusion-based video restoration framework with low-step inference that achieves 12\u00d7 faster sampling while maintaining state-of-the-art performance.", "motivation": "Existing diffusion-based video restoration methods suffer from prohibitive inference latency and temporal instability when handling complex real-world degradations, limiting their practical deployment.", "method": "1) Degradation-Robust Flow Alignment (DRFA) module with confidence-aware attention to filter unreliable motion cues; 2) Adversarial distillation to compress diffusion sampling into few-step regime; 3) Synergistic optimization strategy to balance perceptual quality and temporal consistency.", "result": "Achieves state-of-the-art performance while accelerating sampling process by 12\u00d7 compared to previous methods.", "conclusion": "D\u00b2-VR successfully addresses the latency and stability limitations of diffusion-based video restoration, enabling practical deployment with fast inference while maintaining high perceptual quality and temporal consistency."}}
{"id": "2602.08397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08397", "abs": "https://arxiv.org/abs/2602.08397", "authors": ["Chiara Lena", "Davide Milesi", "Alessandro Casella", "Luca Carlini", "Joseph C. Norton", "James Martin", "Bruno Scaglioni", "Keith L. Obstein", "Roberto De Sire", "Marco Spadaccini", "Cesare Hassan", "Pietro Valdastri", "Elena De Momi"], "title": "RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications", "comment": null, "summary": "Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.", "AI": {"tldr": "RealSynCol is a highly realistic synthetic colonoscopy dataset that addresses the scarcity of ground truth data for 3D reconstruction and colon exploration algorithms.", "motivation": "Deep learning can improve colonoscopy through 3D reconstruction and lesion identification, but development is limited by the scarcity of large-scale ground truth data from real colonoscopies.", "method": "Created RealSynCol by extracting colon geometries from 10 CT scans, importing them into a virtual environment mimicking intraoperative conditions, and rendering with realistic vascular textures. Produced 28,130 frames with paired ground truth depth maps, optical flow, 3D meshes, and camera trajectories.", "result": "Benchmark study showed RealSynCol's high realism and variability significantly enhance generalization performance on clinical images compared to other synthetic colon datasets for depth and pose estimation tasks.", "conclusion": "RealSynCol is a powerful synthetic dataset that effectively supports the development of deep learning algorithms for endoscopic diagnosis by providing realistic training data that generalizes well to clinical settings."}}
{"id": "2602.08430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08430", "abs": "https://arxiv.org/abs/2602.08430", "authors": ["Qiang Wang"], "title": "Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features", "comment": null, "summary": "We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.", "AI": {"tldr": "The paper identifies overlooked design choices in attention-based sparse image matching, finds detectors (not descriptors) are the primary performance factor, and proposes a universal detector-agnostic model that achieves zero-shot matching accuracy comparable to specialized models.", "motivation": "To improve transformer-based sparse image matching models by addressing previously overlooked design choices, understanding the relative importance of detectors vs. descriptors, and creating a universal model that works across different detectors without retraining.", "method": "1) Identified critical design choices in LightGlue model, 2) Investigated role of detectors vs. descriptors in transformer matching framework, 3) Proposed fine-tuning approach using keypoints from diverse detectors to create universal detector-agnostic model.", "result": "The universal model achieves or exceeds accuracy of models specifically trained for individual features when used as zero-shot matcher for novel detectors, demonstrating detector-agnostic capability.", "conclusion": "Detectors are more critical than descriptors for performance in transformer-based matching, and universal detector-agnostic models can match specialized models' accuracy, offering insights for deployment and future local feature design."}}
{"id": "2602.08439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08439", "abs": "https://arxiv.org/abs/2602.08439", "authors": ["Yuhao Dong", "Shulin Tian", "Shuai Liu", "Shuangrui Ding", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Jiaqi Wang", "Ziwei Liu"], "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "comment": null, "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "AI": {"tldr": "The paper introduces Demo-driven Video In-Context Learning, a new task for MLLMs to learn from video demonstrations, along with a benchmark (Demo-ICL-Bench) and a model (Demo-ICL) with two-stage training.", "motivation": "Existing video benchmarks assess MLLMs' static knowledge rather than their ability to learn and adapt from dynamic, novel contexts through few-shot examples. There's a gap in evaluating video in-context learning capabilities.", "method": "1) Proposes Demo-ICL-Bench benchmark with 1200 instructional YouTube videos and two demonstration types (text summaries and video demonstrations). 2) Develops Demo-ICL model with two-stage training: video-supervised fine-tuning and information-assisted direct preference optimization.", "result": "Extensive experiments show Demo-ICL-Bench is challenging for state-of-the-art MLLMs, and Demo-ICL demonstrates effectiveness in video in-context learning, revealing future research directions.", "conclusion": "The paper successfully bridges the gap in evaluating video in-context learning, introduces a novel task and benchmark, and presents an effective model that advances video understanding capabilities of MLLMs."}}
{"id": "2602.08448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08448", "abs": "https://arxiv.org/abs/2602.08448", "authors": ["Haocheng Lu", "Nan Zhang", "Wei Tao", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries", "comment": "Accepted to AAAI 2026 (Main Technical Track)", "summary": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.", "AI": {"tldr": "Vista is a scene-aware framework for streaming video QA that dynamically segments video streams into coherent scenes, compresses them into compact tokens for GPU storage, and selectively recalls relevant scenes for query answering, achieving state-of-the-art performance on StreamingBench.", "motivation": "Streaming Video QA presents unique challenges as video frames arrive sequentially and queries can occur at any time. Existing approaches with fixed-size memory or naive compression suffer from context loss or memory overflow, limiting effectiveness in long-form, real-time scenarios.", "method": "Vista introduces three innovations: (1) scene-aware segmentation that dynamically clusters incoming frames into temporally and visually coherent scenes; (2) scene-aware compression where each scene is compressed into compact tokens stored in GPU memory while full-resolution frames are offloaded to CPU; (3) scene-aware recall that selectively retrieves relevant scenes upon query arrival for reintegration into model input.", "result": "Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding while being model-agnostic and integrating with various vision-language backbones.", "conclusion": "Vista provides an efficient and scalable framework for streaming video QA that enables long-context reasoning without compromising latency or memory efficiency, addressing key limitations of existing approaches through scene-aware segmentation, compression, and recall mechanisms."}}
{"id": "2602.08462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08462", "abs": "https://arxiv.org/abs/2602.08462", "authors": ["Yiyang Cao", "Yunze Deng", "Ziyu Lin", "Bin Feng", "Xinggang Wang", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "comment": null, "summary": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.", "AI": {"tldr": "TriC-Motion: A diffusion-based text-to-motion framework that jointly optimizes spatial, temporal, and frequency domains with causal intervention to eliminate noise and generate high-fidelity, text-aligned motion sequences.", "motivation": "Current text-to-motion methods lack unified joint optimization across spatial, temporal, and frequency domains, leading to suboptimal generation quality. Motion-irrelevant noise cues are entangled with beneficial features, causing motion distortion.", "method": "Proposes Tri-Domain Causal Text-to-Motion Generation (TriC-Motion) with three domain-specific modules: Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. Uses Score-guided Tri-domain Fusion to integrate information and Causality-based Counterfactual Motion Disentangler to eliminate noise.", "result": "Achieves state-of-the-art performance with R@1 of 0.612 on HumanML3D dataset, demonstrating superior ability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences.", "conclusion": "TriC-Motion successfully addresses limitations of existing methods by providing unified tri-domain optimization with causal intervention, resulting in significantly improved motion generation quality across multiple metrics."}}
{"id": "2602.08479", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08479", "abs": "https://arxiv.org/abs/2602.08479", "authors": ["Alif Rizqullah Mahdi", "Mahdi Rezaei", "Natasha Merat"], "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation", "comment": "9th International Conference on Instrumentation, Control, and Automation (ICA)", "summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.", "AI": {"tldr": "A gesture classification framework using 2D pose estimation achieves 87% accuracy in recognizing pedestrian gestures (Stop, Go, Thank & Greet, No Gesture) for autonomous vehicle interactions.", "motivation": "Autonomous vehicles struggle to interpret pedestrian gestures that are crucial for non-verbal communication in traffic when formal rules are insufficient, creating a need for better gesture recognition systems.", "method": "Used 2D pose estimation on real-world video sequences from WIVW dataset, categorized gestures into four classes, and extracted 76 static and dynamic features from normalized keypoints.", "result": "Achieved 87% classification accuracy, with hand position and movement velocity identified as particularly discriminative features for distinguishing between gesture classes.", "conclusion": "The framework improves autonomous vehicle perceptual capabilities and contributes to understanding pedestrian behavior in traffic contexts through effective gesture interpretation."}}
{"id": "2602.08491", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08491", "abs": "https://arxiv.org/abs/2602.08491", "authors": ["Keonvin Park", "Aditya Pal", "Jin Hong Mok"], "title": "Enhanced Food Category Recognition under Illumination-Induced Domain Shift", "comment": null, "summary": "Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.\n  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.\n  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.", "AI": {"tldr": "This paper investigates illumination-induced domain shift in multi-class food recognition, showing accuracy degradation in cross-dataset evaluation and proposing illumination-augmented datasets to improve robustness.", "motivation": "Real-world food recognition systems (like conveyor-belt inspection) are sensitive to illumination changes, but existing research is limited to single categories or controlled settings, and most food datasets lack explicit illumination annotations.", "method": "Used Food-101 and Fruits-360 datasets, demonstrated cross-dataset accuracy degradation, constructed synthetic illumination-augmented datasets by varying light temperature and intensity, and evaluated cross-dataset transfer learning and domain generalization.", "result": "Illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance, with particular focus on illumination-sensitive categories like apple-based classes.", "conclusion": "Illumination robustness is crucial for deploying reliable food recognition systems in real-world inspection scenarios, and illumination-aware augmentation provides practical solutions for domain shift challenges."}}
{"id": "2602.08503", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08503", "abs": "https://arxiv.org/abs/2602.08503", "authors": ["Yi Ding", "Ziliang Qiu", "Bolian Li", "Ruqi Zhang"], "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation", "comment": "17 pages", "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.", "AI": {"tldr": "Octopus is an RL framework that synthesizes dense self-correction examples by recombining existing rollouts, enabling efficient learning of self-correction in vision-language models.", "motivation": "Existing RL methods struggle to learn self-correction in VLMs because effective self-correction behaviors emerge rarely, creating extremely sparse learning signals that make training difficult.", "method": "Proposes correction-specific rollouts (Octopus) - an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. Also introduces response-masking strategy to decouple self-correction from direct reasoning.", "result": "Octopus-8B achieves state-of-the-art performance across 7 benchmarks among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72\u00d7 training time per step.", "conclusion": "The Octopus framework effectively addresses the sparse learning signal problem in self-correction for VLMs through rollout augmentation and response-masking, enabling efficient learning of controllable self-correction capabilities."}}
{"id": "2602.08505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08505", "abs": "https://arxiv.org/abs/2602.08505", "authors": ["Caterina Fuster-Barcel\u00f3", "Virginie Uhlmann"], "title": "Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?", "comment": null, "summary": "Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fr\u00e9chet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.", "AI": {"tldr": "Vision foundation models can achieve good mitochondria segmentation within single EM datasets with lightweight adaptation, but fail to generalize across heterogeneous EM datasets despite visual similarity, revealing persistent domain mismatch.", "motivation": "To understand whether vision foundation models provide general enough latent representations to support effective transfer and reuse across heterogeneous microscopy image datasets, specifically for mitochondria segmentation in electron microscopy.", "method": "Evaluated three VFMs (DINOv2, DINOv3, OpenCLIP) on two EM datasets (Lucchi++, VNC) using two adaptation regimes: frozen-backbone with lightweight segmentation head, and parameter-efficient fine-tuning via LoRA. Analyzed latent representations using PCA, Fr\u00e9chet Dinov2 distance, and linear probes.", "result": "Single-dataset training yields good segmentation performance with LoRA improving in-domain results. Multi-dataset training causes severe performance degradation for all models, with minimal gains from PEFT. Analysis reveals pronounced domain mismatch between EM datasets despite visual similarity.", "conclusion": "VFMs deliver competitive EM segmentation within single domains with lightweight adaptation, but current PEFT strategies are insufficient for robust cross-dataset performance without additional domain-alignment mechanisms due to persistent domain mismatch."}}
{"id": "2602.08524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08524", "abs": "https://arxiv.org/abs/2602.08524", "authors": ["Linger Deng", "Yuliang Liu", "Wenwen Yu", "Zujia Zhang", "Jianzhong Ju", "Zhenbo Luo", "Xiang Bai"], "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving", "comment": null, "summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus", "AI": {"tldr": "GeoFocus is a novel framework that improves geometry problem-solving in Large Multimodal Models by focusing on critical local structures and using a compact topology formal language, achieving 4.7% accuracy improvement over leading specialized models.", "motivation": "Geometry problem-solving is challenging for LMMs because it requires both global shape recognition and attention to intricate local relationships related to geometric theory. Current methods lack sufficient focus on critical local structures and use inefficient global encodings.", "method": "GeoFocus has two core modules: 1) Critical Local Perceptor - automatically identifies and emphasizes critical local structures (angles, parallel lines, distances) using 13 theory-based perception templates; 2) VertexLang - a compact topology formal language that encodes global figures through vertex coordinates and connectivity relations, replacing bulky code-based encodings.", "result": "The framework boosts critical local feature coverage by 61% compared to previous methods, reduces global perception training time by 20%, improves topology recognition accuracy, and achieves 4.7% accuracy improvement over leading specialized models on Geo3K, GeoQA, and FormalGeo7K datasets. It also demonstrates superior robustness in MATHVERSE under diverse visual conditions.", "conclusion": "GeoFocus effectively addresses geometry problem-solving challenges in LMMs by combining focused local perception with efficient global topology encoding, resulting in significant performance improvements and robustness across multiple geometry datasets."}}
{"id": "2602.08528", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08528", "abs": "https://arxiv.org/abs/2602.08528", "authors": ["Chuyang Wu", "Samuli Siltanen"], "title": "Automatic regularization parameter choice for tomography using a double model approach", "comment": null, "summary": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.", "AI": {"tldr": "Automatic regularization parameter selection for X-ray tomography using dual-grid feedback control", "motivation": "X-ray tomography is ill-posed with limited data, requiring regularization. Traditional methods need manual parameter tuning, which is subjective and time-consuming.", "method": "Uses two distinct computational discretizations of the same problem with a feedback control algorithm that dynamically adjusts regularization strength to find the smallest parameter yielding sufficient similarity between reconstructions on both grids.", "result": "Demonstrated effectiveness using real tomographic data, showing the method can automatically select appropriate regularization parameters.", "conclusion": "The dual-grid feedback control approach provides an automatic, objective method for regularization parameter selection in X-ray tomography, addressing the ill-posed nature of limited-data reconstruction problems."}}
{"id": "2602.08531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08531", "abs": "https://arxiv.org/abs/2602.08531", "authors": ["Anastasiia Kornilova", "Ivan Moskalenko", "Arabella Gromova", "Gonzalo Ferrer", "Alexander Menshchikov"], "title": "Thegra: Graph-based SLAM for Thermal Imagery", "comment": null, "summary": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.", "AI": {"tldr": "Thermal SLAM system using learned features (SuperPoint/LightGlue) from visible-spectrum data with preprocessing and confidence-weighted graph optimization for robust performance in degraded visual environments.", "motivation": "Thermal imaging works in visually degraded environments (low light, smoke, weather) but thermal images have low texture, low contrast, and high noise, making feature-based SLAM challenging.", "method": "Sparse monocular graph-based SLAM using SuperPoint detector and LightGlue matcher (trained on visible-spectrum data) with preprocessing pipeline for thermal data adaptation, modified SLAM modules for sparse/outlier-prone matches, and confidence-weighted factor graph using SuperPoint keypoint scores.", "result": "System achieves reliable performance on public thermal datasets without requiring dataset-specific training or fine-tuning, addressing the scarcity of quality thermal data.", "conclusion": "Proposed thermal SLAM system successfully leverages cross-domain learned features with adaptations for thermal imagery, providing robust performance in visually degraded environments without needing thermal-specific training data."}}
{"id": "2602.08540", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08540", "abs": "https://arxiv.org/abs/2602.08540", "authors": ["He Wu", "Xia Yan", "Yanghui Xu", "Liegang Xia", "Jiazhou Chen"], "title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation", "comment": "13 pages, 6 figures, 4 tables", "summary": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.", "AI": {"tldr": "TIBR4D: A learning-free 4D Gaussian segmentation framework that uses two-stage iterative boundary refinement to lift video masks to 4D spaces, producing accurate object Gaussian point clouds with clear boundaries.", "motivation": "Object-level segmentation in dynamic 4D Gaussian scenes is challenging due to complex motion, occlusions, and ambiguous boundaries. Existing methods struggle with these issues, particularly with one-shot threshold-based approaches that don't handle occlusions well or preserve object structure completeness.", "method": "Two-stage iterative boundary refinement (TIBR4D): 1) Iterative Gaussian Instance Tracing (IGIT) at temporal segment level refines Gaussian-to-instance probabilities through iterative tracing to handle occlusions and preserve object structures. 2) Gaussian Rendering Range Control (RCC) suppresses uncertain Gaussians near boundaries while retaining core contributions. Includes temporal segmentation merging strategy to balance identity consistency and dynamic awareness.", "result": "Experiments on HyperNeRF and Neu3D demonstrate accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to state-of-the-art methods.", "conclusion": "The proposed learning-free 4D Gaussian segmentation framework effectively addresses challenges in dynamic scenes through iterative refinement and boundary control, achieving superior segmentation quality and efficiency."}}
{"id": "2602.08550", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.08550", "abs": "https://arxiv.org/abs/2602.08550", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing", "comment": "ICLR 2026. This is a preprint version. The camera-ready version will be updated soon", "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.", "AI": {"tldr": "GOT-Edit integrates 3D geometric cues into 2D object tracking via online model editing with null-space constrained updates, improving robustness to occlusion and clutter.", "motivation": "Current generic object tracking methods rely only on 2D features and neglect 3D geometric cues, making them vulnerable to occlusion, distractors, and appearance variations. Human perception effectively uses 3D knowledge for tracking, suggesting this gap should be addressed.", "method": "GOT-Edit uses online cross-modality model editing that integrates geometry-aware cues from a pre-trained Visual Geometry Grounded Transformer. It performs null-space constrained updates to incorporate geometric information while preserving semantic discrimination, combining 2D semantics with 3D geometric reasoning.", "result": "Extensive experiments on multiple GOT benchmarks show GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter conditions.", "conclusion": "GOT-Edit establishes a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking, demonstrating that integrating geometric cues significantly improves tracking performance in challenging scenarios."}}
{"id": "2602.08558", "categories": ["cs.CV", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08558", "abs": "https://arxiv.org/abs/2602.08558", "authors": ["Guan Yuan Tan", "Ngoc Tuan Vu", "Arghya Pal", "Sailaja Rajanala", "Raphael Phan C. -W.", "Mettu Srinivas", "Chee-Ming Ting"], "title": "FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction", "comment": null, "summary": "We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.", "AI": {"tldr": "FLAG-4D is a novel framework for dynamic 3D scene reconstruction that uses a dual-deformation network to model how 3D Gaussian primitives evolve through time, achieving higher fidelity and temporal coherence than existing methods.", "motivation": "Existing methods for dynamic scene reconstruction typically use a single MLP to model temporal deformations, which struggles to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views.", "method": "FLAG-4D employs a dual-deformation network with an Instantaneous Deformation Network (IDN) for fine-grained local deformations and a Global Motion Network (GMN) for long-range dynamics, refined through mutual learning. It incorporates dense motion features from pretrained optical flow, fuses motion cues from adjacent timeframes, and uses deformation-guided attention to align flow information with evolving 3D Gaussians.", "result": "Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.", "conclusion": "FLAG-4D successfully overcomes limitations of existing dynamic scene reconstruction methods by using a dual-deformation approach with optical flow integration, enabling more accurate and temporally smooth modeling of complex 3D scene dynamics."}}
{"id": "2602.08582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08582", "abs": "https://arxiv.org/abs/2602.08582", "authors": ["Melany Yang", "Yuhang Yu", "Diwang Weng", "Jinwei Chen", "Wei Dong"], "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning", "comment": null, "summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.", "AI": {"tldr": "SemiNFT is a Diffusion Transformer-based color retouching framework that learns like human artists: first imitating with paired data, then developing aesthetic intuition through reinforcement learning on unpaired data.", "motivation": "Manual color retouching requires specialized expertise that's inaccessible to non-experts. Existing reference-based methods perform simple global color mappings without understanding semantic context or human aesthetics.", "method": "Two-stage approach: 1) Train with paired triplets to learn structural preservation and basic color mapping, 2) Use reinforcement learning on unpaired data to develop aesthetic perception with hybrid online-offline rewards to prevent forgetting.", "result": "Outperforms state-of-the-art methods on preset transfer benchmarks and demonstrates strong zero-shot performance on tasks like black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer.", "conclusion": "SemiNFT achieves sophisticated aesthetic comprehension beyond simple statistical matching, effectively mimicking human artistic learning from imitation to intuitive creation."}}
{"id": "2602.08613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08613", "abs": "https://arxiv.org/abs/2602.08613", "authors": ["Wei Gao", "Wenxu Gao", "Xingming Mu", "Changhao Peng", "Ge Li"], "title": "Overview and Comparison of AVS Point Cloud Compression Standard", "comment": "3 figures, 3 tables", "summary": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.", "AI": {"tldr": "This paper reviews China's AVS PCC point cloud compression standard, covering its technologies and performance comparisons with MPEG standards (G-PCC and V-PCC).", "motivation": "Point clouds are important for 3D applications but have large data sizes that challenge transmission and storage, requiring efficient compression standards. While MPEG has established G-PCC and V-PCC standards, China's AVS Workgroup developed its own AVS PCC standard with different technical approaches.", "method": "The paper reviews the AVS PCC standard from two perspectives: 1) related technologies (coding tools and techniques adopted), and 2) performance comparisons with other standards.", "result": "The AVS PCC standard incorporates many new coding tools and techniques that differ from MPEG's G-PCC and V-PCC standards, representing a distinct technical approach to point cloud compression.", "conclusion": "AVS PCC is China's first-generation point cloud compression standard that offers an alternative technical framework to existing MPEG standards, with its own set of coding tools and performance characteristics that warrant review and comparison."}}
{"id": "2602.08615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08615", "abs": "https://arxiv.org/abs/2602.08615", "authors": ["Kfir Goldberg", "Elad Richardson", "Yael Vinker"], "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "comment": "Project page available at https://inspirationseedspaper.github.io/InspirationSeeds/", "summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "AI": {"tldr": "Inspiration Seeds: A text-free generative framework for visual ideation that creates diverse compositions from two input images to reveal latent relationships, supporting early-stage creative exploration.", "motivation": "Current generative models are optimized for executing specific text prompts but lack support for open-ended visual exploration that designers need during early idea formation. Designers often draw inspiration from loosely connected visual references seeking emergent connections.", "method": "Feed-forward model trained on synthetic triplets of decomposed visual aspects using CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. Operates without user-specified text prompts.", "result": "Produces diverse, visually coherent compositions that reveal latent relationships between input images, enabling fast, intuitive recombination for visual ideation.", "conclusion": "The framework shifts image generation from final execution to exploratory ideation, supporting creative work at early, ambiguous stages by removing reliance on language and enabling visual recombination."}}
{"id": "2602.08620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08620", "abs": "https://arxiv.org/abs/2602.08620", "authors": ["Siyu Liu", "Chujie Qin", "Hubery Yin", "Qixin Yan", "Zheng-Peng Duan", "Chen Li", "Jing Lyu", "Chun-Le Guo", "Chongyi Li"], "title": "Improving Reconstruction of Representation Autoencoder", "comment": null, "summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.", "AI": {"tldr": "LV-RAE is a representation autoencoder that augments semantic features from vision foundation models with low-level information to enable high-fidelity reconstruction while maintaining semantic alignment, addressing key bottlenecks in scaling latent diffusion models.", "motivation": "Current approaches using vision foundation models as image encoders for latent diffusion models produce semantic features that lack low-level information (color, texture), leading to degraded reconstruction fidelity which limits further scaling of LDMs.", "method": "Proposes LV-RAE, a representation autoencoder that augments semantic features with missing low-level information. Also addresses decoder sensitivity to latent perturbations by fine-tuning decoders for robustness and smoothing generated latents via controlled noise injection.", "result": "LV-RAE significantly improves reconstruction fidelity while preserving semantic abstraction and achieving strong generative quality, as demonstrated through experiments.", "conclusion": "The proposed LV-RAE framework successfully addresses the low-level information bottleneck in vision foundation model-based LDMs, enabling high-fidelity reconstruction and improved generation quality through robust decoder design and latent smoothing techniques."}}
{"id": "2602.08626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08626", "abs": "https://arxiv.org/abs/2602.08626", "authors": ["Alexis Marouani", "Oriane Sim\u00e9oni", "Herv\u00e9 J\u00e9gou", "Piotr Bojanowski", "Huy V. Vo"], "title": "Revisiting [CLS] and Patch Token Interaction in Vision Transformers", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.", "AI": {"tldr": "Vision Transformers use both class and patch tokens, but process them identically despite their different roles. The paper shows that standard normalization layers implicitly differentiate these tokens, and proposes specialized processing paths that disentangle their computational flow, improving patch representation for dense tasks while maintaining classification accuracy.", "motivation": "Vision Transformers process class tokens and patch tokens identically throughout the model, even though they serve fundamentally different purposes - class tokens capture global features while patch tokens capture local features. This uniform processing creates friction between global and local feature learning, potentially limiting representation quality for dense prediction tasks like segmentation.", "method": "The authors analyze interactions between class and patch tokens under different pre-training strategies, discovering that standard normalization layers already introduce implicit differentiation. Building on this insight, they propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections.", "result": "The specialized processing leads to significantly improved patch representation quality for dense prediction tasks, achieving segmentation performance gains of over 2 mIoU points on standard benchmarks while maintaining strong classification accuracy. The modifications add only 8% more parameters with no additional computational overhead.", "conclusion": "Targeted specialization of processing paths for class and patch tokens in Vision Transformers effectively addresses the friction between global and local feature learning, yielding substantial improvements for dense prediction tasks without compromising classification performance or computational efficiency."}}
{"id": "2602.08652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08652", "abs": "https://arxiv.org/abs/2602.08652", "authors": ["Oskar Thaeter", "Tanja Niedermair", "Johannes Raffler", "Ralf Huss", "Peter J. Sch\u00fcffler"], "title": "Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology", "comment": "17 pages, 8 figures, 7 tables", "summary": "Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to\n  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen\n  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.\n  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from\n  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,\n  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).\n  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and\n  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\\times$\n  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.\n  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for\n  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner\n  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other\n  low-resolution slide annotations.", "AI": {"tldr": "Deep learning model uses low-resolution thumbnail images to predict FFPE vs frozen section fixation types, achieving 0.88 AUROC on TCGA data and processing slides 400\u00d7 faster than full-resolution methods.", "motivation": "Manual fixation type annotation is error-prone and impacts diagnostic accuracy. Existing verification methods require full-resolution whole-slide images, limiting scalability for high-throughput quality control in pathology labs.", "method": "Proposed deep learning model trained on 1,200 WSIs from TUM Institute of Pathology (Leica GT450DX) that predicts fixation types using low-resolution, pre-scan thumbnail images instead of full-resolution WSIs.", "result": "Achieved AUROC of 0.88 on TCGA dataset (n=8,800), outperforming comparable pre-scan methods by 4.8%. Also achieved 0.72 AUROC on external datasets from Augsburg and Regensburg. Processes each slide in 21 ms, 400\u00d7 faster than full-resolution methods.", "conclusion": "The method provides efficient error detection without high-magnification scans, enabling rapid quality control. Future work will improve generalization to additional scanner types and extend to other low-resolution slide annotations."}}
{"id": "2602.08661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08661", "abs": "https://arxiv.org/abs/2602.08661", "authors": ["Yi Dao", "Lankai Zhang", "Hao Liu", "Haiwei Zhang", "Wenbo Wang"], "title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling", "comment": null, "summary": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.", "AI": {"tldr": "WiFlow: A WiFi-based continuous human pose estimation framework using encoder-decoder architecture with spatio-temporal feature extraction, achieving 97% PCK@20 with only 4.82M parameters.", "motivation": "Existing WiFi-based human pose estimation methods struggle with continuous motion and high computational overhead, while vision-based approaches treat CSI as images rather than preserving the original sequential structure of signals.", "method": "WiFlow uses an encoder-decoder architecture: encoder captures spatio-temporal CSI features using temporal/asymmetric convolutions to preserve sequential structure, refines keypoint features with axial attention, and decoder maps features to keypoint coordinates.", "result": "Achieves PCK of 97.00% at PCK@20 and 99.48% at PCK@50, with mean per-joint position error of 0.008m, using only 4.82M parameters on a dataset of 360,000 CSI-pose samples from 5 subjects performing 8 daily activities.", "conclusion": "WiFlow establishes a new performance baseline for practical WiFi-based human pose estimation by significantly reducing model complexity and computational cost while maintaining high accuracy for continuous motion."}}
{"id": "2602.08670", "categories": ["cs.CV", "cs.CE", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.08670", "abs": "https://arxiv.org/abs/2602.08670", "authors": ["Yang Bai"], "title": "A Machine Learning accelerated geophysical fluid solver", "comment": "Master Thesis", "summary": "Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.", "AI": {"tldr": "The paper proposes ML-based PDE solvers using data-driven discretization methods, showing improved performance over traditional solvers like Pyclaw for shallow water and Euler equations.", "motivation": "Machine learning has succeeded in many domains but applying it to mathematically constrained areas like PDE solving remains challenging. The paper aims to bridge this gap by developing ML-based PDE solvers that can improve accuracy, stability, and efficiency compared to traditional numerical methods.", "method": "Implemented classic solvers for shallow water and Euler equations, then proposed four different deep neural network architectures for ML-based solvers using data-driven discretization methods that predict coefficients of quasi-linear stencils on structured grids.", "result": "The classic solver performed significantly better than Pyclaw solver. Among the four proposed neural network approaches, two produced satisfactory solutions, demonstrating the potential of ML-based methods for PDE solving.", "conclusion": "ML-based data-driven discretization methods show promise for improving PDE solvers, offering better accuracy and stability than traditional finite difference/volume schemes while benefiting from conservation properties of classical formulations."}}
{"id": "2602.08682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08682", "abs": "https://arxiv.org/abs/2602.08682", "authors": ["Ying Guo", "Qijun Gan", "Yifu Zhang", "Jinlai Liu", "Yifei Hu", "Pan Xie", "Dongjun Qian", "Yu Zhang", "Ruiqi Li", "Yuqi Zhang", "Ruibiao Lu", "Xiaofeng Mei", "Bo Han", "Xiang Yin", "Bingyue Peng", "Zehuan Yuan"], "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "comment": null, "summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "AI": {"tldr": "ALIVE adapts pretrained Text-to-Video models for Sora-style audio-video generation and animation, achieving state-of-the-art performance through architectural enhancements and high-quality data.", "motivation": "Video generation is evolving toward unified audio-video generation, but existing Text-to-Video models lack audio-visual synchronization and animation capabilities. The paper aims to bridge this gap by adapting T2V models for comprehensive audio-video generation.", "method": "ALIVE augments MMDiT architecture with joint audio-video branch featuring TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Uses comprehensive data pipeline for high-quality finetuning data collection and introduces new benchmark for evaluation.", "result": "After pretraining on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions.", "conclusion": "ALIVE successfully enables audio-video generation and animation capabilities, providing detailed recipes and benchmarks to help the community develop audio-video generation models more efficiently."}}
{"id": "2602.08683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08683", "abs": "https://arxiv.org/abs/2602.08683", "authors": ["Feilong Tang", "Xiang An", "Yunyao Yan", "Yin Xie", "Bin Qin", "Kaicheng Yang", "Yifei Shen", "Yuanhan Zhang", "Chunyuan Li", "Shikun Feng", "Changrui Chen", "Huajie Tan", "Ming Hu", "Manyuan Zhang", "Bo Li", "Ziyong Feng", "Ziwei Liu", "Zongyuan Ge", "Jiankang Deng"], "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence", "comment": null, "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.", "AI": {"tldr": "AGI is a compression problem requiring architectural alignment with data structure. OneVision-Encoder uses codec-inspired patch-level sparsity to focus on signal-rich regions, achieving superior efficiency and accuracy across vision tasks.", "motivation": "Modern vision architectures waste computation on redundant visual data while ignoring sparse discriminative information. The paper argues that visual understanding requires aligning architectures with information-theoretic principles of video codecs to focus on predictive residuals that define motion and meaning.", "method": "OneVision-Encoder uses Codec Patchification to process only 3.1%-25% of signal-rich regions, employs shared 3D RoPE for unified spatial-temporal reasoning with irregular token layouts, and is trained with large-scale cluster discrimination over 1M+ semantic concepts to capture object permanence and motion dynamics.", "result": "OV-Encoder outperforms strong vision backbones (Qwen3-ViT, SigLIP2) across 16 image, video, and document understanding benchmarks despite using fewer visual tokens and pretraining data. Achieves 4.1% average improvement on video tasks over Qwen3-ViT.", "conclusion": "Efficiency and accuracy are positively correlated when architectures align with information-theoretic principles. Codec-aligned patch-level sparsity enables scalable visual generalists, validating that AGI is fundamentally a compression problem requiring architectural resonance with data structure."}}
{"id": "2602.08699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08699", "abs": "https://arxiv.org/abs/2602.08699", "authors": ["Xiaogang Xu", "Kun Zhou", "Tao Hu", "Jiafei Wu", "Ruixing Wang", "Hao Peng", "Bei Yu"], "title": "Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm", "comment": null, "summary": "Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.", "AI": {"tldr": "VLLVE/VLLVE++: View-aware low-light video enhancement framework using video decomposition with view-independent and view-dependent components, plus residual term for scene-adaptive degradations.", "motivation": "Low-light videos suffer from severe invisibility and noise; existing methods need better decomposition strategies to handle dynamic scenes and consistent enhancement across frames.", "method": "1) Video decomposition into view-independent (intrinsic appearance) and view-dependent (shading) components; 2) Dual-structure enhancement network with cross-frame interaction; 3) VLLVE++ adds residual term for scene-adaptive degradations and enables bidirectional learning for enhancement and degradation-aware correspondence refinement.", "result": "Extensive experiments on LLVE benchmarks show strong performance, especially on challenging real-world scenes and high-dynamic videos; framework handles consistent decomposition with minimal parameter overhead.", "conclusion": "VLLVE/VLLVE++ provides effective view-aware decomposition for low-light video enhancement, with VLLVE++ offering superior handling of complex degradations through residual modeling and bidirectional learning."}}
{"id": "2602.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08711", "abs": "https://arxiv.org/abs/2602.08711", "authors": ["Linli Yao", "Yuancheng Wei", "Yaojie Zhang", "Lei Li", "Xinlong Chen", "Feifan Song", "Ziyue Wang", "Kun Ouyang", "Yuanxin Liu", "Lingpeng Kong", "Qi Liu", "Pengfei Wan", "Kun Gai", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions", "comment": null, "summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.", "AI": {"tldr": "Omni Dense Captioning generates continuous, structured audio-visual narratives with timestamps using a six-dimensional schema, with state-of-the-art performance from TimeChat-Captioner-7B.", "motivation": "To create detailed, structured video descriptions that enable vivid scene-by-scene imagination similar to cinematographic screenplays, addressing the need for fine-grained audio-visual narratives with explicit temporal information.", "method": "Introduces a six-dimensional structural schema for \"script-like\" captions, creates OmniDCBench benchmark, proposes SodaM evaluation metric, builds TimeChatCap-42K training dataset, and develops TimeChat-Captioner-7B baseline trained via SFT and GRPO with task-specific rewards.", "result": "TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, and its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA).", "conclusion": "The proposed Omni Dense Captioning task, benchmark, and model effectively generate continuous, fine-grained audio-visual narratives that enhance downstream applications, with all resources made publicly available."}}
{"id": "2602.08713", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08713", "abs": "https://arxiv.org/abs/2602.08713", "authors": ["Lachin Naghashyar", "Hunar Batra", "Ashkan Khakzar", "Philip Torr", "Ronald Clark", "Christian Schroeder de Witt", "Constantin Venhoff"], "title": "Towards Understanding Multimodal Fine-Tuning: Spatial Features", "comment": null, "summary": "Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to \"see\". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.", "AI": {"tldr": "Mechanistic analysis reveals how language models learn to \"see\" during multimodal fine-tuning, identifying vision-preferring features that encode spatial relations through specific attention heads.", "motivation": "Despite strong performance of Vision-Language Models (VLMs), it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. The paper aims to provide the first mechanistic analysis of VLM adaptation.", "method": "Uses stage-wise model diffing technique that isolates representational changes introduced during multimodal fine-tuning. Identifies vision-preferring features that emerge or reorient, analyzes spatial relation encoding through controlled spatial prompt shifts, and traces causal activation to specific attention heads.", "result": "Reveals how language models learn to \"see\" by identifying selective vision-preferring features that reliably encode spatial relations. Shows these features are activated by a small group of attention heads, demonstrating when and where spatially grounded multimodal features arise.", "conclusion": "Stage-wise model diffing provides a clearer view of modality fusion by showing how visual grounding reshapes previously text-only features. This methodology enhances interpretability of multimodal training and provides foundation for understanding how pretrained language models acquire vision-grounded capabilities."}}
{"id": "2602.08717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08717", "abs": "https://arxiv.org/abs/2602.08717", "authors": ["Farnaz Khun Jush", "Grit Werner", "Mark Klemens", "Matthias Lenga"], "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images", "comment": "8 pages, 5 figures, 5 tables", "summary": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.", "AI": {"tldr": "Zero-shot body region detection in CT/MR using foundation models: segmentation-driven rule-based approach outperforms MLLM methods with F1-scores of 0.947 (CT) and 0.914 (MR).", "motivation": "Current body region detection relies on unreliable DICOM metadata and supervised learning, limiting real-world applicability. Need for zero-shot approaches using pre-trained foundation models.", "method": "Three training-free pipelines: (1) segmentation-driven rule-based system using pre-trained multi-organ segmentation models, (2) MLLM guided by radiologist-defined rules, (3) segmentation-aware MLLM combining visual input with anatomical evidence.", "result": "Segmentation-driven rule-based approach achieved best performance: weighted F1-scores of 0.947 (CT) and 0.914 (MR), robust across modalities and atypical scans. MLLM performed competitively in visually distinctive regions.", "conclusion": "Zero-shot body region detection is feasible using foundation models. Segmentation-driven rule-based approach is most effective, demonstrating robustness without training on target data."}}
{"id": "2602.08724", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08724", "abs": "https://arxiv.org/abs/2602.08724", "authors": ["Geng Lin", "Matthias Zwicker"], "title": "Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering", "comment": "Project Page: https://rotlight-ir.github.io/", "summary": "Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.", "AI": {"tldr": "RotLight: A simple rotation-based capture setup that reduces ambiguity in inverse rendering by rotating objects during capture, combined with proxy mesh for better light tracing and global illumination handling.", "motivation": "Current inverse rendering methods suffer from ambiguity in material and lighting estimation, leading to inaccurate color and baked shadows in albedo estimation despite regularization techniques.", "method": "1) RotLight capture setup requiring object rotation during capture (as few as two rotations effective); 2) Proxy mesh for accurate incident light tracing, residual constraints, and improved global illumination handling; 3) 2DGS-based inverse rendering framework.", "result": "Demonstrated superior albedo estimation on both synthetic and real-world datasets while maintaining efficient computation, effectively reducing artifacts compared to prior methods.", "conclusion": "The proposed RotLight capture setup and proxy mesh approach effectively addresses the ambiguity problem in inverse rendering, achieving better material decomposition with minimal additional capture requirements."}}
{"id": "2602.08725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08725", "abs": "https://arxiv.org/abs/2602.08725", "authors": ["Yongwen Lai", "Chaoqun Wang", "Shaobo Min"], "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing", "comment": "Accepted by ICASSP 2026", "summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.", "AI": {"tldr": "FusionEdit is a training-free image editing framework that uses semantic discrepancy analysis to identify editing regions, employs distance-aware latent fusion with soft masks to reduce boundary artifacts, and uses AdaIN-based modulation in DiT attention layers for better editability while preserving source image identity.", "motivation": "Existing text-guided image editing methods use explicit binary masks which create hard boundaries that introduce artifacts and reduce editability. There's a need for more precise and controllable editing that preserves source image identity while achieving smooth transitions.", "method": "1) Automatically identifies editing vs preserved regions by measuring semantic discrepancies between source and target prompts. 2) Uses distance-aware latent fusion along region boundaries to create soft, accurate masks with total variation loss for smooth transitions. 3) Employs AdaIN-based modulation within DiT attention layers for statistical attention fusion in editing regions to enhance editability while maintaining global consistency.", "result": "Extensive experiments show FusionEdit significantly outperforms state-of-the-art methods in text-guided image editing, achieving more precise and controllable edits with fewer artifacts.", "conclusion": "FusionEdit provides an effective training-free framework for text-guided image editing that addresses boundary artifact issues through soft mask generation and attention fusion techniques, resulting in more natural editing outcomes while preserving source image identity."}}
{"id": "2602.08726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08726", "abs": "https://arxiv.org/abs/2602.08726", "authors": ["Khadija Iddrisu", "Waseem Shariff", "Suzanne Little", "Noel OConnor"], "title": "SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training", "comment": "Accepted to the 2nd Workshop on \"Event-based Vision in the Era of Generative AI - Transforming Perception and Visual Innovation, IEEE Winter Conference on Applications of Computer Vision (WACV 2026)", "summary": "The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.", "AI": {"tldr": "Researchers created a synthetic dataset using Blender to simulate eye movements (saccades/fixations) and used Spiking Neural Networks for classification, achieving 0.83 accuracy with better computational efficiency than traditional ANNs.", "motivation": "Event cameras eliminate motion blur and offer superior temporal resolution for studying eye movements, but real event data is scarce. Synthetic data can help overcome this limitation and advance event-based vision research.", "method": "Generated synthetic dataset using Blender to simulate saccades and fixations under controlled conditions. Used Spiking Neural Networks (SNNs) for classification, trained two architectures and fine-tuned on real event data.", "result": "Models achieved up to 0.83 accuracy with consistent performance across varying temporal resolutions. SNNs showed substantial computational efficiency gains over artificial neural network counterparts.", "conclusion": "Synthetic data augmentation with SNNs is effective for eye movement classification using event cameras, demonstrating stability and computational efficiency while advancing event-based vision research."}}
{"id": "2602.08727", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08727", "abs": "https://arxiv.org/abs/2602.08727", "authors": ["Johannes Thalhammer", "Tina Dorosti", "Sebastian Peterhansl", "Daniela Pfeiffer", "Franz Pfeiffer", "Florian Schaff"], "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework", "comment": null, "summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.", "AI": {"tldr": "Hybrid 2D-3D deep learning framework for artifact reduction in undersampled CT volumes, balancing computational efficiency with volumetric consistency.", "motivation": "Undersampled CT volumes reduce acquisition time and radiation exposure but introduce artifacts that degrade image quality and diagnostic utility, requiring artifact reduction for high-quality imaging.", "method": "Two-stage hybrid framework: 1) 2D U-Net processes individual slices to extract feature maps, 2) Stacked feature maps are input to 3D decoder that uses cross-slice contextual information to predict artifact-free 3D CT volume.", "result": "Substantial improvements in inter-slice consistency in coronal and sagittal directions with low computational overhead.", "conclusion": "The hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing, balancing 2D computational efficiency with 3D volumetric consistency."}}
{"id": "2602.08730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08730", "abs": "https://arxiv.org/abs/2602.08730", "authors": ["Shanshan Wang", "Ziying Feng", "Xiaozheng Shen", "Xun Yang", "Pichao Wang", "Zhenwei He", "Xingyi Zhang"], "title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA", "AI": {"tldr": "CLIP-Guided Alignment (CGA) improves Source-Free Domain Adaptation by explicitly modeling class confusion patterns using CLIP's multimodal capabilities for better pseudo-labeling and feature alignment.", "motivation": "Existing SFDA methods struggle with fine-grained scenarios due to asymmetric and dynamic class confusion, where visually similar classes are unequally misclassified by the source model, leading to noisy pseudo-labels.", "method": "Three-part framework: 1) MCA detects directional confusion pairs from source model predictions, 2) MCC uses CLIP to create confusion-aware textual prompts for context-sensitive pseudo-labeling, and 3) FAM builds confusion-guided feature banks and aligns them via contrastive learning.", "result": "CGA consistently outperforms state-of-the-art SFDA methods across various datasets, with particularly strong improvements in confusion-prone and fine-grained scenarios.", "conclusion": "Explicitly modeling inter-class confusion is crucial for effective source-free domain adaptation, and CLIP's multimodal capabilities provide a powerful mechanism for addressing this challenge."}}
{"id": "2602.08735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08735", "abs": "https://arxiv.org/abs/2602.08735", "authors": ["Masanari Oi", "Koki Maeda", "Ryuto Koike", "Daisuke Oba", "Nakamasa Inoue", "Naoaki Okazaki"], "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models", "comment": null, "summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.", "AI": {"tldr": "HATCH is a training framework that improves multi-image spatial reasoning in MLLMs by explicitly incorporating cross-view correspondence and stepwise viewpoint transformation mechanisms inspired by human cognition.", "motivation": "Current MLLMs struggle with multi-image spatial reasoning that requires integrating information from multiple viewpoints. While humans use cross-view correspondence and stepwise viewpoint transformation, existing approaches incorporate these mechanisms only partially and implicitly without explicit supervision.", "method": "HATCH introduces two complementary training objectives: (1) Patch-Level Spatial Alignment that encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning that requires models to generate explicit viewpoint transition actions before predicting final answers.", "result": "HATCH consistently outperforms comparable-size baselines by a clear margin on three benchmarks, achieves competitive results against much larger models, and preserves single-image reasoning capabilities.", "conclusion": "Explicitly incorporating human-inspired cognitive mechanisms (cross-view correspondence and stepwise viewpoint transformation) through HATCH's training framework significantly improves multi-image spatial reasoning in MLLMs while maintaining single-image capabilities."}}
{"id": "2602.08749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08749", "abs": "https://arxiv.org/abs/2602.08749", "authors": ["Carmine Zaccagnino", "Fabio Quattrini", "Enis Simsar", "Marta Tintor\u00e9 Gazulla", "Rita Cucchiara", "Alessio Tonioni", "Silvia Cascianelli"], "title": "Shifting the Breaking Point of Flow Matching for Multi-Instance Editing", "comment": null, "summary": "Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.", "AI": {"tldr": "Proposes Instance-Disentangled Attention for flow matching models to enable multi-instance image editing without semantic interference, addressing limitations of existing methods that struggle with concurrent edits.", "motivation": "Existing flow-based image editors support only global or single-instruction edits and fail at multi-instance scenarios where multiple parts need independent editing without semantic interference, due to globally conditioned velocity fields and joint attention mechanisms.", "method": "Introduces Instance-Disentangled Attention that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation.", "result": "Experimental results show the approach promotes edit disentanglement and locality while preserving global coherence, enabling single-pass, instance-level editing on both natural images and text-dense infographics.", "conclusion": "The proposed Instance-Disentangled Attention mechanism successfully addresses the multi-instance editing limitation in flow matching models, enabling independent editing of multiple regions without semantic interference."}}
{"id": "2602.08753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08753", "abs": "https://arxiv.org/abs/2602.08753", "authors": ["Tianyu Sun", "Zhoujie Fu", "Bang Zhang", "Guosheng Lin"], "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "comment": null, "summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "AI": {"tldr": "MVAnimate is a novel framework that synthesizes 2D and 3D character animation using multi-view prior information to improve video quality and overcome limitations of existing methods.", "motivation": "Existing character animation methods using 2D or 3D pose modeling face problems like low-quality output and training data deficiency, preventing generation of high-quality animation videos.", "method": "MVAnimate synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, leveraging this to produce temporally consistent and spatially coherent animation outputs while optimizing multi-view videos of target characters.", "result": "Experimental results on diverse datasets demonstrate robustness in handling various motion patterns and appearances, showing improvements over existing animation methods.", "conclusion": "MVAnimate enhances generated video quality by effectively utilizing multi-view prior information, addressing key limitations in current character animation generation approaches."}}
{"id": "2602.08775", "categories": ["cs.CV", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.08775", "abs": "https://arxiv.org/abs/2602.08775", "authors": ["Vineet Kumar Rakesh", "Ahana Bhattacharjee", "Soumya Mazumdar", "Tapas Samanta", "Hemendra Kumar Pandey", "Amitabha Das", "Sarbajit Pal"], "title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars", "comment": null, "summary": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg", "AI": {"tldr": "A CPU-friendly talking-head avatar system using symbolic Vedic computation for educational applications, achieving real-time lip-sync on low-end hardware without GPU dependencies.", "motivation": "Current talking-head generation methods rely on GPU-intensive neural rendering, large training sets, or diffusion models, making them unsuitable for offline or resource-constrained educational environments.", "method": "Symbolic Vedic Computation framework: converts speech to phonemes, maps to compact viseme inventory, produces smooth trajectories using Vedic sutra Urdhva Tiryakbhyam for coarticulation, and uses lightweight 2D renderer with ROI warping and mouth compositing.", "result": "Achieves acceptable lip-sync quality with synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, substantially reducing computational load and latency compared to CPU-feasible baselines.", "conclusion": "The deterministic, CPU-oriented approach enables practical educational avatars on low-end hardware, supporting deployment in resource-constrained learning environments without GPU dependencies."}}
{"id": "2602.08792", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08792", "abs": "https://arxiv.org/abs/2602.08792", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems", "comment": null, "summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.", "AI": {"tldr": "Novel multimodal framework combining high-resolution images and force measurements for accurate pantograph-catenary arcing detection, using synchronized datasets and extended DeepSAD algorithm with pseudo-anomaly generation.", "motivation": "Electrical arcing at pantograph-catenary interface causes serious risks including accelerated wear, degraded performance, and service disruptions. Detection is challenging due to transient nature, noisy environment, data scarcity, and difficulty distinguishing arcs from similar phenomena.", "method": "Proposed multimodal framework combining high-resolution image data with force measurements. Built two synchronized datasets (SBB-provided and public videos with synthetic force data). Developed MultiDeepSAD (extension of DeepSAD for multiple modalities) with new loss formulation. Introduced tailored pseudo-anomaly generation techniques for each data type (synthetic arc-like artifacts in images, simulated force irregularities).", "result": "Framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations, as demonstrated through extensive experiments and ablation studies.", "conclusion": "Multimodal approach combining visual and force data with specialized anomaly generation techniques provides robust solution for challenging pantograph-catenary arcing detection, addressing data scarcity and domain shift issues in real-world railway systems."}}
{"id": "2602.08797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08797", "abs": "https://arxiv.org/abs/2602.08797", "authors": ["Jiaming Liu", "Cheng Ding", "Daoqiang Zhang"], "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework", "comment": "10 pages, 7 figures. Submitted to IEEE Journal of Biomedical and Health Informatics (JBHI)", "summary": "Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.", "AI": {"tldr": "Semi-supervised teacher-student framework with uncertainty-aware pseudo-labeling and confidence-based curriculum improves brain tumor segmentation from limited MRI annotations.", "motivation": "Brain tumor segmentation from MRI faces challenges due to expensive manual annotations and data heterogeneity across different scanners and sites, limiting model performance with limited labeled data.", "method": "Proposed a teacher-student framework where the teacher generates probabilistic masks with uncertainty estimates, then uses confidence-based curriculum to progressively introduce unlabeled scans to the student. The student employs dual-loss objective to learn from high-confidence regions and unlearn low-confidence ones, with agreement-based refinement for pseudo-label quality.", "result": "On BraTS 2021, validation DSC improved from 0.393 (10% data) to 0.872 (100%). Teacher reached DSC 0.922, and student surpassed teacher on tumor subregions (NCR/NET 0.797, Edema 0.980) and recovered Enhancing class (DSC 0.620) where teacher failed.", "conclusion": "Confidence-driven curricula and selective unlearning enable robust segmentation under limited supervision and noisy pseudo-labels, demonstrating data efficiency and improved performance over teacher models."}}
{"id": "2602.08794", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08794", "abs": "https://arxiv.org/abs/2602.08794", "authors": ["SII-OpenMOSS Team", ":", "Donghua Yu", "Mingshu Chen", "Qi Chen", "Qi Luo", "Qianyi Wu", "Qinyuan Cheng", "Ruixiao Li", "Tianyi Liang", "Wenbo Zhang", "Wenming Tu", "Xiangyu Peng", "Yang Gao", "Yanru Huo", "Ying Zhu", "Yinze Luo", "Yiyang Zhang", "Yuerong Song", "Zhe Xu", "Zhiyu Zhang", "Chenchen Yang", "Cheng Chang", "Chushu Zhou", "Hanfu Chen", "Hongnan Ma", "Jiaxi Li", "Jingqi Tong", "Junxi Liu", "Ke Chen", "Shimin Li", "Songlin Wang", "Wei Jiang", "Zhaoye Fei", "Zhiyuan Ning", "Chunguo Li", "Chenhui Li", "Ziwei He", "Zengfeng Huang", "Xie Chen", "Xipeng Qiu"], "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "comment": "Technical report for MOVA (open-source video-audio generation model). 38 pages, 10 figures, 22 tables. Project page: https://mosi.cn/models/mova Code: https://github.com/OpenMOSS/MOVA Models: https://huggingface.co/collections/OpenMOSS-Team/mova. Qinyuan Cheng and Tianyi Liang are project leader. Xie Chen and Xipeng Qiu are corresponding authors", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "AI": {"tldr": "MOVA is an open-source 32B parameter Mixture-of-Experts model that generates synchronized audio-visual content including lip-synced speech, sound effects, and music from image-text inputs.", "motivation": "Current audio-visual generation relies on cascaded pipelines that increase cost, accumulate errors, and degrade quality. Existing systems are closed-source, limiting progress, while joint multimodal modeling presents architectural, data, and training challenges.", "method": "Uses a Mixture-of-Experts (MoE) architecture with 32B total parameters (18B active during inference) for IT2VA (Image-Text to Video-Audio) generation. The model supports efficient inference, LoRA fine-tuning, and prompt enhancement.", "result": "Capable of generating high-quality, synchronized audio-visual content including realistic lip-synced speech, environment-aware sound effects, and content-aligned music from image-text inputs.", "conclusion": "MOVA advances audio-visual generation research by providing an open-source solution that addresses limitations of cascaded pipelines and closed-source systems, fostering community development and creative applications."}}
{"id": "2602.08858", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08858", "abs": "https://arxiv.org/abs/2602.08858", "authors": ["Ruihan Xu", "Qingpei Guo", "Yao Zhu", "Xiangyang Ji", "Ming Yang", "Shiliang Zhang"], "title": "FlattenGPT: Depth Compression for Transformer with Layer Flattening", "comment": "Submitted to ICML 2026", "summary": "Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \\textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\\% of zero-shot performance with a compression ratio of 20\\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.", "AI": {"tldr": "FlattenGPT compresses transformer depth by flattening adjacent blocks, preserving knowledge while reducing redundancy and accelerating inference.", "motivation": "Current depth compression methods discard meaningful cues from entire blocks, while channel pruning can't reduce depth and has inconsistent pruning ratios across layers. Need better approach to compress transformers while preserving performance.", "method": "FlattenGPT detects and reduces depth-wise redundancies by flattening two adjacent transformer blocks into one, enabling more effective parameter redundancy detection and removal while maintaining original architecture.", "result": "Outperforms existing pruning methods in zero-shot accuracies and WikiText-2 perplexity across various models. On LLaMA-2/3 and Qwen-1.5, retains 90-96% of zero-shot performance with 20% compression ratio, and accelerates LLM inference better than other methods.", "conclusion": "FlattenGPT provides effective depth compression for transformers with good performance-efficiency trade-off, promising for enhancing transformer efficiency through block flattening approach."}}
{"id": "2602.08961", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08961", "abs": "https://arxiv.org/abs/2602.08961", "authors": ["Ruijie Zhu", "Jiahao Lu", "Wenbo Hu", "Xiaoguang Han", "Jianfei Cai", "Ying Shan", "Chuanxia Zheng"], "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE", "comment": "Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "AI": {"tldr": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion from monocular video using novel joint 3D point maps + scene flow representation and 4D VAE.", "motivation": "Prior methods force 3D values and latents to align with RGB VAE latents despite their fundamentally different distributions, leading to suboptimal performance. There's a need for better joint 4D reconstruction and motion estimation from monocular video.", "method": "Introduces a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, plus a novel 4D VAE to learn this representation. Uses new data normalization and VAE training strategy that better transfers diffusion priors instead of forcing alignment with RGB VAE latents.", "result": "Achieves state-of-the-art performance with 38.64% improvement in geometry reconstruction and 25.0% improvement in motion reconstruction across multiple datasets, all without any post-optimization.", "conclusion": "MotionCrafter demonstrates that forcing alignment between 3D values and RGB VAE latents is unnecessary and suboptimal, and that the proposed joint representation with proper normalization and training strategy enables superior 4D reconstruction and motion estimation from monocular video."}}
{"id": "2602.08820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08820", "abs": "https://arxiv.org/abs/2602.08820", "authors": ["Hao Yang", "Zhiyu Tan", "Jia Gong", "Luozheng Qin", "Hesen Chen", "Xiaomeng Yang", "Yuqing Sun", "Yuetan Lin", "Mengping Yang", "Hao Li"], "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing", "comment": "Technical Report, Project: https://howellyoung-s.github.io/Omni-Video2-project/", "summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.", "AI": {"tldr": "Omni-Video 2 connects pretrained MLLMs with video diffusion models for unified video generation and editing, using MLLMs to interpret complex instructions and guide generation through lightweight adapters.", "motivation": "To create a scalable and efficient system that can handle complex compositional video editing and generation by leveraging the understanding capabilities of multimodal LLMs to interpret user instructions and guide video diffusion models.", "method": "1) Uses pretrained MLLMs to produce explicit target captions from user instructions; 2) Develops lightweight adapters to inject multimodal conditional tokens into pretrained text-to-video diffusion models; 3) Scales to 14B parameters on curated training data.", "result": "Superior performance on FiVE benchmark for fine-grained video editing, achieving competitive/superior quality on VBench for text-to-video generation, with strong ability to follow complex compositional instructions.", "conclusion": "Omni-Video 2 demonstrates an effective approach to unified video generation and editing by connecting MLLMs with video diffusion models, enabling scalable, parameter-efficient systems that handle complex compositional tasks while maintaining high quality."}}
{"id": "2602.08822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08822", "abs": "https://arxiv.org/abs/2602.08822", "authors": ["Yao Pu", "Yiming Shi", "Zhenxi Zhang", "Peixin Yu", "Yitao Zhuang", "Xiang Wang", "Hongzhao Chen", "Jing Cai", "Ge Ren"], "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications", "comment": null, "summary": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.", "AI": {"tldr": "A unified foundation model for any-to-all MRI synthesis in nasopharyngeal carcinoma radiotherapy, combining contrastive learning and vision-language alignment to handle incomplete modalities and improve clinical utility.", "motivation": "MRI is crucial for NPC radiotherapy but practical constraints often lead to incomplete modalities, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, lack anatomical adaptability, and have poor clinical interpretability.", "method": "Developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment. Uses contrastive encoder for modality-invariant representations and CLIP-based text-informed decoder for semantically consistent synthesis, enabling any-to-all MRI synthesis via one unified model.", "result": "Trained on 40,825 images from 13 institutions, achieved consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Unified representation also enhances downstream RT-relevant tasks like segmentation.", "conclusion": "The work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility, providing a unified approach for any-to-all MRI synthesis that addresses practical clinical constraints."}}
{"id": "2602.09014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09014", "abs": "https://arxiv.org/abs/2602.09014", "authors": ["Zihan Yang", "Shuyuan Tu", "Licheng Zhang", "Qi Dai", "Yu-Gang Jiang", "Zuxuan Wu"], "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation", "comment": null, "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.", "AI": {"tldr": "ArcFlow distills diffusion models into few-step generators using non-linear flow trajectories instead of linear shortcuts, achieving 40x speedup with minimal quality loss.", "motivation": "Existing distillation methods use linear shortcuts to approximate teacher trajectories, which fail to match constantly changing tangent directions as velocities evolve across timesteps, leading to quality degradation.", "method": "ArcFlow parameterizes velocity fields as mixtures of continuous momentum processes, enabling capture of velocity evolution and formation of continuous non-linear trajectories. It uses analytical integration to avoid discretization errors and trains via trajectory distillation with lightweight adapters on pre-trained models.", "result": "Built on Qwen-Image-20B and FLUX.1-dev, ArcFlow fine-tunes less than 5% of parameters, achieves 40x speedup with 2 NFEs over original multi-step teachers without significant quality degradation, showing effectiveness on benchmarks.", "conclusion": "ArcFlow provides an effective few-step distillation framework that uses non-linear flow trajectories to better approximate teacher trajectories, enabling significant inference speedup while maintaining generation quality."}}
{"id": "2602.08828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08828", "abs": "https://arxiv.org/abs/2602.08828", "authors": ["Hao Tan", "Jun Lan", "Senyuan Shi", "Zichang Tan", "Zijian Yu", "Huijia Zhu", "Weiqiang Wang", "Jun Wan", "Zhen Lei"], "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning", "comment": "Project: https://github.com/EricTan7/VideoVeritas", "summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "AI": {"tldr": "VideoVeritas framework combines fine-grained perception with fact-based reasoning to detect AI-generated videos, using reinforcement learning with perception pretext tasks and introducing a new evaluation dataset.", "motivation": "Growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. Current MLLMs have strong reasoning but limited granular perception ability.", "method": "Introduces VideoVeritas framework with Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Uses general spatiotemporal grounding and self-supervised object counting as perception pretext tasks rather than direct detection optimization. Also introduces MintVid dataset with 3K videos from 9 SOTA generators and real-world factual error subset.", "result": "Experimental results show existing methods bias toward either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "conclusion": "VideoVeritas effectively addresses the limitations of current methods by integrating fine-grained perception with fact-based reasoning, providing a more balanced approach to AI-generated video detection."}}
{"id": "2602.08861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08861", "abs": "https://arxiv.org/abs/2602.08861", "authors": ["Xiangtian Zheng", "Zishuo Wang", "Yuxin Peng"], "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.", "AI": {"tldr": "TiFRe reduces computational costs in Video MLLMs by using text-guided frame selection and merging non-key frame information into key frames.", "motivation": "Video MLLMs face high computational costs from processing numerous video frames, and simply reducing frames at fixed FPS causes significant performance degradation by overlooking valuable information in non-key frames.", "method": "TiFRe uses Text-guided Frame Sampling (TFS) where LLM generates CLIP-style prompts from user input, then CLIP encoders select relevant frames. Frame Matching and Merging (FMM) integrates non-key frame information into selected key frames to preserve semantics.", "result": "Experiments show TiFRe effectively reduces computational costs while improving performance on video-language tasks compared to simple frame reduction methods.", "conclusion": "TiFRe provides an effective framework for reducing computational overhead in Video MLLMs while maintaining or improving task performance through intelligent frame selection and information preservation."}}
{"id": "2602.08909", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08909", "abs": "https://arxiv.org/abs/2602.08909", "authors": ["Zhendong Wang", "Cihan Ruan", "Jingchuan Xiao", "Chuqing Shi", "Wei Jiang", "Wei Wang", "Wenjie Liu", "Nam Ling"], "title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit", "comment": null, "summary": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.", "AI": {"tldr": "3D Gaussian Splatting solutions exhibit stable statistical patterns called Rendering-Optimal References (RORs) with mixture-structured scales and bimodal radiance. Analysis reveals density-stratification: dense regions allow geometry-based parameter prediction, while sparse regions require multi-view constraints due to visibility heterogeneity.", "motivation": "To understand what structure emerges in 3D Gaussian Splatting solutions from standard multi-view optimization, and to determine what factors govern these parameters, particularly distinguishing between regions where geometry alone suffices versus those requiring rendering constraints.", "method": "Analyze statistical properties of Rendering-Optimal References (RORs) from 3DGS solutions. Use learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Apply variance decomposition to understand parameter coupling in different density regions.", "result": "Reveals stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. Shows fundamental density-stratification: dense regions exhibit geometry-correlated parameters predictable from point clouds, while sparse regions show systematic failure due to visibility heterogeneity creating covariance-dominated coupling between geometric and appearance parameters.", "conclusion": "RORs have dual character: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. This leads to density-aware strategies for improved training robustness and architectural implications for systems balancing feed-forward prediction and rendering-based refinement."}}
{"id": "2602.08958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08958", "abs": "https://arxiv.org/abs/2602.08958", "authors": ["Weihan Luo", "Lily Goli", "Sherwin Bahmani", "Felix Taubner", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "comment": "Project page: https://weihanluo.ca/growflow/", "summary": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "AI": {"tldr": "A 3D Gaussian flow field representation models plant growth as time-varying derivatives over Gaussian parameters, enabling nonlinear continuous-time dynamics by learning reverse growth from mature plants.", "motivation": "Existing motion modeling techniques are ill-suited for plant growth because plants generate new geometry over time, while deformation fields can't introduce new geometry and 4D Gaussian splatting can't track the same Gaussians over time or handle nonlinear growth trajectories.", "method": "Introduces a 3D Gaussian flow field representation that models plant growth as time-varying derivatives over Gaussian parameters (position, scale, orientation, color, opacity). To initialize sufficient Gaussian primitives, reconstructs the mature plant and learns a process of reverse growth, simulating the plant's developmental history in reverse.", "result": "Achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth.", "conclusion": "Provides a new approach for appearance modeling of growing 3D structures that can handle the unique challenges of plant growth dynamics."}}
{"id": "2602.08996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08996", "abs": "https://arxiv.org/abs/2602.08996", "authors": ["Arushi Rai", "Adriana Kovashka"], "title": "Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study", "comment": "to appear WACV 2026", "summary": "While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.", "AI": {"tldr": "Using rock climbing as case study, paper proposes using auxiliary web data (competition videos, coaching manuals) to improve sports feedback generation without expensive finetuning data, plus new evaluation metrics for specificity and actionability.", "motivation": "Video-LLMs struggle with sports feedback generation, requiring expensive finetuning data for each sport and showing poor generalization to unseen sports. Traditional text evaluation metrics fail to capture sports feedback quality aspects.", "method": "Proposes using auxiliary freely-available web data from target domain (competition videos, coaching manuals) combined with existing sports feedback from source domain. Also introduces two new evaluation metrics: specificity and actionability.", "result": "Approach enables more meaningful and practical generation of sports feedback under limited annotations, addressing both the data scarcity problem and evaluation inadequacy for sports feedback tasks.", "conclusion": "Using auxiliary domain data and specialized evaluation metrics improves sports feedback generation performance and evaluation quality, making the approach more practical for real-world applications with limited annotations."}}
{"id": "2602.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09016", "abs": "https://arxiv.org/abs/2602.09016", "authors": ["Hao Phung", "Hadar Averbuch-Elor"], "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction", "comment": "Code: https://anonymous.4open.science/r/Raster2Seq-BE73/", "summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.", "AI": {"tldr": "Raster2Seq: A sequence-to-sequence approach for reconstructing structured vector graphics from floorplan images using autoregressive polygon prediction with learnable anchors.", "motivation": "Existing techniques struggle to faithfully reconstruct the structure and semantics of complex floorplans with many rooms and varying polygon corners, which is crucial for automated understanding and CAD workflows.", "method": "Frames floorplan reconstruction as sequence-to-sequence task where elements are represented as labeled polygon sequences. Uses autoregressive decoder that predicts next corner conditioned on image features and previous corners, guided by learnable anchors that direct attention to informative image regions.", "result": "Achieves state-of-the-art performance on Structure3D, CubiCasa5K, and Raster2Graph benchmarks, and demonstrates strong generalization to more challenging WAFFLE dataset with diverse room structures and complex geometric variations.", "conclusion": "The autoregressive approach with learnable anchors provides flexibility in output format and effectively handles complex floorplans with numerous rooms and diverse polygon structures, advancing floorplan reconstruction capabilities."}}
{"id": "2602.09022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09022", "abs": "https://arxiv.org/abs/2602.09022", "authors": ["Zehan Wang", "Tengfei Wang", "Haiyu Zhang", "Xuhui Zuo", "Junta Wu", "Haoyuan Wang", "Wenqiang Sun", "Zhenwei Wang", "Chenjie Cao", "Hengshuang Zhao", "Chunchao Guo", "Zhou Zhao"], "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models", "comment": "Project page: \\url{https://3d-models.hunyuan.tencent.com/world/}", "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.", "AI": {"tldr": "WorldCompass is an RL post-training framework that improves long-horizon interactive video world models by enhancing exploration accuracy and consistency through clip-level rollouts, complementary rewards, and efficient RL optimization.", "motivation": "To address the challenge of making interactive video-based world models explore the world more accurately and consistently based on interaction signals, particularly for long-horizon scenarios where current methods may lack precision and reliability.", "method": "Three core innovations: 1) Clip-level rollout strategy generating multiple samples at single target clips for efficiency and fine-grained rewards, 2) Complementary reward functions for interaction-following accuracy and visual quality to prevent reward hacking, 3) Efficient RL algorithm using negative-aware fine-tuning with various optimizations.", "result": "Evaluation on state-of-the-art open-source world model WorldPlay shows significant improvements in both interaction accuracy and visual fidelity across various scenarios.", "conclusion": "WorldCompass successfully enhances world model exploration capabilities through targeted RL post-training, demonstrating practical improvements in interactive video generation tasks."}}
{"id": "2602.09024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09024", "abs": "https://arxiv.org/abs/2602.09024", "authors": ["Qihang Yu", "Qihao Liu", "Ju He", "Xinyang Zhang", "Yang Liu", "Liang-Chieh Chen", "Xi Chen"], "title": "Autoregressive Image Generation with Masked Bit Modeling", "comment": "SOTA discrete visual generation defeats diffusion models with 0.99 FID score, project page is available at https://bar-gen.github.io/", "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/", "AI": {"tldr": "Discrete tokenizers can match continuous methods when scaled properly; BAR framework enables scalable discrete generation with state-of-the-art results.", "motivation": "Challenge the assumption that discrete tokenizers are inherently inferior to continuous pipelines in visual generation, and investigate the true performance gap between these approaches.", "method": "Propose masked Bit AutoRegressive modeling (BAR) - a scalable framework that supports arbitrary codebook sizes by equipping an autoregressive transformer with a masked bit modeling head to predict discrete tokens through progressively generating their constituent bits.", "result": "BAR achieves new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches.", "conclusion": "The performance gap between discrete and continuous methods stems from compression ratio differences, not intrinsic inferiority of discrete tokenizers; scaling codebook size effectively bridges this gap, and BAR provides a practical framework to leverage this insight for superior visual generation."}}
