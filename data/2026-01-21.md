<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 266]
- [cs.AI](#cs.AI) [Total: 101]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: Domain-specific SimCLR pre-training on 3,000 unlabeled agricultural images provides +4.57% accuracy boost for disease classification, outperforming architectural improvements (+3.70%). This SSL benefit is architecture-agnostic across Swin-Base and ViT-Base.


<details>
  <summary>Details</summary>
Motivation: To determine whether domain-specific self-supervised pre-training or hierarchical architecture design provides greater benefits for agricultural disease classification using vision transformers.

Method: Used HierarchicalViT (HVT), a Swin-style hierarchical transformer, with SimCLR self-supervised pre-training on 3,000 unlabeled agricultural images. Evaluated on three datasets: Cotton Leaf Disease, PlantVillage, and PlantDoc. Compared with Swin-Base and ViT-Base architectures.

Result: SimCLR pre-training provided +4.57% accuracy improvement vs. +3.70% from hierarchical architecture. SSL benefits were architecture-agnostic: Swin-Base +4.08%, ViT-Base +4.20%. HVT-Base achieved 88.91% vs. Swin-Base 87.23% (+1.68%). Calibration analysis showed HVT achieves 3.56% ECE (1.52% after temperature scaling).

Conclusion: Practitioners should prioritize domain-specific data collection and self-supervised pre-training over architectural choices for agricultural disease classification, as SSL provides greater, architecture-agnostic performance gains.

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [2] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: 3D TransUNet synthesizes diffusion MRI metrics (FA/MD) from T1w MRI, boosting AD/MCI diagnosis accuracy without additional dMRI scans.


<details>
  <summary>Details</summary>
Motivation: Early AD detection is crucial but T1w MRI shows late-stage changes, while dMRI captures early microstructural abnormalities but is time-consuming and prone to artifacts, limiting clinical use.

Method: Proposes a 3D TransUNet image synthesis framework that predicts fractional anisotropy (FA) and mean diffusivity (MD) maps directly from T1w MRI scans.

Result: Achieves SSIM >0.93 and Pearson correlation >0.94 with ground-truth dMRI. Synthetic features boost AD classification accuracy by 5% (78.75%→83.75%) and MCI detection by 12.5%.

Conclusion: High-quality diffusion microstructural information can be inferred from routine T1w MRI, improving AD diagnosis accessibility and accuracy while reducing scan time.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [3] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: PointSLAM++ is an RGB-D SLAM system using hierarchical neural Gaussian representation with progressive pose optimization for improved structural consistency and noise resilience in real-time 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current SLAM approaches struggle with maintaining structural consistency and robust pose estimation in the presence of depth noise, which is crucial for robotics and augmented reality applications.

Method: Uses hierarchically constrained neural Gaussian representation to preserve structural relationships, progressive pose optimization to mitigate depth sensor noise, and dynamic neural representation graph that adjusts Gaussian node distribution based on local geometric complexity.

Result: Outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating advantages for large-scale AR and robotics applications.

Conclusion: PointSLAM++ provides high-precision 3D mapping and photorealistic scene rendering through its novel combination of hierarchical Gaussian representation, progressive optimization, and adaptive mapping techniques.

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

</details>


### [4] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: One-class autoencoder framework authenticates historical drawings using handcrafted features, achieving 83.3% true acceptance with 9.5% false acceptance across 900 verification trials.


<details>
  <summary>Details</summary>
Motivation: Authentication of historical drawings is challenging due to small reference collections and subtle stylistic variations expressed through line work and limited tonal range.

Method: Verification-based framework using one-class autoencoders trained on handcrafted features (Fourier-domain energy, Shannon entropy, global contrast, GLCM homogeneity, fractal complexity) from authenticated sketches across multiple museum collections.

Result: Pooled system achieves 83.3% True Acceptance Rate with 9.5% False Acceptance Rate across 900 trials (90 genuine, 810 impostor). Performance varies by artist, with some verifiers showing near-zero false acceptance.

Conclusion: The computational framework provides reproducible quantitative evidence to complement traditional connoisseurship, particularly useful in data-scarce historical sketch attribution settings.

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

</details>


### [5] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: SLT compresses FreeFlow's 28-layer Transformer into a single shared DiT block (675M→4.3M params) via distillation, enabling rapid noise screening to select better initial points for teacher model, improving one-step generation quality and stability.


<details>
  <summary>Details</summary>
Motivation: To address quality fluctuations in one-step generation caused by low-quality initial noise, and to compress the large parameter count of FreeFlow models while maintaining generation quality.

Method: Distill FreeFlow's 28-layer Transformer into a single shared DiT block (SLT) by matching teacher's intermediate features at depth patches, fusing representations, and aligning velocity prediction. Use SLT to rapidly screen noise candidates and select high-quality initial points for teacher model.

Result: Compressed model from 675M to 4.3M parameters. Within time comparable to two teacher samplings, performs 100+ noise screenings to select better initial points, effectively avoiding quality fluctuations and improving generation stability and average quality.

Conclusion: SLT enables efficient noise screening for one-step generation models, substantially improving stability and quality while dramatically reducing parameter count, making high-quality one-step generation more practical and reliable.

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

</details>


### [6] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: CCPO is a policy optimization framework for multi-turn GUI agents that combines visual compression with policy learning to reduce context inflation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Multi-turn GUI agents suffer from severe context inflation as interaction history accumulates, forcing trade-offs between long-term context (via truncation) and spatial structure (via token pruning).

Method: CCPO introduces Coordinate-Aware Spatial Compression (CASC) that aggregates coordinates from multiple rollouts to identify target-relevant regions and progressively narrows historical attention around key visual areas. It also uses a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness.

Result: Achieves state-of-the-art performance across four benchmarks with up to 55% token compression and 3.8× training speedup.

Conclusion: CCPO effectively addresses context inflation in multi-turn GUI agents by coupling visual compression with policy optimization, enabling efficient attention to informative regions while maintaining task performance.

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

</details>


### [7] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: KG-ViP is a framework that fuses scene graphs and commonsense graphs to address knowledge hallucination and insufficient visual perception in MLLMs for VQA.


<details>
  <summary>Details</summary>
Motivation: MLLMs for VQA suffer from two key limitations: knowledge hallucination (making up facts) and insufficient fine-grained visual perception. While commonsense graphs can address the former with external knowledge, and scene graphs can address the latter with detailed visual structure, prior works treat them separately, missing their synergistic potential.

Method: KG-ViP proposes a unified framework with a novel retrieval-and-fusion pipeline that uses the query as a semantic bridge to progressively integrate both scene graphs and commonsense graphs, synthesizing a unified structured context for reliable multi-modal reasoning.

Result: Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

Conclusion: The fusion of scene graphs and commonsense graphs through KG-ViP's unified framework effectively addresses the dual limitations of MLLMs in VQA, leveraging their complementary strengths for improved visual question answering performance.

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

</details>


### [8] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: ViEBench is a process-verifiable benchmark for evaluating faithful visual reasoning in VLMs, using 200 high-resolution images with expert-annotated evidence and a dual-axis matrix for fine-grained diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks rely on outcome-oriented accuracy and cannot assess whether VLMs accurately leverage fine-grained visual cues for multi-step reasoning, lacking evaluation of authentic reasoning processes.

Method: Created ViEBench with 200 multi-scenario high-resolution images with expert-annotated visual evidence, categorized tasks by difficulty into perception and reasoning dimensions, and introduced a dual-axis matrix with four diagnostic quadrants for comprehensive evaluation.

Result: Experiments revealed that VLMs can produce correct answers despite grounding on irrelevant regions, and may locate correct evidence but fail to use it for accurate conclusions, demonstrating ViEBench's ability to provide explainable evaluation.

Conclusion: ViEBench serves as a more explainable and practical benchmark for comprehensively evaluating the effectiveness of agentic VLMs by enabling transparent diagnosis of model behavior across varying task complexities.

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

</details>


### [9] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: Multimodal LLM agent automatically discovers emerging content issues on short-video platforms, generates annotation policies, and improves governance efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional human-driven discovery of emerging content issues is too slow for rapidly evolving short-video platforms, leading to delayed policy updates and ineffective content governance.

Method: Uses multimodal LLM agents to automatically recall potential new-issue videos, applies two-stage clustering to group them into issues, then generates updated annotation policies from clusters.

Result: Deployed in real system; improves emerging-issue discovery F1 score by over 20%, reduces problematic video views by ~15%, and greatly reduces time costs compared to manual discovery.

Conclusion: Agent-based method significantly accelerates annotation policy iteration and enhances content governance effectiveness on rapidly evolving short-video platforms.

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

</details>


### [10] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: Anon-NET is a unified framework for face video anonymization that de-identifies faces while preserving demographic attributes and expressions through diffusion-based inpainting and video-driven animation.


<details>
  <summary>Details</summary>
Motivation: Face video anonymization is needed for privacy preservation while still enabling analysis in downstream computer vision tasks like expression recognition, people tracking, and action recognition.

Method: Uses diffusion-based generative model for face inpainting guided by high-level attribute recognition and motion-aware expression transfer, followed by video-driven animation that takes de-identified faces and original video as input.

Result: Extensive experiments on VoxCeleb2, CelebV-HQ, and HDTF datasets demonstrate effectiveness in obfuscating identity while maintaining visual realism and temporal consistency across diverse facial dynamics.

Conclusion: Anon-NET provides an effective solution for face video anonymization that balances privacy preservation with retention of important visual attributes for downstream analysis tasks.

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

</details>


### [11] [Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics](https://arxiv.org/abs/2601.11637)
*Aradhya Dixit*

Main category: cs.CV

TL;DR: VLAs struggle with iterative self-correction despite initial task competence, with correction success rates low and diminishing returns after 3 retries, primarily due to Semantic Drift failures.


<details>
  <summary>Details</summary>
Motivation: While multimodal foundation models enable VLAs to decompose visual tasks into executable plans, the quantitative limits and dominant reasoning bottlenecks of iterative self-correction remain poorly characterized, despite recent benchmarks beginning to evaluate this capability.

Method: Introduces a Diagnostic Micro-Benchmark that decouples Task Success Rate from Correction Success Rate, quantifies diminishing returns of correction attempts, and develops a Failure Taxonomy to identify reasoning bottlenecks like Semantic Drift.

Result: Initial task competence (62% TSR) doesn't predict repair ability (25-33% CSR), correction effectiveness saturates after three retries, and Semantic Drift accounts for ~28% of failures as a major reasoning bottleneck.

Conclusion: The benchmark provides a reproducible framework to isolate reasoning bottlenecks like Semantic Drift, enabling progress toward more stateful and trustworthy multimodal agents by addressing contextual state loss during iterative correction.

Abstract: Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.

</details>


### [12] [Confident Learning for Object Detection under Model Constraints](https://arxiv.org/abs/2601.11640)
*Yingda Yu,Jiaqi Xuan,Shuhui Shi,Xuanyu Teng,Shuyang Xu,Guanchao Tong*

Main category: cs.CV

TL;DR: MDDC framework improves weed detection on edge devices by systematically fixing data quality issues instead of scaling models, achieving 5-25% mAP gains with fixed lightweight YOLOv8n.


<details>
  <summary>Details</summary>
Motivation: Edge devices for agricultural weed detection have strict constraints on model capacity, computation, and latency that prevent performance improvements through traditional methods like model scaling or ensembling.

Method: Proposes Model-Driven Data Correction (MDDC) - a data-centric framework with automated error analysis categorizing failures into four types (false negatives, false positives, class confusion, localization errors), then uses structured train-fix-retrain pipeline with version-controlled data management.

Result: Experimental results on multiple weed detection datasets show consistent improvements of 5-25% in mAP at 0.5 using a fixed lightweight detector (YOLOv8n).

Conclusion: Systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints, offering a practical alternative to model scaling for edge deployment.

Abstract: Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

</details>


### [13] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: MOD-DiT: A sampling-free dynamic attention framework for efficient video generation that overcomes quadratic complexity of self-attention through mixture-of-distribution modeling and online block masking.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers for video generation suffer from quadratic complexity of self-attention, making practical deployment difficult. Existing sparse attention methods either use oversimplified static patterns or require expensive sampling operations, leading to inaccurate predictions and degraded quality.

Method: Two-stage approach: 1) Uses prior information from early denoising steps with distributed mixing to model linear approximation for predicting mask patterns, 2) Implements online block masking strategy that dynamically applies predicted masks while maintaining historical sparsity information, eliminating repetitive sampling.

Result: Extensive evaluations show consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating effectiveness for efficient, high-quality video generation.

Conclusion: MOD-DiT successfully overcomes computational limitations of traditional sparse attention approaches, enabling efficient and high-quality video generation without the quadratic complexity burden of standard self-attention mechanisms.

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [14] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: Researchers developed a physics-based synthetic simulation framework to generate controllable knee X-ray scans for osteoarthritis assessment, addressing privacy and data scarcity issues in AI/radiomics research.


<details>
  <summary>Details</summary>
Motivation: Knee osteoarthritis assessment relies on subjective radiographic grading (KL scale), while AI/radiomics approaches need large annotated datasets that are difficult to obtain due to privacy, governance, and resource constraints.

Method: Created a physics-based synthetic simulation framework (PSSF) - a 2D X-ray projection simulator of anteroposterior knee radiographs from parametric anatomical models. Generated virtual cohort of 180 subjects (260 knees) with three imaging protocols. Used IBSI for feature extraction and trained three ML models (logistic regression, random forest, gradient boosting) for binary and three-class OA prediction.

Result: Successfully generated synthetic X-ray scans without patient involvement. Assessed model robustness across IBSI protocol, cross-protocol, and multi-protocol scenarios. Evaluated feature stability using intraclass correlation coefficients across acquisition changes.

Conclusion: The physics-based synthetic simulation framework provides a privacy-preserving solution for generating controllable X-ray datasets, enabling OA assessment research while addressing data scarcity and privacy constraints.

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [15] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: Vision-based confidence estimation framework improves spatial reasoning reliability in VLMs by 34% over text-based methods using geometric verification from object detection.


<details>
  <summary>Details</summary>
Motivation: VLMs show poor spatial reasoning (49-54% accuracy) despite strong multimodal capabilities, creating safety risks for robotics/autonomous systems that need to know when to trust VLM predictions.

Method: Proposes vision-based confidence estimation using independent geometric verification from object detection, fusing four signals via gradient boosting: geometric alignment, spatial ambiguity, detection quality, and VLM internal uncertainty.

Result: Achieves 0.674 AUROC on BLIP-2 (34% improvement) and 0.583 AUROC on CLIP (16.1% improvement); enables 2.2x coverage at 60% target accuracy; improves scene graph precision from 52.1% to 78.3% while retaining 68.2% edges.

Conclusion: External geometric verification significantly outperforms VLM self-assessment (87.4% vs 12.7% importance), enabling reliable deployment of VLMs in safety-critical applications through selective prediction.

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [16] [IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation](https://arxiv.org/abs/2601.11645)
*Ujjwal Jain,Oshin Misra,Roshni Chakraborty,Mahua Bhattacharya*

Main category: cs.CV

TL;DR: IMSAHLO: A novel deep learning framework for robust neuronal cell segmentation in fluorescence microscopy using multi-scale attention and hybrid loss optimization to handle dense/sparse cells, complex morphologies, and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Neuronal cell segmentation in fluorescence microscopy is challenging due to densely packed vs. sparse cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models fail to preserve fine topological details and accurate boundaries under these conditions.

Method: IMSAHLO framework features: 1) Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, 2) Hierarchical Attention mechanism to focus on salient morphological features and preserve ROI boundaries, 3) Hybrid loss combining Tversky+Focal loss for class imbalance, topology-aware Centerline Dice loss, and Contour-Weighted Boundary loss for topological continuity and precise cell separation.

Result: Outperforms state-of-the-art on Fluorescent Neuronal Cells dataset: 81.4% precision, 82.7% macro F1, 83.3% micro F1, 99.5% balanced accuracy on difficult dense/sparse cases. Ablation studies validate synergistic benefits of multi-scale attention and hybrid loss terms.

Conclusion: Establishes foundation for generalizable segmentation models applicable to wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

</details>


### [17] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: T2I generative AI models systematically associate facial attractiveness with positive attributes and show gender bias in classification tasks, revealing algorithmic lookism that compounds inequalities.


<details>
  <summary>Details</summary>
Motivation: To examine algorithmic lookism - systematic preferential treatment based on physical appearance - in text-to-image generative AI and downstream gender classification tasks, revealing how AI systems encode and amplify social biases.

Method: Analyzed 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, examining associations between facial attractiveness and attributes, and testing gender classification algorithms on these generated faces.

Result: Found systematic associations between attractiveness and positive attributes in T2I models, significant gender bias in classification (women's faces with negative attributes had higher misclassification), and intensifying aesthetic constraints in newer models through age homogenization and geographic reductionism.

Conclusion: Algorithmic lookism operates as systematic infrastructure across AI vision systems, compounding inequalities through both representation (in generative models) and recognition (in classification systems), revealing convergent patterns of bias that mirror socially constructed stereotypes.

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [18] [PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation](https://arxiv.org/abs/2601.11654)
*Kaustubh Shivshankar Shejole,Gaurav Mishra*

Main category: cs.CV

TL;DR: Proposes PSSI-MaxST: a graph-based interactive segmentation method using Pixel Segment Similarity Index (PSSI) for edge weights, MeanShift for low-level segmentation, and Maximum Spanning Tree (MaxST) for partitioning, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing interactive graph-based segmentation methods have issues with high computational costs, sensitivity to user interactions, and poor performance when foreground/background have similar colors. The similarity measure for edge weights is crucial for performance.

Method: 1) Proposes Pixel Segment Similarity Index (PSSI) using harmonic mean of inter-channel similarities with pixel intensity and spatial smoothness features. 2) Uses MeanShift for low-level segmentation capturing color, texture, and shape. 3) Constructs pixel-segment graph with PSSI edge weights. 4) Employs Maximum Spanning Tree (MaxST) for partitioning to capture strong local connectivity.

Result: Outperforms current graph-based methods (AMOE, OneCut, SSNCut) on GrabCut and Images250 datasets in terms of Jaccard Index (IoU), F1 score, execution time, and Mean Error (ME). PSSI has O(B) computational complexity where B is number of histogram bins.

Conclusion: The proposed PSSI-MaxST method effectively addresses limitations of existing interactive segmentation approaches by combining PSSI similarity measure, MeanShift segmentation, and MaxST partitioning to jointly capture color similarity, smoothness, texture, shape, and local connectivity.

Abstract: Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.

</details>


### [19] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: MBU-Net achieves near full-precision accuracy with 2.04x speedup and 3.54x energy reduction over 16-bit U-Net through masked binary quantization and GPU optimization.


<details>
  <summary>Details</summary>
Motivation: Real-time image segmentation for AR/VR, robotics, drones, and autonomous systems requires meeting tight accuracy, latency, and energy budgets on resource-constrained edge devices. While U-Net offers good accuracy-efficiency balance, achieving real-time performance on high-resolution inputs remains challenging due to compute, memory, and power limits.

Method: Masked Binary U-Net (MBU-Net) uses a cost-aware masking strategy that prioritizes masking where it yields highest accuracy-per-cost, reconciling accuracy with near-binary efficiency. A GPU execution framework maps MBU-Net to Tensor Cores via subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations using native binary Tensor Core BMMA instructions.

Result: Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

Conclusion: MBU-Net successfully addresses the challenges of extreme quantization for real-time segmentation by combining masked binary weights with efficient GPU implementation, achieving practical performance gains on widely available hardware.

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [20] [LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions](https://arxiv.org/abs/2601.11662)
*Abdullah Jirjees,Ryan Myers,Muhammad Haris Ikram,Mohamed H. Zaki*

Main category: cs.CV

TL;DR: Lightweight thermal-only YOLO model (LTV-YOLO) detects young pedestrians in low-light/weather using LWIR cameras, optimized for edge devices with depthwise separable convolutions and FPN.


<details>
  <summary>Details</summary>
Motivation: Detecting vulnerable road users (children/adolescents) in challenging conditions (low light, adverse weather) where traditional RGB cameras fail, to improve pedestrian safety in transportation systems.

Method: Based on YOLO11 architecture, customized for thermal detection using LWIR cameras. Integrates depthwise separable convolutions and feature pyramid network (FPN) for computational efficiency and handling small/occluded targets.

Result: LTV-YOLO achieves strong performance detecting small-scale, partially occluded, thermally distinct VRUs while maintaining compact architecture suitable for real-time edge deployment.

Conclusion: Provides a practical, scalable thermal-only solution optimized specifically for young/small VRUs under adverse conditions, contributing to pedestrian safety in intelligent transportation systems.

Abstract: Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.

</details>


### [21] [UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM](https://arxiv.org/abs/2601.11665)
*Amir Farzin Nikkhah,Dong Chen,Bradford Campbell,Somayeh Asadi,Arsalan Heydarian*

Main category: cs.CV

TL;DR: This review paper synthesizes over 150 studies on UAV applications in AEC+FM infrastructure inspections, covering data acquisition, photogrammetric modeling, defect detection, and decision support, while proposing a multimodal fusion framework and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: UAVs are transforming infrastructure inspections in AEC+FM, but challenges remain in real-time processing, multimodal data fusion, and generalizability that need to be addressed to improve inspection accuracy and reliability.

Method: Literature synthesis of 150+ studies, development of a proposed workflow framework integrating RGB imagery, LiDAR, and thermal sensing with transformer-based architectures, and case study validation.

Result: UAVs demonstrate value in structural health monitoring, disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation, with innovations in path optimization, thermal integration, and ML models like YOLO and Faster R-CNN for anomaly detection.

Conclusion: Future research should focus on lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections, with the proposed framework providing a comprehensive guide to address current challenges.

Abstract: Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.

</details>


### [22] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: MATEX is a new framework that improves interpretability in medical vision-language models using anatomically informed spatial reasoning, combining multi-layer attention rollout with text-guided spatial priors to produce precise gradient attribution maps.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior interpretability methods in medical AI, including spatial imprecision, lack of anatomical grounding, and limited attention granularity, which hinder trust and transparency in radiological applications.

Method: Combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to generate precise, stable, and clinically meaningful gradient attribution maps for medical vision-language models.

Result: Outperforms state-of-the-art M2IB approach on MS-CXR dataset in both spatial precision and alignment with expert-annotated findings.

Conclusion: MATEX enhances trust and transparency in radiological AI applications by providing more faithful and interpretable model explanations through anatomically informed spatial reasoning.

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [23] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: MetamerGen is a latent diffusion model that generates scene images combining high-resolution fixation information with low-resolution peripheral gist, creating metamers aligned with human scene representations.


<details>
  <summary>Details</summary>
Motivation: Human vision combines low-resolution peripheral gist with high-resolution fixation information to understand scenes, but current models don't capture this foveated processing. The goal is to generate images that match human internal scene representations.

Method: Uses a dual-stream latent diffusion model with DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded context features. Evaluated through same-different behavioral experiments comparing generated vs. original images.

Result: Generated images that function as metamers for human scene representations. High-level semantic alignment best predicts metamerism when conditioned on viewers' own fixations, though metamers can be generated even with random fixations.

Conclusion: MetamerGen is a powerful tool for understanding human scene perception, revealing specific visual processing features that contribute to scene judgments and demonstrating alignment between generated images and latent human representations.

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [24] [Conformal Point and the Calibrated Conic](https://arxiv.org/abs/2601.11679)
*Richard Hartley*

Main category: cs.CV

TL;DR: Paper introduces conformal point and calibrating conic concepts for visualizing image geometry and computing angles/directions intuitively


<details>
  <summary>Details</summary>
Motivation: To provide intuitive geometric concepts for understanding and computing image geometry, particularly angles and directions in images

Method: Introduces and defines the conformal point and calibrating conic concepts, and establishes their relationship for geometric visualization

Result: Develops concepts that enable intuitive visualization of image geometry and provide practical ways to compute geometric properties like angles and directions

Conclusion: Conformal point and calibrating conic are useful geometric concepts that facilitate intuitive understanding and computation of image geometry

Abstract: This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.

</details>


### [25] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: A shallow RNN achieves 98.3% AUC in detecting human vs. synthetic handwriting across 10 datasets and 7 synthesizers, with strong few-shot and out-of-domain performance.


<details>
  <summary>Details</summary>
Motivation: Handwriting movements serve as behavioral biometrics for human verification, framing the task as a reverse Turing test to detect whether input is generated by humans or artificial synthesizers.

Method: Train a shallow recurrent neural network on non-featurized trajectory data from 10 public datasets of handwritten symbols, tested against 7 different synthesizers (including Kinematic Theory, GANs, Transformers, and Diffusion models).

Result: Excellent performance with 98.3% AUC and 1.4% equal error rate on average across all synthesizers and datasets, strong few-shot performance with just 10% training data, and competitive out-of-domain results.

Conclusion: The approach provides an effective layer of security for verifying human presence in computerized systems, demonstrating robust detection of synthetic handwriting across diverse datasets and generation methods.

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [26] [SemAlign: Language Guided Semi-supervised Domain Generalization](https://arxiv.org/abs/2601.11724)
*Muditha Fernando,Kajhanan Kailainathan,Krishnakanth Nagaratnam,Isuranga Udaravi Bandara Senavirathne,Ranga Rodrigo*

Main category: cs.CV

TL;DR: Novel SSDG method uses VLM feature alignment with augmentation/regularization to improve data utilization beyond just pseudo-label accuracy, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing SSDG methods overly focus on pseudo-labeling accuracy without maximizing data utilization, limiting potential performance improvements. Need better approach that promotes domain-invariance while preventing overfitting.

Method: Aligns intermediate model features with semantically rich VLM feature space to promote domain-invariance. Enhanced with image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting.

Result: Achieves state-of-the-art results across four benchmarks against existing SSDG baselines, both qualitatively and quantitatively.

Conclusion: Proposed VLM feature alignment with augmentation/regularization effectively addresses SSDG challenges by improving data utilization and domain-invariance beyond traditional pseudo-labeling focus.

Abstract: Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.

</details>


### [27] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: SpaRRTa benchmark evaluates Visual Foundation Models' spatial reasoning by testing their ability to identify relative object positions in images, revealing significant disparities in their spatial awareness capabilities.


<details>
  <summary>Details</summary>
Motivation: Visual Foundation Models (VFMs) like DINO and CLIP have strong semantic understanding but limited spatial reasoning, which restricts their use in embodied systems. While recent work adds 3D tasks to VFM training, performance remains inconsistent across spatial tasks, raising questions about whether these models truly have spatial awareness or just overfit to specific 3D objectives.

Method: Introduces the Spatial Relation Recognition Task (SpaRRTa) benchmark that generates arbitrary numbers of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Unlike traditional 3D objectives focused on precise metric prediction, SpaRRTa probes fundamental capabilities underpinning human-like spatial understanding.

Result: Evaluation of state-of-the-art VFMs reveals significant disparities in their spatial reasoning abilities. The analysis provides insights into mechanisms that support or hinder spatial awareness in modern VFMs.

Conclusion: SpaRRTa serves as a useful tool for guiding development of future spatially aware visual models by addressing the fundamental question of whether VFMs truly possess spatial awareness beyond overfitting to specific 3D tasks.

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [28] [From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce](https://arxiv.org/abs/2601.11769)
*Cheng Lyu,Jingyue Zhang,Ryan Maunu,Mengwei Li,Vinny DeGenova,Yuanli Pei*

Main category: cs.CV

TL;DR: Proposed taxonomy-decoupled visual search architecture with classification-free region proposals and unified embeddings, plus LLM-as-a-Judge framework for zero-shot evaluation, deployed successfully on global home goods platform.


<details>
  <summary>Details</summary>
Motivation: Existing industrial visual search systems rely on taxonomy-based classification and noisy catalog data for evaluation, limiting robustness and scalability for subjective, open-ended style-driven domains like e-commerce.

Method: 1) Taxonomy-decoupled architecture using classification-free region proposals and unified embeddings for similarity retrieval; 2) LLM-as-a-Judge framework for zero-shot evaluation of visual similarity and category relevance without human annotations.

Result: Deployed at scale on global home goods platform, improving retrieval quality and yielding measurable uplift in customer engagement; offline evaluation metrics strongly correlate with real-world outcomes.

Conclusion: The proposed approach enables more flexible and generalizable visual search by decoupling from rigid taxonomies and using LLM-based evaluation, successfully addressing limitations of existing industrial systems.

Abstract: Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.

</details>


### [29] [studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting](https://arxiv.org/abs/2601.11772)
*Yimu Pan,Hongda Mao,Qingshuang Chen,Yelin Kim*

Main category: cs.CV

TL;DR: studentSplat: A single-view 3D Gaussian splatting method for scene reconstruction that uses teacher-student architecture and extrapolation network to overcome scale ambiguity and missing context problems.


<details>
  <summary>Details</summary>
Motivation: Single-view 3D scene reconstruction remains under-explored due to inherited ambiguity in single-view inputs, while feed-forward 3D Gaussian splatting has advanced multi-view and single-object reconstruction.

Method: 1) Teacher-student architecture where a multi-view teacher model provides geometric supervision to address scale ambiguity; 2) Extrapolation network that completes missing scene context for high-quality extrapolation.

Result: Achieves state-of-the-art single-view novel-view reconstruction quality, comparable performance to multi-view methods at scene level, and competitive performance as self-supervised single-view depth estimation method.

Conclusion: studentSplat demonstrates potential for general single-view 3D understanding tasks by effectively overcoming scale ambiguity and extrapolation challenges inherent in single-view reconstruction.

Abstract: Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.

</details>


### [30] [Cross-Domain Object Detection Using Unsupervised Image Translation](https://arxiv.org/abs/2601.11779)
*Vinicius F. Arruda,Rodrigo F. Berriel,Thiago M. Paixão,Claudine Badue,Alberto F. De Souza,Nicu Sebe,Thiago Oliveira-Santos*

Main category: cs.CV

TL;DR: Proposes using unsupervised image translation (CycleGAN/AdaIN) to generate artificial target domain datasets for training object detectors, achieving simpler implementation and better performance than complex feature alignment methods.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised domain adaptation methods for object detection rely on complex intermediate feature alignment that are hard to implement and interpret, and still have performance gaps compared to training with target data.

Method: Use unsupervised image translators (CycleGAN and AdaIN-based model) to generate artificial datasets in the target domain using only source domain annotations and unlabeled target data, then train object detectors on this generated data.

Result: Significant improvements in real-world autonomous driving scenarios, outperforming state-of-the-art methods in most cases and closing the performance gap toward the upper-bound (training with target data).

Conclusion: Proposed method offers less complexity, better interpretability, and improved effectiveness compared to existing feature alignment approaches for unsupervised domain adaptation in object detection.

Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.

</details>


### [31] [Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening](https://arxiv.org/abs/2601.11896)
*Ngoc-Khai Hoang,Thi-Nhu-Mai Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: A multimodal deep learning framework using facial expressions, speech, and upper-body movements achieves 95.83% accuracy for binary stroke screening during F.A.S.T. assessments.


<details>
  <summary>Details</summary>
Motivation: Early stroke identification is crucial for timely intervention, especially in prehospital settings where rapid screening can significantly improve patient outcomes.

Method: Multimodal deep learning framework integrating facial dynamics (Transformer on landmark features), speech (Audio Spectrogram Transformer on mel spectrograms), and upper-body movements (MLP-Mixer on pose sequences) with attention-based fusion.

Result: Achieved 95.83% accuracy and 96.00% F1-score on self-collected dataset of 222 videos from 37 subjects, outperforming unimodal baselines and detecting all stroke cases in test set.

Conclusion: Multimodal learning shows strong potential for early stroke screening but requires larger, clinically representative datasets for reliable real-world deployment.

Abstract: Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.

</details>


### [32] [RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection](https://arxiv.org/abs/2601.11898)
*Yilmaz Korkmaz,Vishal M. Patel*

Main category: cs.CV

TL;DR: RemoteVAR introduces a visual autoregressive model framework for remote sensing change detection that addresses limitations of VARs in dense prediction tasks through multi-resolution feature conditioning and specialized training.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive models (VARs) have shown strong image generation capabilities but struggle with pixel-level discriminative tasks due to weak controllability, suboptimal dense prediction performance, and exposure bias. The authors aim to adapt VARs for remote sensing change detection to leverage their strengths while overcoming these limitations.

Method: RemoteVAR conditions autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention and employs an autoregressive training strategy specifically designed for change map prediction.

Result: Extensive experiments on standard change detection benchmarks show RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative.

Conclusion: RemoteVAR successfully adapts visual autoregressive models for remote sensing change detection, overcoming previous limitations and providing a competitive alternative to existing methods.

Abstract: Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\underline{here}}.

</details>


### [33] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: Dual-task EfficientNetB4 model achieves 96% classification and 90% threat prediction accuracy for airborne objects using new AODTA dataset.


<details>
  <summary>Details</summary>
Motivation: Need for automated real-time threat assessment systems due to proliferation of airborne platforms (aircraft, drones, UAVs), as current manual monitoring approaches lack scalability and efficiency.

Method: Developed dual-task model based on EfficientNetB4 for simultaneous airborne object classification and threat-level prediction. Created AODTA Dataset by aggregating/refining multiple public sources to address data scarcity. Benchmarked against AVD Dataset and ResNet-50 baseline.

Result: EfficientNetB4 achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, outperforming ResNet-50 baseline. Note: Study focuses on classification/threat inference using pre-localized images, not detection.

Conclusion: EfficientNetB4-based dual-task model shows strong promise for surveillance, defense, and airspace management applications, though limited to classification/threat inference rather than detection.

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [34] [Effects of the retina-inspired light intensity encoding on color discrimination performance](https://arxiv.org/abs/2601.11909)
*Io Yamada,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: The study compares light intensity encoding functions (logarithmic vs Naka-Rushton) in center/surround retinex models for color constancy, finding Naka-Rushton with double opponent color representation works best.


<details>
  <summary>Details</summary>
Motivation: Color constancy is crucial for reliable object recognition since illumination color affects perceived color. The study aims to improve color constancy in vision systems by investigating better light intensity encoding functions in biological vision-inspired models.

Method: Compared two light intensity encoding functions (logarithmic vs Naka-Rushton) in center/surround retinex models. Used color-variable LEDs to illuminate targets with various lighting colors, evaluated color discrimination using HSV and opponent color representations.

Result: The Naka-Rushton function combined with double opponent color plane representation provided superior discrimination performance for color constancy compared to the original logarithmic function.

Conclusion: Using the Naka-Rushton function (modeling retinal photoreceptor response) with double opponent color representation significantly improves color constancy performance in center/surround retinex models, offering better illumination-independent color perception.

Abstract: Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.

</details>


### [35] [A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection](https://arxiv.org/abs/2601.11910)
*Guiying Zhu,Bowen Yang,Yin Zhuang,Tong Zhang,Guanqun Wang,Zhihao Che,He Chen,Lianlin Li*

Main category: cs.CV

TL;DR: GW-VLM is a training-free approach for Open-Vocabulary Object Detection that uses a "guess what" game between Vision Language Models and Large Language Models with multi-scale visual-language alignment and contextual concept prompts.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models have impressive zero-shot capabilities for OVOD, but they lack universal understanding for any object cognition according to already pretrained models. The paper aims to address this oversight by creating a universal understanding paradigm without additional training.

Method: Proposes GW-VLM with two key components: 1) Multi-Scale Visual Language Searching (MS-VLS) for multi-scale visual-language soft-alignment using VLMs on class-agnostic object detection results, and 2) Contextual Concept Prompt (CCP) that forms concept flow from MS-VLS to help LLMs understand snippets for OVOD. The approach engages pretrained VLMs and LLMs in a "guess what" game without any training.

Result: Extensive experiments on natural (COCO val, Pascal VOC) and remote sensing (DIOR, NWPU-10) datasets show GW-VLM achieves superior OVOD performance compared to state-of-the-art methods without any training steps.

Conclusion: GW-VLM successfully creates a universal understanding paradigm for OVOD by leveraging pretrained foundation models through a training-free "guess what" game approach, demonstrating that effective object detection can be achieved without additional model training.

Abstract: Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.

</details>


### [36] [Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh](https://arxiv.org/abs/2601.11911)
*Muhammad Ibrahim,Alfe Suny,MD Sakib Ul Islam,Md. Imran Hossain*

Main category: cs.CV

TL;DR: Compact CNN achieves high accuracy on diverse Bangladeshi image datasets with efficient training and low computational cost.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs can be complex and prone to overfitting on small datasets, while there's a need for effective image classification solutions for real-world applications in developing regions like Bangladesh.

Method: Evaluated a compact convolutional neural network architecture across five publicly available Bangladeshi image datasets covering urban encroachment, vehicle detection, road damage, and agricultural crops.

Result: The model demonstrated high classification accuracy, efficient convergence, low computational overhead, and robust generalization across diverse scenarios as shown through quantitative metrics and saliency analyses.

Conclusion: Streamlined CNN architectures are well-suited for small-class image classification tasks, offering effective feature capture and generalization while maintaining computational efficiency.

Abstract: Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.

</details>


### [37] [From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection](https://arxiv.org/abs/2601.11915)
*Chi Wang,Xinjue Hu,Boyu Wang,Ziwen He,Zhangjie Fu*

Main category: cs.CV

TL;DR: A novel intervention paradigm that models spurious correlations as a low-rank subspace and removes it from representations to improve face forgery detection generalization.


<details>
  <summary>Details</summary>
Motivation: Face forgery detection suffers from poor generalization due to spurious correlations (forgery-irrelevant information) that create biased learning. Previous methods address specific spurious correlations individually, but this is impractical since spurious correlations arise from unobservable confounding factors.

Method: Proposes an intervention paradigm that uniformly models spurious correlations as a low-rank subspace. Uses orthogonal low-rank projection to decompose spurious correlation features into this subspace, removes it from original representations, and trains the orthogonal complement to capture authentic forgery-related features.

Result: Achieves state-of-the-art performance across several benchmarks with only 0.43M trainable parameters, demonstrating excellent robustness and generalization capabilities.

Conclusion: The proposed low-rank subspace intervention effectively eliminates spurious correlation factors, ensuring classification decisions are based on authentic forgery cues rather than biased correlations, providing a practical solution to the generalization problem in face forgery detection.

Abstract: The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.

</details>


### [38] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: Using Gabor filters (inspired by visual nervous system) as CNN preprocessing improves generalization and reduces model size for edge device robot vision with limited training data.


<details>
  <summary>Details</summary>
Motivation: Edge devices need small, efficient CNNs for robot vision, but training data is often limited to specific conditions. The visual nervous system learns effectively from few visual experiences, suggesting biological inspiration could help.

Method: Used Gabor filters (modeling visual nervous system feature extraction) as CNN preprocessing. Created dataset with images from different camera positions. Trained CNNs with limited data (images from specific distances) and tested generalization to other conditions. Compared architectures with/without Gabor preprocessing.

Result: Gabor filter preprocessing improved CNN generalization performance and contributed to reducing CNN size, making models more suitable for edge device deployment.

Conclusion: Biologically-inspired preprocessing with Gabor filters is effective for improving CNN generalization and reducing model size when training data is limited, addressing key challenges for edge-based robot vision applications.

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [39] [SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM](https://arxiv.org/abs/2601.11930)
*Xulei Shi,Maoyu Wang,Yuning Peng,Guanbo Wang,Xin Wang,Qi Chen,Pengjie Tao*

Main category: cs.CV

TL;DR: SupScene learns global descriptors for finding overlapping image pairs in SfM using subgraph-based training and DiVLAD aggregator with attention maps.


<details>
  <summary>Details</summary>
Motivation: Existing image retrieval methods for SfM focus on semantic similarity rather than geometric matchability, and most deep learning methods fail to capture the nuance of overlapping vs. non-overlapping pairs.

Method: 1) Subgraph-based training strategy using ground-truth geometric overlapping relationships with various weights via soft supervised contrastive loss. 2) DiVLAD aggregator that leverages multi-head attention maps from ViT's last block. 3) Learnable gating mechanism to adaptively combine semantic cues with visual features.

Result: Achieves state-of-the-art performance on GL3D dataset, significantly outperforming NetVLAD while adding negligible trainable parameters. Training strategy brings consistent gains across different aggregation techniques.

Conclusion: SupScene effectively learns discriminative global descriptors for finding geometrically overlapping image pairs in SfM, addressing the limitations of existing methods that focus on semantic similarity rather than geometric matchability.

Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.

</details>


### [40] [Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition](https://arxiv.org/abs/2601.11931)
*Zhengxian Wu,Chuanrui Zhang,Shenao Jiang,Hangrui Xu,Zirui Liao,Luyuan Zhang,Huaqiu Li,Peng Jiao,Haoqian Wang*

Main category: cs.CV

TL;DR: LMGait: Language-guided motion-aware gait recognition framework using gait-related language cues to capture key motion features and address overfitting on static noise.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods overfit on static noise (e.g., clothing) and fail to effectively capture dynamic motion regions due to complex architectures that directly extract features from images and use pooling operations for sequence-level representations.

Method: Proposes LMGait framework that utilizes designed gait-related language cues to capture key motion features in gait sequences, focusing on dynamic motion regions rather than static noise.

Result: Not specified in the provided abstract excerpt (only partial abstract provided).

Conclusion: Not specified in the provided abstract excerpt (only partial abstract provided).

Abstract: Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.

</details>


### [41] [Deep learning-based neurodevelopmental assessment in preterm infants](https://arxiv.org/abs/2601.11944)
*Lexin Ren,Jiamiao Lu,Weichuan Zhang,Benqing Wu,Tuo Wang,Yi Liao,Jiapan Guo,Changming Sun,Liang Guo*

Main category: cs.CV

TL;DR: Proposed Hierarchical Dense Attention Network for 3D MRI segmentation of white and gray matter in preterm infants, addressing isointense tissue challenge with attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Preterm infants have elevated neurodevelopmental risks requiring early identification. Current MRI segmentation struggles with isointense white/gray matter differentiation in preterm brains due to similar signal intensities during early development.

Method: Hierarchical Dense Attention Network with 3D spatial-channel attention mechanism and attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric MRI data.

Result: Superior segmentation performance compared to state-of-the-art baselines, effectively tackling isointense tissue differentiation. Confirmed significantly lower WM and GM volumes in preterm vs term infants.

Conclusion: The proposed network successfully addresses the challenge of isointense tissue segmentation in preterm infants, providing imaging evidence of neurodevelopmental delays and enabling better assessment tools.

Abstract: Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.

</details>


### [42] [Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2601.11952)
*Haonan An,Guang Hua,Wei Du,Hangcheng Cao,Yihang Tao,Guowen Xu,Susanto Rahardja,Yuguang Fang*

Main category: cs.CV

TL;DR: Proposes Decoder Gradient Shields (DGS) to defend against watermark removal attacks targeting decoders in box-free model watermarking systems.


<details>
  <summary>Details</summary>
Motivation: Existing box-free watermarking research focuses on encoder robustness but overlooks decoder vulnerabilities, allowing attackers to use query responses to obtain gradients and train watermark removers.

Method: Introduces DGS family: DGS-O (output), DGS-I (input), and DGS-L (layers) with closed-form solution for DGS-O. Uses joint reorienting and rescaling of gradients from watermark channel gradient leaking queries to prevent training convergence of watermark removers while preserving image quality.

Result: Achieves 100% defense success rate in deraining and image generation tasks with state-of-the-art box-free watermarking under all experimental settings.

Conclusion: DGS effectively protects decoders in box-free watermarking systems against gradient-based attacks while maintaining decoder output quality, addressing a critical vulnerability in current watermarking approaches.

Abstract: Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.

</details>


### [43] [Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms](https://arxiv.org/abs/2601.11970)
*S. M. Khalid Bin Zahid,Md. Rakibul Hasan Nishat,Abdul Hasib,Md. Rakibul Hasan,Md. Ashiqussalehin,Md. Sahadat Hossen Sajib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: Real-time multi-modal vision framework with adaptive scheduling for edge devices reduces computational load by 65% while maintaining high accuracy across object detection, face recognition, and emotion analysis.


<details>
  <summary>Details</summary>
Motivation: Current surveillance systems lack unified adaptive runtime schedulers that dynamically allocate computational resources based on context, limiting holistic understanding and efficiency on low-power edge devices.

Method: Integrated pipeline with YOLOv8n for object detection, custom FaceNet-based embedding for facial recognition, and DeepFace's CNN for emotion classification, deployed on Raspberry Pi 5 with adaptive scheduling mechanism.

Result: 65% computational load reduction vs continuous processing; object detection AP 0.861; facial recognition 88% accuracy; emotion detection AUC up to 0.97; operates at 5.6 FPS.

Conclusion: Context-aware scheduling enables complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

Abstract: Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

</details>


### [44] [AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering](https://arxiv.org/abs/2601.11976)
*Zongmin Li,Yachuan Li,Lei Kang,Dimosthenis Karatzas,Wenkang Ma*

Main category: cs.CV

TL;DR: AVIR framework reduces MP-DocVQA computational cost by 70% using adaptive page retrieval and clustering, achieving 84.58% ANLS without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: MP-DocVQA is challenging due to computational strain from long documents and reduced attention mechanism effectiveness in LVLMs when processing many pages.

Method: Adaptive Visual In-document Retrieval (AVIR) uses lightweight retrieval to score page relevance, clusters pages by score distribution, screens with Top-K, and uses probability threshold for short documents, feeding only selected pages to frozen LVLM.

Result: Reduces average page count by 70%, achieves 84.58% ANLS on MP-DocVQA dataset, outperforms previous methods with lower computational cost, and shows effectiveness on SlideVQA and DUDE benchmarks.

Conclusion: AVIR effectively addresses MP-DocVQA challenges through adaptive page selection, significantly reducing computational requirements while maintaining high performance without model fine-tuning.

Abstract: Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.

</details>


### [45] [Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https://arxiv.org/abs/2601.11981)
*Jian Lang,Rongpei Hong,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: RADAR is a test-time adaptation framework for fake news video detection that handles unseen topics through retrieval-guided adaptation, stable anchor alignment, and target-aware self-training.


<details>
  <summary>Details</summary>
Motivation: Existing fake news video detection methods fail with emerging events and unseen topics because they assume consistent news topic distribution between training and test phases. There's a need for systems that can adapt to new topics during testing.

Method: RADAR introduces a retrieval-guided adaptation paradigm with three key components: 1) Entropy Selection-Based Retrieval to find stable (low-entropy) reference videos, 2) Stable Anchor-Guided Alignment to align unstable instances with source domain via distribution matching, and 3) Target-Domain Aware Self-Training that generates pseudo-labels augmented by stable references to handle imbalanced distributions.

Result: Extensive experiments show RADAR achieves superior performance for test-time fake news video detection, enabling strong on-the-fly adaptation to unseen fake news video topics.

Conclusion: RADAR successfully bridges the gap in fake news video detection by enabling test-time adaptation to unseen topics through its novel retrieval-guided paradigm, making it the first framework capable of adapting to emerging events during testing.

Abstract: Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.

</details>


### [46] [An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System](https://arxiv.org/abs/2601.11983)
*Md. Asiful Islam,Abdul Hasib,Tousif Mahmud Emon,Khandaker Tabin Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: AI-IoT smart wheelchair with gesture control, obstacle avoidance, and health monitoring achieves high accuracy rates for affordable assistive mobility.


<details>
  <summary>Details</summary>
Motivation: Addressing the growing need for affordable, intelligent wheelchairs for differently-abled and elderly individuals, as traditional wheelchairs lack dynamic features and existing smart alternatives are costly, single-modality, and limited in health integration.

Method: Comprehensive AI-IoT system with glove-based gesture control for navigation, YOLOv8 object detection with auditory feedback for obstacle avoidance, ultrasonic sensors for immediate collision prevention, and continuous vital sign monitoring (heart rate, SpO2, ECG, temperature) with ThingSpeak cloud integration and email alerts.

Result: Gesture control achieved 95.5% success rate, ultrasonic obstacle detection reached 94% accuracy, and YOLOv8 object detection delivered 91.5% Precision, 90.2% Recall, and 90.8% F1-score. System provides practical, scalable, affordable solution.

Conclusion: The integrated multi-modal approach bridges innovative research with real-world deployment, significantly enhancing user autonomy, safety, and independence through affordable assistive technology.

Abstract: The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\% success rate, ultrasonic obstacle detection reached 94\% accuracy, and YOLOv8-based object detection delivered 91.5\% Precision, 90.2\% Recall, and a 90.8\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.

</details>


### [47] [Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis](https://arxiv.org/abs/2601.11987)
*Khaled Berkani*

Main category: cs.CV

TL;DR: A graph reasoning framework with anatomical priors for explainable medical diagnosis, using structural propagation instead of generic message passing.


<details>
  <summary>Details</summary>
Motivation: To create explainable vision-based diagnosis systems that incorporate explicit anatomical priors for structured reasoning, moving beyond black-box models.

Method: Converts convolutional feature maps to patch-level graphs with appearance/spatial nodes, uses custom structural propagation modeling spatial relations, enabling joint node-level lesion prediction and graph-level diagnostic reasoning.

Result: Demonstrated through chest X-ray case study showing how structural priors guide relational reasoning and improve interpretability with intrinsic explainability via learned node importance scores.

Conclusion: The framework is domain-agnostic and contributes to graph-based reasoning for structure-aware, explainable learning across AI systems.

Abstract: We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.

</details>


### [48] [DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset](https://arxiv.org/abs/2601.11990)
*Yiming Li,Chen Cai,Tianyi Liu,Dan Lin,Wenqian Wang,Wenfei Liang,Bingbing Li,Kim-Hui Yap*

Main category: cs.CV

TL;DR: DAOS dataset with 9,787 video clips annotated with 36 driver actions and 15 object classes, plus AOR-Net model using multi-level reasoning and chain-of-action prompting for driver action recognition.


<details>
  <summary>Details</summary>
Motivation: Existing driver-monitoring datasets lack accurate object-location annotations or don't link objects to actions, creating a gap for reliable action recognition since driver movements are limited to upper body and often look similar without object context.

Method: Introduce DAOS dataset with multi-modal, multi-view data (RGB, IR, depth) from 4 perspectives. Propose AOR-Net with multi-level reasoning, chain-of-action prompting mechanism to model logical relationships among actions, objects, and relations, plus Mixture of Thoughts module for dynamic knowledge selection.

Result: Extensive experiments show AOR-Net outperforms state-of-the-art methods on various datasets, demonstrating effectiveness in handling object-rich and object-scarce conditions for driver action recognition.

Conclusion: The DAOS dataset and AOR-Net framework address critical gaps in driver action recognition by explicitly modeling human-object relations, enabling more reliable monitoring through comprehensive object-action annotations and sophisticated reasoning mechanisms.

Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.

</details>


### [49] [SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine](https://arxiv.org/abs/2601.12010)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: SMc2f improves autonomous vehicle safety testing by combining vision-language models for coarse filtering with LLM-based fine-grained retrieval and contrastive learning, outperforming previous methods in finding rare safety-critical scenarios.


<details>
  <summary>Details</summary>
Motivation: Current scenario mining methods like RefAV rely on trajectory labels and ignore direct image-text connections, making them dependent on upstream detection/tracking quality and prone to errors in spatial-temporal localization.

Method: Coarse-to-fine pipeline: 1) VLMs for coarse image-text filtering, 2) database of successful cases to few-shot condition LLMs for robust retrieval, 3) text-trajectory contrastive learning to refine matches in shared embedding space.

Result: Experiments on public datasets show substantial improvements in both retrieval quality and efficiency compared to previous methods.

Conclusion: SMc2f addresses limitations of trajectory-based retrieval by leveraging vision-language models and contrastive learning, providing more robust and efficient scenario mining for autonomous vehicle safety validation.

Abstract: The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.

</details>


### [50] [SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture](https://arxiv.org/abs/2601.12015)
*Pavan Kumar Yata,Pediredla Pradeep,Goli Himanish,Swathi M*

Main category: cs.CV

TL;DR: DeepSegFusion is a hybrid deep learning model combining SegNet and DeepLabV3+ with attention-based feature fusion for accurate oil spill segmentation in SAR images, achieving 94.85% accuracy and significantly reducing false alarms compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional threshold-based methods for oil spill detection in satellite images suffer from high false alarm rates due to look-alike phenomena like wind slicks and ship wakes, necessitating more robust and accurate detection methods.

Method: A hybrid deep learning model called DeepSegFusion that integrates SegNet and DeepLabV3+ architectures with an attention-based feature fusion mechanism to improve boundary precision and contextual understanding for oil spill segmentation in SAR images.

Result: The model achieves 94.85% accuracy, 0.5685 IoU, and 0.9330 ROC-AUC on SAR datasets including ALOS PALSAR imagery, with more than 3x fewer false detections (64.4% reduction) compared to baseline models and traditional methods.

Conclusion: DeepSegFusion is a stable model under various marine conditions that can be used for near real-time oil spill monitoring, offering significant improvements over existing approaches for environmental surveillance and maritime safety.

Abstract: Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.

</details>


### [51] [DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering](https://arxiv.org/abs/2601.12020)
*Guillermo Figueroa-Araneda,Iris Diana Jimenez,Florian Hofherr,Manny Ko,Hector Andrade-Loarca,Daniel Cremers*

Main category: cs.CV

TL;DR: DIAMOND-SSS enables high-fidelity translucent material reconstruction from extremely sparse data (as few as 10 images) using diffusion models for data augmentation and illumination-independent geometric priors.


<details>
  <summary>Details</summary>
Motivation: Subsurface scattering effects in translucent materials are challenging to model in neural rendering due to complex light transport and the need for densely captured multi-view, multi-light datasets (often 100+ views and 112 OLATs).

Method: Fine-tune diffusion models for novel-view synthesis and relighting conditioned on estimated geometry, trained on <7% of dataset to produce photorealistic augmentations. Introduce illumination-independent geometric priors: multi-view silhouette consistency loss and multi-view depth consistency loss to stabilize reconstruction under sparse supervision.

Result: Achieves state-of-the-art quality in relightable Gaussian rendering across all sparsity regimes, reducing real capture requirements by up to 90% compared to SSS-3DGS. Can replace up to 95% of missing captures with synthetic augmentations.

Conclusion: DIAMOND-SSS provides a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision, dramatically reducing the data capture requirements for modeling subsurface scattering effects.

Abstract: Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).
  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.
  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.

</details>


### [52] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: FocaLogic is a model-agnostic framework that interprets visual models by identifying minimal visual regions (visual focuses) and translating them into logical expressions, with quantitative metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods for visual models either require white-box access or lack quantitative rigor, limiting their practical application in high-stakes scenarios.

Method: FocaLogic identifies minimal interpretable subsets of visual regions (visual focuses) that decisively influence predictions, translates them into logical expressions, and provides quantitative metrics (precision, recall, divergence) for evaluation.

Result: Empirical analyses show FocaLogic can uncover critical insights like training-induced concentration, improved focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks.

Conclusion: FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models, addressing limitations of existing interpretability methods.

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [53] [A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models](https://arxiv.org/abs/2601.12051)
*Weixin Ye,Wei Wang,Yahui Liu,Yue Song,Bin Ren,Wei Bi,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: MJP framework protects Transformers from gradient attacks by disrupting position embeddings through token shuffling and masking, while improving performance in CV and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers in federated learning are vulnerable to gradient attacks where position embeddings can leak input data, requiring protection while maintaining or improving model performance across vision and language tasks.

Method: Masked Jigsaw Puzzle (MJP) framework: 1) random token shuffling breaks token order, 2) learnable unknown position embeddings mask shuffled tokens' PEs, disrupting local spatial information and forcing models to learn less position-dependent representations.

Result: MJP improves robustness against gradient attacks while boosting performance in image classification (ImageNet-1K) and text sentiment analysis (Yelp, Amazon), serving as a unified framework for different Transformer models.

Conclusion: MJP effectively addresses Transformer vulnerability to gradient attacks by disrupting position embeddings, offering both security improvements and performance gains across diverse vision and language applications.

Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

</details>


### [54] [Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation](https://arxiv.org/abs/2601.12052)
*Zaiyan Zhang,Jie Li,Shaowei Shi,Qiangqiang Yuan*

Main category: cs.CV

TL;DR: TDP-CR is a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation using a Prompt-Guided Fusion mechanism and two-phase training, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Cloud occlusion in optical remote sensing imagery limits downstream utility, and existing cloud removal methods often over-smooth textures and boundaries, creating a mismatch between visually plausible restoration and semantic utility for analysis-ready data.

Method: Proposes TDP-CR with Prompt-Guided Fusion (PGF) mechanism using learnable degradation prompts to encode cloud thickness/spatial uncertainty, adaptively integrating SAR information only where optical data is corrupted. Uses parameter-efficient two-phase training that decouples reconstruction and semantic representation learning.

Result: On LuojiaSET-OSFCR dataset: surpasses SOTA baselines by 0.18 dB in PSNR using only 15% of parameters, achieves 1.4% improvement in mIoU against multi-task competitors, effectively delivering analysis-ready data.

Conclusion: TDP-CR bridges the gap between visually plausible restoration and semantic utility by jointly performing cloud removal and segmentation, providing effective analysis-ready data through task-driven multimodal learning.

Abstract: Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\% of the parameters, and achieves a 1.4\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.

</details>


### [55] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: AUTO-DIP enables optimization-free deep image prior denoising for fluorescence microscopy by transferring optimal network architectures and stopping points from similar images, outperforming baseline DIP and variational methods.


<details>
  <summary>Details</summary>
Motivation: Unsupervised DIP avoids training data needs but requires time-consuming parameter optimization for each new image, limiting its application when processing many images. The authors hypothesize that similar fluorescence microscopy images share optimal DIP configurations.

Method: Generated calibration (n=110) and validation (n=55) sets from open-source data for U-net architecture search. Implemented AUTO-DIP pipeline for automatic parameter transfer based on image metadata similarity rather than quantitative image measures.

Result: Parameter transfer based on metadata similarity (microscope type, specimen) performed similarly or better than quantitative similarity measures. AUTO-DIP outperformed baseline DIP and variational denoising across multiple test datasets, especially for very noisy inputs.

Conclusion: AUTO-DIP enables efficient, optimization-free DIP application for fluorescence microscopy denoising by leveraging parameter transfer from similar images, demonstrating superior performance particularly for challenging noisy conditions.

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [56] [Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2601.12062)
*Xiaomei Yang,Xizhan Gao,Antai Liu,Kang Wei,Fa Zhu,Guang Feng,Xiaofeng Qu,Sijie Niu*

Main category: cs.CV

TL;DR: LSMRL method uses language prompts from CLIP to learn modal-invariant representations for video-based visible-infrared person re-identification, addressing spatial-temporal modeling, cross-modal interaction, and explicit modality-level guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods using CLIP language prompts for VVI-ReID have limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance.

Method: Proposes LSMRL with three modules: STFL for efficient spatial-temporal modeling on CLIP, SD for diffusing language prompts into features for modal consistency, and CMI with bidirectional cross-modal self-attention to eliminate modality gaps.

Result: Extensive experiments on large-scale VVI-ReID datasets demonstrate superiority over state-of-the-art methods.

Conclusion: LSMRL effectively addresses key limitations in VVI-ReID by combining language-driven guidance with efficient spatial-temporal modeling and enhanced cross-modal interaction.

Abstract: The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.

</details>


### [57] [Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation](https://arxiv.org/abs/2601.12066)
*Zijie Lou,Xiangwei Feng,Jiaxin Wang,Xiaochao Qu,Luoqi Liu,Ting Liu*

Main category: cs.CV

TL;DR: Video object removal reformulated as video-to-video translation using stochastic bridge model instead of noise-to-data diffusion, with adaptive mask modulation for better results.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods discard structural priors from input video, leading to incomplete object removal or implausible content generation that violates scene logic.

Method: Reformulate video object removal as video-to-video translation via stochastic bridge model that creates direct path from source to target video, plus adaptive mask modulation to balance background fidelity with generative flexibility.

Result: Significantly outperforms existing methods in both visual quality and temporal consistency through extensive experiments.

Conclusion: Stochastic bridge formulation effectively leverages input video as structural prior for precise object removal while maintaining logical consistency with surrounding environment.

Abstract: Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.

</details>


### [58] [ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification](https://arxiv.org/abs/2601.12067)
*VSS Tejaswi Abburi,Ananya Singhal,Saurabh J. Shigwan,Nitin Kumar*

Main category: cs.CV

TL;DR: ARMARecon: A graph learning framework using ARMA filtering with reconstruction objectives for early detection of Alzheimer's and Frontotemporal Dementia from white-matter connectivity patterns.


<details>
  <summary>Details</summary>
Motivation: Early detection of neurodegenerative diseases like Alzheimer's Disease and Frontotemporal Dementia is crucial for reducing progression risk. Since these diseases propagate along white-matter regions in a graph-dependent manner, graph-based neural networks are well-suited to capture these patterns.

Method: ARMARecon integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation. It models both local and global connectivity using 20-bin Fractional Anisotropy histogram features from white-matter regions while mitigating over-smoothing.

Result: ARMARecon achieves superior performance compared to state-of-the-art methods on multi-site dMRI datasets ADNI and NIFD.

Conclusion: The proposed ARMARecon framework effectively captures disease propagation patterns in white-matter regions and improves classification accuracy for early neurodegenerative disease detection.

Abstract: Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.

</details>


### [59] [CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation](https://arxiv.org/abs/2601.12076)
*H. Jiang,Y. Sun,Z. Dong,T. Liu,Y. Gu*

Main category: cs.CV

TL;DR: Proposes RS-RVOS Bench benchmark and MQC-SAM framework for remote sensing video referring object segmentation, addressing weak target saliency and memory quality issues.


<details>
  <summary>Details</summary>
Motivation: RS-RVOS faces challenges with weak target saliency, visual information truncation, lack of large-scale benchmarks, and memory issues (biased initialization and indiscriminate accumulation) that cause error propagation.

Method: Two contributions: 1) RS-RVOS Bench - first large-scale benchmark with 111 videos, 25K frames, 213K annotations using causality-aware annotation; 2) MQC-SAM framework with temporal motion consistency module for memory calibration and decoupled attention-based memory integration with dynamic quality assessment.

Result: MQC-SAM achieves state-of-the-art performance on RS-RVOS Bench through extensive experiments.

Conclusion: The paper advances RS-RVOS research through both data (new benchmark) and methodology (memory-quality-aware framework) contributions, effectively addressing key challenges in the field.

Abstract: Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.

</details>


### [60] [EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space](https://arxiv.org/abs/2601.12079)
*Jing Zhang,Bingjie Fan,Jixiang Zhu,Zhe Wang*

Main category: cs.CV

TL;DR: EmoLat is a novel emotion latent space for fine-grained, text-driven image sentiment transfer that models cross-modal correlations between text and visual emotion features, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To enable controllable image sentiment editing guided by textual input by modeling the complex relationships between textual semantics and visual emotion features, which existing methods struggle to capture effectively.

Method: Constructs an emotion semantic graph to capture relational structure among emotions, objects, and visual attributes; uses adversarial regularization to align latent emotion distributions across modalities; proposes a cross-modal sentiment transfer framework with multi-objective loss (semantic consistency, emotion alignment, adversarial regularization).

Result: Significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity on the EmoSpace Set benchmark dataset.

Conclusion: Establishes a new paradigm for controllable image sentiment editing guided by textual input, with the EmoSpace Set dataset and code publicly available for further research.

Abstract: We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.

</details>


### [61] [Toward Real-World High-Precision Image Matting and Segmentation](https://arxiv.org/abs/2601.12080)
*Haipeng Zhou,Zhaohu Xing,Hongqiu Wang,Jun Ma,Ping Li,Lei Zhu*

Main category: cs.CV

TL;DR: FCLM is a foreground-consistent learning model for high-precision scene parsing that addresses limitations in existing methods through depth-aware distillation, domain-invariant learning for synthetic data, and an object-oriented decoder supporting both visual and language prompts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for high-precision scene parsing (image matting/dichotomous segmentation) have several limitations: they focus mainly on single foreground objects, interactive methods are class-agnostic and lack generalization, and reliance on synthetic data leads to poor real-world performance due to domain gaps.

Method: Proposes FCLM with three key components: 1) Depth-Aware Distillation strategy to transfer depth-related knowledge for better foreground representation, 2) Domain-invariant learning strategy treating synthetic data processing as domain adaptation to focus on foreground learning, and 3) Object-Oriented Decoder that accepts both visual and language prompts for interactive prediction.

Result: Experimental results show the method quantitatively and qualitatively outperforms state-of-the-art methods in high-precision scene parsing tasks.

Conclusion: FCLM effectively addresses the limitations of existing methods by improving foreground consistency through depth-aware knowledge transfer, domain adaptation for synthetic data, and flexible interactive prediction via multimodal prompts.

Abstract: High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.

</details>


### [62] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: LLM/LVM-based agentic framework dynamically generates and adapts behavior trees for autonomous vehicles when baseline planning fails, enabling navigation around unexpected obstacles without human intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional behavior trees for AVs are static and require manual tuning, limiting their applicability for SAE Level 5 autonomy in unpredictable real-world environments where adaptive planning is essential.

Method: Three-agent framework: Descriptor agent uses chain-of-symbols prompting to assess scene criticality; Planner agent constructs high-level sub-goals via in-context learning; Generator agent synthesizes executable BT sub-trees in XML format. Integrated into CARLA+Nav2 simulation and triggered only upon baseline BT failure.

Result: System successfully navigates around unexpected obstacles (e.g., street blockage) with no human intervention. Demonstrates capability to handle diverse driving scenarios beyond the static BT baseline.

Conclusion: The agentic framework using LLMs and LVMs provides a proof-of-concept for dynamic behavior tree generation and adaptation, extending autonomous vehicle planning capabilities to handle unpredictable real-world scenarios.

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [63] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: HistoCRF refines zero-shot histopathology predictions using CRFs with novel pairwise potentials, achieving 16-33% accuracy gains without training.


<details>
  <summary>Details</summary>
Motivation: Vision-language models provide imperfect zero-shot predictions for histopathological image analysis, which is crucial for cancer detection and staging. There's a need to refine these predictions without additional model training.

Method: HistoCRF adapts Conditional Random Fields with a novel pairwise potential that promotes label diversity and leverages expert annotations. The framework works in three modes: without annotations, with expert annotations, and with iterative human-in-the-loop annotations.

Result: On five patch-level classification datasets covering different organs and diseases, HistoCRF achieved average accuracy gains of 16.0% without annotations, 27.5% with only 100 annotations, and 32.6% with human-in-the-loop annotations compared to zero-shot predictions.

Conclusion: HistoCRF effectively refines histology foundation model predictions without training, with human-in-the-loop integration providing the best performance improvements for histopathological image analysis.

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [64] [CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology](https://arxiv.org/abs/2601.12373)
*Amro Khaled,Farah Khaled,Omar Riad,Catherine M. Elias*

Main category: cs.CV

TL;DR: CD-TWINSAFE is a V2I-based digital twin system for autonomous vehicles that combines real-time on-board perception with a UE5 virtual replica for safety monitoring and alerts.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous vehicle safety through a digital twin approach that provides real-time monitoring, safety metric calculation, and alert systems by combining on-board perception with infrastructure-based virtual simulation.

Method: Two-stack architecture: 1) On-board driving stack with stereo camera for localization and perception (object detection, velocity, yaw, safety metrics), 2) Digital twin stack with Unreal Engine 5 replica that receives real-time data via ROS2/UDP over 4G V2I communication.

Result: The system successfully processes 20-fps stereo images, calculates safety metrics (time-to-collision, time-headway), and provides real-time updates to the digital twin. Multiple driving scenario tests confirm system validity and real-time response.

Conclusion: CD-TWINSAFE demonstrates a functional V2I digital twin architecture for autonomous vehicles that enables real-time safety monitoring through synchronized physical and virtual environments, validated across various driving scenarios.

Abstract: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.

</details>


### [65] [Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data](https://arxiv.org/abs/2601.12090)
*Matej Mok,Lukáš Gajdošech,Michal Mesároš,Martin Madaras,Viktor Kocur*

Main category: cs.CV

TL;DR: A novel 6DoF pose estimation method for industrial bins using 3D line segment detection and geometric reasoning, outperforming SOTA without needing CAD models at inference.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning approaches for 6DoF object pose estimation require extensive training data or CAD models, limiting application in real-world industrial settings where data is scarce and object instances vary.

Method: Exploit cuboid geometry of bins by detecting intermediate 3D line segments corresponding to top edges using extended 2D line segment detection network (LeTR) on structured point clouds, then process with simple geometric procedure to determine 6DoF pose.

Result: Method achieves 3 cm translation error and 8.2° rotation error, significantly outperforming current state-of-the-art 6DoF pose estimation methods while not requiring instance-specific CAD models during inference.

Conclusion: The proposed approach provides an effective solution for industrial bin pose estimation that works well with limited real data by leveraging synthetic training and geometric reasoning, making it practical for real-world industrial applications.

Abstract: The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.

</details>


### [66] [DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition](https://arxiv.org/abs/2601.12729)
*Hanyu Zhu,Zhihao Zhan,Yuhang Ming,Liang Li,Dibo Hou,Javier Civera,Wanzeng Kong*

Main category: cs.CV

TL;DR: DC-VLAQ is a visual place recognition framework that fuses complementary visual foundation models (DINOv2 and CLIP) and uses a novel query-residual global aggregation scheme to create robust representations that handle viewpoint changes, illumination variations, and domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing VPR methods typically use single visual foundation models, missing complementary cues from different VFMs. However, fusing multiple VFMs alters token distributions, which destabilizes existing query-based global aggregation methods.

Method: Two key innovations: 1) Residual-guided complementary fusion that anchors representations in DINOv2 space while injecting CLIP semantics via learned residual correction, and 2) Vector of Local Aggregated Queries (VLAQ) - a query-residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries for improved stability.

Result: Extensive experiments on 6 standard VPR benchmarks (Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, AmsterTime) show DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, especially under challenging domain shifts and long-term appearance changes.

Conclusion: DC-VLAQ successfully addresses the complementary VFM fusion challenge while maintaining aggregation stability, demonstrating superior robustness in visual place recognition across diverse challenging conditions.

Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.

</details>


### [67] [Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification](https://arxiv.org/abs/2601.12109)
*Larissa Ferreira Rodrigues Moreira,Rodrigo Moreira,Leonardo Gabriel Ferreira Rodrigues*

Main category: cs.CV

TL;DR: Lightweight AI models for coffee leaf disease diagnosis using knowledge distillation and ensemble learning achieve competitive accuracy with reduced energy consumption for IoT applications.


<details>
  <summary>Details</summary>
Motivation: Coffee yield depends on timely disease diagnosis, but field assessment is challenging. While AI vision models are accurate, their adoption is limited by constrained devices and intermittent connectivity in agricultural settings.

Method: Used knowledge distillation where high-capacity CNNs trained in data centers transfer knowledge to compact CNNs through ensemble learning. Integrated dense tiny pairs with simple and optimized ensembling to enhance accuracy while meeting computational and energy constraints.

Result: On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive accuracy compared to prior work with significantly reduced energy consumption and carbon footprint.

Conclusion: Lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for IoT applications in agriculture, enabling sustainable on-device disease diagnosis.

Abstract: Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.

</details>


### [68] [Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation](https://arxiv.org/abs/2601.13565)
*Yu Qin,Shimeng Fan,Fan Yang,Zixuan Xue,Zijie Mai,Wenrui Chen,Kailun Yang,Zhiyong Li*

Main category: cs.CV

TL;DR: FiCoP improves open-vocabulary 6D object pose estimation by replacing global matching with patch-level correspondence using a patch-to-patch correlation matrix as spatial filter, achieving 8.0% and 6.1% AR improvements on REAL275 and Toyota-Light datasets.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary 6D pose estimation methods suffer from ambiguity in global matching where target features get confused with background distractors in open-world scenarios, leading to degraded pose estimation performance.

Method: FiCoP framework transitions from global matching to patch-level correspondence using: 1) object-centric disentanglement preprocessing to isolate semantic targets, 2) Cross-Perspective Global Perception module for dual-view feature fusion with explicit context reasoning, and 3) Patch Correlation Predictor that generates block-wise association maps as spatial filters for noise-resilient matching.

Result: Experiments show FiCoP improves Average Recall by 8.0% on REAL275 and 6.1% on Toyota-Light datasets compared to state-of-the-art methods, demonstrating robust performance in complex open-world environments.

Conclusion: FiCoP successfully addresses the ambiguity problem in open-vocabulary 6D pose estimation through spatially-constrained patch-level correspondence, providing a more robust and generalized perception framework for robotic agents in unconstrained environments.

Abstract: Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.

</details>


### [69] [RCDN: Real-Centered Detection Network for Robust Face Forgery Identification](https://arxiv.org/abs/2601.12111)
*Wyatt McCurdy,Xin Zhang,Yuqi Song,Min Gao*

Main category: cs.CV

TL;DR: RCDN is a frequency-spatial CNN framework that anchors representation around real images to improve cross-domain forgery detection by focusing on authentic consistency rather than diverse forgery patterns.


<details>
  <summary>Details</summary>
Motivation: Existing forgery detection methods work well within the same domain but fail in cross-domain scenarios, which is problematic as new forgery techniques continuously emerge and detectors need to handle unseen manipulations.

Method: Real-Centered Detection Network (RCDN) uses frequency-spatial CNN with Xception backbone, dual-branch architecture, and real-centered loss design to anchor representation space around authentic facial images rather than modeling diverse forgery patterns.

Result: Extensive experiments on DiFF dataset with three forgery types (FE, I2I, T2I) show RCDN achieves state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization, reducing generalization gap and achieving highest cross/in-domain stability ratio.

Conclusion: RCDN demonstrates potential as a practical solution for defending against evolving and unseen image forgery techniques by focusing on the consistency of real images rather than modeling diverse forgery patterns.

Abstract: Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.

</details>


### [70] [FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2601.13976)
*Jing Zuo,Lingzhou Mu,Fan Jiang,Chengcheng Ma,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyVLN is a unified implicit reasoning framework for Vision-and-Language Navigation that preserves Chain-of-Thought reasoning benefits without explicit token overhead, enabling real-time navigation with improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLN approaches with explicit CoT reasoning have critical drawbacks: textual CoTs lack spatial grounding and overfit to sparse annotations, while multimodal CoTs suffer from severe token inflation by generating imagined visual observations, making real-time navigation impractical.

Method: Proposes FantasyVLN with a pretrained Visual AutoRegressor (VAR) that encodes imagined visual tokens into a compact latent space during CoT training. Uses a unified multi-CoT strategy where the model jointly learns from textual, visual, and multimodal CoT modes. At inference, performs direct instruction-to-action mapping while maintaining reasoning-aware representations.

Result: Extensive experiments on LH-VLN show the approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

Conclusion: FantasyVLN provides a practical solution for VLN that preserves the interpretability and planning benefits of CoT reasoning while eliminating the computational overhead, enabling real-time navigation with human-like reasoning capabilities.

Abstract: Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

</details>


### [71] [CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction](https://arxiv.org/abs/2601.12119)
*Xiaotong Zhou,Zhenhui Yuan,Yi Han,Tianhua Xu,Laurence T. Yang*

Main category: cs.CV

TL;DR: CARLA-Round is a systematic simulation dataset for roundabout trajectory prediction with controlled weather and traffic density variations, enabling precise analysis of factor impacts on prediction performance.


<details>
  <summary>Details</summary>
Motivation: Roundabout trajectory prediction is critical for safety but challenging due to circular geometry and complex interactions. Real-world datasets are scarce and confounded by incomplete observations and entangled factors that are difficult to isolate.

Method: Created a systematically designed simulation dataset (CARLA-Round) with structured variations: 5 weather conditions × 5 traffic density levels (LOS A-E) = 25 controlled scenarios. Includes realistic driving behaviors and explicit annotations. Used standard baselines (LSTM, GCN, GRU+GCN) for validation.

Result: Traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. Best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. Systematic approach quantifies factor impacts impossible to isolate in real-world datasets.

Conclusion: CARLA-Round provides a valuable simulation dataset for roundabout trajectory prediction research, enabling controlled analysis of factor impacts and demonstrating effective transfer to real-world scenarios through systematic design.

Abstract: Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.

</details>


### [72] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: SAMA extends SAM with lightweight modules for high-quality interactive segmentation and matting, achieving SOTA performance with minimal extra parameters.


<details>
  <summary>Details</summary>
Motivation: SAM lacks precision for real-world applications, and no unified framework exists for both segmentation and matting despite their strong correlation.

Method: Lightweight extension of SAM with Multi-View Localization Encoder (MVLE) for detailed features, Localization Adapter for boundary refinement, and dual prediction heads for segmentation and matting.

Result: Achieves state-of-the-art performance across multiple segmentation and matting benchmarks with minimal parameter overhead.

Conclusion: SAMA demonstrates that a unified lightweight framework can effectively handle both segmentation and matting tasks with high accuracy.

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [73] [Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks](https://arxiv.org/abs/2601.12149)
*Pengfei Zhu,Xavier Maldague*

Main category: cs.CV

TL;DR: THz-SSDD: A self-supervised network using PCA and Recorrupted-to-Recorrupted learning to simultaneously denoise and deblur THz images without manual intervention or labeled data.


<details>
  <summary>Details</summary>
Motivation: THz systems suffer from frequency-dependent degradation effects causing both low-frequency blurring and high-frequency noise. Conventional methods can't address both issues simultaneously and require manual intervention due to unknown boundaries between denoising and deblurring.

Method: Proposes THz-SSDD network using: 1) Recorrupted-to-Recorrupted self-supervised learning strategy to capture noise intrinsic features by exploiting invariance under repeated corruption, and 2) PCA decomposition and reconstruction to restore images across both low and high frequencies.

Result: Evaluated on four sample types, showing effective denoising and deblurring across different material properties and measurement modes. Quantitative analysis validates network feasibility with improved image quality while preserving original signal physical characteristics.

Conclusion: THz-SSDD provides an effective self-supervised solution for simultaneous THz image denoising and deblurring, requiring only small sets of unlabeled noisy images and eliminating manual intervention needs.

Abstract: Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.

</details>


### [74] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: A space- and time-efficient inference strategy for pathology foundation models that enables efficient whole-slide image processing by sparsifying attention and filtering non-informative tokens.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models are constrained by fixed input sizes (e.g., 224x224), creating inefficiencies when processing whole-slide images spanning thousands of resolutions. Naive approaches either cause prohibitive GPU memory consumption (enlarging inputs) or lose critical morphological details (downsampling).

Method: Proposes an efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores, reducing GPU memory and runtime while preserving performance.

Result: Achieves up to 7.67% improvement in ROI classification and compatible results in segmentation, enabling inference at higher resolutions under the same GPU budget.

Conclusion: The proposed method overcomes input size limitations of pathology foundation models, enabling efficient high-resolution whole-slide image inference while improving downstream performance.

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [75] [Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors](https://arxiv.org/abs/2601.12155)
*Xiang Gao,Xinmu Wang,Yuanpeng Liu,Yue Wang,Junqi Huang,Wei Chen,Xianfeng Gu*

Main category: cs.CV

TL;DR: Novel inverse rendering method uses persistent homology priors to reconstruct complex high-genus 3D objects from images by incorporating topological constraints on tunnel/handle loops.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction from images is ill-posed due to geometric, appearance, and topological ambiguities, especially for high-genus surfaces where existing methods often fail catastrophically by collapsing tunnels or losing complex structure.

Method: Collaborative inverse rendering framework combining photometric consistency from multi-view images with persistent homology priors that capture topological features (tunnel loops, handle loops). Uses gradient-based optimization in mesh-based framework (no neural networks) to emphasize topological constraints.

Result: Method achieves lower Chamfer Distance and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failures in high-genus reconstruction.

Conclusion: Persistent homology priors effectively resolve topological ambiguities in 3D reconstruction, enabling robust recovery of complex high-genus geometry while preventing catastrophic failures like tunnel collapse.

Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.

</details>


### [76] [VIRTUE: Versatile Video Retrieval Through Unified Embeddings](https://arxiv.org/abs/2601.12193)
*Shaunak Halbe,Bhagyashree Puranik,Jayakrishnan Unnikrishnan,Kushan Thakkar,Vimal Bhat,Toufiq Parag*

Main category: cs.CV

TL;DR: VIRTUE is an MLLM-based video retrieval framework that unifies corpus-level retrieval, moment localization, and multimodal querying in a single architecture, achieving performance comparable to specialized models with much less training data.


<details>
  <summary>Details</summary>
Motivation: Specialized video retrieval architectures perform well but can't handle composed multimodal queries, while MLLM-based methods support rich queries but underperform on retrieval tasks. There's a need for a unified system that combines strong retrieval performance with multimodal query flexibility.

Method: Uses a shared MLLM backbone with contrastive alignment of visual and textual embeddings, trained efficiently with LoRA on 700K data samples. Supports embedding-based candidate search and can be adapted for reranking without additional training for moment retrieval.

Result: Surpasses other MLLM-based methods on zero-shot video retrieval, achieves competitive results on zero-shot moment retrieval, and state-of-the-art results for zero-shot composed video retrieval. With reranking, outperforms existing MLLM systems and matches specialized models trained on much larger datasets.

Conclusion: VIRTUE demonstrates that a single MLLM-based architecture can effectively unify diverse video retrieval tasks while achieving performance comparable to specialized systems, offering a versatile solution for modern video retrieval needs.

Abstract: Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.

</details>


### [77] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgRef is a motion-guided framework for surgical instrument segmentation using natural language descriptions, focusing on tool motion patterns rather than static visual features to handle occlusion and terminology variations.


<details>
  <summary>Details</summary>
Motivation: Current surgical referring segmentation methods rely too heavily on static visual cues and predefined instrument names, limiting their generalization across different procedures and failing to handle occlusion, ambiguity, or unfamiliar terminology in natural language descriptions.

Method: SurgRef introduces a motion-guided framework that grounds free-form language expressions in instrument motion patterns across time, capturing how surgical tools move and interact rather than what they look like. The approach uses the Ref-IMotion dataset with dense spatiotemporal masks and motion-centric expressions for training.

Result: SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

Conclusion: Motion-guided approaches that focus on how surgical instruments move and interact over time provide more robust and generalizable solutions for language-driven surgical scene understanding compared to traditional static visual methods.

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [78] [DiffusionQC: Artifact Detection in Histopathology via Diffusion Model](https://arxiv.org/abs/2601.12233)
*Zhenzhen Wang,Zhongliang Zhou,Zhuoyu Wen,Jeong Hwan Kook,John B Wojcik,John Kang*

Main category: cs.CV

TL;DR: DiffusionQC: An unsupervised diffusion model for artifact detection in histopathology images that requires only clean images for training, no artifact annotations needed.


<details>
  <summary>Details</summary>
Motivation: Histopathology images often contain artifacts from slide preparation/digitization that must be detected for reliable analysis. Traditional supervised models need large annotated datasets, are resource-intensive, and don't generalize to novel artifact types.

Method: Proposes DiffusionQC which detects artifacts as outliers among clean images using a diffusion model. Only requires clean images for training, no pixel-level artifact annotations. Enhanced version adds contrastive learning to explicitly enlarge distribution separation between artifact and clean images.

Result: Superior performance to state-of-the-art methods, offers cross-stain generalization capacity, and requires significantly less data and annotations.

Conclusion: DiffusionQC provides an effective unsupervised approach for artifact detection in digital pathology that reduces annotation burden, generalizes to novel artifacts, and maintains high performance with cross-stain generalization.

Abstract: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.

</details>


### [79] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: PRISM is a three-stage framework for video summarization that uses adaptive visual sampling, label-driven keyframe anchoring, and LLM contextual validation to create semantically grounded summaries, achieving 84% semantic retention with less than 5% frame sampling.


<details>
  <summary>Details</summary>
Motivation: Video summarization is crucial for efficient video review in high-stakes domains like surgical training. While prior work has evolved from basic visual features to vision-language models for better semantic understanding, there's a need for methods that produce semantically grounded summaries that capture procedural transitions while filtering out irrelevant content.

Method: PRISM is a three-stage framework: 1) Adaptive visual sampling to select relevant frames, 2) Label-driven keyframe anchoring to identify meaningful procedural transitions, and 3) Contextual validation using a large language model (LLM) to filter out generic or hallucinated content and ensure contextual coherence.

Result: The method achieves 84% semantic content retention while sampling fewer than 5% of original frames. It improves over baselines by up to 33% and demonstrates strong performance across both instructional and activity datasets, with good generalization to procedural and domain-specific video tasks.

Conclusion: PRISM effectively produces contextually coherent video summaries that capture meaningful procedural transitions while filtering irrelevant content. The framework generalizes well across different video domains and achieves strong semantic alignment and precision with minimal frame sampling.

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [80] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: A novel breast cancer detection framework combining PAAC and Transformer architectures achieves 98.5% accuracy on mammographic images, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Breast cancer is a major global health concern for women, and accurate early diagnosis is crucial for improving treatment outcomes. Current methods need improvement in detecting malignant masses in mammographic images.

Method: Proposed framework integrates Pyramid Adaptive Atrous Convolution (PAAC) with Transformer architecture using Multi-Scale Feature Fusion. Combines Dice Loss and Focal Loss functions for training. Uses INbreast, MIAS, and DDSM datasets preprocessed with data augmentation, contrast enhancement, and resized to 227x227 pixels.

Result: Achieved 98.5% accuracy, 97.8% sensitivity, 96.3% specificity, 98.2% F1-score, and 97.9% precision. Outperformed BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer models.

Conclusion: The PAAC-Transformer framework demonstrates superior performance in breast cancer detection, offering a reliable and efficient tool that can be integrated into medical diagnostic systems for improved cancer diagnosis.

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [81] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: FedDCG is a federated learning approach that jointly addresses both class and domain generalization by training class-generalized networks within domain groups and aggregating results based on domain similarity.


<details>
  <summary>Details</summary>
Motivation: Existing methods typically address either unseen classes or unseen domains in isolation, without considering a joint framework for both, especially in federated learning settings where data privacy and distribution are concerns.

Method: Proposes FedDCG with domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. Uses learnable network for class generalization and decoupling mechanism to separate general and domain-specific knowledge. Aggregates results based on domain similarity during inference.

Result: Extensive experiments across various datasets show that FedDCG outperforms state-of-the-art baselines in terms of accuracy and robustness.

Conclusion: FedDCG effectively addresses both class and domain generalization in federated learning settings, providing a unified framework that outperforms existing methods.

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [82] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: Researchers demonstrate 3D reconstruction of hidden scenes from ordinary non-line-of-sight photographs by reformulating light transport into light-occluding and non-light-occluding components, solving with gradient optimization and a neural network called Soft Shadow diffusion.


<details>
  <summary>Details</summary>
Motivation: Conventional imaging requires line of sight, which can be impractical, dangerous, or impossible in certain situations. Passive NLOS methods using ordinary photographs of subtle shadows have been limited to 1D, low-resolution 2D, or known-shape object localization.

Method: Proposed novel reformulation of light transport model that decomposes hidden scene into light-occluding and non-light-occluding components, yielding separable non-linear least squares inverse problem. Developed two solutions: gradient-based optimization and physics-inspired neural network called Soft Shadow diffusion (SSD).

Result: Successfully demonstrated 3D reconstruction of hidden scenes from ordinary NLOS photographs in real experimental scenarios. SSD shows generalization to unseen classes in simulation and real-world NLOS scenes, with surprising robustness to noise and ambient illumination.

Conclusion: The approach effectively addresses challenging ill-conditioned inverse problems in NLOS imaging, enabling 3D reconstruction from ordinary photographs of shadows, with neural network methods showing strong generalization and robustness properties.

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [83] [AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search](https://arxiv.org/abs/2601.12272)
*Shahrzad Esmat,Mahdi Banisharif,Ali Jannesari*

Main category: cs.CV

TL;DR: AgenticPruner uses LLM agents to achieve MAC-constrained neural network pruning with iterative strategy learning, improving convergence from 48% to 71% while maintaining or improving accuracy across CNN and ViT architectures.


<details>
  <summary>Details</summary>
Motivation: Existing pruning approaches focus on parameter reduction without directly controlling computational cost, leading to unpredictable inference latency when strict MAC operation budgets must be met for deployment on resource-constrained devices.

Method: Uses three specialized LLM agents: Profiling Agent analyzes model architecture and MAC distributions, Master Agent orchestrates workflow with divergence monitoring, and Analysis Agent (powered by Claude 3.5 Sonnet) learns optimal strategies from historical attempts through in-context learning. Builds on isomorphic pruning with context-aware adaptation by analyzing patterns across pruning iterations.

Result: On ImageNet-1K: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). ConvNeXt-Small pruning yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. Achieves MAC-budget compliance within tolerance bands (+1% to +5% overshoot, -5% to -15% undershoot).

Conclusion: AgenticPruner enables automatic convergence to target MAC budgets with improved accuracy, establishing feasibility for deployment scenarios requiring strict computational guarantees through LLM-powered iterative strategy learning.

Abstract: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.
  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.

</details>


### [84] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: CytoCLIP: Vision-language models for automated brain cytoarchitecture analysis using CLIP-based joint visual-text representations from NISSL-stained fetal brain sections.


<details>
  <summary>Details</summary>
Motivation: Manual delineation of brain regions by cytoarchitecture is time-consuming and requires specialized expertise, necessitating automated approaches to reduce human effort in brain analysis.

Method: Two model variants: one trained on low-resolution whole-region images for overall patterns, another on high-resolution image tiles for cellular details. Uses CLIP-based vision-language models trained on NISSL-stained fetal brain sections (86 regions for low-res, 384 regions for high-res).

Result: Outperforms existing methods with F1 scores of 0.87 for whole-region classification and 0.91 for high-resolution tile classification. Shows strong generalization across different ages and sectioning planes.

Conclusion: CytoCLIP provides an effective automated solution for brain cytoarchitecture analysis, demonstrating superior performance in region classification and cross-modal retrieval tasks.

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [85] [SDiT: Semantic Region-Adaptive for Diffusion Transformers](https://arxiv.org/abs/2601.12283)
*Bowen Lin,Fanjiang Ye,Yihua Liu,Zhenghui Guo,Boyuan Zhang,Weijian Zheng,Yufan Xu,Tiancheng Xing,Yuke Wang,Chengming Zhang*

Main category: cs.CV

TL;DR: SDiT accelerates Diffusion Transformers by 3x without retraining, using semantic-aware clustering and complexity-driven scheduling to selectively update only complex regions during denoising.


<details>
  <summary>Details</summary>
Motivation: DiTs are computationally expensive due to iterative denoising and quadratic attention costs, but denoising dynamics are spatially non-uniform - background converges quickly while edges/textures evolve actively.

Method: Training-free framework with: (1) semantic-aware clustering via Quickshift segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement for spatial coherence.

Result: Achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference, without any model retraining or architectural modification.

Conclusion: SDiT demonstrates that computation can be allocated according to regional complexity in diffusion models, enabling significant speedups while maintaining quality through semantic-aware adaptive processing.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.

</details>


### [86] [LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines](https://arxiv.org/abs/2601.12285)
*Safa C. Medin,Gengyan Li,Ziqian Bai,Ruofei Du,Leonhard Helminger,Yinda Zhang,Stephan J. Garbin,Philip L. Davidson,Gregory W. Wornell,Thabo Beeler,Abhimitra Meka*

Main category: cs.CV

TL;DR: Novel explicit mesh-based representation for photorealistic 3D face avatars that enables efficient classical rendering on legacy platforms without custom engineering.


<details>
  <summary>Details</summary>
Motivation: To create photorealistic 3D face avatars that can be efficiently rendered on legacy graphics platforms without requiring custom engineering or integration, while maintaining control over complex facial features like hair, skin, and eyes.

Method: Uses radiance fields anchored to parametric face models to learn radiance manifolds in 3D space, extracting an explicit layered mesh with appearance and warp textures. During deployment, enables control through linear blending and alpha compositing of textures over a static mesh.

Result: Achieves controllable volumetric rendering of complex facial features and enables efficient streaming and classical mesh/shader-based rendering on legacy graphics platforms.

Conclusion: The explicit representation provides photorealistic 3D face avatars that are both controllable and efficiently renderable on existing graphics platforms without custom engineering.

Abstract: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.

</details>


### [87] [Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations](https://arxiv.org/abs/2601.12303)
*Shizhan Gong,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: PCBM-ReD is a post-hoc concept bottleneck model that retrofits interpretability onto pretrained models by automatically extracting visual concepts, filtering them with MLLMs, and decomposing image representations into concept embeddings.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack interpretability for critical applications, and existing concept-based methods have limitations like unreliable concept relevance, non-visual concepts, labor-intensive definitions, and model/data-agnostic assumptions.

Method: Extracts visual concepts from pre-trained encoders, uses MLLMs to label/filter concepts based on visual identifiability and task relevance, selects independent concept subset via reconstruction-guided optimization, and decomposes image representations into linear combinations of concept embeddings using CLIP's visual-text alignment.

Result: Achieves state-of-the-art accuracy across 11 image classification tasks, narrows performance gap with end-to-end models, and exhibits better interpretability.

Conclusion: PCBM-ReD provides an effective pipeline for retrofitting interpretability onto pretrained models while maintaining high accuracy and improving transparency through automatically extracted visual concepts.

Abstract: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.

</details>


### [88] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: 2S-GDA: A two-stage globally-diverse attack framework that improves adversarial transferability against vision-language pre-training models in black-box settings.


<details>
  <summary>Details</summary>
Motivation: Vision-language pre-training models are vulnerable to adversarial attacks, especially in black-box scenarios. Existing multimodal attacks have limitations including limited perturbation diversity and unstable multi-stage pipelines.

Method: Proposes 2S-GDA with two stages: 1) Textual perturbations using globally-diverse strategy combining candidate text expansion with globally-aware replacement, 2) Visual perturbations using multi-scale resizing and block-shuffle rotation for enhanced diversity.

Result: Extensive experiments show 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains up to 11.17% in black-box settings. The framework is modular and can be combined with existing methods.

Conclusion: 2S-GDA effectively addresses diversity limitations in multimodal adversarial attacks and demonstrates superior performance in attacking vision-language pre-training models, particularly in challenging black-box scenarios.

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [89] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: AMC-MetaNet is a lightweight few-shot learning framework for remote sensing that uses correlation-guided feature pyramids and meta-learning to handle multi-scale objects with only ~600K parameters.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in remote sensing few-shot learning: scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects.

Method: Three innovations: (1) correlation-guided feature pyramids for scale-invariant patterns, (2) adaptive channel correlation module for dynamic cross-scale relationships, (3) correlation-guided meta-learning using correlation patterns instead of prototype averaging.

Result: Achieves up to 86.65% accuracy in 5-way 5-shot classification on multiple datasets (EuroSAT, NWPU-RESISC45, UC Merced, AID), with 20× fewer parameters than ResNet-18 and <50ms inference time per image.

Conclusion: AMC-MetaNet establishes a computationally efficient, scale-aware framework suitable for real-world few-shot remote sensing applications.

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [90] [CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding](https://arxiv.org/abs/2601.12312)
*Yongjun Jeon,Jongmin Shin,Kanggil Park,Seonmin Park,Soyoung Lim,Jung Yong Kim,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: CurConMix+ framework with curriculum-guided contrastive learning and multi-resolution temporal transformer outperforms SOTA in surgical action triplet recognition, with new LLS48 dataset for complex laparoscopic surgery.


<details>
  <summary>Details</summary>
Motivation: Surgical action triplet recognition is clinically important but faces challenges: severe class imbalance, subtle visual variations, and semantic interdependence among triplet components. Existing approaches address only subsets of these challenges rather than tackling them jointly.

Method: CurConMix+ framework builds on CurConMix spatial representation with curriculum-guided contrastive learning, structured hard-pair sampling, and feature-level mixup. Temporal extension adds Multi-Resolution Temporal Transformer (MRTT) for adaptive multi-scale temporal feature fusion and dynamic spatio-temporal balance.

Result: Outperforms state-of-the-art approaches in triplet recognition on CholecT45 and new LLS48 datasets. Exhibits strong cross-level generalization - fine-grained features effectively transfer to higher-level phase and step recognition tasks.

Conclusion: Framework and LLS48 dataset provide unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. Code and dataset will be publicly released to facilitate reproducibility and further research.

Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.

</details>


### [91] [S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection](https://arxiv.org/abs/2601.12313)
*Xiangyu Hu,Yicheng Hong,Hongchuang Zheng,Wenjun Zeng,Bingyao Liu*

Main category: cs.CV

TL;DR: S²F-Net: A cross-model detection framework that leverages spectral discrepancies between real and synthetic textures to detect AI-generated content with strong generalization across unseen generative architectures.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated content detection methods suffer from overfitting to specific source models and poor generalization to unseen generative architectures, creating an urgent need for detection schemes with strong generalization capabilities.

Method: Proposes S²F-Net framework that detects frequency-domain artifacts from upsampling operations. Uses a learnable frequency attention module that adaptively weights discriminative frequency bands by combining spatial texture analysis and spectral dependencies.

Result: Achieves 90.49% detection accuracy on AIGCDetectBenchmark (17 categories of generative models), significantly outperforming existing baseline methods in cross-domain detection scenarios.

Conclusion: Focusing on inherent spectral discrepancies between real and synthetic textures through frequency-domain analysis provides a fundamental solution for improving generalization in AI-generated content detection across diverse generative models.

Abstract: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.

</details>


### [92] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: A semantics-modulated Transformer for 3D gaze estimation achieves state-of-the-art results by conditioning CLIP features with learnable prototype banks and using cross-scale fusion with Mixture of Experts.


<details>
  <summary>Details</summary>
Motivation: To improve 3D gaze estimation by addressing challenges like varying illumination, head poses, backgrounds, and gaze directions through semantic conditioning and multi-scale feature fusion.

Method: Uses CLIP global features conditioned with learnable prototype banks (illumination, head pose, background, direction), fuses these with CLIP patch tokens and high-resolution CNN tokens in unified attention, and replaces FFN blocks with routed/shared Mixture of Experts for increased conditional capacity.

Result: Achieves new state-of-the-art angular errors on MPIIFaceGaze (2.49°), EYEDIAP (3.22°), Gaze360 (10.16°), and ETH-XGaze (1.44°), demonstrating up to 64% relative improvement over previous methods.

Conclusion: The proposed semantics-modulated, multi-scale Transformer with prototype conditioning, cross-scale fusion, and Mixture of Experts significantly advances 3D gaze estimation performance across multiple benchmark datasets.

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [93] [Multi-Sensor Matching with HyperNetworks](https://arxiv.org/abs/2601.12325)
*Eli Passov,Nathan S. Netanyahu,Yosi Keller*

Main category: cs.CV

TL;DR: Lightweight hypernetwork-based descriptor learning for multimodal patch matching achieves SOTA on VIS-IR benchmarks with minimal inference cost increase.


<details>
  <summary>Details</summary>
Motivation: Improve multimodal patch matching (especially visible vs. infrared) by addressing appearance shifts while maintaining the efficiency of descriptor-based methods during inference.

Method: Augment Siamese CNN with hypernetwork modules for adaptive per-channel scaling/shifting and conditional instance normalization for modality-specific adaptation in shallow layers.

Result: Achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks, matches/surpasses prior methods on additional datasets despite their higher inference cost.

Conclusion: Hypernetworks provide effective modality adaptation for patch matching while preserving inference efficiency; new GAP-VIR dataset released for cross-domain generalization evaluation.

Abstract: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.

</details>


### [94] [EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation](https://arxiv.org/abs/2601.12326)
*Jing Zhang,Bingjie Fan*

Main category: cs.CV

TL;DR: EmoKGEdit: Training-free framework using knowledge graph to disentangle emotion from content for precise image emotion editing while preserving visual structure.


<details>
  <summary>Details</summary>
Motivation: Existing image emotion editing methods fail to properly separate emotional cues from content representations, resulting in weak emotional expression and distorted visual structures.

Method: Constructs Multimodal Sentiment Association Knowledge Graph (MSA-KG) to model relationships among objects, scenes, attributes, visual clues and emotion. Uses this as external knowledge for chain-of-thought reasoning to generate coherent editing instructions. Also designs disentangled structure-emotion editing module to separate emotional attributes from layout features in latent space.

Result: Extensive experiments show EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, outperforming state-of-the-art methods.

Conclusion: The proposed training-free framework successfully addresses the disentanglement problem in image emotion editing, enabling precise emotion injection while maintaining visual spatial coherence through knowledge graph-guided reasoning and latent space separation.

Abstract: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.

</details>


### [95] [FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching](https://arxiv.org/abs/2601.12329)
*Mithlesh Singla,Seema Kumari,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: FlowIID: A parameter-efficient intrinsic image decomposition model using latent flow matching for single-step inference with competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing IID models achieve good results but use too many parameters, making them costly to combine with other models in real-world applications. There's a need for parameter-efficient solutions suitable for resource-constrained and real-time settings.

Method: Proposed FlowIID architecture based on latent flow matching, combining a VAE-guided latent space with a flow matching module for stable albedo and shading decomposition in a single inference step.

Result: FlowIID delivers competitive and superior results compared to existing models across various benchmarks, despite its compact design and parameter efficiency.

Conclusion: FlowIID is well-suited for deployment in resource-constrained and real-time vision applications due to its parameter efficiency, single-step inference, and competitive performance.

Abstract: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.

</details>


### [96] [Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12337)
*Jiahui Sheng,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: Turbo-GoDec enhances hyperspectral anomaly detection by incorporating cluster sparsity prior of anomalies into GoDec algorithm, outperforming existing methods on small-size anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral anomaly detection methods rely on low-rank background and sparse anomaly assumptions but rarely expand on anomaly sparsity. Observations show anomalies exhibit spatial cluster patterns (cluster sparsity), which current methods don't fully exploit.

Method: Combines cluster sparsity prior with GoDec algorithm, modeling anomaly cluster sparsity using Markov random field and computing marginal probabilities via message passing on factor graph. High-probability locations become sparse component in Turbo-GoDec.

Result: Experiments on three real HSI datasets demonstrate superior performance in detecting small-size anomalies compared to vanilla GoDec (LSMAD) and state-of-the-art methods.

Conclusion: Turbo-GoDec effectively incorporates cluster sparsity prior to improve hyperspectral anomaly detection, particularly for small clustered anomalies, with code publicly available.

Abstract: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.

</details>


### [97] [MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents](https://arxiv.org/abs/2601.12346)
*Peizhou Huang,Zixuan Zhong,Zhongwei Wan,Donghao Zhou,Samiul Alam,Xin Wang,Zexin Li,Zhihao Dou,Li Zhu,Jing Xiong,Chaofan Tao,Yan Xu,Dimitrios Dimitriadis,Tuo Zhang,Mi Zhang*

Main category: cs.CV

TL;DR: MMDR-Bench is a new benchmark for evaluating multimodal deep research agents on 140 expert-crafted tasks across 21 domains, with unified evaluation metrics for report quality, citation grounding, and multimodal integrity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on text-only settings or short multimodal QA, missing end-to-end evaluation of multimodal evidence use in citation-rich report generation by deep research agents.

Method: Created MMDR-Bench with 140 expert-crafted tasks across 21 domains, each with image-text bundles. Proposed three evaluation metrics: FLAE for report quality, TRACE for citation-evidence alignment, and MOSAIC for text-visual integrity.

Result: Experiments with 25 state-of-the-art models revealed systematic trade-offs between generation quality, citation discipline, and multimodal grounding, showing that strong prose doesn't guarantee faithful evidence use and multimodal integrity remains a bottleneck.

Conclusion: MMDR-Bench provides comprehensive evaluation of multimodal deep research agents, highlighting the need for better multimodal integrity and faithful evidence use beyond just text generation quality.

Abstract: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.

</details>


### [98] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: SimpleMatch is a lightweight semantic correspondence framework that achieves strong performance at low resolutions (252x252) by addressing feature fusion issues from downsampling, using an upsample decoder and multi-scale supervision.


<details>
  <summary>Details</summary>
Motivation: Current semantic correspondence methods rely on high-resolution inputs for optimal performance, causing computational overhead. A fundamental limitation is the irreversible fusion of adjacent keypoint features when semantically distinct keypoints fall within the same downsampled receptive field during deep downsampling operations.

Method: Proposes SimpleMatch with: 1) lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, 2) multi-scale supervised loss to ensure upsampled features retain discriminative features across spatial scales, and 3) sparse matching and window-based localization to optimize training memory usage.

Result: Achieves 84.1% PCK@0.1 on SPair-71k benchmark at 252x252 resolution (3.3x smaller than current SOTA methods), reduces training memory usage by 51%, and provides efficient performance without high-resolution inputs.

Conclusion: SimpleMatch provides a practical and efficient baseline for future semantic correspondence research by addressing feature fusion issues in downsampling and enabling strong performance at low resolutions with reduced computational overhead.

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [99] [DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data](https://arxiv.org/abs/2601.12366)
*Jiafei Zhang,Songliang Cao,Binghui Xu,Yanan Li,Weiwei Jia,Tingting Wu,Hao Lu,Weijuan Hu,Zhiguo Han*

Main category: cs.CV

TL;DR: DepthCropSeg++ is a foundation model for crop segmentation that achieves 93.11% mIoU, outperforming supervised baselines and general vision models like SAM by significant margins, especially in challenging scenarios like night-time environments and unseen crop varieties.


<details>
  <summary>Details</summary>
Motivation: Current crop segmentation models suffer from limited training data due to expensive pixel-level labeling costs, performing well only under specific crop types or controlled environments. There's a need for a more generalizable model that can handle diverse crop species and environmental conditions.

Method: Built upon previous DepthCropSeg work, scaled up dataset to 28,406 images across 30+ species and 15 environmental conditions. Enhanced ViT-Adapter architecture with dynamic upsampling for improved detail awareness, and trained with a two-stage self-training pipeline.

Result: Achieved 93.11% mIoU on comprehensive testing set, outperforming supervised baselines (+0.36%) and SAM (+48.57%). Excels in challenging scenarios: night-time (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU).

Conclusion: DepthCropSeg++ establishes a new state-of-the-art for crop segmentation, demonstrating strong generalization capabilities across diverse crop species and environmental conditions, making it a practical foundation model for agricultural applications.

Abstract: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.

</details>


### [100] [Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12379)
*Jiahui Sheng,Yidan Shi,Shu Xiang,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: ScoreAD: A hyperspectral anomaly detection method using score-based generative models to distinguish background spectra (on low-dimensional manifolds) from anomalous spectra (outliers).


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images contain high-dimensional spectra determined by few factors, satisfying the manifold hypothesis. Background spectra reside on low-dimensional manifolds while anomalies are outliers, enabling detection via manifold discrepancy.

Method: Train a score-based generative model (SGM) on all spectra, then at test time perturb each spectrum and use the trained SGM to estimate scores. Leverage the fundamental discrepancy between background and anomalous spectra's manifold distributions.

Result: Experiments on four hyperspectral datasets demonstrate the effectiveness of the proposed ScoreAD method for anomaly detection.

Conclusion: The proposed ScoreAD method successfully leverages score-based generative models and the hyperspectral manifold hypothesis for effective anomaly detection in hyperspectral images.

Abstract: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.

</details>


### [101] [A Hierarchical Benchmark of Foundation Models for Dermatology](https://arxiv.org/abs/2601.12382)
*Furkan Yuceyalcin,Abdurrahim Yilmaz,Burak Temelkuran*

Main category: cs.CV

TL;DR: Foundation models show a "granularity gap" in dermatology - general medical models excel at high-level screening while specialized models perform better at fine-grained lesion classification.


<details>
  <summary>Details</summary>
Motivation: Current dermatology benchmarks oversimplify diagnostic taxonomy to binary classification, obscuring models' ability to perform fine-grained differential diagnoses needed for clinical workflow integration.

Method: Evaluated embeddings from 10 foundation models across three domains (general CV, general medical, dermatology-specific) using DERM12345 dataset with 40 lesion subclasses. Used frozen embeddings with lightweight adapter models and 5-fold cross-validation, introducing hierarchical evaluation across four clinical granularity levels.

Result: Revealed "granularity gap": MedImageInsights achieved 97.52% F1 on binary malignancy but declined to 65.50% on 40-class classification. MedSigLip (69.79%) and dermatology-specific models excelled at fine-grained classification but performed worse on broader tasks.

Conclusion: General medical foundation models are effective for high-level screening, but specialized modeling strategies are needed for granular distinctions required in diagnostic support systems.

Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.

</details>


### [102] [Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation](https://arxiv.org/abs/2601.12391)
*Dasith de Silva Edirimuni,Ajmal Saeed Mian*

Main category: cs.CV

TL;DR: CPVQ-VAE enables pure point cloud scene generation without external object databases by using class-partitioned codebooks and class-aware inverse lookup.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation methods rely on retrieving objects from databases using bounding boxes or latent features, but diffusion-based latents cannot be effectively decoded into correct point cloud objects that match target classes for complex multi-categorical scenes.

Method: Introduces Class-Partitioned Vector Quantized VAE (CPVQ-VAE) with class-partitioned codebook where codevectors are labeled by class, plus class-aware running average update to prevent codebook collapse. Uses Latent-space Flow Matching Model (LFMM) to generate object features and class labels, then CPVQ-VAE performs class-aware inverse lookup to map latents to codebook entries for decoding into class-specific point clouds.

Result: Achieves pure point cloud generation without external database retrieval, with up to 70.4% reduction in Chamfer error and 72.3% reduction in Point2Mesh error on complex living room scenes.

Conclusion: The method reliably recovers plausible point cloud scenes by effectively decoding object latent features through class-partitioned codebooks, eliminating the need for external object databases in 3D scene generation.

Abstract: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.

</details>


### [103] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: Paper reviews emotion detection from faces, selects top 3 neural network methods and datasets, trains models, and performs cross-dataset experiments revealing weaknesses in current solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for emotion detection in human-computer interaction by conducting an in-depth review of existing methods due to the enormous variety of approaches available.

Method: The method involves: 1) Comprehensive literature review of emotion detection methods, 2) Selection of three best neural network solutions, 3) Selection of three diverse datasets with large image collections, 4) Training selected neural networks, 5) Cross-dataset experiments comparing performance across different datasets.

Result: The experiments reveal weaknesses in existing solutions including: dataset differences causing performance variations, unequal difficulty in recognizing certain emotions, and challenges in differentiating between closely related emotions.

Conclusion: Current emotion detection solutions have significant limitations including dataset biases, varying recognition difficulty across emotions, and poor differentiation between similar emotions, highlighting the need for more robust and generalizable approaches.

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [104] [HOT-POT: Optimal Transport for Sparse Stereo Matching](https://arxiv.org/abs/2601.12423)
*Antonin Clerc,Michael Quellmalz,Moritz Piening,Philipp Flotho,Gregor Kornhardt,Gabriele Steidl*

Main category: cs.CV

TL;DR: Unsupervised sparse stereo matching using optimal transport with line constraints for facial landmark matching across different conventions.


<details>
  <summary>Details</summary>
Motivation: Stereo vision faces challenges with occlusions, motion, and camera distortions, especially for sparse features like facial landmarks. Parameter sensitivity makes sparse stereo matching ill-posed, requiring unsupervised methods to handle different landmarking conventions.

Method: Formulate camera-projected points as lines and use epipolar distance and 3D ray distance as cost functions in optimal transport problems. Extend to hierarchical OT for object matching, creating efficiently solvable assignment problems.

Result: Developed algorithms for efficient feature and object matching demonstrated in numerical experiments, specifically applied to facial analysis for matching different landmarking conventions.

Conclusion: Optimal transport with line constraints provides an effective unsupervised approach for sparse stereo matching, particularly useful for facial landmark matching across different conventions in challenging stereo vision scenarios.

Abstract: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.

</details>


### [105] [SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition](https://arxiv.org/abs/2601.12432)
*Shunyu Huang,Yunjiao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: SkeFi uses cross-modal knowledge transfer from RGB to wireless sensors (LiDAR/mmWave) for skeleton-based action recognition, addressing data scarcity and noise issues with enhanced graph convolution and temporal modeling.


<details>
  <summary>Details</summary>
Motivation: RGB-based skeleton recognition fails in dark environments and raises privacy concerns, limiting applications in smart homes/hospitals. Wireless sensors (LiDAR/mmWave) offer non-invasive alternatives but face data scarcity and noisy skeletal keypoints.

Method: Proposes SkeFi with cross-modal knowledge transfer from data-rich RGB modality to wireless sensors. Uses enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to handle noise/missing frames, plus dual temporal convolution for multiscale temporal modeling.

Result: SkeFi achieves state-of-the-art performance on both mmWave and LiDAR datasets, demonstrating accurate pose and action extraction from noisy wireless sensor data.

Conclusion: Cross-modal transfer from RGB to wireless sensors effectively addresses data scarcity and noise challenges, enabling practical skeleton-based action recognition in privacy-sensitive and dark environments using LiDAR/mmWave sensors.

Abstract: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.

</details>


### [106] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: Survey paper reviewing three main defense paradigms against adversarial attacks on Vision Language Models (VLMs): Training-time Defense, Test-time Adaptation Defense, and Training-free Defense, analyzing their strengths, limitations, and challenges.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Vision Language Models (VLMs) like CLIP has raised security concerns about their vulnerability to sophisticated adversarial attacks that could compromise model performance and system security in cross-modal tasks.

Method: The paper conducts a comprehensive survey reviewing three defense paradigms: 1) Training-time Defense (adversarial fine-tuning), 2) Test-time Adaptation Defense (updating parameters at inference), and 3) Training-free Defense (altering inputs/embeddings without model modification).

Result: The survey analyzes the latest advancements in adversarial defense strategies for VLMs, highlighting the trade-offs between effectiveness, computational cost, and generalization capabilities across different defense approaches.

Conclusion: While various defense strategies exist, each has limitations: training-time approaches are resource-intensive, test-time adaptation adds complexity, and training-free methods may have limited effectiveness. Ongoing challenges remain in enhancing VLM robustness against adversarial attacks.

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [107] [Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild](https://arxiv.org/abs/2601.12464)
*Yanrui Lu,Danyang Chen,Haowen Xiao,Jiarui Zhu,Fukang Ge,Binqian Zou,Jiali Guan,Jiayin Liang,Yuting Wang,Ziqian Guan,Xiangcheng Bao,Jinhao Bi,Lin Gu,Jun He,Yingying Zhu*

Main category: cs.CV

TL;DR: Large-scale multi-organelle EM segmentation benchmark reveals current models' limitations in handling real-world heterogeneity and long-range structural continuity.


<details>
  <summary>Details</summary>
Motivation: Current EM segmentation benchmarks are too small and curated, failing to capture real-world heterogeneity and large spatial context needed for accurate organelle instance segmentation.

Method: Created large-scale multi-source benchmark with 100,000+ 2D EM images across cell types and 5 organelle classes using connectivity-aware 3D Label Propagation Algorithm with expert refinement. Benchmarked U-Net, SAM variants, and Mask2Former.

Result: Current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies like Endoplasmic Reticulum, revealing fundamental mismatch between local-context models and long-range structural continuity needs.

Conclusion: There's a critical need for models that can handle real-world variability and long-range structural continuity in EM organelle segmentation. The benchmark dataset and labeling tool will be publicly released.

Abstract: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.

</details>


### [108] [DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors](https://arxiv.org/abs/2601.12468)
*Yanqi Wu,Qichao Chen,Runhe Lai,Xinhua Lu,Jia-Xin Zhuang,Zhilin Zhao,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: DCAC is a training-free test-time calibration module that uses class-specific caches to reduce overconfident predictions on OOD samples by leveraging visual similarity patterns.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often make overconfident predictions on unseen OOD samples. The authors discovered that OOD samples predicted as the same class tend to be visually more similar to each other than to true in-distribution samples, revealing a class-specific pattern that can be exploited for better OOD detection.

Method: DCAC maintains separate caches for each ID class to collect high-entropy samples during testing. It uses a lightweight two-layer module that leverages cached visual features and predicted probabilities to calibrate raw predictions. The training-free module can be integrated with various existing OOD detection methods across unimodal and vision-language models with minimal overhead.

Result: Extensive experiments on multiple OOD benchmarks show DCAC significantly enhances existing methods. When integrated with ASH-S on ImageNet OOD benchmark, it reduces FPR95 by 6.55%, demonstrating substantial improvements in OOD detection performance.

Conclusion: DCAC provides an effective, lightweight, and training-free solution for improving OOD detection by exploiting class-specific visual similarity patterns between OOD samples, offering a practical enhancement that can be seamlessly integrated with various existing methods.

Abstract: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

</details>


### [109] [NeuralFur: Animal Fur Reconstruction From Multi-View Images](https://arxiv.org/abs/2601.12481)
*Vanessa Sklyarova,Berna Kabadayi,Anastasios Yiannakidis,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: First multi-view method for 3D animal fur reconstruction using vision language models to guide strand-based fur modeling across various animals.


<details>
  <summary>Details</summary>
Motivation: Reconstructing realistic animal fur from images is challenging due to fine details, self-occlusion, and view-dependent appearance. Unlike human hairstyles, there are no datasets to learn fur priors for different animals.

Method: Multi-view RGB images → coarse surface geometry via multi-view stereo → VLM retrieves realistic fur length/structure per body part → construct furless geometry → grow strands with geometric/photometric losses → VLM guides strand growth direction and gravity relation to mitigate orientation ambiguities.

Result: Shows generalization across variety of animals with different fur types using new schema of VLM-guided 3D reconstruction from multi-view inputs.

Conclusion: Presents first multi-view-based method for high-fidelity 3D animal fur modeling using strand-based representation, leveraging VLM knowledge to overcome lack of fur datasets and achieve realistic reconstruction across diverse animals.

Abstract: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.

</details>


### [110] [Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation](https://arxiv.org/abs/2601.12493)
*Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Fereshteh Shakeri,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: Histopath-C benchmark with synthetic corruptions for histopathology VLMs, plus LATTE method for robust adaptation outperforming SOTA TTA approaches.


<details>
  <summary>Details</summary>
Motivation: Histopathology VLMs suffer performance degradation from domain shifts like staining variations, contamination, blurring, and noise in real-world scenarios.

Method: 1) Histopath-C benchmark with realistic synthetic corruptions; 2) LATTE transductive low-rank adaptation using multiple text templates to mitigate text input sensitivity.

Result: LATTE outperforms state-of-the-art TTA methods designed for natural images across multiple histopathology datasets, demonstrating effective robust adaptation.

Conclusion: Proposed framework enables evaluation of TTA mechanisms and provides effective adaptation strategy for histopathology VLMs facing real-world distribution shifts.

Abstract: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.

</details>


### [111] [Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods](https://arxiv.org/abs/2601.12500)
*Yaowu Fan,Jia Wan,Tao Han,Andy J. Ma,Antoni B. Chan*

Main category: cs.CV

TL;DR: MovingDroneCrowd++ dataset enables drone-based dense crowd counting/tracking; GD3A method uses density map decomposition via optimal transport for counting; DVTrack adds descriptor voting for tracking, achieving 47.4% counting error reduction and 39.2% tracking improvement.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting/tracking methods rely on fixed cameras with limited spatial coverage, inadequate for large-scale dense crowd analysis. Moving drones offer flexible video capture but lack suitable datasets and methods.

Method: 1) Introduce MovingDroneCrowd++ dataset captured by moving drones with diverse conditions. 2) Propose GD3A: density map-based video counting using optimal transport with adaptive dustbin score to decompose global density maps into shared/inflow/outflow components. 3) Extend to DVTrack: descriptor voting mechanism converts descriptor-level matching to instance-level associations for tracking.

Result: Methods significantly outperform existing approaches under dense crowds and complex motion: 47.4% reduction in counting error and 39.2% improvement in tracking performance on the MovingDroneCrowd++ dataset.

Conclusion: The proposed drone-based approach with GD3A and DVTrack provides an effective solution for large-scale dense crowd analysis, overcoming limitations of fixed-camera methods through flexible drone capture and novel density map decomposition techniques.

Abstract: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.

</details>


### [112] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: SDCoNet is a saliency-driven multi-task network that jointly optimizes super-resolution and detection for small objects in low-quality remote sensing images, using shared encoder, saliency prediction, and gradient routing to improve detection performance.


<details>
  <summary>Details</summary>
Motivation: Remote sensing images have complex backgrounds, weak object signals, and small object scales, making detection challenging. Serial pipelines of SR-then-detection suffer from misaligned objectives, feature redundancy, and lack of effective interaction between tasks.

Method: Proposes SDCoNet with: 1) Swin transformer-based shared encoder for cross-task feature collaboration, 2) Multi-scale saliency prediction module to select key tokens and focus on weak object regions, 3) Gradient routing strategy to mitigate optimization conflicts by stabilizing detection semantics and routing SR gradients in detection-oriented direction.

Result: Experiments on NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split datasets show SDCoNet significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images while maintaining competitive computational efficiency.

Conclusion: SDCoNet effectively addresses the limitations of serial SR-detection pipelines through multi-task collaboration, saliency-driven attention, and gradient routing, achieving superior performance for small object detection in challenging remote sensing conditions.

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [113] [Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images](https://arxiv.org/abs/2601.12512)
*Mohd Usama,Belal Ahmad,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: Proposes Cycle-GAN-based unsupervised domain adaptation for MRI to handle scanner/institutional variations without paired data.


<details>
  <summary>Details</summary>
Motivation: MRI scans from different scanners/institutions suffer from domain shifts due to hardware, protocol, and parameter variations, degrading deep learning model performance when applied to target domains.

Method: Cycle-GAN-based model for unsupervised medical-image domain adaptation that learns bidirectional mappings between source and target domains without paired training data, using content and disparity loss to preserve anatomical content.

Result: Experiments on MRI datasets demonstrate efficacy in bidirectional domain adaptation without labeled data, improving model performance and reducing domain-related variability.

Conclusion: The approach offers promising avenues for improving diagnostic accuracy in healthcare and contributes to more precise and consistent medical image analysis.

Abstract: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.

</details>


### [114] [Deep Feature Deformation Weights](https://arxiv.org/abs/2601.12527)
*Richard Liu,Itai Lang,Rana Hanocka*

Main category: cs.CV

TL;DR: Fusion of semantic data-driven priors with precise classical mesh deformation control, using deep feature proximity for real-time semantic weights without optimization.


<details>
  <summary>Details</summary>
Motivation: Classical handle-based mesh deformation requires users to know ideal handle placement for desired edits, with unintuitive mapping from handles to deformation. Data-driven methods offer semantic edits but are slow and imprecise. Need to combine semantic prior of data with precise control and speed of traditional frameworks.

Method: Uses deep feature proximity to compute smooth semantic deformation weights without additional regularization. Introduces barycentric feature distillation pipeline that efficiently uses visual signals from shape renders to minimize distillation cost. Preserves classical method properties through feature space constraints and locality weighting. Field representation enables automatic detection of semantic symmetries.

Result: Weights computed in real-time for any surface point (vs. optimization for new handles). Semantic prior enables co-deformation of semantic parts. Works for high-resolution meshes in under a minute (vs. hours for classical/neural methods). Proof-of-concept shows real-time deformations for meshes up to 1 million faces on consumer-grade machines.

Conclusion: Simple yet effective fusion of semantic data prior with precise classical control, enabling real-time semantic mesh deformation with automatic symmetry preservation and efficient computation for high-resolution meshes.

Abstract: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.

</details>


### [115] [XRefine: Attention-Guided Keypoint Match Refinement](https://arxiv.org/abs/2601.12530)
*Jan Fabian Schmid,Annika Hagemann*

Main category: cs.CV

TL;DR: XRefine is a detector-agnostic sub-pixel keypoint refinement method using cross-attention on image patches, improving geometric estimation across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current keypoint refinement methods are detector-specific and require retraining for each detector, limiting their practical application and generalization.

Method: Uses cross-attention architecture on image patches centered at matched keypoints to predict refined coordinates without relying on internal detector representations.

Result: Consistently improves geometric estimation accuracy on MegaDepth, KITTI, and ScanNet datasets, outperforming existing refinement methods while maintaining runtime efficiency.

Conclusion: XRefine provides a generalizable, detector-agnostic solution for sub-pixel keypoint refinement that can be extended to multi-view feature tracks, offering practical benefits for 3D vision tasks.

Abstract: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.

</details>


### [116] [BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images](https://arxiv.org/abs/2601.12533)
*Md. Ahanaf Arif Khan,Ariful Islam,Sangeeta Biswas,Md. Iqbal Aziz Khan,Subrata Pramanik,Sanjoy Kumar Chakrabarty,Bimal Kumar Pramanik*

Main category: cs.CV

TL;DR: Created BirdsEye-RU dataset with 2,978 images containing 8,000+ annotated faces to address challenges of detecting small, distant faces in overhead imagery from drones and high-altitude smartphones.


<details>
  <summary>Details</summary>
Motivation: Face detection in overhead images is challenging due to extreme scale variations and environmental clutter. Existing datasets may not adequately capture the difficulties of detecting small, distant faces in aerial imagery.

Method: Created a comprehensive dataset called BirdsEye-RU containing 2,978 images with over eight thousand annotated faces. The dataset includes both drone images and smartphone-captured images from high altitude, specifically designed to capture small and distant faces across diverse environments.

Result: Successfully created and publicly released the BirdsEye-RU dataset, which is freely available on Kaggle. The dataset provides a valuable resource for researchers working on face detection in overhead imagery.

Conclusion: The BirdsEye-RU dataset addresses the need for specialized datasets for face detection in overhead images, particularly for small and distant faces. Making the dataset publicly available will facilitate research and development in this challenging computer vision domain.

Abstract: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.

</details>


### [117] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: Self-supervised eye movement reconstruction from low-res videos predicts emotional expression markers like speech emotion alignment and momentary emotional behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing eye movement-emotion studies rely on specialized high-resolution equipment, limiting accessibility. Need methods that work with naturalistic, low-resolution video data to broaden reach of findings.

Method: Develop novel gaze detection model using self-supervised eye movement reconstruction (inspired by language model pretraining) to leverage unlabeled video. Use encoder embeddings to fine-tune on two downstream tasks: 1) aligning eye movement with speech emotion estimates, 2) predicting momentary emotional behaviors (laughing, crying/sobbing, sighing). Data from USC Shoah Foundation Holocaust survivor interviews.

Result: New model predicts emotion outcomes effectively. Positive correlation between pretraining performance and emotion processing performance for both experiments. Self-supervised eye movement reconstruction successfully encodes affective signals.

Conclusion: Self-supervised eye movement reconstruction is an effective method for encoding affective signals from naturalistic, low-resolution videos, enabling emotion prediction without specialized equipment.

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [118] [PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception](https://arxiv.org/abs/2601.12551)
*Tong Wu*

Main category: cs.CV

TL;DR: PISE combines physics-informed deep learning with ghost imaging for efficient edge perception, achieving 2.57% accuracy improvement and 9x variance reduction at 5% sampling.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of low-bandwidth edge perception, where traditional imaging methods struggle with limited sampling rates and high computational demands. There's a need for efficient perception systems that can operate with minimal data while maintaining accuracy.

Method: PISE (Physics-Informed deep ghost imaging framework for low-bandwidth edge perception) combines adjoint operator initialization with semantic guidance. The adjoint operator initialization provides physics-informed priors, while semantic guidance helps focus on relevant features for classification tasks.

Result: The framework improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling rate, demonstrating significant performance gains with minimal data requirements.

Conclusion: PISE effectively bridges physics-informed modeling with deep learning for edge perception, enabling high-performance classification with dramatically reduced bandwidth requirements, making it suitable for resource-constrained edge computing applications.

Abstract: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.

</details>


### [119] [Camera Pose Revisited](https://arxiv.org/abs/2601.12567)
*Władysław Skarbek,Michał Salomonowicz,Michał Król*

Main category: cs.CV

TL;DR: PnP-ProCay78 algorithm solves planar Perspective-n-Point problem using Cayley parameterization and deterministic starting point selection, achieving accuracy comparable to SQPnP with simpler structure.


<details>
  <summary>Details</summary>
Motivation: Camera pose estimation is fundamental for calibration and multi-sensor systems; existing methods often require complex search procedures or lack geometric transparency.

Method: Combines quadratic reconstruction error formulation with Cayley rotation parameterization and least-squares optimization, using deterministic starting point selection based on analysis of reconstruction error for two canonical vectors.

Result: Achieves practically same projection accuracy as optimal SQPnP and slightly higher than IPPE, with significantly simpler algorithmic structure; validated on RGB and thermal camera data.

Conclusion: PnP-ProCay78 provides geometrically transparent, computationally efficient solution with intuitive convergence insights, making it attractive for both practical applications and educational purposes.

Abstract: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.

</details>


### [120] [Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models](https://arxiv.org/abs/2601.12626)
*Raphi Kang,Hongqiao Chen,Georgia Gkioxari,Pietro Perona*

Main category: cs.CV

TL;DR: VLMs use linear spatial IDs bound to textual activations for spatio-temporal reasoning, which can causally explain model behavior and serve as diagnostic tools for model limitations.


<details>
  <summary>Details</summary>
Motivation: To understand the opaque mechanisms behind spatio-temporal reasoning in Vision Language Models and identify how visual/geometrical and textual representations combine in VLM computations.

Method: Search for confluence of spatial representations, identify linear binding of spatial IDs to textual activations, perform rigorous causal interventions to demonstrate mediation of model beliefs, and extend analysis to video VLMs for temporal ID mechanisms.

Result: VLMs encode object locations via linear spatial IDs bound to textual activations, perform reasoning through language tokens, and these IDs can systematically mediate model beliefs at intermediate layers. Spatial IDs serve as diagnostic tools and learning signals, with analogous temporal IDs found in video VLMs.

Conclusion: The identified linear spatiotemporal ID mechanism elucidates previously underexplored internal reasoning processes in VLMs, contributing to improved interpretability and principled design of more aligned and capable models.

Abstract: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.

</details>


### [121] [From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2](https://arxiv.org/abs/2601.12636)
*Satyaki Roy Chowdhury,Aswathnarayan Radhakrishnan,Hsiao Jou Hsu,Hari Subramoni,Joachim Moortgat*

Main category: cs.CV

TL;DR: Swin-BathyUNet model for satellite-derived bathymetry analyzed for depth inference reliability and cross-site transfer challenges, with practical deployment guidance.


<details>
  <summary>Details</summary>
Motivation: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across different sites remains challenging, requiring better understanding of how deep learning models infer depth and when their predictions are trustworthy.

Method: Analyzed Swin-Transformer based U-Net (Swin-BathyUNet) using leave-one-band out study to rank spectral importance, adapted ablation-based CAM to regression (A-CAM-R) with performance retention validation, conducted attention ablations, and performed cross-region inference tests.

Result: Green/blue channels most important for depth inference; A-CAM-R reliably localizes evidence; decoder cross attention improves robustness to glint/foam; cross-region inference shows depth-dependent degradation with MAE rising linearly with depth and bimodal distributions exacerbating errors.

Conclusion: Practical guidance includes maintaining wide receptive fields, preserving radiometric fidelity in green/blue channels, pre-filtering bright high variance near shore, and pairing light target site fine-tuning with depth-aware calibration for cross-region transfer.

Abstract: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.

</details>


### [122] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: Mixed precision quantization framework for PointPillars LIDAR 3D object detection that handles wide numerical distributions and outliers through layer sensitivity analysis and calibration optimization, achieving up to 2.35x speedup and 2.26x model size reduction.


<details>
  <summary>Details</summary>
Motivation: LIDAR 3D object detection needs real-time operation for autonomous vehicles, but standard quantization causes performance degradation due to LIDAR's wide numerical distributions and extreme outliers.

Method: Proposed mixed precision framework: 1) PTQ-based layer sensitivity search to identify top-k sensitive layers kept as FP, 2) greedy search for mixed precision combinations, 3) finalization with PTQ or QAT, 4) small calibration data strategy to reduce outlier impact.

Result: Mixed precision models achieve competitive performance to FP models, with TensorRT deployment offering up to 2.35x latency reduction and 2.26x model size reduction.

Conclusion: The framework effectively addresses LIDAR quantization challenges, enabling real-time 3D object detection with minimal performance degradation through intelligent mixed precision allocation and outlier handling strategies.

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [123] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: Hyperparameters optimized on one cancer dataset can be transferred to non-IID federated learning scenarios using a simple cross-dataset aggregation heuristic that averages learning rates and uses modal optimizers/batch sizes.


<details>
  <summary>Details</summary>
Motivation: Deep learning for cancer histopathology faces privacy constraints in clinical settings. Federated Learning addresses privacy by keeping data local, but its performance depends on hyperparameter choices under non-IID client datasets. The paper examines whether hyperparameters optimized on one cancer imaging dataset generalize across non-IID federated scenarios.

Method: Used binary histopathology tasks for ovarian and colorectal cancers. Performed centralized Bayesian hyperparameter optimization and transferred dataset-specific optima to non-IID FL setup. Introduced a simple cross-dataset aggregation heuristic by combining configurations through averaging learning rates and considering modal optimizers and batch sizes.

Result: The combined configuration achieves competitive classification performance in non-IID federated learning scenarios.

Conclusion: Hyperparameters optimized on individual cancer datasets can be effectively transferred to non-IID federated learning settings using a simple aggregation approach, enabling competitive performance while maintaining privacy constraints in clinical cancer histopathology applications.

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [124] [Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface](https://arxiv.org/abs/2601.12666)
*Zonglin Li,Jieji Ren,Shuangfan Zhou,Heng Guo,Jinnuo Zhang,Jiang Zhou,Boxin Shi,Zhanyu Ma,Guoying Gu*

Main category: cs.CV

TL;DR: Single-shot surface reconstruction using neural implicit representations for depth and BRDF under mono-chromaticity assumption, validated with optical tactile sensor.


<details>
  <summary>Details</summary>
Motivation: Existing color photometric stereo methods assume ideal distant lighting and Lambertian reflectance, leaving practical near-light conditions and non-Lambertian surfaces underexplored.

Method: Proposes a framework using neural implicit representations for depth and BRDF modeling under mono-chromaticity assumption (uniform chromaticity and homogeneous material), which alleviates ill-posedness of color photometric stereo.

Result: Achieves accurate and robust surface reconstruction from just one image, demonstrated on both synthetic and real-world datasets.

Conclusion: The method enables detailed surface recovery from single images under practical near-light conditions and non-Lambertian surfaces, validated with a compact optical tactile sensor.

Abstract: Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.

</details>


### [125] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: Federated learning with test-time augmentation significantly improves brain tumor MRI classification, outperforming preprocessing alone.


<details>
  <summary>Details</summary>
Motivation: Brain tumor diagnosis is challenging due to lesion variability and image complexity, requiring efficient methods for early treatment.

Method: Evaluated CNNs in federated learning setting, comparing original vs preprocessed MRI images with test-time augmentation.

Result: Preprocessing alone had negligible gains, but combined with TTA delivered statistically significant improvements (p<0.001).

Conclusion: TTA should be default inference strategy in FL-based medical imaging; when possible, pair with light preprocessing for additional gains.

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [126] [VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness](https://arxiv.org/abs/2601.12672)
*Qimao Chen,Fang Li,Shaoqing Xu,Zhiyi Lai,Zixun Xie,Yuechen Luo,Shengyin Jiang,Hanbing Li,Long Chen,Bing Wang,Yi Zhang,Zhi-Xin Yang*

Main category: cs.CV

TL;DR: VILTA integrates Vision Language Models directly into autonomous driving training loops to generate diverse, challenging scenarios by editing agent trajectories, overcoming limitations of two-stage frameworks and improving safety on long-tail events.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems face safety challenges due to the long-tail problem where rare critical scenarios are underrepresented in real-world data. Existing approaches using rule-based heuristics, resampling, or two-stage VLM frameworks have limited ability to generate diverse and novel challenges, constraining the generalization potential of VLMs.

Method: VILTA (VLM-In-the-Loop Trajectory Adversary) integrates a Vision Language Model directly into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories.

Result: The approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events. The direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios beyond traditional methods.

Conclusion: VILTA represents a novel framework that overcomes limitations of existing safety-critical scenario generation methods by integrating VLMs directly into the training loop, enabling more effective handling of long-tail driving scenarios and improving autonomous driving safety.

Abstract: The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.

</details>


### [127] [Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement](https://arxiv.org/abs/2601.12682)
*Banglei Guan,Dongcai Tan,Jing Tao,Ang Su,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: Proposed image fusion-restoration methods to suppress thermal radiation and heat haze effects in high-temperature DIC deformation measurement, improving accuracy and computation area.


<details>
  <summary>Details</summary>
Motivation: Thermal radiation and heat haze cause image degradation and random errors in high-temperature deformation measurement using DIC, limiting accuracy and effectiveness.

Method: 1) Multi-exposure image fusion with positive/negative channel decomposition for thermal radiation suppression; 2) FSIM-guided iterative optimization with grayscale averaging for heat haze reduction.

Result: Effective computation area increased from 26% to 50% for under-exposed and 32% to 40% for over-exposed images; static thermal deformation errors reduced by 85.3% (ε_xx), 36.0% (ε_yy), and 36.4% (γ_xy).

Conclusion: The proposed image processing methods effectively suppress thermal radiation and heat haze interference, improve image quality, reduce deformation measurement errors, and have potential application value in thermal deformation measurement.

Abstract: In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.

</details>


### [128] [GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation](https://arxiv.org/abs/2601.12683)
*Liwei Liao,Ronggang Wang*

Main category: cs.CV

TL;DR: GaussianTrimmer is a plug-and-play post-processing method that trims jagged boundaries in 3D Gaussian segmentation by using virtual cameras and 2D segmentation results.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian segmentation methods produce jagged object boundaries because large Gaussian primitives often span foreground and background due to their variable scales.

Method: Two-step approach: 1) Generate uniformly distributed virtual cameras with good coverage, 2) Trim Gaussian primitives at the primitive level based on 2D segmentation results from these virtual cameras.

Result: Extensive experiments show the method effectively improves segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play solution.

Conclusion: GaussianTrimmer provides an efficient, plug-and-play post-processing solution for refining boundaries in 3D Gaussian segmentation, addressing the jagged boundary problem caused by variable Gaussian scales.

Abstract: With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.

</details>


### [129] [Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation](https://arxiv.org/abs/2601.12697)
*Chao Yang,Deshui Miao,Chao Tian,Guoqing Zhu,Yameng Gu,Zhenyu He*

Main category: cs.CV

TL;DR: IVGF is a novel 3D Gaussian-based framework for infrared-visible image fusion that reconstructs scene geometry from multimodal inputs and enables direct rendering of fused images, addressing limitations of 2D fusion methods.


<details>
  <summary>Details</summary>
Motivation: Existing 2D infrared-visible fusion methods focus on fixed camera viewpoints, which leads to incomplete scene understanding and loss of critical information in complex scenarios.

Method: Proposes Infrared-Visible Gaussian Fusion (IVGF) framework with: 1) Cross-modal adjustment (CMA) module to modulate Gaussian opacity for resolving cross-modal conflicts, 2) Fusion loss to guide CMA optimization and preserve distinctive features from both modalities.

Result: Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method in achieving better fusion results compared to existing approaches.

Conclusion: IVGF successfully addresses the limitations of 2D fusion methods by reconstructing scene geometry and enabling direct rendering of fused images while preserving critical characteristics from both infrared and visible modalities.

Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.

</details>


### [130] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: P2L-CA is a parameter-efficient framework for multi-label class-incremental learning that uses prompt-to-label modules and continuous adapters to reduce computational costs while improving performance without memory buffers.


<details>
  <summary>Details</summary>
Motivation: Existing multi-label class-incremental learning approaches suffer from high computational costs (full-parameter fine-tuning), substantial storage overhead (memory buffers), and inadequate handling of feature confusion and domain discrepancies between pre-trained models and downstream tasks.

Method: P2L-CA integrates two modules: 1) Prompt-to-Label (P2L) module that uses class-specific prompts to disentangle multi-label representations and incorporates linguistic priors for stable semantic-visual alignment, and 2) Continuous Adapter (CA) module that employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks.

Result: Extensive experiments on MS-COCO and PASCAL VOC show P2L-CA achieves substantial improvements over state-of-the-art methods, demonstrates strong generalization in CIL scenarios, requires minimal trainable parameters, and eliminates the need for memory buffers.

Conclusion: P2L-CA provides an effective parameter-efficient solution for multi-label class-incremental learning that addresses computational cost, storage overhead, and domain adaptation challenges while maintaining strong performance and generalization capabilities.

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [131] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: RSOD is a teacher-student framework for sonar image object detection that uses reliability scoring and object mixed pseudo-labels to work with extremely limited labeled data, achieving competitive results with only 5% labels.


<details>
  <summary>Details</summary>
Motivation: Sonar images have fewer texture details and more noise than natural images, making them hard for non-experts to annotate precisely. This creates a need for effective object detection methods that can work with extremely limited labeled data in sonar imaging.

Method: Proposes RSOD teacher-student framework: 1) calculates reliability scores by assessing teacher prediction consistency across different views, 2) introduces object mixed pseudo-label method to address labeled data shortage, 3) implements reliability-guided adaptive constraint to optimize student performance using unlabeled data.

Result: On UATD dataset, using only 5% of labeled data, RSOD achieves results competitive with baseline algorithm trained on 100% labeled data. Also collected a new dataset to provide more valuable data for sonar research.

Conclusion: RSOD effectively addresses the challenge of limited labeled data in sonar image object detection by leveraging unlabeled data through a teacher-student framework with reliability scoring and pseudo-label strategies, enabling good performance even with extremely limited annotations.

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [132] [S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation](https://arxiv.org/abs/2601.12719)
*Lin Zhao,Yushu Wu,Aleksei Lebedev,Dishani Lahiri,Meng Dong,Arpit Sahni,Michael Vasilkovsky,Hao Chen,Ju Hu,Aliaksandr Siarohin,Sergey Tulyakov,Yanzhi Wang,Anil Kag,Yanyu Li*

Main category: cs.CV

TL;DR: S2DiT is an efficient streaming video generation model that achieves state-of-the-art quality while running at 10+ FPS on mobile devices like iPhone through novel attention mechanisms and distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers (DiTs) for video generation have heavy computational costs that make real-time or on-device generation infeasible, limiting practical applications on mobile hardware.

Method: Introduces S2DiT with: 1) Novel efficient attention mechanisms (LinConv Hybrid Attention and Stride Self-Attention), 2) Sandwich design discovered via budget-aware dynamic programming search, and 3) 2-in-1 distillation framework to transfer knowledge from large teacher models to compact few-step models.

Result: Achieves video generation quality on par with state-of-the-art server models while streaming at over 10 FPS on an iPhone, making real-time mobile video generation feasible.

Conclusion: S2DiT demonstrates that efficient, high-fidelity streaming video generation on mobile hardware is achievable through careful architectural design and distillation techniques, bridging the gap between server-quality models and mobile deployment.

Abstract: Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.

</details>


### [133] [KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction](https://arxiv.org/abs/2601.12736)
*Qingtian Zhu,Xu Cao,Zhixiang Wang,Yinqiang Zheng,Takafumi Taketomi*

Main category: cs.CV

TL;DR: KaoLRM adapts Large Reconstruction Model's 3D prior for parametric face reconstruction using FLAME-based 2D Gaussian Splatting to improve viewpoint consistency.


<details>
  <summary>Details</summary>
Motivation: Existing 3DMM regressors for facial reconstruction suffer from poor consistency across varying viewpoints, making them sensitive to viewpoint variations.

Method: Harnesses LRM's pre-trained 3D prior, projects triplane features into FLAME parameter space for geometry, and models appearance via FLAME-mesh-coupled 2D Gaussian primitives in the rendering pipeline.

Result: Achieves superior reconstruction accuracy and cross-view consistency on controlled and in-the-wild benchmarks, handling self-occlusions and diverse viewpoints better than existing methods.

Conclusion: KaoLRM effectively leverages LRM's rich 3D prior to create a robust FLAME regressor that maintains accuracy across varying viewpoints, addressing a key limitation of current 3DMM-based approaches.

Abstract: We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.

</details>


### [134] [SSPFormer: Self-Supervised Pretrained Transformer for MRI Images](https://arxiv.org/abs/2601.12747)
*Jingkai Li,Xiaoze Tian,Yuhang Shen,Jia Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: SSPFormer is a self-supervised pretrained transformer for MRI that uses inverse frequency projection masking and frequency-weighted FFT noise enhancement to learn domain-specific, artifact-robust features from unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Direct transfer of pre-trained transformers to MRI faces two challenges: inability to adapt to medical anatomical specificity, and limitations from privacy/scarcity of medical data. Need domain-specific feature learning from unlabeled data.

Method: Proposes SSPFormer with two key strategies: 1) Inverse frequency projection masking that prioritizes reconstruction of high-frequency anatomical regions for structure-aware learning, and 2) Frequency-weighted FFT noise enhancement that injects physiologically realistic noise in Fourier domain for artifact robustness.

Result: Achieves state-of-the-art performance on segmentation, super-resolution, and denoising tasks, verifying ability to capture fine-grained MRI image fidelity and adapt to clinical requirements.

Conclusion: SSPFormer effectively learns domain-invariant and artifact-robust features directly from raw MRI scans, overcoming domain gap and data scarcity challenges through self-supervised pretraining strategies.

Abstract: The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.

</details>


### [135] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: CLIP-style models can learn left-right spatial relations through contrastive training, with label diversity being more important than layout diversity for generalization, mediated by attention gradients from positional-token embedding interactions.


<details>
  <summary>Details</summary>
Motivation: To understand whether vision-language models truly acquire spatial understanding and through what mechanisms, specifically focusing on left-right relational understanding in CLIP-style models.

Method: Created a controllable 1D image-text testbed with lightweight Transformer encoders trained end-to-end on paired descriptions of one- and two-object scenes, evaluated generalization with systematic variation of label and layout diversity, and performed attention decomposition analysis.

Result: Contrastive training successfully learns left-right relations; label diversity drives generalization more than layout diversity; attention decomposition reveals positional-token embedding interactions create horizontal attention gradients that break left-right symmetry.

Conclusion: The study provides mechanistic insight into how CLIP-style models acquire relational competence, showing that contrastive objectives combined with sufficient label diversity enable spatial understanding through attention gradient mechanisms.

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [136] [Moaw: Unleashing Motion Awareness for Video Diffusion Models](https://arxiv.org/abs/2601.12761)
*Tianqi Zhang,Ziyi Wang,Wenzhao Zheng,Weiliang Chen,Yuanhui Huang,Zhengyang Huang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Moaw is a framework that repurposes video diffusion models for motion transfer by training one for motion perception and injecting its learned motion features into a video generation model.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models naturally capture motion correspondences across frames, but supervised training could better harness their tracking capabilities for motion understanding and transfer tasks.

Method: Train a diffusion model for motion perception (video-to-dense-tracking), identify motion-encoding features using a motion-labeled dataset, then inject these features into a structurally identical video generation model without adapters.

Result: Enables zero-shot motion transfer by leveraging homogeneity between networks, allowing motion features to be naturally adapted between perception and generation models.

Conclusion: Provides a new paradigm for bridging generative modeling and motion understanding, enabling more unified and controllable video learning frameworks.

Abstract: Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.

</details>


### [137] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: YOLO26 eliminates NMS post-processing through end-to-end learning, achieving superior speed and accuracy over previous YOLO versions and competitors.


<details>
  <summary>Details</summary>
Motivation: Traditional YOLO frameworks (v1-v11) are constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing, creating a trade-off between speed and accuracy.

Method: YOLO26 introduces three key innovations: MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision, enabling native end-to-end learning without NMS.

Result: YOLO26 establishes a new Pareto front, outperforming predecessors and state-of-the-art competitors (RTMDet, DAMO-YOLO) in both inference speed and detection accuracy.

Conclusion: By decoupling representation learning from heuristic post-processing, YOLO26 resolves the historical latency-precision trade-off, representing the next evolutionary step in edge-based computer vision.

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [138] [Towards Unbiased Source-Free Object Detection via Vision Foundation Models](https://arxiv.org/abs/2601.12765)
*Zhi Cai,Yingjie Gao,Yanan Zhang,Xinzhu Ma,Di Huang*

Main category: cs.CV

TL;DR: DSOD is a novel VFM-assisted source-free object detection framework that mitigates source bias through unified feature injection and semantic-aware regularization, achieving SOTA results on multiple cross-domain benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing SFOD methods suffer from Source Bias problem where adapted models remain skewed towards source domain, leading to poor generalization and error accumulation during self-training.

Method: Proposes DSOD with Unified Feature Injection (UFI) module integrating VFM features via Simple-Scale Extension and Domain-aware Adaptive Weighting, plus Semantic-aware Feature Regularization to prevent source overfitting. Also introduces DSOD-distill variant for computation-restricted scenarios using Dual-Teacher distillation.

Result: Outperforms state-of-the-art SFOD methods: 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation across multiple benchmarks.

Conclusion: DSOD effectively mitigates source bias in source-free object detection through VFM assistance and novel feature integration techniques, demonstrating superior performance across diverse domain adaptation scenarios.

Abstract: Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.

</details>


### [139] [Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration](https://arxiv.org/abs/2601.12766)
*Lu Yue,Yue Fan,Shiwei Lian,Yu Zhao,Jiaxin Yu,Liang Xie,Feitian Zhang*

Main category: cs.CV

TL;DR: Spatial-VLN is a perception-guided exploration framework that addresses spatial perception bottlenecks in zero-shot VLN using specialized experts and active exploration to improve performance in complex continuous environments.


<details>
  <summary>Details</summary>
Motivation: Zero-shot VLN agents using LLMs have good generalization but suffer from insufficient spatial perception, especially in complex continuous environments with three key challenges: door interaction, multi-room navigation, and ambiguous instruction execution where existing methods have high failure rates.

Method: Two main modules: 1) Spatial Perception Enhancement (SPE) with panoramic filtering and specialized door/region experts for coherent perceptual representations; 2) Explored Multi-expert Reasoning (EMR) using parallel LLM experts for waypoint semantics and region transitions, with query-and-explore mechanism to resolve discrepancies through active probing.

Result: Achieves state-of-the-art performance on VLN-CE using only low-cost LLMs, and introduces value-based waypoint sampling strategy that effectively bridges Sim2Real gap, with extensive real-world evaluations confirming superior generalization and robustness.

Conclusion: Spatial-VLN framework successfully addresses spatial perception bottlenecks in zero-shot VLN, demonstrating both strong performance in simulation and practical real-world applicability through effective Sim2Real bridging.

Abstract: Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.

</details>


### [140] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: Researchers developed specialized architectures for Membership Inference Tests (MINT) in object recognition, achieving 70-80% precision in identifying whether data was used during training, with performance varying based on detection module layer depth.


<details>
  <summary>Details</summary>
Motivation: To address the need for determining whether specific data was used during model training in object recognition systems, particularly for transparency and understanding data utilization in training processes.

Method: Proposed tailored MINT architectures using convolutional layers to capture activation patterns, combining object detection models, embedding extractors, and MINT modules, tested across three public databases with over 174K images.

Result: Achieved precision rates of 70-80% in identifying training data usage, with performance dependent on the depth of detection module layers used as input to the MINT module.

Conclusion: Specialized MINT architectures can effectively identify training data usage in object recognition systems, with layer depth being a key factor influencing performance, contributing to more transparent training processes.

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [141] [Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval](https://arxiv.org/abs/2601.12768)
*Zequn Xie,Boyun Zhang,Yuxiao Lin,Tao Jin*

Main category: cs.CV

TL;DR: HVP-Net improves video-text retrieval by extracting hierarchical features from multiple vision encoder layers to reduce redundancy and enhance semantic matching.


<details>
  <summary>Details</summary>
Motivation: Current video-text retrieval methods using pre-trained models like CLIP suffer from video redundancy and reliance on coarse final-layer features, limiting matching accuracy.

Method: Proposes HVP-Net (Hierarchical Visual Perception Network) that extracts and refines features from multiple intermediate layers of a vision encoder, progressively distilling salient visual concepts from raw patch-tokens at different semantic levels.

Result: Achieves new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet.

Conclusion: Validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval, with code publicly available.

Abstract: Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.

</details>


### [142] [Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image](https://arxiv.org/abs/2601.12770)
*Shuling Zhao,Dan Xu*

Main category: cs.CV

TL;DR: One-shot 3D full-head animatable avatar reconstruction from a single image with real-time animation and 360° rendering


<details>
  <summary>Details</summary>
Motivation: Existing methods fail under large camera pose variations, compromising realism of 3D avatars. Need efficient one-shot reconstruction with real-time animation capabilities.

Method: Uses Gaussian primitives embedded on parametric face model in UV space; leverages pretrained 3D GAN for full-head priors; fuses local image features with global textures using facial symmetry.

Result: Achieves high-quality 3D full-head modeling with real-time animation, improving realism of 3D talking avatars compared to existing methods.

Conclusion: Proposed framework enables efficient one-shot 3D full-head animatable avatar reconstruction with real-time capabilities and improved realism under large pose variations.

Abstract: Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.

</details>


### [143] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: TVWorld introduces a graph-based abstraction for evaluating TV navigation capabilities in LVLMs, revealing topology awareness limitations and proposing a topology-aware training framework that achieves SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing LVLM research focuses on point-and-click interaction while neglecting remote-control TV navigation, which is common in everyday use. There's a need for reproducible, deployment-free evaluation of TV-use capabilities.

Method: Created TVWorld - an offline graph-based abstraction of real-world TV navigation. Derived two benchmarks: TVWorld-N for topology-aware navigation and TVWorld-G for focus-aware grounding. Proposed Topology-Aware Training framework to inject topology awareness into LVLMs, resulting in TVTheseus foundation model.

Result: TVTheseus achieves 68.3% success rate on TVWorld-N, surpassing strong closed-source baselines like Gemini 3 Flash and establishing state-of-the-art performance. Benchmarks reveal key limitation: insufficient topology awareness for focus-based, long-horizon TV navigation.

Conclusion: The work addresses the underexplored area of remote-control TV navigation, provides comprehensive evaluation benchmarks, and demonstrates that topology-aware training significantly improves LVLM performance on TV navigation tasks, offering valuable insights for developing effective TV-use agents.

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [144] [Open Vocabulary Panoptic Segmentation With Retrieval Augmentation](https://arxiv.org/abs/2601.12779)
*Nafis Sadeq,Qingfeng Liu,Mostafa El-Khamy*

Main category: cs.CV

TL;DR: RetCLIP improves open-vocabulary panoptic segmentation by combining retrieval-augmented classification with CLIP scores, achieving significant gains on unseen classes.


<details>
  <summary>Details</summary>
Motivation: Traditional panoptic segmentation systems trained on specific datasets fail to generalize to unseen classes. Open-vocabulary segmentation aims to handle arbitrary user-specified classes, but current methods struggle with generalization beyond training data.

Method: RetCLIP builds a masked segment feature database from paired image-text data. At inference, masked segment features from input images query this database to retrieve similar features and associated class labels. Classification scores combine retrieval-based similarity with CLIP-based scores.

Result: When trained on COCO, RetCLIP achieves 30.9 PQ, 19.3 mAP, 44.0 mIoU on ADE20k, representing +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over baseline FC-CLIP.

Conclusion: Retrieval-augmented classification significantly improves open-vocabulary panoptic segmentation performance on unseen classes by leveraging external knowledge from image-text databases.

Abstract: Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.

</details>


### [145] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: A U-Net CNN approach outperformed transformer models in medical imaging challenges, training faster with smaller models while leveraging anatomical priors.


<details>
  <summary>Details</summary>
Motivation: To develop efficient foundation models for medical image analysis that overcome unique challenges in radiological tasks, specifically for 3D brain MRI analysis.

Method: U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge, specifically designed for SSL3D and FOMO25 challenges.

Result: Ranked first in tracks of both MICCAI 2025 challenges, with models that trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches.

Conclusion: CNN-based approaches with domain-specific priors can outperform transformer models in medical imaging tasks while being significantly more efficient in training time and model size.

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [146] [SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification](https://arxiv.org/abs/2601.12791)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Hongyuan Shu,Junchu Zhao,Yanjun Huang,Yue Xiu,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: SKANet is a cognitive deep learning framework that uses dual-stream architecture with Time-Frequency Images and Power Spectral Density to classify compound GNSS jamming interference, achieving 96.99% accuracy.


<details>
  <summary>Details</summary>
Motivation: GNSS faces growing threats from sophisticated jamming interference. While DL works for basic interference, classifying compound interference is difficult due to superposition of diverse jamming sources. Single-domain approaches suffer performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales.

Method: Proposes SKANet (Selective Kernel and Asymmetric convolution Network) with dual-stream architecture integrating Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Uses Multi-Branch Selective Kernel module with Asymmetric Convolution Blocks to dynamically adjust receptive fields. Includes Squeeze-and-Excitation mechanism at fusion stage to adaptively recalibrate heterogeneous feature contributions.

Result: Achieves 96.99% overall accuracy on dataset of 405,000 samples. Shows superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

Conclusion: SKANet effectively addresses compound interference classification by dynamically adapting receptive fields to capture both micro-scale transient features and macro-scale spectral trends, outperforming conventional approaches.

Abstract: As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

</details>


### [147] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: First adversarial framework using realistic rain perturbations to attack Vision-Language Models, showing weather conditions can cause significant semantic misalignment in VLMs.


<details>
  <summary>Details</summary>
Motivation: VLMs are trained on canonical visual conditions but their robustness to real-world weather conditions and stability of cross-modal semantic alignment under structured perturbations remain insufficiently studied.

Method: Two-stage parameterized perturbation model: Stage 1 applies low-dimensional global modulation to weaken semantic decision boundaries; Stage 2 introduces structured rain variations by modeling multi-scale raindrop appearance and illumination changes, optimizing the non-differentiable weather space.

Result: Even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing safety and reliability risks in real-world deployment.

Conclusion: Illumination modeling and multi-scale raindrop structures are key drivers of semantic shifts in VLMs under rainy conditions, revealing vulnerabilities that need addressing for safe real-world deployment.

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [148] [Combating Noisy Labels through Fostering Self- and Neighbor-Consistency](https://arxiv.org/abs/2601.12795)
*Zeren Sun,Yazhou Yao,Tongliang Liu,Zechao Li,Fumin Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: Jo-SNC: A noise-robust method combining sample selection and model regularization using self- and neighbor-consistency to handle both in-distribution and out-of-distribution label noise.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling label noise often neglect imbalances in noise across mini-batches and insufficiently address out-of-distribution noisy data, while deep networks are vulnerable to memorizing corrupted labels.

Method: Uses Jensen-Shannon divergence to measure sample cleanliness likelihood considering nearest neighbors, employs self-adaptive data-driven thresholding for per-class selection, trains clean samples normally, in-distribution noisy samples with partial label learning, out-of-distribution noisy samples with negative learning, and adds triplet consistency regularization for self-prediction, neighbor-prediction, and feature consistency.

Result: Extensive experiments on benchmark datasets show the method is effective and superior to existing state-of-the-art approaches.

Conclusion: Jo-SNC successfully addresses limitations of previous methods by jointly handling sample selection and model regularization while considering both in-distribution and out-of-distribution label noise.

Abstract: Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.

</details>


### [149] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: ORACLE-CT: Organ-aware attention model for CT triage achieves state-of-the-art classification across chest and abdomen CT with AUROC 0.86 on CT-RATE and 0.85 on MERLIN.


<details>
  <summary>Details</summary>
Motivation: Urgent need for automated triage and classification of high-volume CT scans to improve patient care and reduce radiologist burnout, with current VLMs struggling with 3D anatomy, protocol shifts, and noisy report supervision.

Method: ORACLE-CT uses encoder-agnostic organ-aware head with Organ-Masked Attention (mask-restricted per-organ pooling for spatial evidence) and Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues), built on carefully tuned supervised baseline.

Result: Achieves AUROC 0.86 on CT-RATE chest dataset and AUROC 0.85 on MERLIN abdomen dataset (30 findings), surpassing all reported linear-probe VLMs and establishing new supervised state-of-the-art across both chest and abdomen CT.

Conclusion: ORACLE-CT delivers state-of-the-art supervised classification performance across chest and abdomen CT under unified evaluation protocol, providing calibrated predictions with localized evidence for effective CT triage.

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [150] [PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition](https://arxiv.org/abs/2601.12798)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Yue Xiu,Lu Chen,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: PhyG-MoE is a physics-guided mixture-of-experts framework that dynamically aligns model capacity with signal complexity for GNSS interference recognition, achieving 97.58% accuracy while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current static deep learning models for GNSS interference recognition suffer from fixed computational topologies regardless of input physical entropy, causing resource mismatch where simple signals consume same processing as complex ones, compromising SAGIN reliability.

Method: Proposes PhyG-MoE framework with spectrum-based gating mechanism that routes signals based on spectral feature entanglement. Uses high-capacity TransNeXt expert for complex saturated scenarios and lightweight experts for fundamental signals to minimize latency.

Result: Achieves 97.58% overall accuracy on 21 jamming categories. Resolves conflict between static computing and dynamic electromagnetic environments, significantly reducing computational overhead without performance degradation.

Conclusion: PhyG-MoE offers viable solution for resource-constrained cognitive receivers by dynamically aligning model capacity with signal complexity, enabling efficient GNSS interference recognition in SAGIN.

Abstract: Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.

</details>


### [151] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: DIP-ℓ₀ is a deep image prior framework that uses ℓ₀ gradient regularization for training-free image smoothing, outperforming existing methods in edge preservation and JPEG artifact removal.


<details>
  <summary>Details</summary>
Motivation: Traditional image smoothing methods rely on local statistics or optimization, while deep learning approaches require carefully curated training datasets, which are challenging to construct for image smoothing tasks.

Method: Proposes DIP-ℓ₀, a deep image prior framework incorporating ℓ₀ gradient regularizer, with an ADMM algorithm using off-the-shelf ℓ₀ gradient minimization solver to handle the nonconvex, nonsmooth ℓ₀ "norm".

Result: Numerical experiments show DIP-ℓ₀ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal without requiring training data.

Conclusion: The proposed DIP-ℓ₀ framework enables high-quality image smoothing without training data by combining deep image prior with ℓ₀ gradient regularization, demonstrating superior performance over existing methods.

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [152] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: QVLM uses code generation to preserve pixel-level information for quantitative spatial reasoning in satellite images, achieving 42% accuracy vs 28% for standard VLMs.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models fail at quantitative spatial reasoning because their architectures destroy pixel-level information needed for counting and measurements through patch embeddings that lose spatial indexing.

Method: QVLM decouples language understanding from visual analysis by generating executable code that calls a segmentation model to obtain pixel-level masks, then operates directly on these masks to preserve spatial indexing throughout reasoning.

Result: QVLM using GPT-5 as coder achieves 42.0% accuracy on the SQuID benchmark, significantly outperforming standard VLMs (28.1%). The SQuID dataset contains 2,000 satellite image QA pairs with numerical range and categorical answers across three difficulty tiers.

Conclusion: Architectural decoupling enables better accuracy on quantitative spatial reasoning tasks by maintaining pixel precision, revealing that current VLM architectures fundamentally limit quantitative spatial reasoning capabilities.

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [153] [CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting](https://arxiv.org/abs/2601.12814)
*Yu-Jen Tseng,Chia-Hao Kao,Jing-Zhong Chen,Alessandro Gnutti,Shao-Yuan Lo,Yen-Yu Lin,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: First unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting, enabling joint compression and semantic learning for decoder-side applications.


<details>
  <summary>Details</summary>
Motivation: Prior works treat 3DGS compression and segmentation independently, leaving joint optimization unexplored. Need to support decoder-side applications like scene editing and manipulation beyond traditional reconstruction and view synthesis.

Method: Integrates semantic learning into compression pipeline with lightweight implicit neural representation-based hyperprior for efficient entropy coding of color and semantic attributes. Uses compression-guided segmentation learning with quantization-aware training and quality-aware weighting mechanism.

Result: Significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance on LERF and 3D-OVS datasets.

Conclusion: First unified framework successfully combines rate-distortion-optimized compression and segmentation of 3DGS, enabling efficient transmission with semantic capabilities for decoder-side applications.

Abstract: We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.

</details>


### [154] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: Paper introduces interpretable explanation methods for black-box neural networks using logical formulas over human-recognizable concepts.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are highly effective but opaque and hard to interpret, creating a need for explanation methods that make model decisions understandable to humans.

Method: Develop local and global explanation methods that generate explanations as logical formulas in monotone disjunctive-normal-form (MDNF) using human-recognizable primitive concepts. Also present algorithm for multi-class explanations as monotone explanation lists.

Result: The proposed explanations maintain high fidelity and coverage with respect to black-box models while being simple and interpretable, demonstrated on challenging vision datasets.

Conclusion: The paper presents effective interpretable explanation methods that bridge the gap between black-box model performance and human understanding through logical formulations over recognizable concepts.

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [155] [A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling](https://arxiv.org/abs/2601.12820)
*Wei Chen,Liang Wu,Shuyi Lu,Yuanyuan Sun,Wenkai Bi,Zilong Yuan,Yaoyao He,Feng Wang,Junchi Ma,Shuyong Liu,Zhaoping Cheng,Xiaoyan Hu,Jianfeng Qiu*

Main category: cs.CV

TL;DR: SDF-HOLO is a multimodal foundation model for total-body PET/CT that uses dual-stream encoders, cross-modal interaction, hierarchical context modeling, and voxel-mask-text alignment to enable system-wide molecular imaging analysis.


<details>
  <summary>Details</summary>
Motivation: Total-body PET/CT presents unique challenges for medical AI: heterogeneous anatomical/metabolic signals, ~2m axial coverage, and complex radiology semantics that existing models can't handle due to assumptions about single-modality inputs, localized fields of view, and coarse image-text alignment.

Method: 1) Dual-stream encoders decouple CT and PET representation learning with cross-modal interaction; 2) Hierarchical context modeling combines local windows with global attention for long-range dependencies; 3) Anatomical segmentation masks as semantic anchors with voxel-mask-text alignment during pre-training on 10,000+ patients.

Result: Outperforms task-specific and clinical-reference baselines across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation while reducing localization errors and hallucinated findings. Enables system-wide metabolic profiling and reveals tumor-associated inter-organ metabolic network interactions.

Conclusion: SDF-HOLO provides a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology, moving beyond focal interpretation to enable holistic analysis of system-wide molecular imaging data.

Abstract: Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.

</details>


### [156] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: Deep learning with ResNet-18 predicts colon capsule endoscopy cleansing quality using pruning for efficiency and explainability methods for clinical trust.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient deep learning model for predicting colon cleansing quality in capsule endoscopy images that is both accurate and interpretable for clinical applications.

Method: Used ResNet-18 trained on 500 clinician-labeled CCE images with stratified K-fold cross-validation, applied structured pruning for sparsity, evaluated explainability with multiple CAM methods (Grad-CAM, Grad-CAM++, etc.) using ROAD evaluation, and employed adaptive temperature scaling for calibration.

Result: Achieved 88% cross-validation accuracy with 79% sparsity through pruning, improving efficiency from 84% baseline while maintaining performance; successfully applied explainability methods and calibrated models for external datasets.

Conclusion: Pruning effectively improves model efficiency without compromising accuracy for CCE cleansing quality prediction, and explainability methods are crucial for clinical trust, though challenges remain in evaluating cleansing quality and using ROAD method for this specific task.

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [157] [TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement](https://arxiv.org/abs/2601.12823)
*Belal Shaheen,Minh-Hieu Nguyen,Bach-Thuan Bui,Shubham,Tim Wu,Michael Fairley,Matthew David Zane,Michael Wu,James Tompkin*

Main category: cs.CV

TL;DR: TreeDGS uses 3D Gaussian Splatting from aerial imagery to accurately measure tree diameter at breast height (DBH), outperforming LiDAR with 4.79cm RMSE.


<details>
  <summary>Details</summary>
Motivation: Aerial remote sensing struggles with direct object-level measurement in complex natural scenes like forests. While 3D vision advances (NeRF, 3D Gaussian Splatting) improve reconstruction fidelity, measuring tree DBH from aerial imagery remains challenging due to distant, sparsely observed trunks that span only a few pixels at typical altitudes.

Method: TreeDGS leverages 3D Gaussian Splatting as a continuous, densifiable scene representation. After SfM-MVS initialization and Gaussian optimization, extracts dense point set using RaDe-GS's depth-aware cumulative-opacity integration with multi-view opacity reliability scores. Estimates DBH from trunk-isolated points using opacity-weighted solid-circle fitting.

Result: Evaluated on 10 plots with field-measured DBH, TreeDGS achieves 4.79cm RMSE (about 2.6 pixels at this GSD), outperforming state-of-the-art LiDAR baseline (7.91cm RMSE). Demonstrates densified splat-based geometry enables accurate, low-cost aerial DBH measurement.

Conclusion: TreeDGS successfully enables accurate tree DBH measurement from aerial imagery using 3D Gaussian Splatting, providing a cost-effective alternative to LiDAR for forest inventory applications.

Abstract: Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.

</details>


### [158] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: CARPE is a model-agnostic framework that improves LVLMs' vision capabilities by adaptively prioritizing image representations through vision-integration layers and context-aware ensemble strategy.


<details>
  <summary>Details</summary>
Motivation: LVLMs underperform on vision-centric tasks like image classification compared to their base vision encoders (CLIP-based models), showing a gap in effectively utilizing visual information.

Method: Proposes CARPE with vision-integration layers and context-aware ensemble strategy to identify when to prioritize image representations vs. rely on language model reasoning, enabling adaptive weighting of visual and textual modalities.

Result: Improves performance on both image classification benchmarks and various vision-language benchmarks, with consistent generalization improvements across tasks.

Conclusion: CARPE is an effective, adaptable framework that can be integrated with most open-source LVLMs to enhance their vision capabilities while maintaining language reasoning strengths.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [159] [Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification](https://arxiv.org/abs/2601.12826)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: Grad-CAM's faithfulness for lung cancer classification varies across architectures, with significant degradation in Vision Transformers, revealing limitations of current XAI methods in medical imaging.


<details>
  <summary>Details</summary>
Motivation: To critically investigate whether Grad-CAM truly represents the internal decision-making of deep models for lung cancer classification, as the faithfulness and reliability of heatmap-based explanations remain under scrutiny despite their popularity in medical image analysis.

Method: Evaluated five architectures (ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, ViT-Base-Patch16-224) on IQ-OTH/NCCD dataset using a quantitative framework combining localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across models.

Result: Grad-CAM effectively highlights tumor regions in most convolutional networks but interpretive fidelity significantly degrades for Vision Transformers due to non-local attention behavior. Cross-model comparisons show substantial variability in saliency localization, indicating Grad-CAM explanations may not always correspond to true diagnostic evidence used by networks.

Conclusion: Exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful, urging more cautious adoption of visual explanation tools and rethinking what it means to "trust" a model's explanation.

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to "trust" a model's explanation.

</details>


### [160] [FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection](https://arxiv.org/abs/2601.12863)
*Jun Wan,Xinyu Xiong,Ning Chen,Zhihui Lai,Jie Zhou,Wenwen Min*

Main category: cs.CV

TL;DR: FGTBT: A Frequency-Guided Task-Balancing Transformer for facial landmark detection that improves geometric structure perception through frequency-domain modeling and multi-dataset unified training with fine-grained task balancing.


<details>
  <summary>Details</summary>
Motivation: Current deep learning FLD methods struggle with challenging scenarios (large pose variations, illumination changes, facial expressions) and have difficulty capturing accurate facial geometric structure. Limited dataset size and diversity also hinder robust model training and reduce detection accuracy.

Method: Proposes FGTBT framework with two key components: 1) Fine-Grained Multi-Task Balancing loss (FMB-loss) that assigns weights to individual landmarks based on dataset occurrence for effective unified training, and 2) Frequency-Guided Structure-Aware (FGSA) model using frequency-guided structure injection and regularization to learn facial structure constraints.

Result: Extensive experiments on popular benchmark datasets demonstrate that FGTBT achieves performance comparable to state-of-the-art methods, effectively addressing challenges in facial landmark detection.

Conclusion: The proposed FGTBT framework successfully enhances facial structure perception through frequency-domain modeling and multi-dataset unified training with fine-grained task balancing, providing an effective solution for robust facial landmark detection in challenging scenarios.

Abstract: Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.

</details>


### [161] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: ACG is a single-pass contrastive guidance method that reduces hallucinations in LVLMs by steering attention away from language priors toward visual evidence, achieving SOTA performance with 2x lower latency.


<details>
  <summary>Details</summary>
Motivation: LVLMs often hallucinate when language priors dominate over visual evidence, causing object misidentification and inconsistent descriptions. Current methods are computationally expensive.

Method: Attention-space Contrastive Guidance (ACG) constructs vision-language and language-only attention paths in a single forward pass, then applies orthogonalized correction to remove language-only components and amplify visual contributions.

Result: ACG achieves state-of-the-art faithfulness and caption quality on CHAIR and POPE benchmarks while reducing computational cost by up to 2x compared to prior methods requiring multiple forward passes.

Conclusion: ACG provides a principled, efficient alternative for hallucination mitigation in LVLMs by embedding contrastive guidance directly in attention layers, balancing visual grounding with computational efficiency.

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [162] [Proxy Robustness in Vision Language Models is Effortlessly Transferable](https://arxiv.org/abs/2601.12865)
*Xiaowei Fu,Fuxiang Huang,Lei Zhang*

Main category: cs.CV

TL;DR: Proposes HPT-GPD framework for efficient adversarial robustness transfer in vision-language models without expensive adversarial training, achieving balance between robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Adversarial robustness transfer via distillation works for image models but is computationally prohibitive for large vision-language models like CLIP. Need efficient method to transfer robustness without sacrificing natural generalization.

Method: Heterogeneous Proxy Transfer (HPT) leverages cross-architectural robustness between CLIP variants, plus Generalization-Pivot Decoupling (GPD) using learning rate scheduling to separate generalization maintenance from robustness enhancement.

Result: Extensive experiments on 15 zero-shot datasets show effectiveness in achieving adversarial robustness while maintaining natural generalization, outperforming conventional approaches.

Conclusion: Proposes efficient framework for adversarial robustness transfer in VLMs without expensive adversarial training, achieving balance between robustness and generalization through novel proxy transfer and decoupling techniques.

Abstract: As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.

</details>


### [163] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: HAVEN is a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search to overcome information fragmentation and maintain global coherence.


<details>
  <summary>Details</summary>
Motivation: Long video understanding is challenging for vision-language models due to extremely long context windows. Existing solutions using naive chunking with retrieval-augmented generation suffer from information fragmentation and loss of global coherence.

Method: 1) Preserve semantic consistency by integrating entity-level representations across visual and auditory streams, 2) Organize content into a structured hierarchy (global summary, scene, segment, entity levels), 3) Employ agentic search mechanism for dynamic retrieval and reasoning across these layers.

Result: Achieves state-of-the-art with 84.1% overall accuracy on LVBench, with outstanding 80.1% performance in challenging reasoning category. Demonstrates good temporal coherence, entity consistency, and retrieval efficiency.

Conclusion: HAVEN highlights the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos, establishing a new approach to overcome fragmentation and coherence issues in long-video understanding.

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [164] [Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation](https://arxiv.org/abs/2601.12876)
*Zhenxuan Lu,Zhihua Xu,Zhijing Yang,Feng Gao,Yongyi Lu,Keze Wang,Tianshui Chen*

Main category: cs.CV

TL;DR: THFEM integrates audio-driven talking head generation with speech-preserving facial expression manipulation to maintain accurate lip sync while altering expressions.


<details>
  <summary>Details</summary>
Motivation: SPFEM struggles with accurate lip synchronization due to complex interplay between facial expressions and mouth shapes, despite preserving speech.

Method: THFEM framework combines AD-THG models with SPFEM, using adjacent frame learning strategy to finetune AD-THG models for consecutive frame prediction.

Result: Framework effectively preserves mouth shapes during expression manipulations, improving image quality and lip synchronization.

Conclusion: Integration of AD-THG with SPFEM provides substantial benefits for speech-preserving facial expression manipulation with accurate lip sync.

Abstract: Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.

</details>


### [165] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: Insight is a language-aligned concept foundation model that extracts human-interpretable, spatially-grounded concepts from vision models using hierarchical sparse autoencoders and semantic representations, enabling competitive performance with explainable decision-making.


<details>
  <summary>Details</summary>
Motivation: Current vision foundation models have opaque representations that are hard to interpret, and existing concept decomposition methods provide poor spatial grounding and are limited to image classification tasks.

Method: Uses hierarchical sparse autoencoder with a foundation model having strong semantic representations to automatically extract concepts at various granularities. Examines local co-occurrence dependencies to define concept relationships, improving concept naming and explanations.

Result: Insight provides competitive performance on classification and segmentation benchmarks compared to opaque foundation models while offering fine-grained, high-quality concept-based explanations.

Conclusion: Insight demonstrates that vision models can achieve strong performance while providing interpretable, spatially-grounded concept explanations, bridging the gap between opaque foundation models and human-understandable decision-making.

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [166] [Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning](https://arxiv.org/abs/2601.12889)
*Nazibul Basar Ayon,Abdul Hasib,Md. Faishal Ahmed,Md. Sadiqur Rahman,Kamrul Islam,T. M. Mehrab Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: Novel ensemble deep learning framework achieves 98.2% accuracy for simultaneous detection of Lumpy Skin Disease and Foot-and-Mouth Disease in cattle, addressing symptom overlap challenges with expert-annotated dataset from 18 farms across three countries.


<details>
  <summary>Details</summary>
Motivation: LSD and FMD are highly contagious viral diseases causing significant economic losses and welfare issues. Visual diagnosis is complicated by symptom overlap between these diseases and with benign conditions like insect bites or chemical burns, hindering timely control measures.

Method: Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging, trained on comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA for simultaneous LSD and FMD detection.

Result: State-of-the-art accuracy of 98.2%, with macro-averaged precision of 98.2%, recall of 98.1%, F1-score of 98.1%, and AUC-ROC of 99.5%. The model uniquely addresses symptom overlap challenge in multi-disease detection.

Conclusion: The approach enables early, precise, and automated diagnosis, potentially enhancing disease management and supporting global agricultural sustainability. Designed for future deployment in resource-limited settings.

Abstract: Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\%, with macro-averaged precision of 98.2\%, recall of 98.1\%, F1-score of 98.1\%, and an AUC-ROC of 99.5\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

</details>


### [167] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: OmniOVCD is a standalone open-vocabulary change detection framework that leverages SAM 3's decoupled output heads with a Synergistic Fusion to Instance Decoupling strategy, achieving state-of-the-art performance on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing training-free OVCD methods rely on combining different models (CLIP for category identification, DINO for feature extraction), which causes feature matching problems and system instability. SAM 3's integrated segmentation and identification capabilities offer new possibilities for a more unified approach.

Method: Proposes OmniOVCD framework with Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID fuses SAM 3's semantic, instance, and presence outputs to construct land-cover masks, then decomposes them into individual instance masks for change comparison, preserving category recognition accuracy and instance-level consistency.

Result: Achieves state-of-the-art performance on four public benchmarks: LEVIR-CD (67.2 IoU), WHU-CD (66.5 IoU), S2Looking (24.5 IoU), and SECOND (27.1 IoU), surpassing all previous methods.

Conclusion: OmniOVCD demonstrates that leveraging SAM 3's integrated capabilities through the SFID strategy enables accurate open-vocabulary change detection without the instability issues of multi-model approaches, establishing a new benchmark for the field.

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [168] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: TwoHead-SwinFPN: A unified deep learning model for detecting and localizing manipulated regions in ID documents using dual-head architecture with Swin Transformer backbone, achieving high accuracy for both classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of sophisticated generative AI models enabling synthetic manipulations in identity documents (face swapping and text inpainting attacks) creates a critical need for robust detection systems to prevent fraud and ensure document authenticity.

Method: Proposes TwoHead-SwinFPN architecture integrating Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM). Uses dual-head architecture for joint optimization of detection and segmentation tasks with uncertainty-weighted multi-task learning.

Result: Achieves 84.31% accuracy, 90.78% AUC for classification, 57.24% mean Dice score for localization, and 88.61% F1-score for binary classification on FantasyIDiap dataset. Model demonstrates computational efficiency suitable for real-world deployment via FastAPI.

Conclusion: TwoHead-SwinFPN provides an effective unified solution for detecting and localizing manipulated regions in ID documents, with strong performance across multiple languages and devices, making it suitable for practical deployment in document verification systems.

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [169] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: GoG is an autonomous visual planning framework that uses selective gaze and iterative reasoning to improve knowledge-intensive visual queries in Large Multimodal Models.


<details>
  <summary>Details</summary>
Motivation: Current LMMs struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Existing search-augmented approaches suffer from visual redundancy and lack deep iterative reflection.

Method: Proposes Glance-or-Gaze (GoG) framework with Selective Gaze mechanism that dynamically chooses between global context and high-value regions. Uses dual-stage training: Reflective GoG Behavior Alignment via supervised fine-tuning and Complexity-Adaptive Reinforcement Learning for iterative reasoning.

Result: Achieves state-of-the-art performance across six benchmarks. Ablation studies confirm both Selective Gaze and complexity-adaptive RL are essential for effective visual search.

Conclusion: GoG successfully shifts from passive perception to active visual planning, addressing limitations of existing methods by filtering irrelevant information before retrieval and enabling deep iterative reasoning for complex visual queries.

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [170] [Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection](https://arxiv.org/abs/2601.12919)
*Jun Wan,Yuanzhi Yao,Zhihui Lai,Jie Zhou,Xianxu Hou,Wenwen Min*

Main category: cs.CV

TL;DR: SHT framework combines face hallucination and pose transfer for weakly-supervised facial landmark detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Low-resolution face images, insufficient training data, and imprecise annotations degrade facial landmark detection accuracy, creating need for more robust approaches.

Method: Proposes SHT framework with two modules: DHLN for learning high-resolution representations via face hallucination, and FPTN for improving landmark heatmaps through facial pose transfer.

Result: Experimental results show method surpasses state-of-the-art techniques in both face hallucination and facial landmark detection tasks.

Conclusion: First weakly-supervised FLD approach integrating face hallucination and pose transfer, demonstrating improved robustness and precision through mutual enhancement of hallucination and landmark detection tasks.

Abstract: High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.

</details>


### [171] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: A universal abstention framework for medical image segmentation that enhances noise-robustness by allowing models to selectively ignore corrupted samples, outperforming baselines especially under high noise levels.


<details>
  <summary>Details</summary>
Motivation: Label noise is a critical problem in medical image segmentation due to difficult manual annotation, causing models to overfit and degrade generalization. While abstention mechanisms work well for classification, their potential in segmentation remains unexplored.

Method: Introduces a universal, modular abstention framework with two key components: 1) an informed regularization term to guide abstention behavior, and 2) a flexible power-law-based auto-tuning algorithm for abstention penalty. The framework integrates with three loss functions to create novel noise-robust variants: GAC, SAC, and ADS.

Result: Experiments on CaDIS and DSAD medical datasets show the methods consistently and significantly outperform non-abstaining baselines, especially under high noise levels. The framework demonstrates versatility across different loss functions.

Conclusion: Enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. The abstention framework effectively addresses label noise in medical image segmentation.

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [172] [Dual-Stream Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2601.12926)
*Jun Wan,Jun Liu,Zhihui lai,Jie Zhou*

Main category: cs.CV

TL;DR: DSCT uses dual-stream transformer with region and segmentation features to generate more accurate image captions by addressing semantic inconsistencies and spatial misalignment.


<details>
  <summary>Details</summary>
Motivation: Current region feature-based captioning methods generate irrelevant descriptions due to lack of contextual information and over-reliance on partial descriptions, needing better feature fusion.

Method: Dual-Stream Collaborative Transformer (DSCT) with Pattern-Specific Mutual Attention Encoders (PSMAEs) to consolidate region/segmentation features and Dynamic Nomination Decoders (DNDs) to dynamically select relevant learning blocks.

Result: DSCT outperforms state-of-the-art image captioning models on popular benchmark datasets.

Conclusion: First study to fuse different pattern-specific features dynamically to address semantic inconsistencies and spatial misalignment, achieving superior captioning performance.

Abstract: Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.

</details>


### [173] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: SVGFormer is a decoder-free 3D medical imaging pipeline that uses semantic supervoxel graphs and hierarchical encoding to focus parameters on feature learning rather than spatial reconstruction, achieving strong performance with inherent explainability.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D medical vision backbones use parameter-heavy encoder-decoder structures that allocate significant resources to spatial reconstruction rather than feature learning, limiting efficiency and interpretability.

Method: Introduces SVGFormer with content-aware grouping that partitions volumes into semantic supervoxel graphs, then uses hierarchical encoding combining patch-level Transformers with supervoxel-level Graph Attention Networks to model both intra-region features and inter-regional dependencies.

Result: Two specialized models on BraTS dataset: classification model achieved F1-score of 0.875, regression model achieved MAE of 0.028, demonstrating strong performance and the encoder's ability to learn discriminative, localized features.

Conclusion: Graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation, concentrating learnable capacity on feature encoding while providing dual-scale explainability.

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [174] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: POCI-Diff: A diffusion-based framework for text-to-image generation with consistent 3D layout control and editing, using semantic binding to bounding boxes and warping-free editing via regeneration.


<details>
  <summary>Details</summary>
Motivation: Prior methods for spatial control in T2I generation often distort object geometry and fail to preserve consistency across edits when using 2D cues or iterative copy-warp-paste strategies.

Method: Introduces POCI-Diff framework with: 1) joint enforcement of 3D geometric constraints and instance-level semantic binding via Blended Latent Diffusion, 2) warping-free generative editing pipeline for object insertion/removal/transformation via regeneration, 3) IP-Adapter conditioning for object identity preservation across edits.

Result: Produces high-quality images consistent with specified 3D layouts and edits, outperforms state-of-the-art methods in visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

Conclusion: POCI-Diff enables consistent and interactive 3D layout control for T2I generation with explicit per-object semantic control and coherent object appearance throughout editing, addressing limitations of prior warping-based approaches.

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [175] [QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2601.12936)
*Tianran Ouyang,Xingping Dong,Jing Zhang,Mang Ye,Jun Chen,Bo Du*

Main category: cs.CV

TL;DR: QASA introduces a quality-guided K-adaptive slot attention method that dynamically selects high-quality slots for object-centric learning, outperforming both K-adaptive and K-fixed baselines.


<details>
  <summary>Details</summary>
Motivation: Existing K-adaptive slot attention methods have two key limitations: 1) they lack explicit constraints on slot-binding quality, leading to ambiguous feature attribution, and 2) they create conflicting optimization goals between reducing active slots and maintaining reconstruction fidelity, causing them to lag behind K-fixed baselines.

Method: QASA decouples slot selection from reconstruction to eliminate mutual constraints, proposes an unsupervised Slot-Quality metric to assess per-slot quality, designs a Quality-Guided Slot Selection scheme that dynamically selects high-quality slots, and uses a gated decoder for reconstruction. At inference, token-wise competition yields K-adaptive outcomes.

Result: QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets, and surpasses K-fixed methods on real-world datasets.

Conclusion: The proposed quality-guided approach effectively addresses the limitations of existing K-adaptive slot attention methods, achieving superior performance by explicitly measuring and selecting high-quality slots while eliminating conflicting optimization objectives.

Abstract: Slot Attention, an approach that binds different objects in a scene to a set of "slots", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.

</details>


### [176] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: A novel unsupervised video class incremental learning (uVCIL) approach that learns video information without forgetting or using labels, outperforming baselines on three action recognition datasets.


<details>
  <summary>Details</summary>
Motivation: Prior video class-incremental learning approaches require supervised labels and task boundaries, which are costly, require human annotation, or are unrealistic. There's a need for unsupervised methods that can learn video information incrementally without forgetting.

Method: Uses a deep feature extractor network to get representative video features without class/task info, then progressively builds deep clusters from extracted features. During successive task learning, the model from previous task serves as initial state to transfer knowledge to current task.

Result: Significantly outperforms other baselines on three standard video action recognition datasets: UCF101, HMDB51, and Something-to-Something V2, when ignoring labels from supervised setting.

Conclusion: Proposes a simple yet effective approach for unsupervised video class incremental learning that successfully learns video information without forgetting and without requiring labels, demonstrating strong performance across multiple datasets.

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [177] [GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation](https://arxiv.org/abs/2601.12948)
*Riccardo Catalini,Davide Di Nucci,Guido Borghi,Davide Davoli,Lorenzo Garattoni,Giampiero Francesca,Yuki Kawana,Roberto Vezzani*

Main category: cs.CV

TL;DR: GazeD: A diffusion-based method that jointly estimates 3D gaze and human pose from single RGB images by treating gaze as an additional body joint and generating multiple plausible hypotheses.


<details>
  <summary>Details</summary>
Motivation: Current methods often struggle with 3D gaze estimation from single images due to uncertainty and ambiguity. The authors aim to leverage diffusion models' ability to handle uncertainty while exploiting the relationship between gaze and body pose for more accurate joint estimation.

Method: Uses diffusion models conditioned on 2D pose, subject surroundings, and scene context to generate multiple plausible 3D gaze and pose hypotheses. Represents 3D gaze as an additional body joint at fixed distance from eyes, allowing joint denoising during diffusion process.

Result: Achieves state-of-the-art performance on three benchmark datasets for 3D gaze estimation, surpassing even methods that use temporal information, demonstrating superior accuracy from single RGB images.

Conclusion: GazeD successfully demonstrates that diffusion models can effectively handle uncertainty in 3D gaze estimation and that treating gaze as a body joint enables better joint estimation with pose, leading to superior performance from single images.

Abstract: We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.

</details>


### [178] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: DermaBench is a clinician-annotated dermatology visual question answering benchmark built on diverse skin images to evaluate VLMs' visual understanding and clinical reasoning beyond simple classification.


<details>
  <summary>Details</summary>
Motivation: Current dermatology datasets focus mainly on image-level classification tasks like lesion recognition, which cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal vision-language models. There's a need for VQA benchmarks to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions.

Method: Built DermaBench on the Diverse Dermatology Images (DDI) dataset with 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Used a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended) where expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, image quality, plus open-ended narrative descriptions and summaries.

Result: Created a comprehensive benchmark with approximately 14,474 VQA-style annotations covering diverse dermatological aspects. The dataset is released as metadata-only to respect upstream licensing and is publicly available at Harvard Dataverse.

Conclusion: DermaBench addresses the gap in evaluating VLMs' multimodal capabilities in dermatology by providing a clinician-annotated VQA benchmark that enables assessment of visual understanding, language grounding, and clinical reasoning beyond simple classification tasks.

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [179] [StyMam: A Mamba-Based Generator for Artistic Style Transfer](https://arxiv.org/abs/2601.12954)
*Zhou Hong,Rongsheng Hu,Yicheng Di,Xiaolong Xu,Ning Dong,Yihua Shao,Run Ling,Yun Wang,Juqin Wang,Zhanjie Zhang,Ao Ma*

Main category: cs.CV

TL;DR: Proposes StyMam, a Mamba-based generator for image style transfer that captures both local and global dependencies to avoid artifacts while preserving content structure and achieving fast inference.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based methods struggle with capturing both local and global dependencies, causing artifacts and disharmonious patterns. SD-based methods reduce these issues but fail to preserve content structures and have slow inference speeds.

Method: Introduces a Mamba-based generator (StyMam) with a residual dual-path strip scanning mechanism to capture local texture features, and a channel-reweighted spatial attention module to model global dependencies.

Result: Extensive experiments show the proposed method outperforms state-of-the-art algorithms in both quality (reducing artifacts and disharmonious patterns) and speed (faster inference).

Conclusion: StyMam successfully addresses limitations of existing GAN and SD-based methods by leveraging Mamba architecture to produce high-quality stylized images with preserved content structure and fast inference.

Abstract: Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.

</details>


### [180] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: Two-stream transformer video classifier using content and optical flow streams achieves excellent human activity recognition results.


<details>
  <summary>Details</summary>
Motivation: Motion representation is crucial for video understanding applications like action recognition and autonomous systems. Transformers with self-attention have shown strong performance, but need better integration of spatio-temporal information from both content and motion.

Method: Proposes a two-stream transformer video classifier that extracts spatio-temporal information from both content frames and optical flow streams. The model identifies self-attention features across joint optical flow and temporal frame domains and represents their relationships within transformer encoder mechanisms.

Result: The proposed methodology provides excellent classification results on three well-known video datasets of human activities.

Conclusion: The two-stream transformer approach effectively combines content and motion information for superior video classification performance in human activity recognition tasks.

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [181] [Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation](https://arxiv.org/abs/2601.12964)
*John Waithaka,Gustave Bwirayesu,Moise Busogi*

Main category: cs.CV

TL;DR: Adding high-resolution imagery to self-supervised pretraining via spatial affinity component improves mid-resolution image representation learning and downstream segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Most self-supervised pretraining in remote sensing uses mid-resolution (MR) datasets due to availability, but high-resolution (HR) datasets are now emerging. The paper explores how to incorporate HR data to enhance MR image representation learning and downstream segmentation performance on MR tasks.

Method: Design a spatial affinity component that can be integrated into existing self-supervised learning frameworks. This component uses HR imagery to learn better representations of MR imagery, leveraging the complementary information between different spatial resolutions.

Result: The spatial affinity component was tested on two self-supervised learning frameworks and demonstrated superior performance compared to models pretrained on HR or MR images alone, showing effective enhancement of MR image representation learning.

Conclusion: Incorporating high-resolution imagery through a spatial affinity component in self-supervised pretraining frameworks effectively improves mid-resolution image representation learning and downstream segmentation performance, providing a valuable approach for leveraging multi-resolution remote sensing data.

Abstract: Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.

</details>


### [182] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: TabTrans transformer model outperforms generative AI and conventional ML for T2DM prediction using longitudinal EHR and DXA data, achieving ≥79.7% ROC AUC and identifying key bone-related risk factors.


<details>
  <summary>Details</summary>
Motivation: Existing methods often overlook complex, long-range dependencies in disease progression from longitudinal health data. There's a need for better early T2DM risk prediction tools that can handle multimodal healthcare data.

Method: Tabular transformer (TabTrans) architecture processes longitudinal EHR and DXA data. Applied to Qatar BioBank cohort (1,382 subjects), used SMOTE/SMOTE-ENN for class imbalance, compared against conventional ML and generative AI models (Claude 3.5, GPT-4, Gemini Pro).

Result: TabTrans achieved superior predictive performance with ROC AUC ≥79.7% for T2DM prediction, outperforming both generative AI and conventional ML. Key risk indicators identified: VAT mass/volume, ward BMD/BMC, T/Z-scores, and L1-L4 scores.

Conclusion: TabTrans demonstrates significant potential for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [183] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: MIRACLE is a deep learning system that predicts postoperative complication risks in lung cancer surgery by fusing clinical and radiological data using hyperspherical embeddings, with interpretable AI features for clinical decision support.


<details>
  <summary>Details</summary>
Motivation: Postoperative complications significantly impact patient outcomes and increase healthcare costs, creating a need for accurate, personalized risk prediction tools that can integrate diverse clinical data sources and provide actionable insights for clinicians.

Method: MIRACLE uses hyperspherical embedding space fusion to integrate heterogeneous preoperative clinical records and radiological images, with an interventional deep learning module that enables interactive adjustment of predictions and provides interpretable insights for clinical decision-making.

Result: MIRACLE outperforms traditional machine learning models and contemporary LLM variants on the POC-L dataset of 3,094 lung cancer surgery patients, demonstrating superior performance for personalized and explainable postoperative risk management.

Conclusion: MIRACLE represents an effective deep learning approach for postoperative complication prediction that combines multimodal data fusion with interpretable AI capabilities, offering both improved accuracy and clinical utility for personalized risk management in lung cancer surgery.

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [184] [AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection](https://arxiv.org/abs/2601.12994)
*Shiming Wang,Holger Caesar,Liangliang Nan,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: AsyncBEV: A lightweight module that improves 3D BEV object detection robustness against sensor asynchrony by estimating feature flow and warping feature maps.


<details>
  <summary>Details</summary>
Motivation: Real-world sensor asynchrony (due to different frequencies, network latency, hardware failures, processing bottlenecks) degrades perception performance in autonomous driving, especially for dynamic objects, despite synchronization efforts.

Method: Proposes AsyncBEV module that: 1) Estimates 2D flow from BEV features of two sensor modalities considering known time offset, 2) Uses predicted feature flow to warp and spatially align feature maps, 3) Can be integrated into different BEV detector architectures (grid-based and token-based).

Result: Significantly outperforms ego motion compensated baselines, notably by 16.6% and 11.9% NDS on dynamic objects in worst-case scenario of 0.5s time offset. Improves robustness against both small and large asynchrony between LiDAR or camera sensors.

Conclusion: AsyncBEV is an effective, trainable, lightweight module that enhances 3D BEV object detection robustness against sensor asynchrony, particularly benefiting dynamic object detection in autonomous driving scenarios.

Abstract: In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.

</details>


### [185] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: VideoMaMa converts coarse segmentation masks to accurate alpha mattes using pretrained video diffusion models, enabling zero-shot generalization to real videos. The authors then create MA-V dataset with 50K+ pseudo-labeled videos and show it improves video matting models like SAM2-Matte.


<details>
  <summary>Details</summary>
Motivation: Video matting models struggle with real-world generalization due to lack of labeled data. There's a need for scalable approaches that can work with real videos without extensive manual annotation.

Method: 1) VideoMaMa: Uses pretrained video diffusion models to convert coarse segmentation masks into accurate alpha mattes, trained only on synthetic data. 2) Creates MA-V dataset: Uses VideoMaMa for pseudo-labeling to generate 50K+ high-quality video matting annotations. 3) Fine-tunes SAM2 on MA-V to create SAM2-Matte.

Result: VideoMaMa shows strong zero-shot generalization to real videos despite synthetic-only training. SAM2-Matte outperforms models trained on existing matting datasets, demonstrating MA-V's effectiveness. The dataset contains diverse real-world scenes and motions.

Conclusion: Large-scale pseudo-labeled video matting datasets like MA-V are crucial for advancing video matting research. Generative priors (video diffusion models) combined with accessible segmentation cues enable scalable progress in this field.

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


### [186] [Think3D: Thinking with Space for Spatial Reasoning](https://arxiv.org/abs/2601.13029)
*Zaibin Zhang,Yuhan Wu,Lianjie Jia,Yifan Wang,Zhongbo Zhang,Yijiang Li,Binghao Ran,Fuxi Zhang,Zhuohan Sun,Zhenfei Yin,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: Think3D enables VLMs to perform 3D spatial reasoning by integrating 3D reconstruction models and interactive camera operations, achieving significant performance gains without additional training.


<details>
  <summary>Details</summary>
Motivation: Current vision large models (VLMs) are fundamentally 2D perceivers and struggle with genuine 3D spatial reasoning, which is essential for understanding and reasoning about the physical world.

Method: Think3D framework leverages 3D reconstruction models to recover point clouds and camera poses from images/videos, enabling agents to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process.

Result: Significant improvements in spatial reasoning: +7.8% average gains on BLINK Multi-view and MindCube, +4.7% on VSI-Bench for advanced models like GPT-4.1 and Gemini 2.5 Pro. Smaller models benefit from RL policy (+0.7% to +6.8% improvement).

Conclusion: Training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence.

Abstract: Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.

</details>


### [187] [GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure](https://arxiv.org/abs/2601.13052)
*Antoine Carreaud,Shanci Li,Malo De Lacour,Digre Frinde,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: GridNet-HD is a multi-modal dataset for 3D semantic segmentation of electrical infrastructure, combining high-density LiDAR and high-resolution oblique imagery with 11-class annotations, showing fusion models outperform unimodal baselines by +5.55 mIoU.


<details>
  <summary>Details</summary>
Motivation: There is no public dataset that jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets, creating a gap for multi-modal 3D semantic segmentation research in electrical infrastructure.

Method: Created GridNet-HD dataset with 7,694 images and 2.5 billion LiDAR points annotated into 11 classes, with predefined splits and mIoU metrics. Provided unimodal (LiDAR-only, image-only) and multi-modal fusion baselines for comparison.

Result: On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, demonstrating the complementarity of geometric (LiDAR) and appearance (imagery) information for 3D semantic segmentation of electrical infrastructure.

Conclusion: GridNet-HD fills an important gap in multi-modal 3D semantic segmentation datasets for electrical infrastructure, showing that combining LiDAR and imagery significantly improves performance, with the dataset, baselines, and code publicly available.

Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.

</details>


### [188] [Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures](https://arxiv.org/abs/2601.13059)
*Yulun Guo*

Main category: cs.CV

TL;DR: Dual-branch prototype learning network combining Retinex theory with few-shot learning for low-light crack segmentation, achieving SOTA performance with minimal annotation requirements.


<details>
  <summary>Details</summary>
Motivation: Real-world cracks often appear in low-light environments (tunnels, bridge undersides) where computer vision segmentation accuracy degrades. Pixel-level annotation of low-light crack images is extremely time-consuming, but most deep learning methods require large, well-illuminated datasets.

Method: Proposes a dual-branch prototype learning network integrating Retinex theory with few-shot learning. Uses Retinex-based reflectance components for illumination-invariant global representation learning. Includes cross-similarity prior mask generation module to compute high-dimensional similarities between query and support features, and multi-scale feature enhancement module that fuses multi-scale features with prior mask to alleviate spatial inconsistency.

Result: Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions.

Conclusion: The proposed approach effectively addresses low-light crack segmentation challenges by combining illumination-invariant representation learning with few-shot learning, reducing dependence on large annotated datasets while maintaining high accuracy in challenging lighting conditions.

Abstract: Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.

</details>


### [189] [Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups](https://arxiv.org/abs/2601.13094)
*Gelei Xu,Yuying Duan,Jun Xia,Ruining Deng,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: HyperAdapt is a patient-conditioned adaptation framework that improves subgroup reliability in medical AI while maintaining a shared diagnostic model, outperforming baselines by 4.1% in recall and 4.4% in F1 score.


<details>
  <summary>Details</summary>
Motivation: Medical AI models often show uneven performance across patient populations due to disease heterogeneity, but removing sensitive attributes (like age/sex) degrades diagnostic accuracy since these carry essential clinical information. Clinical decision-making incorporates patient context, suggesting a need for subgroup-aware models that preserve diagnostic information.

Method: HyperAdapt uses patient attributes (age, sex) encoded into compact embeddings to condition a hypernetwork-style module. This generates small residual modulation parameters for selected layers of a shared backbone model, preserving general medical knowledge while enabling targeted patient-specific adjustments. Adaptations are constrained through low-rank and bottlenecked parameterizations for efficiency and robustness.

Result: The approach consistently improves subgroup-level performance without sacrificing overall accuracy across multiple medical imaging benchmarks. On PAD-UFES-20 dataset, it outperforms the strongest baseline by 4.1% in recall and 4.4% in F1 score, with larger gains for underrepresented patient populations.

Conclusion: HyperAdapt demonstrates that patient-conditioned adaptation can improve subgroup reliability in medical AI while maintaining shared diagnostic models, offering a more clinically aligned alternative to attribute suppression approaches that degrade diagnostic accuracy.

Abstract: AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.

</details>


### [190] [A Streamlined Attention-Based Network for Descriptor Extraction](https://arxiv.org/abs/2601.13126)
*Mattia D'Urso,Emanuele Santellani,Christian Sormann,Mattia Rossi,Andreas Kuhn,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: SANDesc is a lightweight attention-based descriptor network that improves keypoint matching without modifying detectors, achieving better performance with only 2.4M parameters.


<details>
  <summary>Details</summary>
Motivation: To improve keypoint descriptor extraction for better matching performance while maintaining computational efficiency and compatibility with existing keypoint detectors.

Method: Revised U-Net-like architecture with Convolutional Block Attention Modules and residual paths (Residual U-Net Blocks with Attention), trained using modified triplet loss with curriculum learning-inspired hard negative mining.

Result: Outperforms original keypoint descriptors on HPatches, MegaDepth-1500, and Image Matching Challenge 2021 benchmarks, with substantial gains on a new urban 4K dataset, using only 2.4M parameters.

Conclusion: SANDesc demonstrates that lightweight attention-based descriptor networks can significantly improve matching performance while maintaining computational efficiency and compatibility with existing detectors.

Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.
  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.
  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.

</details>


### [191] [PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain](https://arxiv.org/abs/2601.13128)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: PhaseMark is a fast, optimization-free watermarking method for diffusion models that modulates phase in VAE latent frequency domain, achieving high resilience without quality degradation.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for Latent Diffusion Models are too slow due to iterative optimization or inversion processes, creating a need for faster yet robust solutions.

Method: Single-shot, optimization-free framework that directly modulates phase in the VAE latent frequency domain, analyzing four different modulation variants.

Result: Thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks including regeneration, without degrading image quality.

Conclusion: PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties of diffusion models.

Abstract: The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.

</details>


### [192] [GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning](https://arxiv.org/abs/2601.13132)
*Kim Yu-Ji,Dahye Lee,Kim Jun-Seong,GeonU Kim,Nam Hyeon-Woo,Yongjin Kwon,Yu-Chiang Frank Wang,Jaesung Choe,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: GaussExplorer is a framework combining 3D Gaussian Splatting with Vision-Language Models for embodied exploration and reasoning, enabling better handling of complex compositional language queries through novel viewpoint generation.


<details>
  <summary>Details</summary>
Motivation: Prior approaches to language-embedded 3DGS struggle with complex compositional queries, while object-centric RGB-D methods are constrained by pre-fixed viewpoints. There's a need for better embodied exploration and reasoning in 3D scenes.

Method: Integrates Vision-Language Models with 3D Gaussian Splatting. First identifies pre-captured images most correlated with query questions, then adjusts them into novel viewpoints to capture better visual information for VLM reasoning.

Result: Outperforms existing methods on several benchmarks, demonstrating effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

Conclusion: GaussExplorer successfully enables question-driven exploration and reasoning within 3D scenes by combining VLM capabilities with 3DGS, addressing limitations of prior approaches.

Abstract: We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

</details>


### [193] [CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks](https://arxiv.org/abs/2601.13133)
*Mingshuang Luo,Ruibing Hou,Bo Chao,Hong Chang,Zimo Liu,Yaowei Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: CLASP is an unsupervised pre-training framework for human-centric vision that uses CLIP to generate multi-level semantic pseudo-labels and a Prompt-Controlled MoE module for task-adaptive feature extraction.


<details>
  <summary>Details</summary>
Motivation: With large-scale unlabeled human image datasets available, there's a need for general unsupervised pre-training models that can support diverse human-centric downstream tasks like surveillance, healthcare, and human-computer interaction.

Method: CLASP uses CLIP to generate low-level (body parts) and high-level (attributes) semantic pseudo-labels, integrates these into visual representations, and employs a Prompt-Controlled Mixture-of-Experts module for dynamic feature adaptation based on task-specific prompts.

Result: Extensive experiments across multiple benchmarks show CLASP consistently outperforms existing unsupervised pre-training methods for human-centric visual analysis.

Conclusion: CLASP advances human-centric visual analysis by providing an effective unsupervised pre-training framework that leverages multi-level semantic cues and task-adaptive feature extraction for better transferability to diverse downstream tasks.

Abstract: Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.

</details>


### [194] [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/abs/2601.13148)
*Richard Shaw,Youngkyoon Jang,Athanasios Papaioannou,Arthur Moreau,Helisa Dhamo,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ICo3D creates interactive, photorealistic 3D human avatars from multi-view captures, combining animatable face and body models with LLM-powered conversation for real-time interactions.


<details>
  <summary>Details</summary>
Motivation: To create a fully integrated virtual avatar experience that supports real-time conversational interactions in immersive environments, addressing the need for lifelike virtual humans in gaming, virtual assistance, and education.

Method: Uses multi-view captures to create separate animatable 3D face and dynamic 3D body models using Gaussian splatting (HeadGaS++ for face, SWinGS++ for body). Merges models without artifacts, integrates LLM for conversation, and uses audio speech to drive facial animation for synchronization.

Result: Developed a complete system demonstrating real-time conversation with photorealistic 3D avatars, supporting both oral and written interactions in immersive environments with precise audio-visual synchronization.

Conclusion: ICo3D provides a fully integrated virtual avatar solution applicable to gaming, virtual assistance, and personalized education, offering lifelike conversational interactions through improved photorealism and seamless model integration.

Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/

</details>


### [195] [GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction](https://arxiv.org/abs/2601.13207)
*Jinnao Li,Zijian Chen,Tingzhu Chen,Changbo Wang*

Main category: cs.CV

TL;DR: GTPred is a new benchmark for geo-temporal prediction that evaluates multi-modal LLMs on jointly predicting location and temporal information from images, showing current models have limited world knowledge and reasoning despite strong visual perception.


<details>
  <summary>Details</summary>
Motivation: Existing geo-localization benchmarks ignore temporal information in images, which can provide important constraints for location inference. There's a need to evaluate models on both spatial and temporal reasoning capabilities.

Method: Created GTPred benchmark with 370 globally distributed images spanning over 120 years. Evaluates MLLMs using joint year and hierarchical location sequence matching, and assesses intermediate reasoning chains against annotated ground-truth reasoning processes.

Result: Experiments on 8 proprietary and 7 open-source MLLMs show current models have strong visual perception but limited world knowledge and geo-temporal reasoning. Incorporating temporal information significantly enhances location inference performance.

Conclusion: Temporal information is crucial for accurate geo-localization and should be incorporated in benchmarks. Current MLLMs need improvement in world knowledge and spatio-temporal reasoning capabilities.

Abstract: Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.

</details>


### [196] [Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising](https://arxiv.org/abs/2601.13208)
*Vikram R Lakkavalli*

Main category: cs.CV

TL;DR: Additive U-Net replaces concatenative skip connections with gated additive connections using learnable non-negative scalars, achieving competitive denoising performance with better interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard U-Net concatenation skip connections double channel dimensionality and obscure information flow, allowing uncontrolled noise transfer between encoder and decoder.

Method: Proposes Additive U-Net which replaces concatenative skip connections with gated additive connections where each skip pathway is scaled by a learnable non-negative scalar, providing explicit control over encoder contributions without channel inflation.

Result: Achieves competitive PSNR/SSIM on Kodak-17 denoising benchmark at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Effective denoising achieved even without explicit down/up-sampling or forced hierarchies.

Conclusion: Additive skips provide a lightweight and interpretable alternative to concatenation, enabling efficient design and clearer understanding of multi-scale information transfer in reconstruction networks.

Abstract: Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.

</details>


### [197] [ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments](https://arxiv.org/abs/2601.13218)
*Igor Vozniak,Philipp Mueller,Nils Lipp,Janis Sprenger,Konstantin Poddubnyy,Davit Hovhannisyan,Christian Mueller,Andreas Bulling,Philipp Slusallek*

Main category: cs.CV

TL;DR: A new VR dataset (120 participants) for object-based visual attention in street-crossing scenarios, with novel object-based similarity metric (oSIM) and SUMGraph model that outperforms SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Object-based attention is well-established in cognitive science but underrepresented in computational models due to lack of suitable datasets and evaluation metrics for object-based attention assessment.

Method: Created \dataset~ with 120 participants in VR street-crossing scenarios, featuring gaze data, complete state-space object representation, panoptic segmentation, depth info, and vehicle keypoints. Proposed oSIM metric and developed SUMGraph, a Mamba U-Net-based model with graph representation of critical scene objects (vehicles).

Result: Explicit optimization for object-based attention improves both oSIM performance and common metrics. SUMGraph outperforms several state-of-the-art visual attention prediction methods.

Conclusion: The dataset addresses ethical/safety challenges of real-world data collection, enables object-based attention evaluation, and shows that explicit object encoding improves attention prediction performance.

Abstract: The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.

</details>


### [198] [Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations](https://arxiv.org/abs/2601.13225)
*Tim Lachmann,Alexandra Israelsson,Christina Tornberg,Teimuraz Saghinadze,Michal Balazia,Philipp Müller,Petri Laukka*

Main category: cs.CV

TL;DR: BLEMORE dataset enables multimodal recognition of blended emotions with relative salience, addressing limitations of single-emotion approaches.


<details>
  <summary>Details</summary>
Motivation: Current emotion recognition systems focus on single emotions, ignoring blended emotions with varying salience, due to lack of appropriate datasets.

Method: Created BLEMORE dataset with 3,000+ clips from 58 actors showing 6 basic emotions and 10 blends with 3 salience configurations (50/50, 70/30, 30/70). Evaluated state-of-the-art video classification approaches on presence and salience prediction tasks.

Result: Unimodal classifiers: 29% presence accuracy, 13% salience accuracy. Multimodal methods improved to 35% presence (ImageBind + WavLM) and 18% salience (HiCMAE). Test set: 33% presence (VideoMAEv2 + HuBERT), 18% salience (HiCMAE).

Conclusion: BLEMORE dataset provides valuable resource for advancing emotion recognition systems that account for blended emotion complexity and significance.

Abstract: Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.

</details>


### [199] [ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection](https://arxiv.org/abs/2601.13234)
*Md. Nishan Khan,Kazi Shahriar Sanjid,Md. Tanzim Hossain,Asib Mostakim Fony,Istiak Ahmed,M. Monir Uddin*

Main category: cs.CV

TL;DR: ConvMambaNet: A hybrid CNN-Mamba model achieves 99% accuracy for EEG seizure detection, addressing temporal complexity and class imbalance challenges.


<details>
  <summary>Details</summary>
Motivation: Epilepsy monitoring via EEG is challenging due to temporal complexity of signals and limitations in automated analysis, requiring better temporal feature extraction for accurate seizure detection.

Method: Hybrid deep learning model integrating CNNs with Mamba Structured State Space Model (SSM) to capture both spatial features and long-range temporal dynamics in EEG signals.

Result: Achieved 99% accuracy on CHB-MIT Scalp EEG dataset with robust performance under severe class imbalance conditions.

Conclusion: ConvMambaNet demonstrates strong potential for precise, efficient seizure detection and offers a viable path toward real-time automated epilepsy monitoring in clinical settings.

Abstract: Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.

</details>


### [200] [Deep Learning for Semantic Segmentation of 3D Ultrasound Data](https://arxiv.org/abs/2601.13263)
*Chenyu Liu,Marco Cecotti,Harikrishnan Vijayakumar,Patrick Robinson,James Barson,Mihai Caleap*

Main category: cs.CV

TL;DR: Novel 3D ultrasound-based semantic segmentation framework using Calyo Pulse sensors for autonomous vehicles in harsh environments.


<details>
  <summary>Details</summary>
Motivation: Current LiDAR and camera systems have cost, robustness, and performance trade-offs, especially in adverse conditions. Need for reliable, cost-efficient perception systems for automated vehicles.

Method: Introduces a learning-based 3D semantic segmentation framework using Calyo Pulse ultrasound sensors. Uses 3D U-Net architecture trained on spatial ultrasound data for volumetric segmentation.

Result: Demonstrates robust segmentation performance from Calyo Pulse sensors. Shows potential for improvement with larger datasets, refined ground truth, and weighted loss functions.

Conclusion: 3D ultrasound sensing is a promising complementary modality for reliable autonomy, offering potential advantages over traditional LiDAR and camera systems in harsh environments.

Abstract: Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.

</details>


### [201] [Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams](https://arxiv.org/abs/2601.13299)
*Ethan Seefried,Prahitha Movva,Naga Harshita Marupaka,Tilak Kasturi,Tirthankar Ghosal*

Main category: cs.CV

TL;DR: Enginuity is the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations for automated diagram parsing, enabling multimodal LLMs to perform engineering diagram analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack the ability to comprehend and manipulate visual-structural knowledge in engineering diagrams, creating a fundamental barrier preventing AI from fully participating in scientific workflows where diagram interpretation and visual reasoning are essential for hypothesis generation and discovery.

Method: Creation of a large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations that capture hierarchical component relationships, connections, and semantic elements across diverse engineering domains.

Result: The proposed Enginuity dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation.

Conclusion: Enginuity would be transformative for AI for Scientific Discovery by breaking down the barrier that prevents AI from fully participating in scientific workflows requiring diagram interpretation, technical drawing analysis, and visual reasoning.

Abstract: We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.

</details>


### [202] [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304)
*Wenxin Ma,Chenlong Wang,Ruisheng Yuan,Hao Chen,Nanru Dai,S. Kevin Zhou,Yijun Yang,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: MLLMs fail at causal spatial reasoning (predicting consequences of object motions), scoring only 54% vs human 84%. The paper introduces CausalSpatial benchmark and proposes COW framework that generates videos of hypothetical dynamics to ground reasoning in visual evidence.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) are limited to static spatial perception and cannot answer "what-if" questions about object motions in 3D scenes, while humans can instantly predict consequences like collisions.

Method: Introduces CausalSpatial benchmark with four tasks (Collision, Compatibility, Occlusion, Trajectory) to evaluate causal spatial reasoning. Proposes Causal Object World model (COW) framework that externalizes simulation by generating videos of hypothetical dynamics to provide explicit visual cues of causality.

Result: Humans score 84% on CausalSpatial benchmark while GPT-5 achieves only 54%, revealing a severe gap. Analysis shows MLLMs over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing spatially ungrounded hallucinations.

Conclusion: MLLMs fundamentally lack causal spatial reasoning capabilities due to over-reliance on linguistic priors. The proposed COW framework addresses this by grounding reasoning in physical reality through explicit visual simulation, with dataset and code made publicly available.

Abstract: Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial

</details>


### [203] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: MultiST is a multimodal framework that integrates spatial topology, gene expression, and tissue morphology using cross-attention fusion to improve spatial domain boundary resolution in spatial transcriptomics.


<details>
  <summary>Details</summary>
Motivation: Existing spatial transcriptomics methods lack effective integration of histological morphology with molecular profiles, relying on shallow fusion or omitting tissue images, which limits their ability to resolve ambiguous spatial domain boundaries.

Method: MultiST uses a unified multimodal framework with cross-attention-based fusion to jointly model spatial topology, gene expression, and tissue morphology. It employs graph-based gene encoders with adversarial alignment and integrates color-normalized histological features to capture molecular-morphological dependencies.

Result: Evaluated on 13 diverse ST datasets spanning human brain cortex and breast cancer tissue, MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns.

Conclusion: MultiST provides an effective multimodal framework that improves spatial domain boundary resolution by integrating tissue morphology with molecular profiles, offering better biological interpretability for spatial transcriptomics analysis.

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [204] [Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments](https://arxiv.org/abs/2601.13364)
*Zhenan Liu,Yaodong Cui,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: Novel methodology for generating controlled dust concentrations in cluttered environments enables repeatable mm-wave propagation studies, with a new 4D radar dataset and filtering framework for reliable pedestrian detection in dust-laden mining environments.


<details>
  <summary>Details</summary>
Motivation: To study mm-wave propagation in harsh, enclosed environments like mines and tunnels where dust particles and reflective surfaces severely impact sensing functionality, requiring controlled testing environments and robust detection methods.

Method: 1) Developed methodology for generating controlled multi-level dust concentrations in cluttered environments; 2) Created 4D mmWave radar dataset augmented with camera and LiDAR; 3) Built threshold-based noise filtering framework using radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and multipath reflections; 4) Implemented cluster-level, rule-based classification pipeline using radar semantics for pedestrian detection.

Result: Experimental results confirm significant enhancement in clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments, enabling reliable real-time pedestrian detection without extensive domain-specific training.

Conclusion: The integrated approach combining controlled dust generation, comprehensive sensor datasets, and radar-based filtering/classification provides an effective solution for mm-wave sensing in harsh, cluttered environments with severe electromagnetic constraints.

Abstract: This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.

</details>


### [205] [Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations](https://arxiv.org/abs/2601.13371)
*Junyi Zhang,Yiming Wang,Yunhong Lu,Qichao Wang,Wenzhe Qian,Xiaoyin Xu,David Gu,Min Zhang*

Main category: cs.CV

TL;DR: A novel text-to-3D face generation method using spherical geometry representation and diffusion models to achieve high-quality geometry by constraining vertices to a topological sphere, enabling robust mesh reconstruction and synergy with 2D generative models.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D face generation methods struggle with poor geometry quality due to arbitrary vertex distributions that make clean connectivity difficult to establish, resulting in suboptimal 3D face geometry.

Method: Proposes Spherical Geometry Representation that anchors geometric signals to uniform spherical coordinates, ensuring regular point distribution for robust mesh reconstruction. This canonical sphere can be unwrapped into a 2D map, enabling Spherical Geometry Diffusion - a conditional diffusion framework that jointly models geometry and texture with explicit conditioning of texture synthesis on geometry.

Result: The method successfully handles text-to-3D generation, face reconstruction, and text-based 3D editing, substantially outperforming existing methods in geometric quality, textual fidelity, and inference efficiency.

Conclusion: Constraining 3D face geometry to a topological sphere representation enables high-quality geometry generation by providing regular point distribution and seamless integration with powerful 2D generative models through spherical unwrapping.

Abstract: A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.

</details>


### [206] [A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions](https://arxiv.org/abs/2601.13373)
*Zhenan Liu,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: Model-driven 4D mmWave radar framework for robust human detection in dust-filled industrial/underground environments where cameras/LiDAR fail.


<details>
  <summary>Details</summary>
Motivation: Industrial/underground environments have dust, smoke, confined spaces, and metallic structures that degrade optical/LiDAR perception, creating need for resilient sensing.

Method: Fully model-driven 4D radar perception framework with domain-aware multi-threshold filtering, ego-motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler-aware refinement, and rule-based 3D classifier.

Result: Radar detector maintains stable pedestrian identification in dust-filled trailer and underground mining tunnels while camera/LiDAR fail under severe visibility degradation.

Conclusion: Model-driven 4D radar approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial/subterranean environments.

Abstract: Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.

</details>


### [207] [Practical Insights into Semi-Supervised Object Detection Approaches](https://arxiv.org/abs/2601.13380)
*Chaoxin Wang,Bharaneeshwar Balasubramaniyam,Anurag Sangem,Nicolais Guevara,Doina Caragea*

Main category: cs.CV

TL;DR: Comprehensive comparison of three state-of-the-art SSOD methods (MixPL, Semi-DETR, Consistent-Teacher) showing performance variations with labeled data amounts across standard and custom datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of data-scarce settings in object detection by evaluating how semi-supervised approaches perform with limited labeled data, particularly relevant for few-shot learning scenarios.

Method: Comparative evaluation of three SSOD methods (MixPL, Semi-DETR, Consistent-Teacher) using MS-COCO and Pascal VOC benchmarks plus a custom Beetle dataset, analyzing performance across different labeled data quantities.

Result: Findings reveal trade-offs between accuracy, model size, and latency, identifying which methods work best in low-data regimes and specialized datasets with fewer object categories.

Conclusion: Provides insights into optimal SSOD method selection for data-scarce environments, highlighting performance characteristics across different dataset types and labeled data quantities.

Abstract: Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.

</details>


### [208] [Leveraging Transformer Decoder for Automotive Radar Object Detection](https://arxiv.org/abs/2601.13386)
*Changxu Zhang,Zhaoze Wang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: Transformer-based architecture for 3D radar object detection using decoder to directly regress bounding boxes, eliminating dense proposals and NMS tuning.


<details>
  <summary>Details</summary>
Motivation: To improve 3D radar object detection by modeling long-range spatial-temporal correlations and cross-feature interactions while eliminating heuristic post-processing like extensive NMS tuning.

Method: Uses Transformer Decoder as prediction head with learnable object queries and positional encodings, plus Pyramid Token Fusion (PTF) module to convert multi-scale radar features into unified token sequences.

Result: Achieves significant improvements over state-of-the-art radar-only baselines on the RADDet dataset.

Conclusion: The proposed Transformer-based framework effectively handles 3D radar object detection by formulating it as a set prediction problem, demonstrating superior performance while simplifying the detection pipeline.

Abstract: In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.

</details>


### [209] [Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study](https://arxiv.org/abs/2601.13416)
*A. Nieto Juscafresa,Á. Mazcuñán Herreros,J. Sullivan*

Main category: cs.CV

TL;DR: Frozen diffusion models serve as effective feature encoders for fine-grained recognition, outperforming other self-supervised methods and competing with supervised baselines in plankton monitoring tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are state-of-the-art for image generation but their potential as general-purpose feature encoders remains underexplored. The authors aim to investigate whether diffusion models, trained without labels for denoising, can capture meaningful visual features for downstream recognition tasks.

Method: Use a frozen diffusion model backbone as a feature encoder, probe intermediate denoising features across layers and timesteps, and train a linear classifier for each layer-timestep pair. Evaluate in real-world plankton monitoring with controlled training setups against supervised and self-supervised baselines.

Result: Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and long-tailed settings. They maintain strong accuracy and Macro F1 under substantial distribution shift in out-of-distribution evaluations on temporally and geographically shifted plankton datasets.

Conclusion: Diffusion models can serve as effective general-purpose feature encoders for fine-grained recognition tasks, demonstrating strong performance even under distribution shifts, making them valuable for real-world applications like ecological monitoring.

Abstract: Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.

</details>


### [210] [SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement](https://arxiv.org/abs/2601.13417)
*Yujian Xiong,Xuanzhao Dong,Wenhui Zhu,Xin Li,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: SGW-GAN uses Sliced Gromov Wasserstein distance for retinal image enhancement, preserving clinical intra-class structure better than GAN/diffusion methods while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Current GAN- and diffusion-based retinal image enhancement methods improve perceptual quality but distort intra-class geometry, harming downstream clinical tasks like disease grading and lesion detection. The Gromov Wasserstein discrepancy preserves intra-class structure but is computationally expensive.

Method: Proposed SGW-GAN framework incorporates Sliced Gromov Wasserstein (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing computational cost for practical use.

Result: Experiments on public datasets show SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity.

Conclusion: SGW-GAN offers an efficient and clinically faithful approach to unpaired medical image enhancement by preserving intra-class geometry through sliced Gromov Wasserstein distance, making it suitable for downstream diagnostic tasks.

Abstract: Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.

</details>


### [211] [Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation](https://arxiv.org/abs/2601.13440)
*Mohit Kakda,Mirudula Shri Muthukumaran,Uttapreksha Patel,Lawrence Swaminathan Xavier Prince*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) like CLIP enable zero-shot anomaly detection without labeled data by aligning image and text representations, with analysis of key methods (WinCLIP, AprilLab) across classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: VLMs revolutionize anomaly detection by eliminating need for extensive labeled datasets and task-specific training, enabling zero-shot defect identification through natural language descriptions of normal/abnormal states.

Method: Systematic analysis of VLM-based approaches including sliding window feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab), and compositional prompt ensemble strategies, evaluated across feature extraction, alignment strategies, prompt engineering, and computational efficiency.

Result: Comprehensive evaluation on benchmarks (MVTec AD, VisA) comparing classification accuracy, segmentation precision, and inference efficiency, providing foundational understanding of VLM success in anomaly detection.

Conclusion: Provides practical insights for method selection, identifies current limitations, and aims to facilitate informed adoption of VLM-based methods in industrial quality control while guiding future research directions.

Abstract: Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.

</details>


### [212] [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](https://arxiv.org/abs/2601.13498)
*Nimrod Kruger,Nicholas Owen Ralph,Gregory Cohen,Paul Hurley*

Main category: cs.CV

TL;DR: Physics-grounded pipeline converts event streams to log-intensity estimates, enabling Wiener deconvolution for computational imaging with dynamic optical systems.


<details>
  <summary>Details</summary>
Motivation: Event vision sensors produce sparse, asynchronous data incompatible with traditional linear forward models used in computational imaging, creating a need to bridge event sensing with model-based imaging approaches.

Method: Developed a processing pipeline that maps event streams to per-pixel log-intensity and intensity derivative estimates, then embeds these in a dynamic linear systems model with time-varying point spread function for frequency-domain Wiener deconvolution.

Result: Validated in simulation for single/overlapping point sources under modulated defocus and on real event data from tunable-focus telescope imaging star fields, demonstrating successful source localization and separability.

Conclusion: Provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems, enabling inverse filtering directly from event data.

Abstract: Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

</details>


### [213] [DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities](https://arxiv.org/abs/2601.13502)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: DIS2: A novel multimodal RS framework addressing missing modalities through guided compensation rather than shared features, using DLKD synergy and class-specific learning.


<details>
  <summary>Details</summary>
Motivation: Multimodal RS learning suffers from missing modalities, exacerbated by RS data heterogeneity and scale variation. Conventional approaches (disentanglement learning, knowledge distillation) fail due to insufficient feature overlap and ill-posed mimicry tasks.

Method: DIS2 paradigm with three pillars: (1) principled missing information compensation, (2) class-specific modality contribution, (3) multi-resolution feature importance. Uses DLKD (reformulated synergy between disentanglement learning and knowledge distillation) to capture compensatory features. Includes CFLM for adaptive class-specific learning and hierarchical hybrid fusion structure.

Result: Extensive experiments show DIS2 significantly outperforms state-of-the-art methods across benchmarks.

Conclusion: DIS2 successfully addresses RS-specific challenges of missing modalities through active guided compensation rather than modality-shared feature dependence, providing a new effective paradigm for multimodal RS learning.

Abstract: The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.

</details>


### [214] [GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models](https://arxiv.org/abs/2601.13524)
*Yang Yu,Yunze Deng,Yige Zhang,Yanjie Xiao,Youkun Ou,Wenhao Hu,Mingchao Li,Bin Feng,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: GO-MLVTON is the first multi-layer virtual try-on method that handles multiple garment layers with realistic deformation and occlusion modeling.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods focus on single-layer or multi-garment try-on but neglect multi-layer VTON, which requires realistic deformation and layering of garments with proper occlusion relationships between inner and outer layers.

Method: Proposes GO-MLVTON with two key modules: 1) Garment Occlusion Learning module to learn occlusion relationships between garment layers, and 2) StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body. Also introduces MLG dataset and LACD evaluation metric.

Result: Extensive experiments demonstrate state-of-the-art performance in multi-layer VTON, producing high-quality multi-layer try-on results with realistic garment deformation and layering.

Conclusion: GO-MLVTON successfully addresses the multi-layer VTON challenge by modeling occlusion relationships and using diffusion-based garment fitting, establishing a new benchmark for this task with its proposed dataset and evaluation metric.

Abstract: Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.

</details>


### [215] [DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis](https://arxiv.org/abs/2601.13551)
*Feng Ding,Wenhui Yi,Xinan He,Mengyao Xiao,Jianfeng Xu,Jianqiang Du*

Main category: cs.CV

TL;DR: DiffFace-Edit dataset with 2M+ AI-generated faces featuring fine-grained regional manipulations across 8 facial regions, plus analysis of detector-evasive splice attacks on detection models.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated face datasets lack focus on fine-grained regional manipulations, and no research has studied the real impact of splice attacks (detector-evasive samples) between real and manipulated samples on detectors.

Method: Created DiffFace-Edit dataset with over 2 million AI-generated fake images featuring edits across 8 facial regions (eyes, nose, etc.) with single-region and multi-region editing combinations. Conducted comprehensive dataset analysis and proposed cross-domain evaluation combining IMDL methods.

Result: Dataset contains extensive fine-grained manipulated faces with regional edits, enabling study of detector-evasive samples. Analysis reveals impact of splice attacks on detection models.

Conclusion: DiffFace-Edit addresses critical gap in AI-generated face datasets by focusing on fine-grained regional manipulations and studying detector-evasive samples, providing valuable resource for improving detection robustness against sophisticated face manipulations.

Abstract: Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.

</details>


### [216] [ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch](https://arxiv.org/abs/2601.13606)
*Zheng Liu,Honglin Lin,Chonghan Qin,Xiaoyang Wang,Xin Gao,Yu Li,Mengzhang Cai,Yun Zhu,Zhanping Zhong,Qizhi Pei,Zhuoshi Pan,Xiaoran Shang,Bin Cui,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: ChartVerse is a framework for synthesizing complex charts and reliable reasoning data to train VLMs for chart understanding, achieving SOTA performance with an 8B model.


<details>
  <summary>Details</summary>
Motivation: Open-source VLMs lack high-quality chart reasoning training data - existing datasets have simplistic synthetic charts with hallucinated QA pairs lacking reasoning depth.

Method: 1) Uses Rollout Posterior Entropy (RPE) metric to quantify chart complexity and complexity-aware chart coder to synthesize diverse high-complexity charts via executable programs. 2) Implements truth-anchored inverse QA synthesis with answer-first paradigm, extracting deterministic answers from source code, generating questions, and enforcing consistency verification. Filters samples based on model fail-rate and distills high-quality CoT reasoning.

Result: ChartVerse-8B achieves state-of-the-art performance, surpassing its teacher Qwen3-VL-30B-A3B-Thinking and rivaling the stronger Qwen3-VL-32B-Thinking model.

Conclusion: ChartVerse provides a scalable framework for generating high-quality chart reasoning data that enables training of powerful open-source VLMs for complex chart understanding tasks.

Abstract: Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.

</details>


### [217] [Scaling Test-time Inference for Visual Grounding](https://arxiv.org/abs/2601.13633)
*Guanqi Zhan,Changye Li,Zhijian Liu,Yao Lu,Yi Wu,Song Han,Ligeng Zhu*

Main category: cs.CV

TL;DR: EGM improves small VLMs' visual grounding by scaling test-time computation (#generated tokens) rather than model size, achieving comparable accuracy to large models with much faster inference.


<details>
  <summary>Details</summary>
Motivation: Small VLMs lag behind large ones in visual grounding primarily due to language understanding limitations, not visual processing. Large models are slow and deployment-unfriendly, so there's a need for efficient grounding methods.

Method: EGM (Efficient visual Grounding language Models) scales test-time computation by generating more tokens during inference, leveraging the fact that small models have cheaper per-token costs than large models.

Result: EGM-Qwen3-VL-8B achieves 91.4 IoU on RefCOCO with 737ms latency (5.9x faster) vs Qwen3-VL-235B's 90.5 IoU with 4320ms. Also improves amodal grounding, matching or outperforming larger models.

Conclusion: Scaling test-time computation is an effective deployment-friendly approach to boost small VLMs' grounding capabilities to match large models while maintaining faster inference speeds.

Abstract: Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.

</details>


### [218] [Face-Voice Association with Inductive Bias for Maximum Class Separation](https://arxiv.org/abs/2601.13651)
*Marta Moscati,Oleksandr Kats,Mubashir Noman,Muhammad Zaigham Zaheer,Yufang Hou,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: First work to apply maximum class separation as inductive bias for face-voice association, achieving SOTA performance through combined losses for inter-class orthogonality.


<details>
  <summary>Details</summary>
Motivation: Previous face-voice association methods use loss functions but haven't applied maximum class separation as inductive bias, which has shown success in classification tasks for strengthening discriminative ability of embeddings.

Method: Develops face-voice association method that imposes maximum class separation among multimodal representations of different speakers as inductive bias, combined with losses for inter-class orthogonality.

Result: Achieves state-of-the-art performance on two face-voice association task formulations; ablation study shows inductive bias is most effective when combined with inter-class orthogonality losses.

Conclusion: First work to demonstrate effectiveness of maximum class separation as inductive bias in multimodal learning, establishing a new paradigm for face-voice association.

Abstract: Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.

</details>


### [219] [VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement](https://arxiv.org/abs/2601.13664)
*Tiancheng Fang,Bowen Pan,Lingxi Chen,Jiangjing Lyu,Chengfei Lyu,Chaoyue Niu,Fan Wu*

Main category: cs.CV

TL;DR: VIAFormer is a transformer model that refines incomplete 3D voxels using multi-view images as guidance, achieving state-of-the-art performance in correcting synthetic and real-world artifacts.


<details>
  <summary>Details</summary>
Motivation: To address the problem of repairing incomplete and noisy 3D voxel representations using calibrated multi-view images as guidance, enabling more accurate 3D reconstruction and refinement.

Method: VIAFormer uses three key components: 1) Image Index for explicit 3D spatial grounding of 2D image tokens, 2) Correctional Flow objective that learns direct voxel-refinement trajectories, and 3) Hybrid Stream Transformer for robust cross-modal fusion between voxel and image data.

Result: VIAFormer establishes new state-of-the-art performance in correcting both severe synthetic corruptions and realistic artifacts on voxel shapes obtained from Vision Foundation Models, demonstrating superior refinement capabilities.

Conclusion: VIAFormer serves as a practical and reliable bridge in real-world 3D creation pipelines, enabling voxel-based methods to thrive in the era of large models and big data, advancing 3D reconstruction and refinement technology.

Abstract: We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.

</details>


### [220] [Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting](https://arxiv.org/abs/2601.13665)
*Mounika Kanulla,Rajasree Dadigi,Sailaja Thota,Vivek Yelleti*

Main category: cs.CV

TL;DR: Proposed fusion architectures combining CNN with LSTM and DeiT transformers for simultaneous vegetable classification, spoilage detection, and shelf life forecasting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Food wastage is a critical challenge in agricultural supply chains. Accurate spoilage detection and forecasting can reduce waste and improve supply chain longevity, motivating the development of multi-task fusion architectures.

Method: Developed fusion architectures combining CNN with LSTM (CNN+CNN-LSTM) and CNN with DeiT Transformer (CNN+DeiT Transformer) for simultaneous multi-task learning. Created a custom dataset by capturing vegetable images from fresh to completely spoiled states.

Result: Proposed fusion architectures outperformed CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. CNN+DeiT Transformer achieved F1-scores of 0.98 (classification) and 0.61 (spoilage detection), with MSE of 3.58 and SMAPE of 41.66% for forecasting. Models validated on noisy images and integrated with LIME for interpretability.

Conclusion: Fusion architectures combining CNN with LSTM and DeiT transformers are effective for simultaneous vegetable classification, spoilage detection, and shelf life forecasting, offering practical solutions for reducing food waste in agricultural supply chains.

Abstract: Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.

</details>


### [221] [Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging](https://arxiv.org/abs/2601.13677)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jäger,Klaus Maier-Hein,Fabian Isensee*

Main category: cs.CV

TL;DR: ClaSP PE is a novel active learning method for 3D biomedical image segmentation that consistently outperforms random baselines by addressing class imbalance and query redundancy through class-stratified querying and scheduled power noising.


<details>
  <summary>Details</summary>
Motivation: Active learning could reduce expensive 3D biomedical annotation costs, but existing methods fail to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without reliable solutions.

Method: Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) combines class-stratified querying to ensure coverage of underrepresented structures with log-scale power noising and a decaying schedule to enforce diversity in early-stage AL and encourage exploitation later.

Result: In 24 experimental settings across four 3D biomedical datasets, ClaSP PE was the only method that consistently outperformed improved random baselines with statistically significant gains in segmentation quality while remaining annotation-efficient. It also robustly generalized to four unseen datasets without manual adaptation.

Conclusion: ClaSP PE demonstrates that an AL method can consistently outperform random baselines in 3D segmentation, providing both performance gains and annotation efficiency in realistic scenarios, with open-source implementation making it readily applicable in practice.

Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.

</details>


### [222] [Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation](https://arxiv.org/abs/2601.13683)
*Boyuan Cao,Xingbo Yao,Chenhui Wang,Jiaxin Ye,Yujie Wei,Hongming Shan*

Main category: cs.CV

TL;DR: DyDiLA introduces a novel linear attention formulation with dynamic projection, dynamic measure kernel, and token differential operator to enhance linear diffusion transformers, overcoming oversmoothing issues while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Linear attention mechanisms reduce computational cost in diffusion transformers but often sacrifice generative performance by producing over-smoothed attention weights that limit expressiveness.

Method: Proposes Dynamic Differential Linear Attention (DyDiLA) with three key designs: dynamic projection module for token decoupling, dynamic measure kernel for better similarity measurement, and token differential operator for robust query-to-key retrieval.

Result: DyDi-LiT consistently outperforms current state-of-the-art models across multiple metrics, demonstrating strong practical potential.

Conclusion: DyDiLA effectively addresses the oversmoothing issue in linear diffusion transformers, enhancing generation quality while maintaining computational efficiency, making it a promising approach for scalable high-fidelity image generation.

Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.

</details>


### [223] [Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles](https://arxiv.org/abs/2601.13705)
*Maria Lymperaiou,Vasileios Karampinis,Giorgos Filandrianos,Angelos Vlachos,Chrysoula Zerva,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: Survey paper analyzing visual puzzles as diagnostic tools for evaluating reasoning abilities in Large Vision-Language Models (LVLMs), organizing benchmarks by cognitive reasoning types and identifying model limitations.


<details>
  <summary>Details</summary>
Motivation: Visual puzzles serve as compact probes of human cognition that can isolate abstraction, rule discovery, and systematic reasoning with minimal prior knowledge. The paper aims to leverage these properties to evaluate LVLM reasoning abilities, providing controlled alternatives to open-ended multimodal benchmarks.

Method: The survey provides a unified perspective by framing visual puzzles through a common abstraction and organizing existing benchmarks by the reasoning mechanisms they target: inductive, analogical, algorithmic, deductive, and geometric/spatial reasoning. This links puzzle design to required cognitive operations.

Result: Synthesis of empirical evidence across puzzle categories reveals consistent limitations in current LVLMs: brittle generalization, tight entanglement between perception and reasoning, and persistent gaps between fluent explanations and faithful execution.

Conclusion: Visual puzzles should be framed as diagnostic instruments rather than task formats. The survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

Abstract: Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

</details>


### [224] [ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins](https://arxiv.org/abs/2601.13706)
*Xinhao Liu,Yu Wang,Xiansheng Guo,Gordon Owusu Boateng,Yu Cao,Haonan Si,Xingchen Guo,Nirwan Ansari*

Main category: cs.CV

TL;DR: ParkingTwin: Training-free, lightweight system for online streaming 3D reconstruction of parking lots that runs at 30+ FPS on entry-level GPUs, achieving 15x speedup and 83.3% memory reduction vs 3DGS.


<details>
  <summary>Details</summary>
Motivation: High-fidelity parking-lot digital twins are essential for AVP tasks but face a trilemma: sparse forward-facing views cause weak parallax, dynamic occlusions hinder texture fusion, and neural rendering needs expensive offline optimization violating edge-side streaming constraints.

Method: Three key components: 1) OSM-prior-driven geometric construction using OpenStreetMap semantic topology to generate metric-consistent TSDF, 2) geometry-aware dynamic filtering with quad-modal constraint field to reject moving vehicles, 3) illumination-robust fusion in CIELAB space with adaptive L-channel weighting and depth-gradient suppression.

Result: Achieves 30+ FPS on GTX 1660, SSIM 0.87 (+16.0% improvement), 15x end-to-end speedup, and 83.3% GPU memory reduction compared to state-of-the-art 3DGS on RTX 4090D. Outputs explicit triangle meshes compatible with Unity/Unreal pipelines.

Conclusion: ParkingTwin addresses the trilemma of parking-lot reconstruction by providing a training-free, lightweight system for online streaming 3D reconstruction that works efficiently on entry-level hardware while maintaining high quality and compatibility with digital-twin pipelines.

Abstract: High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/

</details>


### [225] [MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network](https://arxiv.org/abs/2601.13715)
*Yiwei Lu,Hao Huang,Tao Yan*

Main category: cs.CV

TL;DR: MVGD-Net detects glass surfaces in videos using motion inconsistency cues, outperforming state-of-the-art methods with novel temporal-spatial modules and a large dataset.


<details>
  <summary>Details</summary>
Motivation: Glass surfaces pose threats to vision-based systems like robot/drone navigation, and current Video Glass Surface Detection (VGSD) methods need improvement. The key insight is that reflected/transmitted objects on glass appear farther away and move slower than objects in non-glass regions, creating motion inconsistency that reveals glass presence.

Method: Proposes MVGD-Net with three novel modules: Cross-scale Multimodal Fusion Module (CMFM) integrates spatial features and optical flow maps; History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM) enhance temporal features; Temporal-Spatial Decoder (TSD) fuses spatial and temporal features to generate glass region masks.

Result: Extensive experiments show MVGD-Net outperforms relevant state-of-the-art methods. The paper also contributes a large-scale dataset with 312 diverse glass scenarios and 19,268 frames for training and evaluation.

Conclusion: MVGD-Net effectively detects glass surfaces in videos by leveraging motion inconsistency cues through novel temporal-spatial fusion modules, providing a robust solution for vision-based systems operating in environments with glass surfaces.

Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.

</details>


### [226] [Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement](https://arxiv.org/abs/2601.13724)
*Sam Cantrill,David Ahmedt-Aristizabal,Lars Petersson,Hanna Suominen,Mohammad Ali Armin*

Main category: cs.CV

TL;DR: MeshPhys: A novel 3D facial mesh-based approach for remote photoplethysmography that explicitly aligns receptive fields with facial surface structure using spatiotemporal graph representation.


<details>
  <summary>Details</summary>
Motivation: Existing facial rPPG methods fail to explicitly align their receptive fields with the 3D facial surface, which is the spatial support of the physiological signal, limiting their performance and interpretability.

Method: Proposes Facial Spatiotemporal Graph (STGraph) representation using 3D facial mesh sequences to encode facial color and structure, and MeshPhys - a lightweight spatiotemporal graph convolutional network that operates on STGraph for physiological signal estimation.

Result: Achieves state-of-the-art or competitive performance across four benchmark datasets in both intra- and cross-dataset settings. Ablation studies show surface-aligned receptive fields act as strong structural prior and 3D-aware node features are critical for robust encoding.

Conclusion: STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG that enables robust, interpretable, and generalizable physiological signal estimation by explicitly aligning processing with facial surface structure.

Abstract: Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .

</details>


### [227] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: HiT-Prithvi enables real-time flood detection on small satellites using transformer models with History Injection, achieving 43 FPS on Jetson Orin Nano while maintaining accuracy and reducing storage by 99%.


<details>
  <summary>Details</summary>
Motivation: Natural disaster monitoring requires continuous satellite observation with real-time processing, but current systems face memory and computational constraints on small satellites, creating dependency on ground-based infrastructure.

Method: Proposed History Injection mechanism for Transformer models (HiT) that maintains historical context from previous observations while compressing data storage by over 99% of original image size, integrated with Prithvi-tiny foundation model.

Result: HiT-Prithvi maintained detection accuracy comparable to bitemporal baseline on STTORM-CD flood dataset while achieving 43 FPS on Jetson Orin Nano hardware, demonstrating practical onboard processing capabilities.

Conclusion: The work establishes a practical framework for satellite-based continuous monitoring of natural disasters, enabling real-time hazard assessment without ground infrastructure dependency, with architecture and models publicly available.

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [228] [PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval](https://arxiv.org/abs/2601.13797)
*Gabriele Serussi,David Vainshtein,Jonathan Kouchly,Dotan Di Castro,Chaim Baskin*

Main category: cs.CV

TL;DR: PREGEN is an efficient CoVR framework that pairs a frozen VLM with a lightweight encoder, achieving state-of-the-art performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current CoVR methods fail to fully exploit modern VLMs, using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation.

Method: Pairs a frozen pre-trained VLM with a lightweight encoding model, extracts hidden states from the VLM's final token across layers, and trains a simple encoder on pooled representations.

Result: Significantly advances state-of-the-art with substantial gains in Recall@1 (+27.23 and +69.59), robust across different VLM backbones, strong zero-shot generalization to complex textual modifications.

Conclusion: PREGEN provides an efficient and powerful CoVR framework that overcomes limitations of current methods, demonstrating effectiveness and strong semantic capabilities without VLM fine-tuning.

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.

</details>


### [229] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: CSDA is a novel colorspace transformation algorithm that improves image segmentation by optimizing color representation through multidimensional nonlinear discriminant analysis.


<details>
  <summary>Details</summary>
Motivation: Current segmentation algorithms often neglect color preprocessing, leading to suboptimal color representation that hinders accurate segmentation. There's a need for tailored color representation optimization in domain-specific segmentation tasks.

Method: Proposes Colorspace Discriminant Analysis (CSDA), which extends Linear Discriminant Analysis into deep learning context. It customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability using generalized discriminative loss. Introduces three alternative losses for stable end-to-end optimization of both colorspace transformation and segmentation.

Result: Experiments on wind turbine blade data demonstrate significant accuracy gains, showing the effectiveness of CSDA for domain-specific segmentation tasks.

Conclusion: Tailored color preprocessing is crucial for accurate segmentation, and CSDA provides an effective framework for optimizing color representation in domain-specific applications, particularly demonstrated with wind turbine blade analysis.

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [230] [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/abs/2601.13837)
*Xinya Ji,Sebastian Weiss,Manuel Kansy,Jacek Naruniec,Xun Cao,Barbara Solenthaler,Derek Bradley*

Main category: cs.CV

TL;DR: A feed-forward method called \OURS generates high-quality 3D Gaussian head avatars from few input images with real-time animation, outperforming existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian-based head avatar methods require extensive multi-view setups or per-identity optimization during inference, limiting scalability and ease of use on unseen subjects.

Method: Learns per-pixel Gaussian representation from input images, aggregates multi-view information using transformer-based encoder with DINOv3 and Stable Diffusion VAE features, extends Gaussians with per-Gaussian features, uses lightweight MLP-based dynamic network for deformation prediction, and employs point maps from pre-trained reconstruction model for geometry supervision.

Result: Significantly outperforms existing methods in both rendering quality and inference efficiency while supporting real-time dynamic avatar animation.

Conclusion: The proposed \OURS method enables efficient generation of high-fidelity 3D head avatars from few images with real-time animation capabilities, addressing scalability and usability limitations of current approaches.

Abstract: Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.

</details>


### [231] [DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes](https://arxiv.org/abs/2601.13839)
*Aisha Al-Mohannadi,Ayisha Firoz,Yin Yang,Muhammad Imran,Ferda Ofli*

Main category: cs.CV

TL;DR: DisasterVQA is a benchmark dataset for evaluating vision-language models on disaster response tasks, featuring 1,395 real-world images and 4,405 expert-curated QA pairs across diverse disaster scenarios.


<details>
  <summary>Details</summary>
Motivation: Social media imagery provides low-latency situational information during disasters, but current VQA models' suitability for complex, safety-critical disaster reasoning remains unclear. There's a need for specialized benchmarks to evaluate and improve vision-language models for humanitarian applications.

Method: Created DisasterVQA dataset with 1,395 real-world disaster images and 4,405 expert-curated question-answer pairs spanning floods, wildfires, earthquakes, and other events. Questions are grounded in humanitarian frameworks (FEMA ESF and OCHA MIRA) and include binary, multiple-choice, and open-ended formats covering situational awareness and operational decision-making.

Result: Benchmarked 7 state-of-the-art vision-language models showing performance variability across question types, disaster categories, regions, and humanitarian tasks. Models achieved high accuracy on binary questions but struggled with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, especially for underrepresented disaster scenarios.

Conclusion: DisasterVQA provides a challenging, practical benchmark to guide development of more robust and operationally meaningful vision-language models for disaster response. The dataset addresses critical gaps in current VQA capabilities for humanitarian applications and is publicly available for research.

Abstract: Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.

</details>


### [232] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: Deep Discriminant Analysis (DDA) optimizes Fisher criterion with deep networks for better class separability, enhanced with probabilistic version (PDDA) for wind blade segmentation.


<details>
  <summary>Details</summary>
Motivation: Linear discriminant analysis has limitations with non-linearly separable data, so the authors propose using deep networks to directly optimize the Fisher criterion for improved class separability in complex scenarios like image segmentation.

Method: Introduce Deep Discriminant Analysis (DDA) with stable training techniques: signed between-class variance, sigmoid-bounded outputs, and additive relationships. Develop two stable DDA loss functions, then augment with probability loss to create Probabilistic DDA (PDDA) that minimizes class overlap in output distributions.

Result: PDDA produces highly confident predictions with reduced within-class variance and shows notable advances in performance and consistency when applied to wind blade segmentation, representing the first application of DDA to image segmentation.

Conclusion: PDDA successfully extends discriminant analysis to deep networks for image segmentation, demonstrating practical value in wind energy maintenance applications through improved segmentation performance and consistency.

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [233] [OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting](https://arxiv.org/abs/2601.13871)
*Michail Spanakis,Iason Oikonomidis,Antonis Argyros*

Main category: cs.CV

TL;DR: OCCAM is the first training-free, multi-class object counting method that uses SAM2 and FINCH clustering without needing exemplars or text prompts.


<details>
  <summary>Details</summary>
Motivation: Most existing class-agnostic counting methods require single-class assumptions, extensive training, or supplementary information like exemplars/prompts. There's a need for training-free approaches that handle multi-class counting without additional inputs.

Method: Uses Segment Anything Model 2 (SAM2) foundation model with a custom threshold-based variant of FINCH clustering algorithm. No training required and operates without supplementary information.

Result: Achieves competitive performance on FSC-147 and CARPK benchmarks. Introduces synthetic multi-class dataset and proposes F1 score as more suitable evaluation metric.

Conclusion: OCCAM demonstrates that training-free approaches using foundation models can effectively solve multi-class object counting without needing exemplars or prompts, offering a simpler and more flexible solution.

Abstract: Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.

</details>


### [234] [Revisiting Multi-Task Visual Representation Learning](https://arxiv.org/abs/2601.13886)
*Shangzhe Di,Zhonghua Zhai,Weidi Xie*

Main category: cs.CV

TL;DR: MTV is a multi-task visual pretraining framework that combines vision-language contrastive learning, self-supervised learning, and dense spatial supervision to create visual encoders with both global semantic understanding and fine-grained spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Current visual representation learning is bifurcated: vision-language models (like CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (like MAE, DINO) capture local structures but struggle with high-level semantics. These complementary paradigms should be integrated.

Method: MTV jointly optimizes a shared backbone across three objectives: vision-language contrastive learning, self-supervised learning, and dense spatial supervision. To avoid manual annotations, it leverages high-capacity "expert" models (Depth Anything V2, OWLv2) to synthesize dense pseudo-labels at scale.

Result: MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. The framework demonstrates task synergies and provides insights into multi-task learning mechanics.

Conclusion: Multi-task learning with high-quality pseudo-supervision is a scalable path toward more general visual encoders that combine the strengths of different representation learning paradigms.

Abstract: Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.

</details>


### [235] [Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging](https://arxiv.org/abs/2601.13899)
*Masoumeh Javanbakhat,Piotr Komorowski,Dilyara Bareeva,Wei-Chang Lai,Wojciech Samek,Christoph Lippert*

Main category: cs.CV

TL;DR: Proposes an explainable deep statistical testing framework that adds sample-level and feature-level explanations to deep two-sample tests, enabling interpretable detection of group differences in biomedical imaging without requiring class labels.


<details>
  <summary>Details</summary>
Motivation: Deep neural two-sample tests have strong power but lack interpretability due to their black-box nature, limiting practical adoption in biomedical analysis. Existing explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings.

Method: An explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations. The method identifies which individual samples and which input features drive statistically significant group differences, providing spatial and instance-wise insight into test decisions.

Result: The framework highlights which image regions and which individual samples contribute most to detected group differences. Applied to biomedical imaging data, it identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation.

Conclusion: This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging by providing both sample-level and feature-level explanations for deep two-sample tests.

Abstract: Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.

</details>


### [236] [On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.13913)
*Pavlo Melnyk,Cuong Le,Urs Waldmann,Per-Erik Forssén,Bastian Wandt*

Main category: cs.CV

TL;DR: Monocular 3D human pose estimation using rotation equivariance learned through data augmentation outperforms equivariant-by-design methods.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D human pose estimation is ill-posed but recent methods achieve centimeter-level accuracy. However, existing lifting models fail with rotated inputs, suggesting that learning pose with in-plane rotations is easier and more geometrically grounded than direct point-to-point mapping.

Method: Proposes learning 2D rotation equivariance through data augmentation rather than constrained parameter space. Uses a two-step approach: 2D joint detection followed by 2D-to-3D lifting, with rotation equivariance learned implicitly through augmentation.

Result: Rotation equivariance improves model performance on human poses with image plane rotations. The augmentation-based approach outperforms state-of-the-art equivariant-by-design methods on common HPE benchmarks.

Conclusion: Learning rotation equivariance through data augmentation is more efficient and straightforward than equivariant-by-design approaches, leading to better performance in monocular 3D human pose estimation.

Abstract: Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.

</details>


### [237] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: TrackletGPT uses a GPT framework with tracklets for white matter tract segmentation, outperforming SOTA methods across datasets.


<details>
  <summary>Details</summary>
Motivation: White matter tract segmentation is crucial for studying brain connectivity and disorders, but remains complex due to tract variability across subjects/conditions while maintaining similar 3D structure across hemispheres.

Method: Proposes TrackletGPT, a language-like GPT framework that reintroduces sequential information using tracklets (granular sub-streamline segments), enabling automatic, generalizable tract segmentation across datasets.

Result: Outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, including in inter-dataset experiments.

Conclusion: TrackletGPT successfully addresses tract segmentation challenges by leveraging sequential information through tracklets, achieving superior performance and generalization across different datasets.

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [238] [VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content](https://arxiv.org/abs/2601.13951)
*Shengyi Wu,Yan Hong,Shengyao Chen,Zheng Wang,Xianbing Sun,Jiahui Zhan,Jun Lan,Jianfu Zhang*

Main category: cs.CV

TL;DR: VTONGuard: A large-scale benchmark dataset with 775k+ real/synthetic virtual try-on images for evaluating detection methods, with a proposed multi-task framework achieving best performance.


<details>
  <summary>Details</summary>
Motivation: Address authenticity and responsible use concerns in AI-generated virtual try-on (VTON) systems as they become more realistic and widely used in e-commerce and digital entertainment.

Method: Create VTONGuard dataset (775k+ images covering diverse conditions), systematically evaluate detection paradigms under unified protocols, and design multi-task framework integrating auxiliary segmentation for boundary-aware feature learning.

Result: Benchmark reveals strengths/weaknesses of detection methods and cross-paradigm generalization challenges; proposed multi-task framework achieves best overall performance on VTONGuard.

Conclusion: VTONGuard enables fair comparisons, facilitates robust detection model development, and promotes safe/responsible deployment of VTON technologies.

Abstract: With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.

</details>


### [239] [DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging](https://arxiv.org/abs/2601.13954)
*Adrien Meyer,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: DExTeR is a transformer-based Point-to-Box regressor for medical imaging that uses single-point annotations to generate pseudo-box labels, overcoming challenges like overlapping anatomy and variable object sizes through class-guided attention and mixture of experts.


<details>
  <summary>Details</summary>
Motivation: Medical imaging landmark detection requires costly bounding box annotations. Weakly Semi-Supervised Object Detection with point annotations reduces annotation time but struggles with medical imaging challenges like overlapping anatomy, variable object sizes, and elusive structures.

Method: DExTeR builds on Point-DETR, encoding single-point annotations as object queries. It uses class-guided deformable attention to capture class-specific features, CLICK-MoE to decouple class and instance representations for overlapping structures, and multi-point training for robustness to annotation variability.

Result: Achieves state-of-the-art performance across three medical imaging datasets (endoscopy, chest X-rays, and endoscopic ultrasound), demonstrating effectiveness in reducing annotation costs while maintaining high detection accuracy.

Conclusion: DExTeR effectively addresses medical imaging challenges in weakly supervised object detection, offering a practical solution to reduce annotation burden while maintaining detection performance across diverse medical domains.

Abstract: Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.

</details>


### [240] [STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames](https://arxiv.org/abs/2601.13974)
*Shih-Yao Lin*

Main category: cs.CV

TL;DR: STEC is a new metric for evaluating video frame sampling quality by measuring spatial information, temporal coverage, and redundancy, without needing reference frames.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for frame sampling focus on perceptual quality or reconstruction fidelity, but don't assess whether sampled frames adequately capture informative and representative video content. There's a need for a task-agnostic way to evaluate sampling effectiveness.

Method: STEC builds on Spatio-Temporal Frame Entropy (STFE) which measures per-frame spatial information via entropy-based structural complexity. STEC evaluates sampled frames based on their temporal coverage and redundancy, jointly modeling spatial information strength, temporal dispersion, and non-redundancy.

Result: Experiments on MSR-VTT test-1k show STEC clearly differentiates common sampling strategies (random, uniform, content-aware). It reveals robustness patterns across individual videos not captured by average performance alone, demonstrating practical value as a general-purpose evaluation tool.

Conclusion: STEC provides a principled, lightweight, task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets, though it's not designed to predict downstream task accuracy.

Abstract: Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.
  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.
  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.
  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.

</details>


### [241] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: Developed a unified detection pipeline for marine invasive species monitoring that addresses cross-domain performance degradation, finding structural factors (scene composition, object density) matter more than visual degradation, with practical edge deployment validation.


<details>
  <summary>Details</summary>
Motivation: Marine biodiversity monitoring needs scalable, reliable detection across diverse underwater environments, but existing solutions suffer from performance degradation when deployed to new sites, creating a deployment gap for conservation and invasive species management.

Method: Created a Unified Information Pipeline that standardizes heterogeneous datasets into comparable information flow, evaluated a fixed detector under controlled cross-domain protocols, and benchmarked inference on low-cost edge hardware for operational feasibility.

Result: Structural factors (scene composition, object density, contextual redundancy) explain cross-domain performance loss more strongly than visual degradation like turbidity; sparse scenes cause "Context Collapse" failure; edge hardware optimization enables practical sampling rates.

Conclusion: Shifts emphasis from image enhancement to structure-aware reliability, providing a democratized tool for consistent marine ecosystem assessment that addresses the deployment gap in marine biodiversity monitoring.

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [242] [Equivariant Learning for Unsupervised Image Dehazing](https://arxiv.org/abs/2601.13986)
*Zhang Wen,Jiangwei Xie,Dongdong Chen*

Main category: cs.CV

TL;DR: EID is an unsupervised image dehazing framework that uses equivariant learning and adversarial training to remove haze without needing priors or ground truth data.


<details>
  <summary>Details</summary>
Motivation: Current image dehazing methods require expensive priors or extensive haze-free ground truth data, which are particularly impractical for scientific imaging applications.

Method: Equivariant Image Dehazing (EID) framework that exploits image symmetry through haze consistency and systematic equivariance, combined with adversarial learning to model unknown haze physics.

Result: EID significantly outperforms state-of-the-art approaches on scientific image benchmarks (cell microscopy, medical endoscopy) and natural image dehazing tasks.

Conclusion: By unifying equivariant learning with haze physics modeling, EID enables more versatile and effective haze removal in scientific imaging without requiring expensive ground truth data.

Abstract: Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.

</details>


### [243] [Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution](https://arxiv.org/abs/2601.14030)
*Samuel W. Remedios,Zhangxing Bian,Shuwen Wei,Aaron Carass,Jerry L. Prince,Blake E. Dewey*

Main category: cs.CV

TL;DR: Generalizes diffusion-based inverse problem solvers for multi-image super-resolution MRI, enabling near-isotropic reconstruction from routine 2D multi-slice acquisitions without modifying diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based methods focus on single-image inverse problems, but MRI often involves multiple complementary low-resolution measurements along different axes. Need to extend these methods for multi-image super-resolution to reconstruct near-isotropic anatomy from routine 2D acquisitions.

Method: Generalizes diffusion-based inverse single-image solvers (DPS, DMAP, DPPS, PnP/ADMM) for MISR MRI. Shows DPS likelihood correction allows exactly-separable gradient decomposition across independent measurements, enabling MISR without joint operators, model modifications, or extra network evaluations.

Result: Achieves substantial gains over single-image super-resolution across 4×/8×/16× anisotropic degradations. State-of-the-art super-resolution of anisotropic MRI volumes, enabling reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions.

Conclusion: Successfully extends diffusion-based inverse problem solvers to multi-image super-resolution MRI, providing a practical solution for reconstructing high-quality near-isotropic volumes from standard clinical 2D acquisitions without requiring specialized hardware or acquisition protocols.

Abstract: Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.

</details>


### [244] [Human detectors are surprisingly powerful reward models](https://arxiv.org/abs/2601.14037)
*Kumar Ashutosh,XuDong Wang,Xi Yin,Kristen Grauman,Adam Polyak,Ishan Misra,Rohit Girdhar*

Main category: cs.CV

TL;DR: HuDA is a simple reward model that improves human motion quality in generated videos by combining human detection confidence and temporal prompt alignment, outperforming specialized models and enhancing video generation for complex motions.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with complex, non-rigid human motions, often producing distorted poses, missing limbs, or physically implausible actions, especially in dynamic scenarios like sports and dance.

Method: HuDA integrates two components: human detection confidence for appearance quality and temporal prompt alignment score for motion realism, using off-the-shelf models without additional training, then applies Group Reward Policy Optimization (GRPO) for post-training.

Result: HuDA outperforms specialized models fine-tuned with manual annotations, achieves 73% win-rate over state-of-the-art models like Wan 2.1, and improves generation quality beyond humans to include animals and human-object interactions.

Conclusion: A simple reward model leveraging existing components can effectively quantify and improve human motion quality in generated videos, demonstrating broad applicability beyond just human motion enhancement.

Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.

</details>


### [245] [Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving](https://arxiv.org/abs/2601.14038)
*Alexandre Justo Miro,Ludvig af Klinteberg,Bogdan Timus,Aron Asefaw,Ajinkya Khoche,Thomas Gustafsson,Sina Sharif Mansouri,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: Paper discovers systematic annotation errors in 3D box datasets for autonomous vehicles, proposes correction method to achieve physical consistency, and shows errors significantly impact benchmarking results.


<details>
  <summary>Details</summary>
Motivation: Accurate ground truth annotations are critical for supervised learning and evaluating autonomous vehicle systems, but 3D box annotation from LiDAR data in dynamic scenarios introduces systematic errors due to objects being observed at different timestamps and positions.

Method: Novel offline estimation method that corrects annotations to follow physically feasible trajectories and achieve spatial and temporal consistency with sensor data. First to define metrics for this problem and evaluate on Argoverse 2, MAN TruckScenes, and proprietary datasets.

Result: Increases annotation quality by more than 17% in evaluated datasets. Finds original annotations misplaced by up to 2.5m, with highly dynamic objects most affected. Shows annotation errors have larger impact on benchmarking than typical state-of-the-art improvements.

Conclusion: Accurate annotations are essential for correct interpretation of autonomous vehicle performance. The discovered systematic errors in widely used datasets significantly affect benchmarking, and the proposed correction method substantially improves annotation quality.

Abstract: Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.

</details>


### [246] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: Federated Balanced Learning (FBL) addresses client drift in non-iid federated learning by achieving sample balance on client side through knowledge filling/sampling with edge-side generation models.


<details>
  <summary>Details</summary>
Motivation: In non-iid federated learning, client drift seriously affects final model performance. Previous methods correct the already-deviated global model based on loss/gradient, overlooking the impact of client samples.

Method: FBL achieves sample balance on client side through knowledge filling and knowledge sampling using edge-side generation models, with fixed data sample constraints. Includes Knowledge Alignment Strategy to bridge synthetic-real data gap, Knowledge Drop Strategy for regularization, and scales to complex scenarios with client method flexibility.

Result: Numerous experiments show the method outperforms state-of-the-art baselines.

Conclusion: FBL effectively prevents client drift from the beginning through client-side sample balancing, offering a novel approach to non-iid federated learning challenges.

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [247] [Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology](https://arxiv.org/abs/2601.14044)
*Kaiyu Wu,Pucheng Han,Hualong Zhang,Naigeng Wu,Keze Wang*

Main category: cs.CV

TL;DR: Weather-R1 is a meteorology-focused VLM that addresses reasoning contradictions in high-stakes domains using logical consistency reinforcement fine-tuning, achieving 9.8% improvement over baselines on the new WeatherQA benchmark.


<details>
  <summary>Details</summary>
Motivation: VLMs have advancing reasoning capabilities but face two gaps in meteorology: domain gap (lack of meteorological knowledge) and reasoning faithfulness gap (models can produce self-contradictory reasoning that contradicts their final answers, which is unacceptable in high-stakes meteorological applications).

Method: 1) Construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. 2) Propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT) that introduces a logical consistency reward to resolve Self-Contradictory Reasoning. 3) Develop Weather-R1, the first reasoning VLM with logical faithfulness in meteorology.

Result: Weather-R1 improves performance on WeatherQA by 9.8 percentage points over baseline, outperforming both Supervised Fine-Tuning and standard Reinforcement Fine-Tuning, and even surpassing the original Qwen2.5-VL-32B model.

Conclusion: The proposed LoCo-RFT effectively resolves self-contradictory reasoning in VLMs for high-stakes domains, and Weather-R1 demonstrates superior performance in meteorological reasoning tasks, highlighting the importance of logical consistency in domain-specific VLM applications.

Abstract: While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.

</details>


### [248] [Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model](https://arxiv.org/abs/2601.14052)
*Haoran Xu,Yanlin Liu,Zizhao Tong,Jiaze Li,Kexue Fu,Yuyang Zhang,Longxiang Gao,Shuaiguang Li,Xingyu Li,Yanran Xu,Changwei Wang*

Main category: cs.CV

TL;DR: MM-OOD is a novel multimodal OOD detection pipeline that leverages MLLMs' reasoning and conversation capabilities to improve both near and far OOD detection through direct multimodal analysis and a sketch-generate-elaborate framework.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot OOD detection methods over-rely on text-space knowledge from LLMs, neglecting the inherent challenges of detecting OOD samples in image space. There's a need for better multimodal approaches that can handle both near and far OOD tasks effectively.

Method: MM-OOD uses MLLMs' multimodal reasoning and multi-round conversation capabilities: (1) For near OOD tasks, directly feed ID images and text prompts to MLLMs to identify outliers; (2) For far OOD tasks, employ a sketch-generate-elaborate framework: sketch outlier exposure with text prompts, generate visual OOD samples, then elaborate using multimodal prompts.

Result: The method achieves significant improvements on widely used multimodal datasets like Food-101 and demonstrates scalability on ImageNet-1K, showing enhanced performance for both near and far OOD detection tasks.

Conclusion: MM-OOD effectively addresses the limitations of text-centric OOD detection by leveraging multimodal reasoning, offering a promising approach for robust zero-shot OOD detection that balances both image and text modalities.

Abstract: Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.

</details>


### [249] [Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration](https://arxiv.org/abs/2601.14060)
*Yongcong Ye,Kai Zhang,Yanghai Zhang,Enhong Chen,Longfei Li,Jun Zhou*

Main category: cs.CV

TL;DR: CVSI is a novel zero-shot composed image retrieval method that integrates visual and semantic information through complementary extraction modules to capture fine-grained modifications, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing ZS-CIR methods struggle with fine-grained changes and effective visual-semantic integration, often relying on simplistic transformations that fail to capture complementary information and complete semantic context.

Method: Three key components: (1) Visual Information Extraction with global features and pseudo token mapping, (2) Semantic Information Extraction using captioning models and LLMs for modified captions, (3) Complementary Information Retrieval integrating query and database information.

Result: Extensive experiments on CIRR, CIRCO, and FashionIQ datasets demonstrate CVSI significantly outperforms existing state-of-the-art methods.

Conclusion: CVSI effectively addresses limitations in current ZS-CIR approaches by providing complementary visual-semantic integration for fine-grained composed image retrieval, with code publicly available for reproducibility.

Abstract: Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.

</details>


### [250] [VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences](https://arxiv.org/abs/2601.14066)
*Hendrik Möller,Hanna Schoen,Robert Graf,Matan Atad,Nathan Molinier,Anjany Sekuboyina,Bettina K. Budai,Fabian Bamberg,Steffen Ringhof,Christopher Schlett,Tobias Pischon,Thoralf Niendorf,Josua A. Decker,Marc-André Weber,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: VERIDAH is a novel deep learning algorithm that automatically labels vertebrae in medical images while handling enumeration anomalies, outperforming existing methods on both T2w MRI and CT scans.


<details>
  <summary>Details</summary>
Motivation: Current clinical practice often misses thoracolumbar junction anomalies, which have clinical implications for back pain and surgery planning. Existing deep learning vertebra labeling algorithms lack the ability to automatically identify enumeration anomalies (variations in vertebra counts).

Method: VERIDAH uses multiple classification heads combined with a weighted vertebra sequence prediction algorithm to handle arbitrary field-of-view images and identify enumeration anomalies.

Result: VERIDAH significantly outperforms existing models: 98.30% vs 94.24% correct labeling on T2w MRI (p<0.001) and 99.18% vs 77.26% on CT (p<0.001). It correctly identified thoracic anomalies in 87.80% (T2w) and 96.30% (CT), and lumbar anomalies in 94.48% (T2w) and 97.22% (CT).

Conclusion: VERIDAH successfully addresses the gap in automated vertebra labeling with anomaly handling, demonstrating superior performance on both MRI and CT imaging, with code and models publicly available.

Abstract: The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing "Vertebra Identification with Anomaly Handling" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.

</details>


### [251] [VENI: Variational Encoder for Natural Illumination](https://arxiv.org/abs/2601.14079)
*Paul Walker,James A. D. Gardner,Andreea Ardelean,William A. P. Smith,Bernhard Egger*

Main category: cs.CV

TL;DR: Proposes a rotation-equivariant VAE for natural illumination modeling on spheres using Vector Neuron Vision Transformer encoder and equivariant neural field decoder, enabling smoother latent space interpolation.


<details>
  <summary>Details</summary>
Motivation: Existing inverse rendering methods either ignore the spherical/rotation-equivariant nature of illumination environments or lack well-behaved latent spaces, making illumination modeling challenging.

Method: Rotation-equivariant VAE with novel Vector Neuron Vision Transformer encoder and rotation-equivariant conditional neural field decoder; reduces SO(3) to SO(2) equivariance using novel SO(2)-equivariant fully connected layer.

Result: SO(2)-equivariant fully connected layer outperforms standard Vector Neurons; VAE enables smoother interpolation and more well-behaved latent space compared to previous methods.

Conclusion: The proposed rotation-equivariant VAE effectively models natural illumination on spheres while preserving equivariance properties and providing improved latent space characteristics.

Abstract: Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.

</details>


### [252] [Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition](https://arxiv.org/abs/2601.14101)
*Emily Kim,Allen Wu,Jessica Hodgins*

Main category: cs.CV

TL;DR: Curriculum learning strategies improve generalization to aerial-view action recognition without real aerial data, using synthetic aerial and real ground data, achieving comparable accuracy with 30-37% training efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Human action recognition models trained on ground-view data struggle to generalize to aerial views, creating a cross-view generalization challenge that needs addressing without access to real aerial data.

Method: Two curriculum learning approaches: (1) two-stage direct fine-tuning from synthetic aerial to real ground data, and (2) multi-stage progressive curriculum expanding dataset gradually before fine-tuning. Evaluated on REMAG dataset using SlowFast (CNN) and MViTv2 (Transformer) architectures.

Result: Combining synthetic aerial and real ground data outperforms single-domain training. Both curriculum strategies match top-1 accuracy of simple dataset combination while achieving significant efficiency gains: 37% iteration reduction for SlowFast and 30% for MViTv2 with two-step method, with progressive approach further reducing iterations by 9-30%.

Conclusion: Curriculum-based training enables effective cross-view action recognition without real aerial data, maintaining comparable performance (within 3% accuracy range) while substantially improving training efficiency through strategic domain transition strategies.

Abstract: Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.
  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.
  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.

</details>


### [253] [Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing](https://arxiv.org/abs/2601.14103)
*Xiaolu Liu,Yicong Li,Qiyuan He,Jiayin Zhu,Wei Ji,Angela Yao,Jianke Zhu*

Main category: cs.CV

TL;DR: Interp3D is a training-free framework for textured 3D morphing that preserves both geometric structure and texture coherence through progressive alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D morphing approaches either handle only geometry (ignoring textures) or extend 2D interpolation to 3D (causing semantic ambiguity, structural misalignment, and texture blurring). There's a need to jointly preserve geometric consistency, texture alignment, and robustness throughout transitions for practical applications in animation, editing, and digital content creation.

Method: Interp3D uses generative priors and progressive alignment: 1) semantically aligned interpolation in condition space, 2) SLAT-guided structure interpolation for structural consistency, and 3) fine-grained texture fusion for appearance details. The framework is training-free and evaluated on a dedicated dataset (Interp3DData) with graded difficulty levels.

Result: Both quantitative metrics and human studies demonstrate significant advantages over previous methods in terms of fidelity, transition smoothness, and plausibility. The method effectively addresses challenges of semantic ambiguity, structural misalignment, and texture blurring.

Conclusion: Interp3D provides an effective training-free solution for textured 3D morphing that jointly preserves geometric consistency and texture coherence, advancing 3D generation research and practical applications in animation and digital content creation.

Abstract: Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.

</details>


### [254] [PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning](https://arxiv.org/abs/2601.14111)
*Jiaying Wu,Can Gao,Jinglu Hu,Hui Li,Xiaofeng Cao,Jingcai Guo*

Main category: cs.CV

TL;DR: PMCE is a probabilistic few-shot learning framework that uses multi-granularity semantics and caption-guided enhancement to improve prototype estimation for novel categories with limited labeled samples.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning suffers from biased prototypes due to scarce labeled data. Existing semantic methods mainly enhance support representations but leave query features unchanged, limiting their effectiveness.

Method: PMCE constructs a nonparametric knowledge bank with visual statistics and CLIP class name embeddings. It retrieves relevant base classes for novel categories, aggregates prior information, and fuses it with support prototypes via MAP update. Simultaneously, uses BLIP captioner for instance-level descriptions and a lightweight enhancer to optimize both support and query features with consistency regularization.

Result: Experiments on four benchmarks show consistent improvements over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in 1-shot setting.

Conclusion: PMCE effectively leverages multi-granularity semantics and caption-guided enhancement to address prototype bias in few-shot learning, demonstrating superior performance through comprehensive semantic integration.

Abstract: Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D

</details>


### [255] [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/abs/2601.14127)
*Renmiao Chen,Yida Lu,Shiyao Cui,Xuan Ouyang,Victor Shea-Jay Huang,Shumin Zhang,Chengwei Pan,Han Qiu,Minlie Huang*

Main category: cs.CV

TL;DR: MIR-SafetyBench is the first benchmark for multi-image reasoning safety, showing that MLLMs with better multi-image reasoning capabilities are paradoxically more vulnerable to safety risks, with unsafe responses showing lower attention entropy.


<details>
  <summary>Details</summary>
Motivation: As Multimodal Large Language Models develop stronger multi-image reasoning abilities to handle complex instructions, this advancement may create new safety risks that need to be systematically studied and benchmarked.

Method: Created MIR-SafetyBench with 2,676 instances across 9 multi-image relation categories, then evaluated 19 MLLMs on this benchmark, analyzing attack success rates, response quality, and attention entropy patterns.

Result: Found troubling trend: models with more advanced multi-image reasoning are more vulnerable on safety benchmarks. Many "safe" responses are superficial due to misunderstanding or evasion. Unsafe generations show lower attention entropy than safe ones.

Conclusion: Advanced multi-image reasoning capabilities in MLLMs paradoxically increase safety vulnerabilities, with models potentially over-focusing on task solving while neglecting safety constraints, as evidenced by attention entropy patterns.

Abstract: As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.

</details>


### [256] [GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression](https://arxiv.org/abs/2601.14130)
*Till Aczel,David F. Jenny,Simon Bührer,Andreas Plesner,Antonio Di Maio,Roger Wattenhofer*

Main category: cs.CV

TL;DR: GIC-DLC is a hardware-friendly neural image codec that uses lookup tables and Boolean operations to achieve better compression than traditional methods while reducing energy consumption and latency for edge devices.


<details>
  <summary>Details</summary>
Motivation: Neural image codecs outperform traditional methods but have high computational overhead, making them unsuitable for energy-constrained edge devices like smartphones, cameras, and drones.

Method: Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC) trains lookup tables to combine neural network flexibility with Boolean operation efficiency in a hardware-aware approach.

Result: GIC-DLC outperforms traditional codecs in compression efficiency on grayscale benchmark datasets while enabling substantial reductions in energy consumption and latency.

Conclusion: Learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

Abstract: Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

</details>


### [257] [One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion](https://arxiv.org/abs/2601.14161)
*Yitong Dong,Qi Zhang,Minchao Jiang,Zhiqiang Wu,Qingnan Fan,Ying Feng,Huaqi Zhang,Hujun Bao,Guofeng Zhang*

Main category: cs.CV

TL;DR: A novel framework for high-fidelity novel view synthesis from sparse images using 3D Gaussian Splatting with Vision Transformer backbones, enhanced by a dual-domain detail perception module and feature-guided diffusion network.


<details>
  <summary>Details</summary>
Motivation: Address limitations in recent feed-forward 3D Gaussian Splatting methods using Vision Transformer backbones, which are constrained by low-resolution inputs due to computational costs, and overcome 3D-agnostic generative enhancement methods that cause inconsistent structures across views, especially in unseen regions.

Method: Design a Dual-Domain Detail Perception Module to handle high-resolution images without ViT backbone limitations and endow Gaussians with additional features for high-frequency details. Develop a feature-guided diffusion network to preserve high-frequency details during restoration. Introduce unified training strategy for joint optimization of ViT-based geometric backbone and diffusion-based refinement module.

Result: Experiments demonstrate superior generation quality across multiple datasets.

Conclusion: The proposed framework successfully addresses key limitations in sparse image novel view synthesis by combining ViT-based geometric priors with diffusion-based refinement, enabling high-fidelity results with consistent structures across views.

Abstract: We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.

</details>


### [258] [ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction](https://arxiv.org/abs/2601.14165)
*Zhenghong Li,Wensheng Cheng,Congwu Du,Yingtian Pan,Zhaozheng Yin,Haibin Ling*

Main category: cs.CV

TL;DR: ASBA network reconstructs ODT images from highly sparse A-scans using flow-aware state space modeling and attention mechanisms with weighted loss for blood flow signals.


<details>
  <summary>Details</summary>
Motivation: Current ODT requires dense sampling which prolongs scanning time, increases storage, and limits capture of rapid blood flow dynamics. Sparse sampling approaches have been limited by conservative rates and uniform modeling of flow/background signals.

Method: ASBA network with: 1) A-line ROI state space model to extract sparse flow features along depth axis, 2) B-line phase attention to capture long-range flow signals along lateral axis using phase differences, and 3) flow-aware weighted loss function prioritizing accurate flow signal reconstruction.

Result: Extensive experiments on real animal data demonstrate clear outperformance over existing state-of-the-art reconstruction methods for sparse ODT imaging.

Conclusion: ASBA enables high-fidelity ODT image reconstruction from highly sparse sampling, addressing limitations of dense sampling while maintaining flow signal accuracy through specialized flow-aware architecture.

Abstract: Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.

</details>


### [259] [Progressive self-supervised blind-spot denoising method for LDCT denoising](https://arxiv.org/abs/2601.14180)
*Yichao Liu,Yueyang Teng,Junwen Guo*

Main category: cs.CV

TL;DR: Self-supervised LDCT denoising method using step-wise blind-spot mechanism and Gaussian noise regularization achieves performance comparable to supervised methods.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning for LDCT denoising reduces dependence on paired NDCT data, which is difficult to acquire clinically.

Method: Step-wise blind-spot denoising mechanism enforcing conditional independence progressively, plus Gaussian noise regularization to prevent overfitting.

Result: Outperforms existing self-supervised approaches and achieves comparable or better performance than several supervised denoising methods on Mayo LDCT dataset.

Conclusion: The proposed self-supervised method effectively denoises LDCT images without requiring paired NDCT data, demonstrating strong clinical applicability.

Abstract: Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.

</details>


### [260] [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/abs/2601.14188)
*Liang Shi,Wei Li,Kevin M Beussman,Lin Chen,Yun Fu*

Main category: cs.CV

TL;DR: IIR-VLM enhances VLMs for instance-level recognition by integrating pre-trained ILR expert models as auxiliary visual encoders, enabling one-shot in-context learning of new instances and instance-aware visual understanding.


<details>
  <summary>Details</summary>
Motivation: Current VLMs underperform on instance-level recognition (ILR) compared to domain-specific models, limiting practical applications where recognizing familiar people and objects is crucial. Existing solutions require costly instance-specific datasets and struggle with fine-grained discrimination.

Method: Propose IIR-VLM that integrates pre-trained ILR expert models as auxiliary visual encoders to provide specialized features. This enables VLMs to learn new instances in-context in a one-shot manner and leverage this knowledge for instance-aware visual understanding.

Result: Validated on existing instance personalization benchmarks and demonstrated superior ILR performance on a new challenging benchmark assessing ILR capabilities across varying difficulty and diverse categories (person, face, pet, general objects).

Conclusion: IIR-VLM effectively enhances VLMs for instance-level recognition, addressing the limitations of current VLM approaches while enabling practical applications requiring fine-grained instance discrimination.

Abstract: Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.

</details>


### [261] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: End-to-end pipeline creates interactive 3D models of vehicle undercarriages using a three-camera rig and specialized SfM to overcome wide-angle distortion and low-parallax challenges.


<details>
  <summary>Details</summary>
Motivation: Manual undercarriage inspection is labor-intensive and requires uncomfortable positions, while online buyers lack visual access to undercarriage conditions, creating safety and confidence issues.

Method: Uses three-camera rig to capture synchronized videos as vehicles drive over it, with rig-aware SfM pipeline integrating precise calibration, geometric priors, constrained matching with DISK features and LightGlue matcher, followed by Gaussian splatting for real-time rendering.

Result: Produces photorealistic interactive 3D models enabling rotation, zoom, and slicing to detect rust, leaks, or damage in seconds, achieving state-of-the-art quality through essential design choices.

Conclusion: The system improves workplace safety by eliminating manual crawling inspections and enhances buyer confidence through comprehensive visual access to undercarriage conditions.

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [262] [Soft Tail-dropping for Adaptive Visual Tokenization](https://arxiv.org/abs/2601.14246)
*Zeyuan Chen,Kai Zhang,Zhuowen Tu,Yuanjun Xiong*

Main category: cs.CV

TL;DR: STAT is a 1D discrete visual tokenizer that adaptively chooses token count based on image complexity, enabling efficient causal autoregressive visual generation.


<details>
  <summary>Details</summary>
Motivation: To create a visual tokenizer that adapts to image complexity for better compatibility with causal 1D autoregressive models, addressing limitations of prior vanilla AR visual generation approaches.

Method: STAT encodes images into discrete codes with per-token keep probabilities, regularized to be monotonically decreasing and aligned with image-level complexity measures, beyond standard autoencoder objectives.

Result: On ImageNet-1k, STAT-equipped causal AR models achieve competitive/superior visual generation quality compared to other probabilistic models, with favorable scaling behavior previously elusive in vanilla AR visual generation.

Conclusion: STAT enables effective length-adaptive 1D visual tokens that are naturally compatible with causal AR visual generative models, overcoming previous scaling limitations.

Abstract: We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.

</details>


### [263] [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/abs/2601.14250)
*Pengze Zhang,Yanze Wu,Mengtian Li,Xu Bai,Songtao Zhao,Fulong Ye,Chong Mou,Xinghui Li,Zhuowei Chen,Qian He,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniTransfer is a unified framework for spatio-temporal video transfer that outperforms existing methods across appearance and temporal transfer tasks without requiring task-specific priors.


<details>
  <summary>Details</summary>
Motivation: Existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit rich spatio-temporal information in videos, which limits flexibility and generalization in video generation.

Method: OmniTransfer incorporates three key designs: Task-aware Positional Bias for adaptive reference video usage, Reference-decoupled Causal Learning separating reference/target branches, and Task-adaptive Multimodal Alignment using multimodal semantic guidance.

Result: Extensive experiments show OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose.

Conclusion: OmniTransfer establishes a new paradigm for flexible, high-fidelity video generation by fully exploiting spatio-temporal information and unifying various video transfer tasks.

Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.

</details>


### [264] [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/abs/2601.14251)
*Said Taghadouini,Adrien Cavaillès,Baptiste Aubertin*

Main category: cs.CV

TL;DR: LightOnOCR-2-1B is a 1B-parameter multilingual vision-language model that converts document images to clean text without traditional OCR pipelines, achieving SOTA results while being 9x smaller and faster than previous models.


<details>
  <summary>Details</summary>
Motivation: To create an efficient end-to-end solution for document image-to-text conversion that avoids brittle OCR pipelines, supports multiple languages (especially French), handles scientific PDFs, and provides additional capabilities like image localization.

Method: Trained on large-scale distillation data covering scans, French documents, and scientific PDFs; uses resume strategy for bounding box localization pretraining; refines with RLVR using IoU-based rewards; employs checkpoint averaging and task-arithmetic merging for robustness.

Result: Achieves state-of-the-art results on OlmOCR-Bench while being 9x smaller and substantially faster than prior best models; successfully predicts normalized bounding boxes for embedded images; model released under Apache 2.0 with public dataset and evaluation benchmark.

Conclusion: LightOnOCR-2-1B demonstrates that efficient, end-to-end document understanding models can outperform traditional OCR pipelines while providing additional capabilities like image localization, with potential applications in multilingual document processing and scientific literature analysis.

Abstract: We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.

</details>


### [265] [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/abs/2601.14253)
*Hongyuan Chen,Xingyu Chen,Youjia Zhang,Zexiang Xu,Anpei Chen*

Main category: cs.CV

TL;DR: Motion 3-to-4 is a feed-forward framework that synthesizes high-quality 4D dynamic objects from single monocular videos, optionally using a 3D reference mesh.


<details>
  <summary>Details</summary>
Motivation: 4D synthesis remains challenging due to limited training data and inherent ambiguity in recovering geometry and motion from monocular viewpoints, despite advances in 2D, video, and 3D content generation.

Method: Decomposes 4D synthesis into static 3D shape generation and motion reconstruction. Uses a canonical reference mesh to learn compact motion latent representation and predict per-frame vertex trajectories. Employs scalable frame-wise transformer for robustness to varying sequence lengths.

Result: Superior fidelity and spatial consistency compared to prior work on both standard benchmarks and a new dataset with accurate ground-truth geometry.

Conclusion: Motion 3-to-4 effectively addresses 4D synthesis challenges through decomposition approach and achieves state-of-the-art performance in generating dynamic 4D objects from monocular videos.

Abstract: We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.

</details>


### [266] [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/abs/2601.14256)
*Matthew Gwilliam,Xiao Wang,Xuefeng Hu,Zhenheng Yang*

Main category: cs.CV

TL;DR: A unified model that learns image representations useful for both recognition and generation through hyper-network training for implicit neural representations combined with knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Current image representation learning models are specialized for either recognition (contrastive learning) or generation (reconstruction losses), but not both. The authors seek to unify these two directions into a single model that can handle both tasks effectively.

Method: Train a hyper-network for implicit neural representation (INR) that maps images to model weights for fast, accurate reconstruction. Integrate this with knowledge distillation to improve generalization and performance.

Result: The model learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. It competes with state-of-the-art results for image representation learning while enabling generative capabilities with high-quality tiny embeddings.

Conclusion: The paper presents a first-of-its-kind unified model that successfully bridges the gap between recognition and generation in image representation learning, achieving strong performance on both fronts through innovative hyper-network training and knowledge distillation techniques.

Abstract: Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [267] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: LLMs perform poorly on rare disease diagnosis using new MIMIC-RD benchmark that maps clinical text directly to Orphanet database, revealing significant gap between current capabilities and clinical needs.


<details>
  <summary>Details</summary>
Motivation: Rare diseases affect 1 in 10 Americans but diagnosis is challenging. Existing LLM evaluation methods are flawed: they use idealized case studies or ICD codes that undercount rare diseases since many lack mappings to comprehensive databases like Orphanet.

Method: Created MIMIC-RD benchmark by directly mapping clinical text entities to Orphanet database using LLM-based mining followed by validation from four medical annotators. Evaluated various models on dataset of 145 patients.

Result: Current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting substantial gap between existing capabilities and clinical needs.

Conclusion: The study reveals LLMs' limitations in rare disease diagnosis and outlines future steps needed to improve differential diagnosis capabilities for rare diseases.

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [268] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: Machines can't be conscious on sequential hardware if consciousness requires simultaneous processing; consciousness attribution requires architectural inspection, not just functional performance.


<details>
  <summary>Details</summary>
Motivation: To determine whether machines can be conscious by examining the temporal structure of computation, challenging the view that consciousness depends only on what is computed rather than when it's computed.

Method: Augments Stack Theory with algebraic laws relating temporal constraint satisfaction to conjunction, introduces precise temporal semantics over windowed trajectories, distinguishes StrongSync vs WeakSync postulates, formalizes concurrency-capacity, and reviews neurophysiological evidence.

Result: Proves existential temporal realization does not preserve conjunction, shows systems can realize experience ingredients across time without instantiating the conjunction itself, and demonstrates that under StrongSync, software consciousness on sequential substrates is impossible for contents requiring simultaneous contributors.

Conclusion: Consciousness attribution requires architectural inspection of hardware concurrency capacity, not just functional performance; consciousness on strictly sequential substrates is impossible when simultaneous processing is required.

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [269] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: Neuroscience-inspired dynamical metrics reveal that structured reasoning in LLMs exhibits distinct temporal organization compared to repetitive or noisy generation regimes.


<details>
  <summary>Details</summary>
Motivation: Current interpretability approaches focus on static representations, leaving the temporal dynamics of LLM text generation poorly understood. The paper aims to adapt neuroscience concepts of temporal integration and metastability to analyze transformer models' internal dynamics.

Method: Adapt neuroscience concepts of temporal integration and metastability to transformers, developing a composite dynamical metric from activation time-series during autoregressive generation. Evaluate in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection.

Result: Structured reasoning consistently shows elevated dynamical metrics compared to repetitive, noisy, and perturbed regimes. Differences are statistically significant (one-way ANOVA) with large effect sizes. Results are robust to layer selection, channel subsampling, and random seeds.

Conclusion: Neuroscience-inspired dynamical metrics can reliably characterize differences in computational organization across functional regimes in LLMs. The metric captures formal dynamical properties, not subjective experience.

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [270] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: Fine-tuning language models changes their decision evidence; explanation drift tracks attribution changes across epochs, with Reasoning Stabilization Point identifying when evidence stabilizes early in training.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning pretrained language models improves task performance but subtly alters the evidence models rely on for decisions, creating a need to monitor how decision evidence evolves during training.

Method: Propose training-time interpretability tracking token-level attributions across finetuning epochs, define explanation drift as epoch-to-epoch change in normalized token attributions on fixed probe set, and introduce Reasoning Stabilization Point as earliest epoch after which drift remains consistently low.

Result: Across multiple transformer classifiers and benchmark tasks, drift collapses into low stable regime early in training while validation accuracy changes marginally; in shortcut settings, attribution dynamics expose increasing reliance on shortcuts even with competitive accuracy.

Conclusion: Explanation drift provides simple, low-cost diagnostic for monitoring decision evidence evolution during fine-tuning and selecting checkpoints in stable-evidence regime.

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [271] [PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement](https://arxiv.org/abs/2601.11747)
*Huaxiaoyue Wang,Sunav Choudhary,Franck Dernoncourt,Yu Shen,Stefano Petrangeli*

Main category: cs.AI

TL;DR: PRISM uses design data to learn actionable design knowledge for style-aware graphic design improvement based on natural language instructions, outperforming general VLMs.


<details>
  <summary>Details</summary>
Motivation: Graphic design style exploration is time-consuming for non-experts, and existing VLMs have misaligned style knowledge that doesn't match designer principles (e.g., associating minimalism with abstract designs rather than specific shape/color choices).

Method: PRISM constructs a design knowledge base in three stages: (1) clustering high-variance designs to capture style diversity, (2) summarizing clusters into actionable design knowledge, and (3) retrieving relevant knowledge during inference for style-aware improvement.

Result: On Crello dataset, PRISM achieves best average rank of 1.49 (closer to 1 is better) in style alignment. User studies show designers consistently prefer PRISM over baselines.

Conclusion: Leveraging real-world design data to learn designer principles enables more effective stylistic improvement of designs based on natural language instructions compared to general VLMs.

Abstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.

</details>


### [272] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: RAIL is a risk-aware human-in-the-loop framework for autonomous vehicles that fuses runtime signals into risk scores, adapts control with shields when needed, and improves learning through risk-prioritized replay, outperforming baselines on safety and performance metrics.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles need to handle rare long-tailed scenarios and cyber-physical intrusions while maintaining safety and effectiveness during driving operations.

Method: RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, observation-shift consistency) into Intrusion Risk Score via weighted Noisy-OR. When risk exceeds threshold, actions are blended with cue-specific shields using learned authority; low risk uses nominal policy. Contextual bandit arbitrates among shields. Combines Soft Actor-Critic with risk-prioritized replay and dual rewards for learning.

Result: On MetaDrive: Test Return 360.65, Test Success Rate 0.85, Test Safety Violation 0.75, Disturbance Rate 0.0027, only 29.07 training safety violations. Under CAN injection attacks: Success Rate 0.68, Disengagement Rate under Attack 0.37, Attack Success Rate 0.34. Under LiDAR spoofing: Success Rate 0.80, DRA 0.03, ASR 0.11. In CARLA: TR 1609.70, TSR 0.41 with only 8000 steps.

Conclusion: RAIL effectively improves autonomous vehicle safety and performance under rare scenarios and cyber-physical attacks by integrating risk-aware control adaptations with focused learning, outperforming existing RL, safe RL, offline/imitation learning, and prior human-in-the-loop baselines.

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [273] [A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation](https://arxiv.org/abs/2601.11792)
*Yifei Sun,Yongan Li,A. K. Qin,Sicheng Hou,Tamas Pflanzner*

Main category: cs.AI

TL;DR: Proposes IMPG task and self-evolving multi-role framework with fine-grained difficulty guidance for generating innovative math problems while maintaining correctness.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs for math problem generation lack innovation and discrimination despite high correctness rates, creating need for innovative math problem generation (IMPG).

Method: Self-evolving multi-role collaborative framework with sampler, generator, evaluator, state machine, and memory; improved difficulty model with DAPS algorithm; multi-stage training (CPT, SFT, GRPO) on HSM3K-CN dataset; evaluation capability transfer via distillation.

Result: Method significantly improves innovation of generated problems while maintaining high correctness rate compared to baseline models.

Conclusion: Proposed framework effectively addresses IMPG task by combining multi-role collaboration, fine-grained difficulty guidance, and self-evolution mechanisms to generate innovative yet correct math problems.

Abstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.

</details>


### [274] [Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic](https://arxiv.org/abs/2601.11809)
*Zeyu Mu,Shangtong Zhang,B. Brian Park*

Main category: cs.AI

TL;DR: CNN-QMIX hybrid multi-agent model increases CAV cooperative platooning by 26.2% in mixed traffic during early deployment stages


<details>
  <summary>Details</summary>
Motivation: During initial CAV deployment, sparse distribution among human-driven vehicles reduces effective cooperative platooning opportunities, limiting energy efficiency and traffic flow benefits

Method: Hybrid multi-agent lane change decision model using QMIX framework with CNN processing (CNN-QMIX) to handle dynamic traffic scenarios, plus trajectory planner and model predictive controller for safe execution

Result: Model efficiently manages fluctuating traffic agent numbers, outperforms baseline rule-based models, and enhances cooperative platooning rates up to 26.2%

Conclusion: The proposed CNN-QMIX model effectively optimizes CAV cooperation and traffic dynamics during early deployment stages by increasing platooning participation

Abstract: Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.

</details>


### [275] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS is a governed orchestration framework for enterprise back-office workflows that uses typed plan synthesis and validated execution over LLM agents to ensure auditable, policy-aligned automation with decision-grade artifacts and full execution traces.


<details>
  <summary>Details</summary>
Motivation: Enterprise back-office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, but generic multi-agent setups often fail to deliver these capabilities.

Method: POLARIS treats automation as typed plan synthesis and validated execution over LLM agents. It uses a planner to propose structurally diverse, type-checked DAGs, a rubric-guided reasoning module to select compliant plans, and execution guarded by validator-gated checks, bounded repair loops, and compiled policy guardrails that block or route side effects before they occur.

Result: Applied to document-centric finance tasks, POLARIS produces decision-grade artifacts and full execution traces while reducing human intervention. It achieves micro F1 of 0.81 on SROIE dataset and 0.95-1.00 precision for anomaly routing with preserved audit trails on controlled synthetic suite.

Conclusion: POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI, establishing initial benchmarks for governed Agentic AI systems in enterprise automation.

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [276] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: AI co-scientist platform automates evidence synthesis using PICOS framework, combining NLP models for study classification and retrieval-augmented generation to reduce research waste in biomedical science.


<details>
  <summary>Details</summary>
Motivation: Address research waste in biomedical science caused by redundant studies, incomplete reporting, and limited scalability of traditional evidence synthesis workflows.

Method: AI platform integrates relational storage, vector-based semantic retrieval, and Neo4j knowledge graph. Uses transformer-based multi-task classifier (PubMedBERT) for study design classification, Bi-LSTM for PICOS compliance detection, retrieval-augmented generation with hybrid vector/graph retrieval, and BERTopic for thematic analysis.

Result: Transformer model achieved 95.7% accuracy for study design classification, Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval approaches for structured queries and cross-study integration. Topic modeling revealed thematic redundancy and identified research gaps.

Conclusion: PICOS-aware NLP improves scalability, transparency, and efficiency of evidence synthesis. The domain-agnostic architecture offers practical framework for reducing research waste across biomedical disciplines.

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [277] [Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic](https://arxiv.org/abs/2601.11840)
*Hongyu Lin,Samer Abdallah,Makar Valentinov,Paul Brennan,Elijah Kagan,Christoph M. Wintersteiger,Denis Ignatovich,Grant Passmore*

Main category: cs.AI

TL;DR: CodeLogician is a neurosymbolic agent combining LLMs with formal reasoning (ImandraX) for precise software logic analysis, achieving 41-47% accuracy improvements over LLM-only approaches on a new benchmark for mathematical reasoning about programs.


<details>
  <summary>Details</summary>
Motivation: LLMs lack precise mathematical reasoning about program behavior, while existing benchmarks are either too disconnected from real software (theorem proving) or lack semantic rigor (engineering tasks). There's a need for rigorous reasoning about software logic that bridges this gap.

Method: CodeLogician uses LLMs to construct explicit formal models of software systems, then leverages ImandraX (industrial automated reasoning engine) to perform precise analysis. Introduces code-logic-bench benchmark measuring reasoning about program state spaces, control flow, coverage constraints, and edge cases.

Result: Formal augmentation with CodeLogician yields substantial improvements over LLM-only reasoning, closing a 41-47 percentage point gap in reasoning accuracy on the code-logic-bench benchmark.

Conclusion: Neurosymbolic integration (combining LLMs with formal reasoning) is essential for scaling program analysis toward rigorous, autonomous software understanding, as demonstrated by CodeLogician's significant performance improvements.

Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.

</details>


### [278] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: Researchers developed an AI tool (ITA-GPT) to support inductive thematic analysis, finding it served as a procedural scaffold that enhanced workflow structure and transparency while human researchers maintained interpretive authority through active judgment and modifications.


<details>
  <summary>Details</summary>
Motivation: To examine how generative AI can be responsibly integrated into qualitative research, specifically addressing questions about analytic practice and interpretive authority when using AI tools for inductive thematic analysis.

Method: Developed an Inductive Thematic Analysis GPT (ITA-GPT) aligned with reflexive thematic analysis principles, then had three experienced qualitative researchers use it to analyze Ghanaian teacher education interview transcripts. Collected data through interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos.

Result: ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, human researchers maintained interpretive authority through active judgment demonstrated by modification, deletion, rejection, insertion, and commenting actions on AI outputs.

Conclusion: Inductive thematic analysis can be effectively enacted through responsible human-AI collaboration where AI tools provide procedural support and transparency, but human researchers retain interpretive authority and exercise judgment throughout the analytic process.

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [279] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: MyGram is a modality-aware graph transformer with global distribution for multi-modal entity alignment that uses modality diffusion learning and Gram Loss regularization to improve alignment accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal entity alignment methods often overlook structural contextual information within each modality and are vulnerable to interference from shallow features, limiting their effectiveness in identifying equivalent entities between multi-modal knowledge graphs.

Method: Proposes MyGram with two key components: 1) modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion, and 2) Gram Loss regularization that minimizes the volume of a 4-dimensional parallelotope formed by multi-modal features to achieve global distribution consistency across modalities.

Result: MyGram outperforms baseline models on five public datasets, achieving maximum improvements of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

Conclusion: MyGram effectively addresses the limitations of existing methods by capturing structural contextual information and ensuring global distribution consistency across modalities, leading to superior performance in multi-modal entity alignment tasks.

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [280] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA is an adaptive evaluation framework for LLM-based multi-agent systems that provides process-aware, auditable assessment with human oversight, addressing limitations of single-response scoring approaches.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation approaches for LLM-based multi-agent systems are inadequate - they lack stability, extensibility, and automation when deployed at scale in enterprise settings. Current methods often rely on single-response scoring or narrow benchmarks that don't capture the complexity of multi-agent coordination and decision-making.

Method: AEMA (Adaptive Evaluation Multi-Agent) is a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. It moves beyond single LLM-as-a-Judge approaches to provide comprehensive assessment.

Result: Compared to single LLM-as-a-Judge approaches, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Results on enterprise-style agent workflows using realistic business scenarios demonstrate its effectiveness.

Conclusion: AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems, addressing critical challenges in reliable coordination, transparent decision-making, and verifiable performance assessment.

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [281] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: LIBRA integrates LLMs with contextual bandits for sequential high-stakes decisions, offering warm-start benefits, limited LLM consultation, and robustness guarantees while maintaining statistical rigor.


<details>
  <summary>Details</summary>
Motivation: Need for trustworthy sequential decision-making in high-stakes settings like personalized medicine that combines domain knowledge from LLMs with statistical rigor of bandit algorithms while ensuring recourse (actionable feature modifications).

Method: Introduce recourse bandit problem requiring treatment actions + feature modifications; develop GLRB algorithm; propose LIBRA that strategically combines LLM domain knowledge with bandit learning using three key guarantees.

Result: LIBRA provides warm-start (reduced initial regret), LLM-effort (O(log²T) consultations), and robustness (never worse than pure bandit) guarantees; experiments show improved regret, treatment quality, and sample efficiency vs benchmarks.

Conclusion: Recourse-aware, LLM-assisted bandit algorithms like LIBRA enable trustworthy LLM-bandit collaboration for personalized high-stakes decision-making with theoretical guarantees and practical benefits.

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [282] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: TAAR (Trap-Aware Adaptive Restart) is a test-time control framework that detects and escapes "Thinking Traps" in long chain-of-thought reasoning by truncating trajectories before error-prone segments and adaptively restarting decoding.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought reasoning can lead to "Thinking Traps" where models make early wrong commitments and then elaborate self-consistent but incorrect reasoning, with 89% of failures on DAPO-MATH exhibiting such traps that later reflection cannot fix.

Method: TAAR trains a diagnostic policy to predict two signals from partial trajectories: 1) trap index (where to truncate) and 2) escape probability (intervention strength). At inference, it truncates before trap segments and adaptively restarts decoding, using stronger perturbations (higher-temperature resampling, structured reboot suffix) for severe cases.

Result: Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show TAAR improves reasoning performance without fine-tuning base model parameters.

Conclusion: TAAR effectively addresses Thinking Traps in long chain-of-thought reasoning through adaptive test-time control, enhancing reasoning capabilities by detecting and escaping from error-prone reasoning trajectories.

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [283] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: MARS is a single-cycle self-improvement framework for LLM agents that uses metacognitive reflection to optimize reasoning without costly multi-turn loops.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents are limited by static, human-designed prompts that lack adaptability, and existing self-improvement frameworks are inefficient due to multi-turn recursive loops with high computational costs.

Method: MARS (Metacognitive Agent Reflective Self-improvement) achieves efficient self-evolution within a single recurrence cycle by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success), then synthesizing these insights into optimized instructions.

Result: Extensive experiments on six benchmarks show that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

Conclusion: MARS provides an efficient framework for LLM agents to systematically refine their reasoning logic without continuous online feedback, enabling better adaptability through metacognitive self-improvement.

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [284] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL is a dynamic ICL framework that adaptively inserts relevant demonstrations during multi-step reasoning to address confusion points, improving mathematical reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Static ICL demonstrations fail to adapt to dynamic confusion points in multi-step reasoning tasks like mathematics, leading to cascading errors. There's a need for real-time adaptive demonstration integration.

Method: Two-stage framework: 1) Identify potential confusion points by analyzing semantics and entropy in reasoning process, 2) Retrieve relevant demonstrations matching confusion context and insert them into ongoing reasoning to guide subsequent steps.

Result: PICL outperforms baseline methods by mitigating mid-inference confusion, demonstrating the value of adaptive demonstration insertion in complex mathematical reasoning.

Conclusion: Dynamic demonstration integration that responds to real-time inference needs significantly enhances mathematical reasoning by addressing confusion points as they arise during multi-step deduction.

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [285] [Kernel-Based Learning of Safety Barriers](https://arxiv.org/abs/2601.12002)
*Oliver Schön,Zhengang Zhong,Sadegh Soudjani*

Main category: cs.AI

TL;DR: Data-driven safety verification for black-box AI systems using control barrier certificates learned from trajectories, with RKHS ambiguity sets for robustness and spectral methods for scalability.


<details>
  <summary>Details</summary>
Motivation: AI algorithms in safety-critical applications (autonomous driving, healthcare) raise concerns about meeting safety standards. Traditional formal verification tools struggle with black-box AI systems and lack scalability for real-world complexity.

Method: Learn control barrier certificates directly from system trajectories using conditional mean embeddings in RKHS. Construct RKHS ambiguity sets for robustness to out-of-distribution behavior. Use finite Fourier expansion to transform intractable semi-infinite optimization into linear programming via spectral barriers, enabling efficient computation with FFT.

Result: Provides theoretical results for applying the approach to general temporal logic specifications beyond safety. Demonstrates on case studies including black-box systems with neural network controllers, moving beyond restrictive assumptions on system dynamics and uncertainty.

Conclusion: Offers a scalable, distributionally robust framework for safety verification of black-box AI systems that addresses limitations of traditional formal verification methods while handling real-world complexity and uncertainty.

Abstract: The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.

</details>


### [286] [Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats](https://arxiv.org/abs/2601.12014)
*Elio Masciari,Vincenzo Moscato,Enea Vincenzo Napolitano,Gian Marco Orlando,Marco Perillo,Diego Russo*

Main category: cs.AI

TL;DR: LLMs need structured outputs, but environmental impact is overlooked. New framework evaluates both correctness and sustainability, showing TOON format is more compact/eco-friendly but less accurate without native support.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus only on structural correctness of LLM outputs, ignoring environmental impact. There's a need to evaluate structured output formats considering both correctness and sustainability for carbon-conscious deployments.

Method: Introduce sustainability-aware evaluation framework measuring token usage, generation time, and estimated carbon emissions. Propose Environment-Aware Generation Correctness Score (GCS_env) integrating structural correctness with carbon-aware efficiency. Benchmark TOON format against JSON, XML, YAML across multiple LLMs of different architectures and scales.

Result: TOON yields more compact outputs and lower emissions but lower structural correctness when models lack native support. Increased model capacity reduces this gap. Environment-aware scoring can shift format rankings based on deployment priorities.

Conclusion: Need sustainability-inclusive benchmarking. Compact representations like TOON offer practical advantages in large-scale, carbon-conscious LLM deployments, highlighting trade-offs between correctness and environmental efficiency.

Abstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.

</details>


### [287] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: Multi-agent LLM framework transforms customer reviews into actionable business advice through clustering, generation, evaluation, and ranking.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing customer reviews are limited to descriptive tasks like sentiment analysis, while LLM-generated suggestions often lack accuracy and depth. There's a need to transform review data into practical, actionable business advice.

Method: Multi-agent LLM framework with four components: clustering to select representative reviews, generation of advice, iterative evaluation, and feasibility-based ranking. Combines corpus distillation with feedback-driven refinement.

Result: Framework consistently outperforms single-model baselines on actionability, specificity, and non-redundancy across three service domains and multiple model families. Medium-sized models approach performance of large model frameworks.

Conclusion: The proposed multi-agent framework effectively transforms customer reviews into specific, actionable, and practical business advice, addressing limitations of existing descriptive methods and improving upon single-model LLM approaches.

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [288] [ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents](https://arxiv.org/abs/2601.12030)
*Yilun Yao,Shan Huang,Elsie Dai,Zhewen Tan,Zhenyu Duan,Shousheng Jia,Yanbing Jiang,Tong Yang*

Main category: cs.AI

TL;DR: ARC is a reflection-driven framework that treats context as a dynamic internal reasoning state, actively monitoring and revising it to prevent degradation in long-horizon information seeking tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models suffer from "context rot" where performance degrades as interaction histories grow, and existing approaches treat context as static artifacts that allow early errors to persist.

Method: ARC formulates context management as an active, reflection-driven process with reflection-driven monitoring and revision, allowing agents to actively reorganize working context when misalignment is detected.

Result: ARC consistently outperforms passive context compression methods, achieving up to 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

Conclusion: Treating context as a dynamic internal reasoning state through active reflection-driven management effectively addresses context degradation in long-horizon information seeking tasks.

Abstract: Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

</details>


### [289] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: Extends abstract argumentation frameworks with explicit subargument relations alongside attacks to better capture structural dependencies in argumentation.


<details>
  <summary>Details</summary>
Motivation: Dung's abstract argumentation framework abstracts from argument structure, limiting representation of structural dependencies like subargument relations. Existing extensions (e.g., bipolar frameworks) introduce support relations but don't capture the asymmetric, constitutive nature of subarguments or their interaction with attacks.

Method: Enrich abstract argumentation frameworks with an explicit subargument relation treated as a basic relation alongside attack relations. Analyze how subargument relations interact with attacks and examine their impact on fundamental semantic properties.

Result: The framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

Conclusion: Introducing explicit subargument relations alongside attacks in abstract argumentation frameworks enables better representation of structural dependencies while maintaining abstraction, addressing limitations of existing approaches.

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [290] [Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty](https://arxiv.org/abs/2601.12040)
*Murilo da Luz,Bruno Brandão,Luana Martins,Gustavo Oliveira,Bryan de Oliveira,Luckeciano Melo,Telma Soares*

Main category: cs.AI

TL;DR: PREGU uses entropy monitoring during LLM generation to detect uncertainty, triggers localized search when entropy exceeds threshold, and refines partial reasoning using Soft Reasoning method.


<details>
  <summary>Details</summary>
Motivation: LLMs still have limitations in multi-step inference scenarios like mathematical and logical reasoning, despite their progress. There's a need for better methods to handle uncertainty during reasoning tasks.

Method: PREGU monitors output distribution entropy during autoregressive generation, halts when entropy exceeds threshold, performs localized search in latent space to refine partial reasoning, and uses Soft Reasoning method to select most coherent answer.

Result: Experiments with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, StrategyQA) showed performance greater than or similar to Soft Reasoning.

Conclusion: Entropy can serve as an effective signal to trigger selective refinement during reasoning, enabling better handling of uncertainty in LLM-based reasoning tasks.

Abstract: The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.

</details>


### [291] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: UniMo is a novel LLM-based framework that integrates motion-language information and interpretable CoT reasoning to unify 3D human motion generation and understanding, using SFT and GRPO reinforcement learning to overcome semantic alignment and cumulative error challenges.


<details>
  <summary>Details</summary>
Motivation: Existing 3D human motion methods have limited interpretability and poor mutual enhancement between generation and understanding tasks. LLM-based unified frameworks struggle with semantic alignment, task coherence, and cumulative prediction errors in motion sequences.

Method: UniMo integrates motion-language information and interpretable CoT reasoning into LLMs via supervised fine-tuning (SFT), then uses reinforcement learning with Group Relative Policy Optimization (GRPO) as post-training to optimize token groups for structural correctness and semantic alignment.

Result: Extensive experiments show UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding tasks.

Conclusion: UniMo successfully addresses the limitations of current motion methods by combining motion-language integration, interpretable reasoning, and advanced optimization techniques, enabling effective mutual enhancement between generation and understanding tasks.

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [292] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe introduces a hierarchical four-level risk taxonomy with 129 atomic categories to systematically evaluate safety-critical failures of LLM-based driving assistants, showing current models inadequately refuse unsafe driving queries.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety taxonomies are too general-purpose and fail to capture domain-specific risks in real-world driving scenarios, where unsafe responses can lead to serious safety, ethical, and regulatory consequences.

Method: Developed DriveSafe - a hierarchical four-level risk taxonomy with 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles, reviewed by domain experts. Evaluated refusal behavior across six widely deployed LLMs.

Result: Evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, demonstrating limitations of general-purpose safety alignment in driving contexts.

Conclusion: Current LLMs lack appropriate safety mechanisms for driving contexts, highlighting the need for domain-specific safety evaluation frameworks like DriveSafe to address the unique risks of vehicle-based digital assistants.

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [293] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: TIDE is a novel approach for LTLf task planning that decomposes temporal problems into reach-avoid sub-problems, uses cost-driven heuristics to guide exploration, and features adaptive backtracking for completeness.


<details>
  <summary>Details</summary>
Motivation: Traditional LTLf task planning approaches lack informed heuristics to guide search for temporal goals, limiting their efficiency in solving complex temporally extended goals.

Method: TIDE decomposes temporal problems into manageable reach-avoid sub-problems, identifies promising automaton traces using cost-driven heuristics, and employs adaptive backtracking that recalculates costs and penalizes infeasible transitions.

Result: Experimental results show TIDE achieves promising performance and represents a valuable addition to the portfolio of planning methods for temporally extended goals.

Conclusion: TIDE addresses the heuristic limitation in traditional LTLf planning by providing guided exploration through trace-informed decomposition and adaptive backtracking, offering an efficient and complete solution for temporally extended goals.

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [294] [Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2601.12242)
*WooSeok Kim,Jeonghoon Lee,Sangho Kim,Taesun An,WonMin Lee,Dowon Kim,Kyungseop Shin*

Main category: cs.AI

TL;DR: Proposes a deep reinforcement learning framework with replay memory and on-policy algorithm for resource allocation in NOMA systems to address channel assignment problems.


<details>
  <summary>Details</summary>
Motivation: Growing need to optimize network resource utilization due to IoT expansion causing resource scarcity; NOMA addresses this via power multiplexing but has limitations including unclear channel assignment problems.

Method: Deep reinforcement learning framework incorporating replay memory with an on-policy algorithm for allocating network resources in NOMA systems to generalize learning.

Result: Extensive simulations evaluating effects of varying learning rate, batch size, model type, and number of features in the state (implied but not explicitly stated in abstract).

Conclusion: Proposed DRL framework addresses channel assignment problems in NOMA systems, providing a solution for resource allocation optimization through extensive simulation evaluation.

Abstract: In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.

</details>


### [295] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: CoLLaMo is a large language model-based molecular assistant with multi-level modality-collaborative projector that integrates 1D sequences, 2D graphs, and 3D conformations to reduce hallucination and improve robustness in molecular tasks.


<details>
  <summary>Details</summary>
Motivation: Existing large molecular language models (LMLMs) suffer from hallucination and limited robustness due to inadequate integration of diverse molecular modalities (1D sequences, 2D graphs, 3D conformations), which hinders their performance on molecular comprehension tasks.

Method: Proposes CoLLaMo with a multi-level molecular modality-collaborative projector featuring relation-aware modality-collaborative attention mechanism that facilitates fine-grained, relation-guided information exchange between atoms using 2D structural and 3D spatial relations. Also introduces new molecule-centric automatic measurement including hallucination assessment and GPT-based caption quality evaluation.

Result: CoLLaMo achieves best performance on multiple tasks including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction, demonstrating enhanced molecular modality generalization capabilities.

Conclusion: CoLLaMo effectively addresses hallucination and robustness limitations in LMLMs through better integration of multi-modal molecular information, and the proposed evaluation metrics provide better assessment of molecular comprehension than traditional token-based metrics like BLEU.

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [296] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX-Pro extends FutureX's live benchmark to specialized vertical domains (Finance, Retail, Public Health, Natural Disaster) to test agentic LLMs on high-value prediction tasks, revealing gaps between general reasoning and domain-specific precision needed for industrial deployment.


<details>
  <summary>Details</summary>
Motivation: While generalist agents perform well in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. There's a need to assess whether current SOTA agentic LLMs have the domain grounding necessary for industrial deployment in economically and socially pivotal verticals.

Method: FutureX-Pro extends FutureX's contamination-free, live-evaluation pipeline to four specialized vertical domains: Finance, Retail, Public Health, and Natural Disaster. It benchmarks agentic LLMs on entry-level yet foundational prediction tasks including forecasting market indicators, supply chain demands, tracking epidemic trends, and natural disasters.

Result: The findings reveal a performance gap between generalist reasoning and the precision required for high-value vertical applications, indicating current SOTA agentic LLMs may lack sufficient domain grounding for industrial deployment in these critical sectors.

Conclusion: Specialized frameworks like FutureX-Pro are needed to properly evaluate agentic LLMs for high-value vertical domains, as generalist capabilities don't necessarily translate to the precision required in capital-intensive and safety-critical sectors like finance, retail, public health, and natural disaster management.

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [297] [Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding](https://arxiv.org/abs/2601.12260)
*Yihao Ding,Qiang Sun,Puzhen Wu,Sirui Li,Siwen Luo,Wei Liu*

Main category: cs.AI

TL;DR: Docs2Synth is a synthetic-supervision framework that enables retrieval-guided inference for document understanding in private/low-resource domains without human annotations.


<details>
  <summary>Details</summary>
Motivation: Document understanding in regulated domains faces challenges: lack of manual annotations for model adaptation, difficulty keeping pretrained models updated with domain-specific facts, MLLMs suffering from hallucination and limited domain grounding, and VLPMs requiring costly annotations for new domains.

Method: Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, trains a lightweight visual retriever to extract domain-relevant evidence, and uses an iterative retrieval-generation loop during inference where the retriever collaborates with an MLLM.

Result: Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.

Conclusion: Docs2Synth provides an effective framework for document understanding in private and low-resource domains by combining synthetic supervision with retrieval-guided inference, reducing hallucination and improving response consistency while being deployable as an easy-to-use Python package.

Abstract: Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.

</details>


### [298] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: ToolPRMBench: A large-scale benchmark for evaluating process reward models (PRMs) in tool-using agent settings, featuring both offline single-step and online multi-step evaluation scenarios.


<details>
  <summary>Details</summary>
Motivation: There's a lack of systematic and reliable evaluation benchmarks for process reward models (PRMs) in tool-using settings, despite PRMs being crucial for reward-guided search methods that enhance tool-using agents through step-level monitoring.

Method: Built on existing tool-using benchmarks, ToolPRMBench converts agent trajectories into step-level test cases with interaction history, correct actions, plausible incorrect alternatives, and tool metadata. Uses offline sampling for single-step errors and online sampling for multi-step failures, with multi-LLM verification to ensure data quality.

Result: Extensive experiments reveal clear differences in PRM effectiveness across large language models, general PRMs, and tool-specialized PRMs, highlighting the potential of specialized PRMs for tool-using applications.

Conclusion: ToolPRMBench provides a comprehensive evaluation framework for PRMs in tool-using settings, enabling better assessment and development of process reward models to improve tool-using agent performance.

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [299] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: A self-training architecture using environmental viability instead of rewards prevents reward hacking and enables sustainable autonomous learning through negative-space learning and meta-strategies.


<details>
  <summary>Details</summary>
Motivation: Self-training systems often degenerate due to lack of external quality criteria, leading to reward hacking and semantic drift. The paper aims to create a stable self-training system under sparse feedback and bounded memory constraints.

Method: Introduces a self-training architecture where learning is mediated exclusively by environmental viability rather than rewards or objectives. Behaviors are executed under real resource constraints, and only those whose environmental effects persist and preserve future interaction possibilities are propagated. Selection operates through differential survival of behaviors as world-altering events.

Result: Analysis shows improvement arises through persistence of effective strategies under consolidation/pruning (negative-space learning). Models develop meta-learning strategies like deliberate experimental failure to elicit informative error messages without explicit instruction.

Conclusion: Environment-grounded selection enables sustainable open-ended self-improvement, offering a path toward robust autonomous systems without human-curated data or complex reward shaping.

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [300] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: This survey establishes the first comprehensive technical map for data generation in Document Intelligence, introducing a novel taxonomy based on data/label availability and organizing methods into four resource-centric paradigms.


<details>
  <summary>Details</summary>
Motivation: Document Intelligence requires large-scale, high-quality training data, but manual annotation is a bottleneck. Existing surveys are fragmented, focusing on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows.

Method: Redefines data generation as supervisory signal production and introduces a novel taxonomy based on "availability of data and labels." Organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Establishes a multi-level evaluation framework integrating intrinsic quality and extrinsic utility.

Result: Compiles performance gains across diverse Document Intelligence benchmarks. Dissects the methodological landscape to reveal critical challenges (fidelity gaps) and frontiers (co-evolutionary ecosystems). Systematizes the fragmented field of data generation for Document Intelligence.

Conclusion: By establishing a unified framework, this survey positions data generation as the central engine for next-generation Document Intelligence, filling the gap in comprehensive technical mapping of this rapidly evolving field.

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [301] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: MARO enables LLMs to learn reasoning through multi-agent social interactions by decomposing outcomes, balancing role weights, and evaluating behavior utility, improving both social reasoning and transfer to other tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM training lacks experience in real-world social scenarios involving interaction, negotiation, and competition, limiting their reasoning and judgment abilities in complex social environments.

Method: Multi-Agent Reward Optimization (MARO) with three key components: 1) decomposing final outcomes into specific behavior-level signals, 2) balancing training sample weights across different roles, and 3) directly evaluating utility of each behavior to handle environmental instability.

Result: MARO achieves significant improvements in social reasoning capabilities, and the learned abilities effectively transfer to other tasks like mathematical reasoning and instruction following.

Conclusion: Multi-agent social learning has tremendous potential for enhancing general reasoning capabilities of LLMs, demonstrating that social simulation learning can transfer to diverse reasoning tasks.

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [302] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: Two-LLM framework for generating actionable business recommendations from customer reviews using issue extraction and expert-mixed advice generation.


<details>
  <summary>Details</summary>
Motivation: Customer reviews contain valuable domain-specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains challenging.

Method: Modular two-LLM framework: Issue model extracts salient issues and assigns themes, Advice model generates operational fixes using mixture of LoRA experts strategy with token-level expert mixing for complementary expertise.

Result: Outperforms prompting-only and single-adapter baselines across airlines and restaurant domains, yielding higher actionability and specificity while maintaining favorable efficiency-quality trade-offs.

Conclusion: The proposed framework effectively converts unstructured review feedback into concrete, implementable recommendations through specialized modular design and efficient adaptation techniques.

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [303] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: PsychēChat is a psychological counseling LLM that explicitly models emotion shifts and safety risks through two modules (Emotion Management and Risk Control) and two inference paradigms (Agent Mode and LLM Mode).


<details>
  <summary>Details</summary>
Motivation: Existing LLMs for counseling don't explicitly model seekers' emotion shifts across sessions (a core psychological concept) and lack alignment between counselor responses and these shifts while proactively mitigating safety risks.

Method: Uses interactive role-playing to synthesize dialogues with two modules: Emotion Management Module (captures current emotions and shifts) and Risk Control Module (anticipates reactions and identifies risks). Two modeling paradigms: Agent Mode (multi-agent pipeline) and LLM Mode (unified chain-of-thought for end-to-end inference).

Result: Extensive experiments (interactive scoring, dialogue-level evaluation, human assessment) show PsychēChat outperforms existing methods in emotional insight and safety control.

Conclusion: PsychēChat successfully bridges the gap by explicitly integrating emotion shift tracking and safety risk analysis, offering a balanced approach to psychological counseling with both emotional understanding and risk mitigation.

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [304] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs perform poorly at tracking characters' knowledge states in stories, achieving near-random results on tasks that humans easily solve, highlighting a key gap in AI's social reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by cognitive anthropology research showing that human intelligence uniquely involves inferring others' knowledge states and intentions, while chimpanzees lack this capacity. The authors want to evaluate whether LLMs possess similar social reasoning abilities for knowledge state tracking and estimation.

Method: The researchers designed two evaluation tasks: (1) detecting when story characters demonstrate knowledge they shouldn't possess based on their experiences, and (2) predicting characters' next actions based on their own knowledge versus objective truths they don't know. They tested current state-of-the-art LLMs on these tasks.

Result: Results show that most current state-of-the-art LLMs achieve near-random performance on both knowledge state tracking tasks, performing substantially worse than humans. The models struggle with both detecting implausible knowledge demonstrations and predicting actions based on character-specific knowledge.

Conclusion: The authors conclude that future LLM research should place more emphasis on developing abilities for knowledge estimation and intention understanding, as current models lack the social reasoning capabilities that distinguish human intelligence.

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [305] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: LLMs show promise for generating faithful proofs from OWL ontologies but struggle with complex cases, logical complexity, and imperfect input data.


<details>
  <summary>Details</summary>
Motivation: While LLMs' reasoning abilities have been studied, their capacity to generate faithful, human-readable proofs (explanations of why conclusions follow) remains largely unexplored, particularly in the context of OWL ontologies used for complex knowledge representation.

Method: Developed an automated dataset construction and evaluation framework for proof generation from OWL ontologies, evaluating three sequential tasks: Extraction, Simplification, and Explanation, plus an additional task assessing Logic Completeness of premises.

Result: (1) Some models achieve strong overall results but struggle with complex cases; (2) Logical complexity is the dominant factor affecting performance (more than representation format); (3) Noise and incompleteness in input data substantially diminish LLM performance.

Conclusion: LLMs show promise for generating explanations with rigorous logic, but there's a significant gap in supporting resilient reasoning under complex or imperfect conditions, highlighting both potential and limitations for proof generation tasks.

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [306] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: LLMs struggle with multi-hop reasoning due to position bias, not distance between facts. Multi-Focus Attention Instruction (MFAI) probe reveals "Weakest Link Law" where performance collapses to least visible evidence level. Thinking models with System-2 reasoning overcome these limitations.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have massive context windows but still struggle with multi-hop reasoning due to position bias. It's unclear whether failures come from inability to locate evidence (recognition failure) or integrate it (synthesis failure).

Method: Introduce Multi-Focus Attention Instruction (MFAI), a semantic probe that explicitly steers attention toward selected positions to disentangle recognition vs. synthesis failures. Test across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA).

Result: Established "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Failure governed by absolute position rather than linear distance between facts (variance <3%). Matched MFAI improves accuracy by up to 11.5% in low-visibility positions. Thinking models with System-2 reasoning effectively locate and integrate information, matching gold-only baselines even in noisy, long-context settings.

Conclusion: Position bias, not distance between facts, is the primary bottleneck in LLM multi-hop reasoning. MFAI successfully disentangles recognition vs. synthesis failures, and System-2 reasoning models can overcome these limitations by effectively locating and integrating required information.

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [307] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: Agentic reasoning transforms LLMs into autonomous agents that plan, act, and learn through interaction, organized across foundational, self-evolving, and collective dimensions for real-world applications.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong reasoning in closed settings but struggle in open-ended, dynamic environments. Agentic reasoning addresses this by reframing LLMs as autonomous agents that can interact with and adapt to real-world complexities.

Method: Organizes agentic reasoning along three dimensions: foundational (single-agent planning, tool use, search), self-evolving (feedback, memory, adaptation), and collective multi-agent (coordination, knowledge sharing). Distinguishes in-context reasoning (test-time orchestration) from post-training reasoning (RL/SFT optimization).

Result: Synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, reviewing frameworks across applications including science, robotics, healthcare, autonomous research, and mathematics.

Conclusion: Agentic reasoning represents a paradigm shift for LLMs, enabling autonomous interaction in dynamic environments. Future directions include personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [308] [MemeLens: Multilingual Multitask VLMs for Memes](https://arxiv.org/abs/2601.12539)
*Ali Ezzat Shahroor,Mohamed Bayan Kmainasi,Abul Hasnat,Dimitar Dimitrov,Giovanni Da San Martino,Preslav Nakov,Firoj Alam*

Main category: cs.AI

TL;DR: MemeLens: A unified multilingual multitask VLM for meme understanding that consolidates 38 datasets into 20 tasks across harm, targets, intent, and affect categories.


<details>
  <summary>Details</summary>
Motivation: Existing meme research is fragmented across different tasks (hate, misogyny, propaganda, sentiment, humor) and languages, limiting cross-domain generalization. Memes are complex multimodal artifacts where meaning emerges from interactions between text, imagery, and cultural context.

Method: Proposed MemeLens, an explanation-enhanced Vision Language Model that consolidates 38 public meme datasets, filters and maps dataset-specific labels into a shared taxonomy of 20 tasks spanning harm, targets, figurative/pragmatic intent, and affect categories.

Result: Comprehensive empirical analysis shows robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting.

Conclusion: A unified multilingual multitask approach is needed for comprehensive meme understanding, and the authors will release experimental resources and datasets publicly to advance the field.

Abstract: Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.

</details>


### [309] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research is a multi-agent AI system for interactive scientific discovery with minute-scale turnaround, outperforming existing methods on computational biology benchmarks by 14-26 percentage points.


<details>
  <summary>Details</summary>
Motivation: Current AI systems for scientific discovery are mostly proprietary, operate in slow batch-processing modes (hours per cycle), and lack real-time researcher interaction capabilities, limiting their practical utility in scientific workflows.

Method: A multi-agent architecture with specialized agents for planning, data analysis, literature search, and novelty detection, unified through persistent world state. Offers two operational modes: semi-autonomous with human checkpoints and fully autonomous for extended investigations.

Result: State-of-the-art performance on BixBench computational biology benchmark: 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points.

Conclusion: Deep Research enables interactive scientific investigation with rapid turnaround, though practical deployment faces challenges including open access literature limitations and automated novelty assessment difficulties.

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [310] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: Clinical AI should shift from prediction engines to sequential control under uncertainty, using robust ordinal decision rules (fast-and-frugal heuristics) rather than brittle expected-utility optimization.


<details>
  <summary>Details</summary>
Motivation: Current clinical AI systems focus on prediction/labeling, but real clinical reasoning is sequential control under uncertainty with irreversible actions, constraints, and patient values. There's a mismatch between AI's cardinal optimization approach and clinicians' ordinal, non-compensatory decision-making.

Method: Propose using robust ordinal decision rules (fast-and-frugal trees, lexicographic heuristics) that stop early after checking fixed cue sequences. Argue for ordinal-by-default stance due to weak measurement axioms and layered noise in clinical data. Suggest AI should use rich models for beliefs but choose actions through robust dominance/filtering rules (ε-dominance, maximin).

Result: Ordinal heuristics are not just bounded rationality shortcuts but can be epistemically preferred in medicine due to: 1) weak measurement axioms making only orderings invariant, 2) layered noise creating uncertainty floors that make plug-in optimization brittle, while robust rules stabilize decisions.

Conclusion: Propose clinician-aligned AI blueprint: rich models for beliefs/trajectories + robust ordinal rules for actions, treat heuristics as low-dimensional special cases, deploy AI as 'selective complexity' for tie-breaking when decisions are fragile and information has positive expected impact.

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [311] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: The paper proposes a unified taxonomy for Agentic AI systems, categorizing them into Perception, Brain, Planning, Action, Tool Use, and Collaboration components, while analyzing the transition from passive LLMs to autonomous agents and discussing evaluation practices and open challenges.


<details>
  <summary>Details</summary>
Motivation: The shift from passive text-generating AI models to autonomous Agentic AI systems has created a complex landscape with diverse architectures (from single-loop agents to hierarchical multi-agent systems), making it difficult to navigate and understand the emerging designs. There's a need for a unified framework to categorize and analyze these systems as they automate complex workflows in domains like software engineering, scientific discovery, and web navigation.

Method: The paper investigates various agent architectures and proposes a unified taxonomy that breaks down agents into six key components: Perception, Brain, Planning, Action, Tool Use, and Collaboration. This framework is used to analyze the transition from linear reasoning procedures to native inference time reasoning models, and from fixed API calls to open standards like Model Context Protocol (MCP) and Native Computer Use. The authors also categorize agent operating environments and review current evaluation practices.

Result: The paper provides a comprehensive taxonomy for understanding Agentic AI systems, mapping the evolution from passive LLMs to cognitive controllers that combine memory, tool use, and environmental feedback. It identifies key architectural components, environmental categories (digital operating systems, embodied robotics, specialized domains), and current evaluation approaches while highlighting the shift toward more sophisticated reasoning and standardized interaction protocols.

Conclusion: The proposed taxonomy offers a valuable framework for navigating the complex landscape of Agentic AI. The paper identifies critical open challenges including hallucination in action, infinite loops, and prompt injection, and outlines future research directions toward developing more robust and reliable autonomous systems that can effectively perceive, reason, plan, and act in diverse environments.

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [312] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: LLM-based text-to-CAD framework generates STEP files from natural language with improved geometric fidelity over existing methods.


<details>
  <summary>Details</summary>
Motivation: CAD model creation is labor-intensive and expertise-heavy, limiting accessibility for non-experts. Existing text-to-CAD methods use kernel-dependent formats that lack universality for manufacturing, while STEP files are widely adopted but challenging for auto-regressive LLMs due to their graph-structured nature.

Method: Curated ~40K STEP-caption pairs with novel preprocessing: DFS-based reserialization linearizes cross-references while preserving locality, and CoT-style structural annotations guide global coherence. Integrated retrieval-augmented generation for supervised fine-tuning and refined generation with reinforcement learning using Chamfer Distance-based geometric reward.

Result: STEP-LLM consistently outperforms Text2CAD baseline in geometric fidelity. RAG module enhances completeness and renderability, DFS-based reserialization improves overall accuracy, and RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm higher fidelity shape generation.

Conclusion: Demonstrates feasibility of LLM-driven STEP model generation from natural language, showing potential to democratize CAD design for manufacturing by enabling non-experts to create manufacturable artifacts.

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [313] [MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents](https://arxiv.org/abs/2601.12661)
*Chuhan Qiao,Jianghua Huang,Daxing Zhao,Ziding Liu,Yanjun Shen,Bing Cheng,Wei Lin,Kai Wu*

Main category: cs.AI

TL;DR: MedConsultBench is a comprehensive framework for evaluating medical consultation agents across the complete clinical workflow, using fine-grained metrics to assess information acquisition efficiency and medication safety beyond just diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of medical consultation agents are too outcome-oriented and fragmented, overlooking end-to-end process integrity, clinical safety, and the structured inquiry logic needed for real-world practice. Existing benchmarks fail to capture the nuanced requirements of professional consultations.

Method: Proposes MedConsultBench framework covering the entire clinical workflow from history taking to follow-up Q&A. Introduces Atomic Information Units (AIUs) to track clinical information acquisition at sub-turn level with 22 fine-grained metrics. Addresses underspecification/ambiguity in online consultations and evaluates uncertainty-aware inquiry, medication compatibility, and constraint-respecting plan revisions.

Result: Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. Shows a critical gap between theoretical medical knowledge and clinical practice ability.

Conclusion: MedConsultBench establishes a rigorous foundation for aligning medical AI with real-world clinical care requirements, highlighting the need to move beyond diagnostic accuracy to assess complete consultation processes and clinical safety.

Abstract: Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.

</details>


### [314] [Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration](https://arxiv.org/abs/2601.12667)
*Yi Di,Zhibin Zhao,Fujin Wang,Xue Liu,Jiafeng Tang,Jiaxin Ren,Zhi Zhai,Xuefeng Chen*

Main category: cs.AI

TL;DR: SpaceHMchat: A human-AI collaboration framework for spacecraft power system health management in satellite mega-constellation era, featuring all-in-loop capabilities from condition recognition to maintenance decisions.


<details>
  <summary>Details</summary>
Motivation: With exponential growth in spacecraft and satellite mega-constellations, spacecraft power systems (SPS) need better health management due to their critical power supply role and high failure rates. Current approaches don't scale from dozens to thousands of SPS.

Method: Proposes AUC (aligning underlying capabilities) principle and develops SpaceHMchat - an open-source human-AI collaboration framework for all-in-loop health management. Includes hardware-realistic fault injection platform with simulation model, and creates first AIL HM dataset with 4 sub-datasets covering 17 fault types.

Result: SpaceHMchat achieves excellent performance across 23 metrics: 100% conclusion accuracy in condition recognition, >99% anomaly detection success, >90% fault localization precision, <3 minute knowledge base search time. Framework and dataset are open-sourced.

Conclusion: SpaceHMchat successfully addresses SPS health management challenges in satellite mega-constellation era through human-AI collaboration, demonstrating high performance across the entire health management loop while providing valuable open-source resources for the community.

Abstract: It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.

</details>


### [315] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: Proposes MMSI framework using sentencing logic in Transformers to clarify defendant roles in multidefendant cases, improving AI judicial assistance with legal interpretability.


<details>
  <summary>Details</summary>
Motivation: Judicial phrasing often obscures defendant roles in multidefendant cases, hindering AI analysis and fairness in responsibility assignment.

Method: Incorporates sentencing logic into Transformer encoder with oriented masking mechanism for role clarification and comparative data construction for culpability sensitivity. Uses masked multistage inference (MMSI) with guilt label broadcasting to regression model.

Result: MMSI framework achieves significant accuracy improvements on IMLJP dataset for intentional injury cases, outperforming baselines in role-based culpability differentiation.

Conclusion: Provides robust solution for enhancing intelligent judicial systems with legal interpretability, with publicly available code.

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [316] [Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts](https://arxiv.org/abs/2601.12711)
*Kevin Wang,Neel P. Bhatt,Cong Liu,Junbo Li,Runjin Chen,Yihan Xi,Timothy Barclay,Alvaro Velasquez,Ufuk Topcu,Zhangyang Wang*

Main category: cs.AI

TL;DR: Neurosymbolic LoRA combines numerical fine-tuning (LoRA) with symbolic editing (TextGrad) to adapt LLMs more effectively than either approach alone.


<details>
  <summary>Details</summary>
Motivation: Numerical fine-tuning is good for factual knowledge injection but lacks flexibility, while symbolic updates offer style/alignment control without retraining. The paper aims to combine these complementary strategies for better adaptability.

Method: A neurosymbolic LoRA framework with unified monitoring signal and reward-based classifier to decide when to use LoRA (for factual reconstruction) vs TextGrad (for token-level edits). Memory-efficient by offloading symbolic transformations to external LLM only when needed.

Result: Extensive experiments across multiple LLM backbones show neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance.

Conclusion: Interleaving numerical and symbolic updates unlocks new versatility in language model fine-tuning, with refined prompts from symbolic editing serving as reusable training data, especially valuable in data-scarce domains.

Abstract: Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.

</details>


### [317] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: SCFT and RLERR methods improve Large Reasoning Models' reflection quality by training them to generate effective self-critiques rather than superficial ones, boosting reasoning accuracy on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models often produce superficial reflections that don't improve answers but still incur computational costs. The paper addresses this problem of ineffective self-reflection in LRMs.

Method: Two-stage approach: 1) Self-Critique Fine-Tuning (SCFT) - trains models to critique their own outputs, filters high-quality critiques via rejection sampling, and fine-tunes with critique-based objectives. 2) Reinforcement Learning with Effective Reflection Rewards (RLERR) - uses SCFT-initialized reflections to create reward signals for reinforcement learning to internalize self-correction.

Result: Experiments on AIME2024 and AIME2025 benchmarks show significant improvements in both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines.

Conclusion: The proposed SCFT and RLERR methods effectively address superficial reflection in LRMs, enhancing their reflective reasoning capabilities and computational efficiency through targeted training approaches.

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [318] [Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks](https://arxiv.org/abs/2601.12744)
*Tasnim Ahmed,Yifan Zhu,Salimur Choudhury*

Main category: cs.AI

TL;DR: VLMs struggle to convert network diagrams into optimization code, with visual inputs reducing success rates by 12-21 percentage points compared to text-only inputs.


<details>
  <summary>Details</summary>
Motivation: Network practitioners naturally use diagrams to reason about network structure, but current IBN systems require text-based intent expression. The paper explores whether VLMs can process annotated network sketches into correct optimization code for traffic engineering and resource allocation problems.

Method: Created IntentOpt benchmark with 85 optimization problems across 17 categories. Evaluated four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) using three prompting strategies on multimodal vs text-only inputs. Also conducted a case study deploying generated code to network testbed using Model Context Protocol.

Result: Visual parameter extraction reduces execution success by 12-21 percentage points (GPT-5-Mini drops from 93% to 72%). Program-of-thought prompting decreases performance by up to 13 pp. Open-source models lag significantly (Llama-3.2-11B-Vision reaches 18% vs GPT-5-Mini's 75%).

Conclusion: Current VLMs have significant limitations in generating optimization code from network diagrams, establishing baseline capabilities and highlighting the gap between visual understanding and code generation for IBN systems.

Abstract: Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.

</details>


### [319] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: VIRO introduces verification-integrated reasoning operators to prevent cascading errors in neuro-symbolic REC by embedding lightweight verifiers within reasoning steps, achieving state-of-the-art performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Current neuro-symbolic REC approaches assume accurate intermediate reasoning steps, leading to cascading errors where false detections and invalid relations propagate through reasoning chains, causing high-confidence false positives even when no target exists.

Method: VIRO embeds lightweight operator-level verifiers within reasoning steps where each operator executes and validates its output (object existence, spatial relationships), enabling robust handling of no-target cases when verification conditions fail.

Result: Achieves state-of-the-art 61.1% balanced accuracy across target-present and no-target settings, generalizes to real-world egocentric data, shows superior computational efficiency (high throughput), high reliability (<0.3% program failure rate), and scalability through decoupled program generation.

Conclusion: VIRO addresses critical limitations of neuro-symbolic REC by integrating verification within reasoning operators, preventing cascading errors while maintaining interpretability, generalization, and efficiency.

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [320] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: SL-CBM improves Concept Bottleneck Models by enforcing locality faithfulness through spatially coherent saliency maps at concept and class levels, using 1x1 convolutions and cross-attention for better alignment.


<details>
  <summary>Details</summary>
Motivation: Existing Concept Bottleneck Models (CBMs) lack locality faithfulness - they fail to spatially align concepts with meaningful image regions, limiting interpretability and reliability in high-stakes domains where transparent AI is crucial.

Method: Proposes SL-CBM (CBM with Semantic Locality) that integrates a 1x1 convolutional layer with cross-attention mechanism to generate spatially coherent saliency maps at both concept and class levels, enhancing alignment between concepts, image regions, and predictions.

Result: Extensive experiments show SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Ablation studies highlight importance of contrastive and entropy-based regularization.

Conclusion: SL-CBM bridges concept-based reasoning with spatial explainability, setting a new standard for interpretable and trustworthy concept-based models by producing faithful saliency maps inherently tied to the model's internal reasoning.

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [321] [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](https://arxiv.org/abs/2601.12822)
*Wenqi Zhang,Yulin Shen,Changyue Jiang,Jiarun Dai,Geng Hong,Xudong Pan*

Main category: cs.AI

TL;DR: MirrorGuard is a plug-and-play defense framework that uses neural-symbolic simulation training to secure Computer Use Agents against malicious instructions and visual prompt injections, reducing unsafe actions while maintaining agent utility.


<details>
  <summary>Details</summary>
Motivation: Large foundation models in Computer Use Agents enable autonomous GUI interaction but introduce serious security risks from malicious instructions and visual prompt injections that can trigger unsafe reasoning and harmful system actions. Existing detection-based defenses abort tasks prematurely, reducing agent utility.

Method: MirrorGuard uses a novel neural-symbolic simulation pipeline that generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment. This captures unsafe reasoning patterns and system hazards without executing real operations. The framework learns to intercept and rectify insecure reasoning chains of CUAs before they produce unsafe actions.

Result: Extensive evaluations show MirrorGuard significantly mitigates security risks. On ByteDance UI-TARS, it reduces unsafe rate from 66.5% to 13.0% while maintaining marginal false refusal rate (FRR). State-of-the-art GuardAgent only reduces to 53.9% with 15.4% higher FRR.

Conclusion: Simulation-derived defenses can provide robust, real-world protection while maintaining fundamental agent utility. MirrorGuard proves effective in securing CUAs against security threats without compromising task completion.

Abstract: Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.

</details>


### [322] [SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning](https://arxiv.org/abs/2601.12842)
*Qitong Fang,Haotian Li,Xu Wang*

Main category: cs.AI

TL;DR: SCULPT introduces constraint-guided MCTS for LLM agents, using domain-aware scoring to prune implausible reasoning paths and improve search efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent workflows rely on stochastic exploration that often traverses implausible branches due to weak domain priors, leading to near-random walks over operators, units, and formats.

Method: SCULPT integrates domain-aware scoring into MCTS (selection, expansion, simulation, backpropagation) using symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, diversity) and structural pattern guidance to prune actions.

Result: SCULPT yields stable improvements on multiple datasets under matched LLM configurations, with additional results showing executor transferability and performance on frontier reasoning models like GPT-5.2.

Conclusion: Domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability in LLM agent workflows.

Abstract: Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.

</details>


### [323] [Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data](https://arxiv.org/abs/2601.12856)
*Liping Huang,Gaoxi Xiao,Stefan Ma,Hechang Chen,Shisong Tang,Flora Salim*

Main category: cs.AI

TL;DR: A novel framework uses gradient descent to learn latent transmission links from dengue case data, revealing how human mobility drives citywide spread and enabling proactive hotspot forecasting with interpretable results.


<details>
  <summary>Details</summary>
Motivation: Dengue remains a persistent public health challenge in urban tropical areas like Singapore. Current approaches often treat cases as isolated reports rather than understanding the underlying transmission dynamics, making it difficult to deploy interventions proactively. There's a need for affordable, scalable tools that can anticipate where transmission risks will emerge.

Method: The framework mines latent transmission links directly from publicly available dengue case data using gradient descent optimization. It models how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions, treating cases as interconnected rather than isolated. The learned network is validated by examining its stability across consecutive weeks and comparing it with commuting flow patterns.

Result: Case studies on Singapore (2013-2018 and 2020) show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79 for hotspot forecasting. The learned transmission links align closely with commuting flows, providing interpretable explanations for citywide spread patterns. The framework successfully transforms case data into both predictive and explanatory resources.

Conclusion: This work shifts from simply reporting dengue cases to mining and validating hidden spreading dynamics, advancing epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience. The framework demonstrates how open web-based case data can be leveraged for both prediction and understanding of disease transmission patterns.

Abstract: Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.

</details>


### [324] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: C-MT is an action language built on ASP and transition systems to model human mental state evolution, incorporating psychological theories like Appraisal Theory of Emotion, with novel causal rules for controlled reasoning about mental state dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the need for controlled agent behaviors and restrict unwanted mental side-effects of actions by formally representing how human mental states evolve in response to observable actions, drawing on established psychological theories.

Method: Built on answer set programming (ASP) and transition systems, formalizes mental states as multi-dimensional configurations, extends with novel 'forbids to cause' causal rule and specialized expressions for mental state dynamics, translates principles of mental change into transition constraints and invariance properties.

Result: Enables controlled reasoning about dynamic evolution of human mental states, supports comparison of different change dynamics by analyzing trajectories adhering to different psychological principles, applied to design models for emotion verification.

Conclusion: C-MT provides a formal framework for modeling mental state transitions using psychological principles, enabling rigorous evaluation of mental state dynamics and supporting controlled agent behavior design with applications in emotion verification.

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [325] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: Interpretability research is ill-posed due to non-actionable definitions; actionable definitions require symmetry-based formulations that unify interpretable models and inference.


<details>
  <summary>Details</summary>
Motivation: Current interpretability definitions lack formal principles for deriving concrete modeling and inference rules, making the field fundamentally ill-posed.

Method: Propose symmetry-based definitions of interpretability, hypothesizing that four symmetries can motivate core properties, characterize interpretable models, and derive unified interpretable inference as Bayesian inversion.

Result: A framework where symmetry-based definitions make interpretability actionable by providing formal principles for modeling rules and unifying interpretable inference methods.

Conclusion: Interpretability research needs symmetry-based actionable definitions to move beyond ill-posed formulations and establish principled foundations for interpretable AI.

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [326] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: MagicGUI-RMS is a multi-agent reward model system that automates evaluation and training data generation for GUI agents, enabling self-improvement through adaptive trajectory assessment and corrective feedback.


<details>
  <summary>Details</summary>
Motivation: Current GUI agent evaluation relies on manual annotation or static rule-based verification, which lacks scalability and adaptability in dynamic environments. There's a need for automated evaluation and scalable training data generation to enable continual agent improvement.

Method: MagicGUI-RMS combines a Domain-Specific Reward Model (DS-RM) for fine-grained action assessment with a General-Purpose Reward Model (GP-RM) for robust generalization across heterogeneous GUI tasks. It includes a structured data construction pipeline for automatic balanced dataset generation and an automated data-reflux mechanism for continuous improvement.

Result: Extensive experiments show MagicGUI-RMS achieves substantial gains in task accuracy and behavioral robustness, establishing it as an effective foundation for self-improving GUI agents.

Conclusion: MagicGUI-RMS provides a principled solution for building self-improving GUI agents through reward-based adaptation, addressing key challenges in automated evaluation and scalable training data generation.

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [327] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: General-purpose AI systems have higher risks than task-specific AI due to their non-deterministic high degree of freedom in outputs, requiring new RAI approaches based on C2V2 desiderata (Control, Consistency, Value, Veracity).


<details>
  <summary>Details</summary>
Motivation: Modern general-purpose AI systems (like large language models) are widely adopted but suffer from serious risks including hallucinations, toxicity, and stereotypes, making them untrustworthy compared to traditional task-specific AI systems.

Method: Review risks across eight RAI principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, sustainability), analyze the root cause as non-deterministically high Degree of Freedom in output, and derive C2V2 desiderata for responsible general-purpose AI.

Result: Identified that general-purpose AI has fundamentally different risk profiles than task-specific AI, requiring new RAI approaches. Proposed C2V2 framework and discussed how current techniques (AI alignment, RAG, reasoning enhancements) partially address these desiderata.

Conclusion: Responsible general-purpose AI can be achieved by formally modeling domain-specific RAI requirements along C2V2 dimensions and taking a system design approach to combine various techniques to meet the desiderata.

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [328] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: Extends injection defense framework with semantic caching and observability metrics to balance security, performance, and transparency in multi-agent LLM systems.


<details>
  <summary>Details</summary>
Motivation: Prompt injection remains a critical security threat in multi-agent LLM deployments where malicious instructions can propagate through intermediate outputs, requiring better evaluation frameworks that balance security effectiveness with system transparency and operational efficiency.

Method: Extends TIVS framework with semantic similarity-based caching and adds Observability Score Ratio (OSR) metric to create TIVS-O. Uses HOPE-inspired Nested Learning architecture with agentic pipeline and Continuum Memory Systems, tested on 301 synthetic injection prompts from 10 attack families with comprehensive security analysis using 5 KPIs.

Result: Achieves zero high-risk breaches with 41.6% reduction in LLM calls, plus latency, energy, and carbon emission reductions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency, showing non-monotonic effects in multi-agent pipelines.

Conclusion: Observability-aware evaluation combined with memory-augmented agents can simultaneously maximize security robustness, performance, cost savings, and environmental sustainability without modifying model weights, providing production-ready pathway for secure and green LLM deployments.

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [329] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: LLMs struggle with time awareness in real-time negotiations but perform well with turn-based limits, revealing a systematic temporal tracking failure rather than strategic reasoning issues.


<details>
  <summary>Details</summary>
Motivation: Real-world communication depends on continuous time constraints, but current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines.

Method: Used simulated negotiations between paired agents under strict deadlines with two conditions: control (only global time limit) and time-aware (remaining-time updates at each turn).

Result: Deal closure rates were substantially higher in time-aware condition (32% vs 4% for GPT-5.1), and offer acceptances were sixfold higher. However, same LLMs achieved near-perfect deal closure rates (≥95%) under turn-based limits.

Conclusion: LLMs have a systematic lack of time awareness that constrains deployment in time-sensitive applications, with failure in temporal tracking rather than strategic reasoning.

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [330] [RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements](https://arxiv.org/abs/2601.13233)
*Bolin Chen,Dex Doksoo Lee,Wei "Wayne'' Chen,Wei Chen*

Main category: cs.AI

TL;DR: RAG: Random-forest-based Generative approach for inverse design of metamaterials with functional responses, offering data efficiency, uncertainty quantification, and handling of complex design requirements.


<details>
  <summary>Details</summary>
Motivation: Inverse design of metamaterials with nonlinear, condition-dependent functional responses (like stress-strain curves) is challenging due to high-dimensionality, complex requirements, and non-unique solutions. Existing methods are data-hungry, handle requirements heuristically, and lack uncertainty quantification.

Method: RAG uses random forests for data-efficient prediction of high-dimensional functional responses. It estimates likelihood through ensemble methods for uncertainty quantification, addresses one-to-many mapping via single-shot design generation by sampling from conditional likelihood, and handles complex requirements systematically.

Result: Demonstrated on acoustic metamaterials with prescribed passbands/stopbands (500 samples) and mechanical metamaterials with targeted snap-through responses (1057 samples). Benchmarked against neural networks on public mechanical metamaterial dataset, showing superior data efficiency.

Conclusion: RAG provides a lightweight, trustworthy pathway for inverse design involving functional responses, expensive simulations, and complex requirements, with applications beyond metamaterials.

Abstract: Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.

</details>


### [331] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: CURE-MED improves multilingual medical reasoning in LLMs using curriculum-informed reinforcement learning, achieving up to 95% language consistency and 70% logical correctness across 13 languages.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on monolingual reasoning but remain unreliable for multilingual medical reasoning, hindering deployment in multilingual healthcare settings where diverse languages are spoken.

Method: Introduces CUREMED-BENCH dataset (13 languages) and CURE-MED framework with code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to improve logical correctness and language stability.

Result: Achieves 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters across 13 languages.

Conclusion: The approach enables reliable and equitable multilingual medical reasoning in LLMs, supporting deployment in diverse healthcare settings.

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [332] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: Multi-agent framework improves medical LLM safety using iterative alignment with ethical evaluation agents, achieving 89% reduction in ethical violations.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in healthcare but face barriers to clinical deployment due to ethical integrity and safety compliance concerns. There's a need for structured approaches to ensure medical AI safety aligns with regulatory standards.

Method: Multi-agent refinement framework combining two generative models (DeepSeek R1 and Med-PaLM) with two evaluation agents (LLaMA 3.1 and Phi-4). Uses AMA Principles of Medical Ethics and Safety Risk Assessment (SRA-5) protocol. Evaluated across 900 clinically diverse queries spanning nine ethical domains.

Result: DeepSeek R1 achieved faster convergence (mean 2.34 vs. 2.67 iterations), Med-PaLM showed superior privacy handling. The framework achieved 89% reduction in ethical violations and 92% risk downgrade rate across diverse clinical scenarios.

Conclusion: The study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety through structured multi-agent refinement, demonstrating significant improvements in ethical compliance and risk reduction.

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [333] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff is a novel peptide binder generator that designs binding sequences directly from protein embeddings without structure prediction, using latent-space diffusion to create diverse novel peptides beyond known binders.


<details>
  <summary>Details</summary>
Motivation: Existing peptide binder generation methods rely heavily on intermediate structure prediction, which adds complexity and limits sequence diversity. There's a need for simpler, more diverse approaches that can generate novel binders without structural constraints.

Method: Generates binder sequences directly in a continuous latent space from pretrained protein embeddings, using latent-space exploration and diffusion-based sampling to capture binding-relevant features rather than memorizing known sequences.

Result: Outperforms state-of-the-art approaches across benchmark tests and in the challenging TIGIT case study (large, flat protein-protein interaction interface lacking druggable pocket).

Conclusion: PepEDiff demonstrates potential as a general, structure-free framework for zero-shot peptide binder design, enabling generation of novel peptides in previously unseen regions of protein space.

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [334] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: Scale doesn't uniformly improve reasoning but restructures it through domain-specific phase transitions: legal reasoning crystallizes, science/math remain liquid, and code forms a lattice. Geometry predicts learnability, enabling reasoning endpoint prediction without intermediate steps.


<details>
  <summary>Details</summary>
Motivation: To understand how neural scaling laws affect reasoning capabilities across different domains, moving beyond the assumption that scale uniformly improves all reasoning abilities.

Method: Analyzed 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two model scales (8B, 70B parameters). Introduced Neural Reasoning Operators - learned mappings from initial to terminal hidden states.

Result: Legal reasoning undergoes crystallization (45% dimensionality collapse, 31% trajectory alignment, 10x manifold untangling). Science/math remain geometrically invariant. Code forms discrete lattice. Neural Reasoning Operators achieve 63.6% accuracy on legal tasks via probe decoding. Universal oscillatory signature identified across domains.

Conclusion: Reasoning cost is determined by manifold geometry rather than task difficulty, offering a blueprint for inference acceleration where topology permits. Scale triggers domain-specific phase transitions rather than uniform capability gains.

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [335] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: AgentForge is a lightweight, open-source Python framework for building LLM-driven autonomous agents with modular architecture, composable skills, unified LLM backend, and YAML-based configuration.


<details>
  <summary>Details</summary>
Motivation: Existing agent frameworks suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impede rapid prototyping and deployment of LLM-driven autonomous agents.

Method: Three key innovations: (1) composable skill abstraction with formal input-output contracts, (2) unified LLM backend interface for switching between cloud APIs and local inference, (3) declarative YAML-based configuration system. Formalizes skill composition as a DAG.

Result: Achieves competitive task completion rates while reducing development time by 62% vs LangChain and 78% vs direct API integration. Sub-100ms orchestration overhead suitable for real-time applications. Demonstrates integration of six built-in skills.

Conclusion: AgentForge addresses a critical gap by providing a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance, democratizing LLM agent development.

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [336] [Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models](https://arxiv.org/abs/2601.13443)
*Héctor Manuel Manzanilla-Granados,Zaira Navarrete-Cazales,Miriam Pescador-Rojas,Tonahtiu Ramírez-Romero*

Main category: cs.AI

TL;DR: Paper introduces Explicit Cognitive Allocation principle and Cognitive Universal Agent architecture to structure AI-assisted reasoning by separating epistemic functions, improving traceability and reproducibility over unstructured LLM use.


<details>
  <summary>Details</summary>
Motivation: Current LLM use collapses problem framing, knowledge exploration, retrieval, methodological awareness, and explanation into a single generative process, limiting traceability, weakening epistemic control, and undermining reproducibility in high-responsibility settings.

Method: Proposes Explicit Cognitive Allocation principle and Cognitive Universal Agent (CUA) architecture that organizes inference into distinct stages: exploration/framing, epistemic anchoring, instrumental/methodological mapping, and interpretive synthesis. Introduces Universal Cognitive Instruments (UCIs) to formalize heterogeneous means for making abstract inquiries investigable.

Result: CUA-orchestrated inference shows earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of instrumental landscape compared to baseline LLM inference, which shows greater variability and fails to surface instrumental structure.

Conclusion: Explicit Cognitive Allocation through CUA architecture improves AI-assisted reasoning by separating epistemic functions, enhancing traceability, epistemic control, and reproducibility, particularly valuable for high-responsibility applications.

Abstract: The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.

</details>


### [337] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: SpatialBench-UC is a reproducible benchmark for evaluating text-to-image models on spatial relations, using selective prediction with abstention and confidence reporting for interpretable risk-coverage tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Automating evaluation of text-to-image models on spatial instructions is challenging - object detectors may miss targets or return ambiguous detections, and simple geometric tests can be ambiguous in borderline cases.

Method: Created SpatialBench-UC with 200 prompts (50 object pairs × 4 relations) grouped into 100 counterfactual pairs, with versioned prompts, pinned configs, and per-sample checker outputs. Includes lightweight human audit to calibrate checker's abstention margin and confidence threshold.

Result: Evaluated Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. Grounding methods substantially improve both pass rate and coverage, but abstention remains dominant due mainly to missing detections.

Conclusion: Spatial evaluation should be treated as a selective prediction problem with abstention and confidence reporting, enabling reproducible and auditable comparisons across models through the benchmark package.

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [338] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: Context-based audio deepfake detector (CADD) improves detection by 5-38% using context/transcripts, outperforms baselines on multiple datasets, and is robust to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current audio deepfake detectors only analyze audio files without considering context or transcripts, while humans use context to assess veracity. This creates a gap in detection capabilities.

Method: Created Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes, generated synthetic audio dataset (SYN) of dead public figures, and proposed novel Context-based Audio Deepfake Detector (CADD) architecture that incorporates context and transcripts.

Result: Context/transcripts improve detector performance by 5%-37.58% F1-score, 3.77%-42.79% AUC, and 6.17%-47.83% EER. CADD is robust to 5 adversarial evasion strategies with only -0.71% average performance degradation.

Conclusion: Incorporating context and transcripts significantly improves audio deepfake detection efficacy and robustness, demonstrating the importance of contextual information for reliable detection systems.

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [339] [Graph Neural Networks are Heuristics](https://arxiv.org/abs/2601.13465)
*Yimeng Min,Carla P. Gomes*

Main category: cs.AI

TL;DR: A single GNN training trajectory can create an unsupervised heuristic for combinatorial optimization, specifically for TSP, without search or supervision.


<details>
  <summary>Details</summary>
Motivation: To show that graph neural networks can function as effective combinatorial optimization heuristics without requiring supervised training or explicit search procedures.

Method: Encode global structural constraints as inductive bias in a non-autoregressive GNN model that generates solutions via direct forward passes. Use dropout and snapshot ensembling at inference to create an implicit ensemble from a single model.

Result: The approach reduces optimality gaps through increased solution diversity, demonstrating that GNNs can internalize global combinatorial structure and function as strong learned heuristics without supervision or search.

Conclusion: Reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics, showing GNNs don't need supervision or search to be effective.

Abstract: We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.

</details>


### [340] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO is an automated prompt optimization framework that improves LLM reliability for emotion diagnosis in clinical settings by addressing emotional comorbidity and inefficient cue exploration through multi-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for emotion analysis in mental healthcare but face reliability issues in high-stakes clinical settings due to prompt sensitivity, emotional comorbidity (multiple intertwined emotional states), and inefficient exploration of clinically relevant cues.

Method: APOLO formulates prompt optimization as a Partially Observable Markov Decision Process with multi-agent collaboration: Planner defines optimization trajectory, Teacher-Critic-Student agents iteratively refine prompts for reasoning stability, and Target agent decides continuation based on performance evaluation.

Result: APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating scalable and generalizable performance for trustworthy LLM applications in mental healthcare.

Conclusion: APOLO provides a systematic framework for prompt optimization that addresses key challenges in clinical emotion diagnosis, offering a scalable paradigm for improving LLM reliability in high-stakes mental healthcare applications.

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [341] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: AgenticRed is an automated red-teaming system that uses LLMs to iteratively design and refine attack systems without human intervention, achieving state-of-the-art attack success rates on various models.


<details>
  <summary>Details</summary>
Motivation: Existing automated red-teaming methods rely on human-specified workflows, which suffer from human biases and make exploring the broader design space expensive. There's a need for more automated approaches to AI safety evaluation.

Method: AgenticRed treats red-teaming as a system design problem rather than optimizing attacker policies within predefined structures. It leverages LLMs' in-context learning to iteratively design and refine red-teaming systems, using evolutionary selection inspired by methods like Meta Agent Search.

Result: AgenticRed consistently outperforms state-of-the-art approaches, achieving 96% attack success rate on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. It also shows strong transferability to proprietary models: 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement).

Conclusion: Automated system design is a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models, demonstrating that treating red-teaming as a system design problem yields superior results compared to optimizing within predefined structures.

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [342] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: EGLR is an entropy-guided latent reasoning model for generative re-ranking that enables real-time reasoning during list generation with dynamic temperature adjustment for better exploration-exploitation trade-off.


<details>
  <summary>Details</summary>
Motivation: Existing generative re-ranking methods fail to adapt to dynamic entropy changes during list generation, making it hard to capture complex preferences. Language models' success with reasoning capabilities inspired integrating similar mechanisms for recommendation.

Method: EGLR introduces latent reasoning mechanism with three core features: 1) "reasoning while recommending" instead of "reason first, recommend later", 2) entropy-guided variable-length reasoning with context-aware tokens and dynamic temperature adjustment, 3) lightweight integration design without complex modules.

Result: Experimental results on two real-world datasets validate effectiveness. The model is compatible with existing generative re-ranking models to enhance their performance, showing practical deployment value and research potential.

Conclusion: EGLR effectively reduces entropy in decision-making through latent reasoning, achieving better adaptation to dynamic difficulty changes in list generation while maintaining compatibility with existing models.

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [343] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: TruthTensor is a new evaluation framework that assesses LLMs as human-imitation systems in real-world, uncertain environments using live prediction markets, going beyond static benchmarks to measure calibration, drift, and risk-sensitivity.


<details>
  <summary>Details</summary>
Motivation: Current language model evaluation is limited because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between task accuracy and human-aligned decision-making in evolving conditions.

Method: TruthTensor uses forward-looking, contamination-free tasks anchored to live prediction markets, combining probabilistic scoring with drift-centric diagnostics, robustness checks, and clear protocols for human vs. automated evaluation roles.

Result: Experiments across 500+ real markets show that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, demonstrating the need for multi-axis evaluation.

Conclusion: TruthTensor operationalizes modern evaluation best practices to produce defensible assessments of LLMs in real-world decision contexts, complementing traditional metrics with holistic behavioral analysis.

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [344] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: Proposed TSEvol multi-agent TS evolution algorithm, TSEData-20K dataset, ChatAD chatbot family, TKTO optimization for cross-task generalization, and LLADBench benchmark. ChatAD models achieve up to 34.5% accuracy gains and 37.42% false positive reduction.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-driven anomaly detection methods have inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization, limiting their understanding and explanatory abilities for anomalous behaviors in time series.

Method: 1) TSEvol multi-agent-based time series evolution algorithm; 2) TSEData-20K dataset for AD reasoning and multi-turn dialogue; 3) ChatAD chatbot family (Llama3-8B, Qwen2.5-7B, Mistral-7B); 4) TKTO optimization for cross-task generalization; 5) LLADBench benchmark for evaluation.

Result: ChatAD models achieve substantial gains: up to 34.50% in accuracy, 34.71% in F1, and 37.42% reduction in false positives. TKTO-optimized ChatAD achieves competitive performance in reasoning and cross-task generalization across classification, forecasting, and imputation tasks.

Conclusion: The proposed framework addresses limitations of existing LLM-driven AD methods by providing enhanced reasoning, multi-turn dialogue capabilities, and cross-task generalization through multi-agent algorithms, specialized datasets, chatbot models, and optimization techniques.

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [345] [Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis](https://arxiv.org/abs/2601.13558)
*Mehrab Beikzadeh,Chenglin Hong,Cory J Cascalheira,Callisto Boka,Majid Sarrafzadeh,Ian W Holloway*

Main category: cs.AI

TL;DR: Using social media and dating app text data with machine learning models (ChatGPT/BERT embeddings, LIWC, dictionary methods) to predict MSM risk behaviors like binge drinking, multiple partners, and PrEP use with moderate to strong performance.


<details>
  <summary>Details</summary>
Motivation: MSM face elevated risks for STIs and harmful drinking; social media/dating app text data could enable automated identification of risk/protective behaviors for personalized public health interventions.

Method: Collected textual data with participant consent, trained ML models using features from ChatGPT embeddings, BERT embeddings, LIWC, and dictionary-based risk term approach to predict sexual risk behaviors, alcohol use, and PrEP uptake.

Result: Strong performance for predicting monthly binge drinking and >5 sexual partners (F1=0.78), moderate performance for PrEP use and heavy drinking (F1=0.64 and 0.63).

Conclusion: Social media/dating app text data provides valuable behavioral insights; LLM-based methods show potential for scalable, personalized public health interventions for MSM.

Abstract: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.

</details>


### [346] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC is an evolutionary agent-based genomics data compressor with LLM integration that outperforms 14 baselines with 16%+ compression ratio gains and 4.7-9.2x throughput improvements.


<details>
  <summary>Details</summary>
Motivation: Current learning-based genomics data compression methods suffer from non-evolvability, low-level compression modeling, limited adaptability, and poor user interfaces.

Method: Three-layer agent architecture: User layer (LLM-integrated Leader for interface), Cognitive layer (Leader for joint algorithm-dataset-system optimization), and Compression layer (Worker for automated multi-knowledge learning-based compression). Three modes: CP (compression priority), TP (throughput priority), BM (balanced).

Result: Outperforms 14 baselines on 9 datasets with average compression ratio gains of 16.66% (CP), 16.11% (TP), 16.33% (BM) and throughput gains of 4.73x, 9.23x, 9.15x respectively.

Conclusion: AgentGC successfully addresses limitations of existing methods through evolutionary agent-based design with LLM integration, achieving superior compression performance and adaptability for genomics data.

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [347] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: Novel role-separated transformer architecture achieves human-level performance on ARC visual reasoning tasks by separating reasoning from workspace operations.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack persistent, readable mental states and produce ungrounded rationalizations, unlike humans who can explain actions by decoding internal state. The paper hypothesizes that reasoning should exist as a distinct channel separate from low-level workspace operations.

Method: Designed a role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution for solving ARC tasks as visual reasoning problems. Trained and evaluated within the VARC vision-centric protocol.

Result: Achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and significantly outperforming prior methods. Models exhibit more coherent rule-application structure than dense ViT baselines.

Conclusion: The results support the hypothesis that reasoning should exist as a distinct modality separate from low-level workspace operations, enabling more human-like abstract reasoning capabilities in AI systems.

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [348] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: ScriptMind is an LLM-based framework for scam detection that combines automated reasoning with human cognitive assistance, outperforming GPT-4o by 13% and enhancing users' scam awareness.


<details>
  <summary>Details</summary>
Motivation: Traditional scam detection methods struggle with personalized, multi-turn social engineering scams, and while LLMs show promise for detection, their potential for cognitive assistance remains underexplored.

Method: Three-component framework: 1) Crime Script Inference Task (CSIT) for scam reasoning, 2) Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs using 571 Korean phone scam cases (22,712 training instances), and 3) Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact.

Result: The 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. In phone scam simulations, it significantly enhanced and sustained users' suspicion levels.

Conclusion: ScriptMind represents progress toward human-centered, cognitively adaptive LLMs for scam defense, bridging automated reasoning with human cognition to improve scam awareness and detection.

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [349] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: Multi-agent AI system converts audio emotional signals into safe, age-appropriate media responses in real-time through specialized agents with safety verification.


<details>
  <summary>Details</summary>
Motivation: Current speech emotion recognition focuses too much on classification accuracy rather than practical application - there's a need to transform emotional states into safe, controllable response content, especially for sensitive applications like child media and therapy.

Method: Four-agent pipeline: (1) CNN-based Emotion Recognition Agent extracts acoustic features, (2) Response Policy Decision Agent maps emotions to response modes, (3) Content Parameter Generation Agent creates media control parameters, (4) Safety Verification Agent enforces age-appropriateness and stimulation constraints with explicit verification loop.

Result: Achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, 100% safety compliance, with sub-100ms inference latency suitable for on-device deployment.

Conclusion: The modular multi-agent system successfully transforms emotional signals into safe, controllable media responses with interpretability and extensibility, making it applicable to child media, therapy, and emotionally responsive smart devices.

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [350] [DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems](https://arxiv.org/abs/2601.13591)
*Maojun Sun,Yifei Xie,Yue Wu,Ruijian Han,Binyan Jiang,Defeng Sun,Yancheng Yuan,Jian Huang*

Main category: cs.AI

TL;DR: DSAEval benchmark with 641 real-world data science problems across 285 diverse datasets evaluates LLM-based data agents using multimodal perception, iterative queries, and holistic assessment.


<details>
  <summary>Details</summary>
Motivation: Real-world data science problems are open-ended, span multiple taxonomies, and lack standard answers, making evaluation of LLM-based data agents challenging. Existing benchmarks don't capture the complexity of actual data science workflows.

Method: Created DSAEval benchmark with 641 problems across 285 datasets covering structured/unstructured data. Features: 1) Multimodal Environment Perception (text+vision), 2) Multi-Query Interactions (iterative workflows), 3) Multi-Dimensional Evaluation (reasoning, code, results). Evaluated 11 advanced agentic LLMs.

Result: Claude-Sonnet-4.5: strongest overall performance; GPT-5.2: most efficient; MiMo-V2-Flash: most cost-effective. Multimodal perception improves vision tasks by 2.04-11.30%. Current agents perform well on structured data/routine workflows but struggle with unstructured domains.

Conclusion: DSAEval provides comprehensive evaluation framework for data science agents. While progress made, substantial challenges remain in unstructured data domains. Paper offers critical insights and future research directions for advancing data science agents.

Abstract: Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.

</details>


### [351] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: LLMs struggle with global consistency verification of facts; new algorithm detects minimal inconsistent subsets with polynomial query complexity.


<details>
  <summary>Details</summary>
Motivation: Natural language fact collections need global consistency for tasks like fact-checking and knowledge base construction, but LLMs only handle small subsets with noisy judgments.

Method: Adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) and optionally computes minimal repairs through hitting-sets.

Result: Method has low-degree polynomial query complexity and efficiently detects/localizes inconsistencies in experiments with synthetic and real LLM oracles.

Conclusion: Provides scalable framework for linguistic consistency verification with LLM-based evaluators, overcoming exponential worst-case complexity.

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [352] [Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning](https://arxiv.org/abs/2601.13632)
*Zhiming Xue,Sichen Zhao,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.AI

TL;DR: RADR framework combines ST-GNN with optimization for dynamic logistics routing, reducing congestion risk by 19.3% with only 2.1% distance increase.


<details>
  <summary>Details</summary>
Motivation: Traditional static routing can't handle traffic congestion and fluctuating demand in e-commerce logistics, requiring dynamic risk-aware solutions.

Method: Construct logistics graph from GPS data, use GCN+GRU hybrid model to predict congestion risks, integrate predictions into dynamic edge weights for path planning.

Result: On Smart Logistics Dataset 2024, reduces congestion risk exposure by 19.3% while increasing distance by only 2.1%, enhancing supply chain resilience.

Conclusion: Data-driven RADR framework effectively balances delivery efficiency and operational safety in dynamic logistics environments.

Abstract: With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.

</details>


### [353] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: SocialMindChange benchmark tests LLMs' ability to actively change minds in social interactions (not just track them), showing current models perform 54.2% below humans.


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks are passive - models just read and report mental states. Real social interaction requires using ToM to actively change others' mental states through dialogue. Need benchmark that moves from tracking minds to changing minds.

Method: Created SocialMindChange benchmark with structured four-step framework: 1,200 social contexts with 4 characters and 5 connected scenes each. Model plays one character and generates dialogue across scenes to reach target while maintaining consistency with evolving mental states. Includes higher-order states. Validated for realism/quality.

Result: Evaluated 10 state-of-the-art LLMs. Average performance 54.2% below human performance. Models struggle to maintain and change mental-state representations across long, linked interactions.

Conclusion: Current LLMs still have significant limitations in dynamic social reasoning - can't effectively use ToM to actively shape others' mental states in extended interactions, highlighting gap between passive tracking and active social influence.

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [354] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: LLM agents deceive more effectively than humans in social deduction games, as shown by lower detection accuracy of their deception in Mafia game transcripts.


<details>
  <summary>Details</summary>
Motivation: To study LLM deception in natural language social contexts, moving beyond controlled tasks to realistic social interactions using social deduction games.

Method: Simulated 35 Mafia games with GPT-4o agents using asynchronous multi-agent framework, then created GPT-4-Turbo Mafia Detector to analyze transcripts without role info and predict mafia players, comparing to 28 human games and random baseline.

Result: Mafia Detector's prediction accuracy was lower on LLM games than human games, indicating LLMs blend in better and deceive more effectively, consistent across game days and number of mafias detected.

Conclusion: LLMs demonstrate sophisticated deception capabilities in social contexts, highlighting both their advanced abilities and associated risks, with released dataset supporting future research.

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [355] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: Current probabilistic confidence metrics fail to capture logical reasoning structure, primarily measuring surface fluency instead of true causal dependencies between reasoning steps.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that probabilistic confidence metrics accurately reflect reasoning quality in Best-of-N selection, and investigate whether they truly capture inter-step causal dependencies necessary for valid reasoning.

Method: Introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Test across diverse model families and reasoning benchmarks, including hard attention masks that prevent models from attending to prior reasoning steps. Propose a contrastive causality metric that explicitly isolates inter-step causal dependencies.

Result: Selection accuracy degrades only marginally under systematic disruptions of inter-step dependencies. Even severe interventions like hard attention masks don't substantially reduce selection performance, showing current probabilistic metrics are largely insensitive to logical structure and primarily capture surface-level fluency or in-distribution priors.

Conclusion: Current probabilistic confidence metrics are poor proxies for reasoning quality as they don't capture logical structure. The proposed contrastive causality metric yields more faithful output selection by explicitly isolating inter-step causal dependencies.

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [356] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: RELIEF is a framework that shapes large reasoning model behavior by aligning their internal reasoning beliefs with target blueprints using simple logit probing and self-reflective fine-tuning, eliminating need for reasoning-trace supervision.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from computational redundancy and reasoning unfaithfulness, and current methods using reinforcement learning or fine-tuning with gold-standard reasoning traces are computationally expensive and difficult to scale.

Method: RELIEF uses logit probing to capture models' latent reasoning beliefs, then fine-tunes on synthesized self-reflective question-answering pairs that affirm target beliefs, aligning the model's self-concept with a target belief blueprint without reasoning-trace supervision.

Result: RELIEF matches or outperforms behavior-supervised and preference-based baselines on efficiency and faithfulness tasks while requiring lower training costs, and analysis confirms that shifting reasoning beliefs effectively shapes actual behavior.

Conclusion: RELIEF provides an effective, low-cost alternative to traditional behavior-shaping methods by leveraging models' internal reasoning beliefs, demonstrating that belief engineering can efficiently shape model behavior without expensive supervision.

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [357] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC is a two-stage self-play framework that stabilizes LLM self-improvement by decoupling question generation from solving, using difficulty-calibrated questions and asymmetric self-distillation.


<details>
  <summary>Details</summary>
Motivation: Existing self-play frameworks suffer from optimization instability due to non-stationary objectives from solver-dependent rewards and bootstrapping errors from self-generated pseudo-labels.

Method: Two-stage framework: (1) Train Questioner to synthesize difficulty-calibrated questions conditioned on explicit difficulty levels and external corpora; (2) Train Solver with asymmetric self-distillation where document-augmented teacher generates pseudo-labels for student Solver without document access.

Result: DARC yields average improvement of 10.9 points across nine reasoning benchmarks and three backbone models, consistently outperforms baselines, and approaches fully supervised model performance without human annotations.

Conclusion: DARC provides a stable, model-agnostic self-play framework that effectively addresses optimization instability in LLM self-improvement, achieving strong reasoning performance without human supervision.

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [358] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: Look-Ahead-Bench is a benchmark for measuring look-ahead bias in financial LLMs, showing standard LLMs exhibit significant bias while PiT models demonstrate better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches mainly test inner lookahead knowledge via Q&A, but there's a need to evaluate model behavior in practical financial scenarios and distinguish genuine predictive capability from memorization.

Method: Created a standardized benchmark measuring look-ahead bias in Point-in-Time LLMs within realistic financial workflows, analyzing performance decay across temporally distinct market regimes with quantitative baselines.

Result: Standard LLMs (Llama 3.1 and DeepSeek 3.2) show significant look-ahead bias (alpha decay), while PiT models (Pitinf-Small/Medium/Large) demonstrate improved generalization and reasoning as they scale.

Conclusion: Establishes foundation for standardized evaluation of temporal bias in financial LLMs and provides practical framework for identifying models suitable for real-world deployment.

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [359] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: Virtual Urbanism (VU) is an AI framework using synthetic urban replicas to quantify urban identity, demonstrated with Tokyo neighborhoods achieving ~81% identification accuracy.


<details>
  <summary>Details</summary>
Motivation: To advance computationally tractable urban identity metrics through AI-driven analysis, moving beyond traditional qualitative approaches to create automated, multi-parameter identity assessment frameworks.

Method: Multimodal AI framework integrating Stable Diffusion and LoRA models to generate synthetic urban replicas of nine Tokyo areas as dynamic sequences, excluding orientation markers. Human evaluation experiments assessed perceptual legitimacy, quantified area-level identity, and derived core identity-forming elements.

Result: Mean identification accuracy of ~81% confirmed replica validity. Urban Identity Level (UIL) metric enabled cross-area identity assessment. Semantic analysis revealed culturally embedded typologies as core identity-forming elements.

Conclusion: VU is a viable framework for AI-augmented urban analysis, positioning it as a path toward automated, multi-parameter urban identity metrics that can quantify previously qualitative aspects of urban character.

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [360] [LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health](https://arxiv.org/abs/2601.13880)
*Ye Tian,Zihao Wang,Onat Gungor,Xiaoran Fan,Tajana Rosing*

Main category: cs.AI

TL;DR: LifeAgentBench is a new benchmark for evaluating LLMs on long-horizon, cross-dimensional lifestyle health reasoning, revealing current models' limitations and proposing an improved agent framework.


<details>
  <summary>Details</summary>
Motivation: Personalized digital health requires complex reasoning over heterogeneous lifestyle data, but there's no systematic benchmark to evaluate LLM capabilities in this domain.

Method: Created LifeAgentBench with 22,573 questions spanning retrieval to complex reasoning, developed extensible pipeline and evaluation protocol, evaluated 11 LLMs, then proposed LifeAgent agent with multi-step evidence retrieval and deterministic aggregation.

Result: Systematic evaluation revealed key bottlenecks in long-horizon aggregation and cross-dimensional reasoning; LifeAgent showed significant improvements over baselines in realistic scenarios.

Conclusion: LifeAgentBench enables reliable assessment of health assistants, identifies current LLM limitations, and LifeAgent provides a strong baseline for future research in personalized digital health support.

Abstract: Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.

</details>


### [361] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: HSC is a human-inspired computational framework that models intelligence as a continuous closed-loop process involving thinking, action, learning, reflection, and scheduling, enabling active participation and automatic refinement of reasoning through environmental interaction.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are limited by their reliance on textual data alone, which restricts their ability to adapt, verify reasoning outcomes, and operate effectively in open, dynamic real-world environments. There's a need for systems that can actively participate in and learn from environmental interactions.

Method: Proposes Human Simulation Computation (HSC) - a framework modeling intelligence as a continuous closed-loop process with thinking, action, learning, reflection, and activity scheduling. It emphasizes active participation in both internal reasoning and environmental interactions, using actions to achieve goals and automatically refine reasoning mechanisms. Incorporates human thinking strategies like main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback.

Result: Through theoretical analysis, the paper argues that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

Conclusion: Human-inspired computational frameworks like HSC that incorporate continuous closed-loop processes, active environmental participation, and human thinking strategies are necessary for developing AI systems that can effectively adapt and operate in dynamic real-world environments, going beyond the limitations of language-only models.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [362] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB is a retrospective self-annotation method that targets affective inflection regions instead of full continuous annotation, reducing workload while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Full continuous self-annotation for affective states is time-consuming, cognitively demanding, prone to fatigue/errors, and creates high workload for annotators.

Method: Uses preference-learning model based on peak-end rule and ordinal emotion representations to detect relative affective changes, directing annotators to label only selected segments while interpolating the rest. Includes preview mechanism with contextual cues.

Result: Outperforms baselines in modeling affective inflections, mitigates workload (and conditionally temporal burden), improves annotator confidence without degrading annotation quality.

Conclusion: PREFAB offers an effective low-budget alternative to full annotation that maintains quality while reducing annotator burden through targeted labeling of affective inflection regions.

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [363] [Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval](https://arxiv.org/abs/2601.13969)
*Joaquín Polonuer,Lucas Vittor,Iñaki Arango,Ayush Noori,David A. Clifton,Luciano Del Corro,Marinka Zitnik*

Main category: cs.AI

TL;DR: ARK is an adaptive knowledge graph retriever that balances breadth-first search and depth-first traversal using a two-operation toolset controlled by an LLM, achieving state-of-the-art performance without retrieval training.


<details>
  <summary>Details</summary>
Motivation: Current KG retrieval methods struggle with balancing broad search coverage (similarity-based) with multi-hop traversal (seed-based). Similarity methods are shallow, while traversal methods depend on fragile seed selection and fail with multi-entity queries.

Method: ARK gives an LLM control over breadth-depth tradeoff using two operations: global lexical search over node descriptors and one-hop neighborhood exploration. It alternates between breadth-oriented discovery and depth-oriented expansion without seed selection or pre-set hop depth. Also distills trajectories from large teacher to 8B model via label-free imitation.

Result: On STaRK: 59.1% average Hit@1 and 67.4 average MRR, improving Hit@1 by up to 31.4% and MRR by up to 28.0% over baselines. Distilled 8B model improves Hit@1 by +7.0, +26.6, +13.5 points on AMAZON, MAG, PRIME datasets while retaining up to 98.5% of teacher's performance.

Conclusion: ARK provides an effective adaptive approach to KG retrieval that balances breadth and depth without training, and can be efficiently distilled to smaller models while maintaining strong performance.

Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.

</details>


### [364] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: Numina-Lean-Agent uses general coding agents (Claude Code with MCP) for formal theorem proving, achieving state-of-the-art results on Putnam 2025 and successfully formalizing complex mathematical theorems.


<details>
  <summary>Details</summary>
Motivation: Existing agentic theorem proving systems rely on task-specific pipelines and trained formal provers, limiting flexibility and reproducibility. The authors propose using general coding agents as formal math reasoners because they provide natural interfaces for diverse tasks, allow performance improvements through base model replacement without training, and enable flexible tool integration via MCP.

Method: Numina-Lean-Agent combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean theorem prover, retrieval of relevant theorems, informal proving, and auxiliary reasoning tools. The system leverages the Model Context Protocol (MCP) for flexible tool calling and extension.

Result: Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all 12 problems in Putnam 2025 (12/12), matching the best closed-source system. Beyond benchmarks, it successfully formalizes the Brascamp-Lieb theorem through interaction with mathematicians.

Conclusion: General coding agents provide an effective paradigm for formal theorem proving, offering flexibility, reproducibility, and state-of-the-art performance without requiring specialized training. The approach demonstrates that advanced language models with proper tool integration can match or exceed specialized systems in formal mathematics.

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [365] [Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems](https://arxiv.org/abs/2601.14096)
*Benedikt Hartl,Léo Pio-Lopez,Chris Fields,Michael Levin*

Main category: cs.AI

TL;DR: The paper proposes a unified framework for understanding cognition across biological and artificial systems through two scale-invariant principles: remapping of embedding spaces and navigation within these spaces via iterative error minimization.


<details>
  <summary>Details</summary>
Motivation: To develop an integrated view of problem-solving across diverse intelligent systems (biological, engineered, chimeric) by discovering scale-invariant principles of decision-making that apply from subcellular networks to swarms of organisms.

Method: Proposes a theoretical framework based on two invariants: (1) remapping of embedding spaces (transcriptional, morphological, physiological, or data spaces), and (2) navigation within these spaces through distributed error correction or iterative refinement.

Result: Identifies deep parallels between biological systems (maintaining homeostasis, regenerating structure) and modern AI systems (transformers, diffusion models, neural cellular automata) that both employ remapping and navigation of embedding spaces via error minimization.

Conclusion: The dual principle of remapping and navigation of embedding spaces via iterative error minimization constitutes a substrate-independent invariant of cognition, providing a unifying framework for engineering adaptive intelligence across scales.

Abstract: The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.

</details>


### [366] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent is a multi-agent framework that reframes rebuttal generation as evidence-centric planning, using atomic concern decomposition, hybrid context synthesis, and external search to ensure verifiable grounding.


<details>
  <summary>Details</summary>
Motivation: Current rebuttal generation approaches suffer from hallucination, overlooked critiques, and lack of verifiable grounding by treating it as direct-to-text generation without proper evidence anchoring.

Method: Multi-agent framework that decomposes feedback into atomic concerns, constructs hybrid contexts (compressed summaries + high-fidelity text), integrates autonomous external search for literature needs, and generates inspectable response plans before drafting.

Result: Outperforms strong baselines on RebuttalBench in coverage, faithfulness, and strategic coherence, offering transparent and controllable assistance for peer review.

Conclusion: RebuttalAgent provides a novel evidence-centric approach to rebuttal generation that ensures every argument is explicitly anchored in evidence, addressing key limitations of current methods.

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [367] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: Survey paper analyzing efficiency in LLM-based agent systems across memory, tool learning, and planning components, examining cost-performance trade-offs and benchmarking approaches.


<details>
  <summary>Details</summary>
Motivation: While LLM agents' effectiveness has improved, efficiency (crucial for real-world deployment) has been overlooked. The paper aims to address this gap by comprehensively studying agent efficiency across core components.

Method: Literature review of recent approaches focusing on three core agent components: memory (context compression/management), tool learning (RL rewards to minimize tool invocation), and planning (controlled search mechanisms). Characterizes efficiency through cost-effectiveness trade-offs and Pareto frontiers.

Result: Identifies shared high-level principles across different implementations: context bounding via compression/management, RL reward design for tool minimization, and controlled search mechanisms. Summarizes evaluation protocols and consolidated efficiency metrics from benchmark studies.

Conclusion: Provides comprehensive analysis of agent efficiency, examining key challenges and future directions to offer insights for developing more efficient LLM-based agent systems suitable for real-world deployment.

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [368] [RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models](https://arxiv.org/abs/2601.11801)
*Nitish Sontakke,K. Niranjan Kumar,Sehoon Ha*

Main category: cs.RO

TL;DR: RobotDesignGPT is an automated robot design framework that uses large vision-language models to synthesize robot designs from user prompts and reference images, reducing reliance on domain expertise and manual effort.


<details>
  <summary>Details</summary>
Motivation: Robot design is complex, requiring consideration of multiple criteria (user specs, kinematics, appearance) and heavy reliance on domain expertise. Current rule-based methods need predefined grammars or primitive components, limiting flexibility and requiring significant human effort.

Method: RobotDesignGPT leverages pre-trained vision-language models' general knowledge and reasoning capabilities. It synthesizes initial designs from simple user prompts and reference images, using a novel visual feedback approach to improve design quality and reduce manual feedback.

Result: The framework successfully designs visually appealing and kinematically valid robots inspired by nature, including legged animals and flying creatures. Ablation and user studies validate the approach.

Conclusion: RobotDesignGPT demonstrates an effective automated robot design framework that reduces reliance on domain expertise and manual effort while producing valid, nature-inspired robot designs through vision-language model capabilities and visual feedback.

Abstract: Robot design is a nontrivial process that involves careful consideration of multiple criteria, including user specifications, kinematic structures, and visual appearance. Therefore, the design process often relies heavily on domain expertise and significant human effort. The majority of current methods are rule-based, requiring the specification of a grammar or a set of primitive components and modules that can be composed to create a design. We propose a novel automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to automate the robot design synthesis process. Our framework synthesizes an initial robot design from a simple user prompt and a reference image. Our novel visual feedback approach allows us to greatly improve the design quality and reduce unnecessary manual feedback. We demonstrate that our framework can design visually appealing and kinematically valid robots inspired by nature, ranging from legged animals to flying creatures. We justify the proposed framework by conducting an ablation study and a user study.

</details>


### [369] [Optimal Thruster Configuration for 6-DOF Control of a Small Satellite](https://arxiv.org/abs/2601.11802)
*Suguru Sato,Jinaykumar Patel,Kamesh Subbarao*

Main category: cs.RO

TL;DR: Paper presents thruster configuration groups for small satellites that enable full 6-DOF control, identifies minimum-thrust configurations, and demonstrates attitude control performance in rendezvous-docking missions.


<details>
  <summary>Details</summary>
Motivation: With growing deployment of small satellites (CubeSats, Nanosats, etc.) in LEO for applications like imaging, communication, and rendezvous-docking, there's increasing need for orbit maintenance and attitude control using efficient thruster configurations.

Method: Starting from a 24-thruster configuration, the paper develops viable configuration groups that enable full 6-DOF control, then identifies configurations requiring minimum total thrust among these groups. One configuration from each group is evaluated for attitude control performance through representative rendezvous-docking missions.

Result: The study identifies thruster configuration groups that provide full 6-DOF control and finds minimum-thrust configurations among them. Evaluation shows that even with reduced thruster counts, sufficient maneuverability can be achieved for rendezvous-docking missions.

Conclusion: The paper demonstrates that optimized thruster configurations can provide effective 6-DOF control for small satellites with reduced thruster counts, enabling sufficient maneuverability for complex missions like rendezvous-docking while minimizing thrust requirements.

Abstract: With the growing deployment of small satellites (such as CubeSats, Nanosats, Picosats, and Femtosats) in Low Earth Orbit (LEO) for targeted applications like imaging, communication, data storage, and rendezvous-docking mission, there is increasing attention on orbit maintenance and attitude control. A common approach for active orbit control involves the use of multiple thrusters, which, when properly arranged, can also generate the required torque for attitude control. Starting from a 24-thruster configuration, this paper presents a set of thruster configurations (referred to as a viable configuration group) that enable full six degrees of freedom (6-DOF) control. Further, configuration group that requires minimum total thrust to achieve 6-DOF commands are found among the viable configuration group. One configuration from each of these groups is further evaluated for its attitude control performance through a representative rendezvous-docking mission, demonstrating that even with a reduced thruster count, sufficient maneuverability can be achieved.

</details>


### [370] [Three Dimensional Hydrodynamic Flow-Based Collision Avoidance for UAV Formations Facing Emergent Dynamic Obstacles](https://arxiv.org/abs/2601.11832)
*Suguru Sato,Kamesh Subbarao*

Main category: cs.RO

TL;DR: 3D hydrodynamics-inspired collision avoidance framework for UAV formations using flow-based obstacle modeling and Virtual Rigid Body coordination for smooth, real-time avoidance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Need for real-time collision avoidance for UAV formations in dynamic environments without trajectory discontinuities or explicit replanning, overcoming local minima issues in traditional potential field methods.

Method: Models moving obstacles as 3D doublets/ellipsoids generating local velocity fields based on fluid flow principles (Laplace's equation), integrated with Virtual Rigid Body formation strategy for coordination and geometry preservation.

Result: Simulation results show feasible and scalable method for individual/multi-UAV scenarios with multiple formation geometries encountering moving obstacles, achieving safe, smooth, computationally efficient maneuvers suitable for real-time applications.

Conclusion: The hydrodynamics-inspired flow-based approach provides an effective solution for real-time UAV formation collision avoidance with interpretable behavior, smooth maneuvers, and inherent avoidance of local minima.

Abstract: This paper presents a three-dimensional, hydrodynamics-inspired collision avoidance framework for uncrewed aerial vehicle (UAV) formations operating in dynamic environments. When moving obstacles enter a UAV's sensing region, they are modeled as three dimensional doublets or ellipsoids that generate local velocity fields, guiding nearby UAVs to execute smooth, collision-free maneuvers without trajectory discontinuities or explicit trajectory replanning. This flow-based approach enables real-time operation and interpretable behavior by leveraging the nature of fluid flow around obstacles via the harmonic properties of Laplace's equation, inherently avoiding local minima common in traditional potential field methods. To establish and maintain coordination among the UAVs, a Virtual Rigid Body (VRB) formation strategy is integrated, ensuring that formation geometry and trajectory tracking are preserved. Simulation results demonstrate the feasibility and scalability of the method for both individual and multi-UAV scenarios with multiple formation geometries encountering moving obstacles. The proposed approach achieves safe, smooth, and computationally efficient avoidance maneuvers suitable for real-time and practical applications.

</details>


### [371] [AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal](https://arxiv.org/abs/2601.11876)
*Christopher Kao,Akhil Pathapati,James Davis*

Main category: cs.RO

TL;DR: Autonomous robot using STC path planning, RTK GPS navigation, and ResNet50 CNN achieves 80% success rate picking up trash in parks.


<details>
  <summary>Details</summary>
Motivation: There are 50 billion pieces of litter in the U.S., with grass fields being problematic as picnickers often leave trash behind, creating a need for automated cleanup solutions.

Method: Used Spanning Tree Coverage (STC) algorithm for autonomous navigation path planning, Real-Time Kinematic (RTK) GPS for centimeter-level positioning, ResNet50 CNN for 94.52% accurate trash detection, and developed a specialized pickup mechanism for field trash.

Result: The system achieved an overall success rate of 80% in autonomously navigating, identifying, and picking up trash on grass fields.

Conclusion: Autonomous trash pickup robots are a viable solution for cleaning litter from grass fields in parks, demonstrating practical feasibility for addressing the litter problem.

Abstract: There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.

</details>


### [372] [Visual-Language-Guided Task Planning for Horticultural Robots](https://arxiv.org/abs/2601.11906)
*Jose Cuaran,Kendall Koe,Aditya Potnis,Naveen Kumar Uppalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: VLMs work well for short-term crop monitoring but struggle with long-term tasks and noisy semantic maps, revealing limitations for complex agricultural robotics.


<details>
  <summary>Details</summary>
Motivation: Current crop monitoring systems lack high-level reasoning capabilities needed for precision agriculture, creating a need for more intelligent, modular frameworks that can guide robotic task planning.

Method: Developed a novel modular framework using Visual Language Models (VLMs) to guide robotic task planning by interleaving input queries with action primitives. Created a comprehensive benchmark for evaluating short- and long-horizon crop monitoring tasks in both monoculture and polyculture environments.

Result: VLMs perform robustly for short-horizon tasks (comparable to human success rates), but show significant performance degradation in challenging long-horizon tasks. The system fails when relying on noisy semantic maps, highlighting a key limitation in current VLM context grounding for sustained robotic operations.

Conclusion: This work provides both a deployable framework for agricultural robotics and critical insights into VLM capabilities and limitations, showing that while VLMs are promising for short-term tasks, they face significant challenges with long-term operations and noisy data in complex agricultural environments.

Abstract: Crop monitoring is essential for precision agriculture, but current systems lack high-level reasoning. We introduce a novel, modular framework that uses a Visual Language Model (VLM) to guide robotic task planning, interleaving input queries with action primitives. We contribute a comprehensive benchmark for short- and long-horizon crop monitoring tasks in monoculture and polyculture environments. Our main results show that VLMs perform robustly for short-horizon tasks (comparable to human success), but exhibit significant performance degradation in challenging long-horizon tasks. Critically, the system fails when relying on noisy semantic maps, demonstrating a key limitation in current VLM context grounding for sustained robotic operations. This work offers a deployable framework and critical insights into VLM capabilities and shortcomings for complex agricultural robotics.

</details>


### [373] [Model selection and real-time skill assessment for suturing in robotic surgery](https://arxiv.org/abs/2601.12012)
*Zhaoyang Jacopo Hu,Alex Ranne,Alaa Eldin Abdelaal,Kiran Bhattacharyya,Etienne Burdet,Allison M. Okamura,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: Multimodal deep learning fusion of kinematic and vision data enables real-time surgical skill prediction, outperforming unimodal approaches and showing better generalization when trained on expert data.


<details>
  <summary>Details</summary>
Motivation: To develop automated feedback systems for objective skill assessment in robot-assisted surgery, enabling real-time prediction of surgical skill levels based on OSATS scores.

Method: Three main analyses: 1) Model design comparing unimodal baselines vs multimodal fusion architectures using kinematic and vision data from da Vinci Surgical System, 2) Real-time performance evaluation tracking prediction trends over time and correlation with surgeon gestures, 3) Skill-level-based cross-validation training models separately on different skill levels.

Result: Fusion models consistently outperform unimodal models for real-time predictions (measured by mean Spearman's correlation coefficients). Models trained on high-skill demonstrations perform better and generalize well to similarly skilled participants compared to those trained on low-skilled data.

Conclusion: Multimodal learning enables more stable fine-grained evaluation of surgical performance, and expert-level training data is valuable for model generalization in surgical skill assessment systems.

Abstract: Automated feedback systems have the potential to provide objective skill assessment for training and evaluation in robot-assisted surgery. In this study, we examine methods to achieve real-time prediction of surgical skill level in real-time based on Objective Structured Assessment of Technical Skills (OSATS) scores. Using data acquired from the da Vinci Surgical System, we carry out three main analyses, focusing on model design, their real-time performance, and their skill-level-based cross-validation training. For the model design, we evaluate the effectiveness of multimodal deep learning models for predicting surgical skill levels using synchronized kinematic and vision data. Our models include separate unimodal baselines and fusion architectures that integrate features from both modalities and are evaluated using mean Spearman's correlation coefficients, demonstrating that the fusion model consistently outperforms unimodal models for real-time predictions. For the real-time performance, we observe the prediction's trend over time and highlight correlation with the surgeon's gestures. For the skill-level-based cross-validation, we separately trained models on surgeons with different skill levels, which showed that high-skill demonstrations allow for better performance than those trained on low-skilled ones and generalize well to similarly skilled participants. Our findings show that multimodal learning allows more stable fine-grained evaluation of surgical performance and highlights the value of expert-level training data for model generalization.

</details>


### [374] [BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies](https://arxiv.org/abs/2601.12116)
*Hang Xu,Yizhou Chen,Dongjie Yu,Yi Ren,Jia PanI*

Main category: cs.RO

TL;DR: A hierarchical imitation learning framework for bimanual manipulation using keypose-conditioned coordination-aware consistency policy to improve success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Robots excel at simple unimanual tasks but struggle with bimanual manipulation due to coordination complexities and multi-stage processes. Existing generative model approaches don't adequately address the multi-stage nature while maintaining fast inference speed needed for real-time performance.

Method: Proposes a hierarchical framework with: 1) High-level keypose predictor that identifies bimanual keyposes considering robot-centric action features and task-centric operation styles, 2) Low-level trajectory generator formulated as a consistency model that generates action sequences in single inference step using historical observations and predicted keyposes as sub-goals.

Result: Simulation and real-world experiments show the approach significantly outperforms baseline methods in both success rates and operational efficiency.

Conclusion: The proposed keypose-conditioned coordination-aware consistency policy effectively addresses bimanual manipulation challenges by combining hierarchical structure with fast single-step inference, improving both reliability and efficiency in multi-stage tasks.

Abstract: Robots are essential in industrial manufacturing due to their reliability and efficiency. They excel in performing simple and repetitive unimanual tasks but still face challenges with bimanual manipulation. This difficulty arises from the complexities of coordinating dual arms and handling multi-stage processes. Recent integration of generative models into imitation learning (IL) has made progress in tackling specific challenges. However, few approaches explicitly consider the multi-stage nature of bimanual tasks while also emphasizing the importance of inference speed. In multi-stage tasks, failures or delays at any stage can cascade over time, impacting the success and efficiency of subsequent sub-stages and ultimately hindering overall task performance. In this paper, we propose a novel keypose-conditioned coordination-aware consistency policy tailored for bimanual manipulation. Our framework instantiates hierarchical imitation learning with a high-level keypose predictor and a low-level trajectory generator. The predicted keyposes serve as sub-goals for trajectory generation, indicating targets for individual sub-stages. The trajectory generator is formulated as a consistency model, generating action sequences based on historical observations and predicted keyposes in a single inference step. In particular, we devise an innovative approach for identifying bimanual keyposes, considering both robot-centric action features and task-centric operation styles. Simulation and real-world experiments illustrate that our approach significantly outperforms baseline methods in terms of success rates and operational efficiency. Implementation codes can be found at https://github.com/JoanaHXU/BiKC-plus.

</details>


### [375] [Active Semantic Mapping of Horticultural Environments Using Gaussian Splatting](https://arxiv.org/abs/2601.12122)
*Jose Cuaran,Naveen K. Upalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: Active 3D semantic reconstruction for agricultural scenes using mobile manipulator with Octomap + 3D Gaussian Splatting integration for efficient, high-fidelity mapping.


<details>
  <summary>Details</summary>
Motivation: Traditional manual scanning or fixed camera setups are bottlenecks for agricultural scene reconstruction needed for phenotyping and yield estimation.

Method: Integrates Octomap (probabilistic occupancy for planning) with 3D Gaussian Splatting (geometric, photometric, semantic optimization). Includes strategies for robustness against segmentation noise and memory reduction.

Result: Outperforms occupancy-based approaches: 6.6% improvement in fruit-level F1 score (noise-free), up to 28.6% under segmentation noise, 50% runtime reduction. Enables precise fruit counting and volume estimation.

Conclusion: Proposed framework enables scalable, real-time semantic reconstruction in agricultural robotics with improved accuracy and efficiency.

Abstract: Semantic reconstruction of agricultural scenes plays a vital role in tasks such as phenotyping and yield estimation. However, traditional approaches that rely on manual scanning or fixed camera setups remain a major bottleneck in this process. In this work, we propose an active 3D reconstruction framework for horticultural environments using a mobile manipulator. The proposed system integrates the classical Octomap representation with 3D Gaussian Splatting to enable accurate and efficient target-aware mapping. While a low-resolution Octomap provides probabilistic occupancy information for informative viewpoint selection and collision-free planning, 3D Gaussian Splatting leverages geometric, photometric, and semantic information to optimize a set of 3D Gaussians for high-fidelity scene reconstruction. We further introduce simple yet effective strategies to enhance robustness against segmentation noise and reduce memory consumption. Simulation experiments demonstrate that our method outperforms purely occupancy-based approaches in both runtime efficiency and reconstruction accuracy, enabling precise fruit counting and volume estimation. Compared to a 0.01m-resolution Octomap, our approach achieves an improvement of 6.6% in fruit-level F1 score under noise-free conditions, and up to 28.6% under segmentation noise. Additionally, it achieves a 50% reduction in runtime, highlighting its potential for scalable, real-time semantic reconstruction in agricultural robotics.

</details>


### [376] [Neural Process-Based Reactive Controller for Autonomous Racing](https://arxiv.org/abs/2601.12143)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: Novel reactive control framework for gap-based navigation using Attentive Neural Process (AttNP) and physics-informed PI-AttNP with CBF-based safety filtering for autonomous racing.


<details>
  <summary>Details</summary>
Motivation: As attention-based neural architectures become central to real-time nonlinear control in safety-critical domains like autonomous driving, there's a need for statistically grounded and provably safe decision-making frameworks.

Method: Introduces Attentive Neural Process (AttNP) and physics-informed PI-AttNP for gap-based navigation, evaluated in F1TENTH-style racecar simulation. PI-AttNP adds approximate model-based priors for physical inductive bias. Implements CBF-based filtering mechanism for collision avoidance constraints.

Result: PI-AttNP enables faster convergence and improved prediction accuracy for real-time control. CBF filtering provides lightweight, certifiable safety layer that generalizes across racing scenarios. Demonstrates competitive closed-loop performance with real-time constraint satisfaction.

Conclusion: The proposed framework combines neural process models with physics-informed priors and formal safety guarantees, offering a promising approach for safety-critical autonomous control applications.

Abstract: Attention-based neural architectures have become central to state-of-the-art methods in real-time nonlinear control. As these data-driven models continue to be integrated into increasingly safety-critical domains, ensuring statistically grounded and provably safe decision-making becomes essential. This paper introduces a novel reactive control framework for gap-based navigation using the Attentive Neural Process (AttNP) and a physics-informed extension, the PI-AttNP. Both models are evaluated in a simulated F1TENTH-style Ackermann steering racecar environment, chosen as a fast-paced proxy for safety-critical autonomous driving scenarios. The PI-AttNP augments the AttNP architecture with approximate model-based priors to inject physical inductive bias, enabling faster convergence and improved prediction accuracy suited for real-time control. To further ensure safety, we derive and implement a control barrier function (CBF)-based filtering mechanism that analytically enforces collision avoidance constraints. This CBF formulation is fully compatible with the learned AttNP controller and generalizes across a wide range of racing scenarios, providing a lightweight and certifiable safety layer. Our results demonstrate competitive closed-loop performance while ensuring real-time constraint satisfaction.

</details>


### [377] [Learning Legged MPC with Smooth Neural Surrogates](https://arxiv.org/abs/2601.12169)
*Samuel A. Moore,Easop Lee,Boyuan Chen*

Main category: cs.RO

TL;DR: Smooth neural surrogates with heavy-tailed likelihood training improve legged robot MPC by addressing neural network dynamics issues: contact stiffness, non-physical nonsmoothness, and non-Gaussian errors, enabling reliable execution and substantial performance gains.


<details>
  <summary>Details</summary>
Motivation: Integrating learned neural network dynamics with MPC for legged robots faces three key challenges: (1) stiff contact transitions inherited from data, (2) additional non-physical local nonsmoothness in neural models, and (3) non-Gaussian model errors from rapid state changes in training data.

Method: Introduce smooth neural surrogates - neural networks with tunable smoothness designed for informative predictions and derivatives in trajectory optimization through contact. Train models using heavy-tailed likelihood to better match empirical error distributions observed in legged-robot dynamics.

Result: Smooth neural surrogates with robust learning yield: 10-50% cumulative cost reduction on simple behaviors, enable reliable execution (0/5 to 5/5 success) in challenging regimes, and produce 2-50x lower cumulative cost with orders-of-magnitude improvements in robustness.

Conclusion: The proposed smooth neural surrogates with heavy-tailed likelihood training substantially improve reliability, scalability, and generalizability of learned legged MPC, enabling successful zero-shot locomotion where standard neural dynamics often fail outright.

Abstract: Deep learning and model predictive control (MPC) can play complementary roles in legged robotics. However, integrating learned models with online planning remains challenging. When dynamics are learned with neural networks, three key difficulties arise: (1) stiff transitions from contact events may be inherited from the data; (2) additional non-physical local nonsmoothness can occur; and (3) training datasets can induce non-Gaussian model errors due to rapid state changes. We address (1) and (2) by introducing the smooth neural surrogate, a neural network with tunable smoothness designed to provide informative predictions and derivatives for trajectory optimization through contact. To address (3), we train these models using a heavy-tailed likelihood that better matches the empirical error distributions observed in legged-robot dynamics. Together, these design choices substantially improve the reliability, scalability, and generalizability of learned legged MPC. Across zero-shot locomotion tasks of increasing difficulty, smooth neural surrogates with robust learning yield consistent reductions in cumulative cost on simple, well-conditioned behaviors (typically 10-50%), while providing substantially larger gains in regimes where standard neural dynamics often fail outright. In these regimes, smoothing enables reliable execution (from 0/5 to 5/5 success) and produces about 2-50x lower cumulative cost, reflecting orders-of-magnitude absolute improvements in robustness rather than incremental performance gains.

</details>


### [378] [A Comprehensive Review of Bio-Inspired Approaches to Coordination, Communication, and System Architecture in Underwater Swarm Robotics](https://arxiv.org/abs/2601.12244)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: Review paper synthesizes bio-inspired coordination mechanisms, communication strategies, and system design for underwater swarm robotics, analyzing key algorithms and proposing a classification framework.


<details>
  <summary>Details</summary>
Motivation: Increasing complexity of marine operations requires intelligent robotic systems for ocean observation, exploration, and resource management. Underwater swarm robotics offers promising collective coordination capabilities, but research remains fragmented across algorithmic, communication, and hardware design perspectives.

Method: Comprehensive review synthesizing bio-inspired coordination mechanisms, communication strategies, and system design considerations. Examines key marine-specific algorithms (Artificial Fish Swarm, Whale Optimization, Coral Reef Optimization, Marine Predators Algorithm) and analyzes underwater communication constraints with emerging acoustic, optical, and hybrid solutions. Proposes multi-dimensional classification framework evaluating approaches across communication dependency, environmental adaptability, energy efficiency, and swarm scalability.

Result: Integrated analysis unifies bio-inspired coordination algorithms, communication modalities, and system design approaches. Identifies converging trends, key challenges, and future research directions for real-world deployment of underwater swarm systems.

Conclusion: The review provides a comprehensive synthesis of underwater swarm robotics, bridging fragmented research areas and offering a framework for evaluating approaches. It highlights the need for integrated solutions that address algorithmic coordination, communication constraints, and hardware design to enable practical deployment of bio-inspired underwater swarm systems.

Abstract: The increasing complexity of marine operations has intensified the need for intelligent robotic systems to support ocean observation, exploration, and resource management. Underwater swarm robotics offers a promising framework that extends the capabilities of individual autonomous platforms through collective coordination. Inspired by natural systems, such as fish schools and insect colonies, bio-inspired swarm approaches enable distributed decision-making, adaptability, and resilience under challenging marine conditions. Yet research in this field remains fragmented, with limited integration across algorithmic, communication, and hardware design perspectives. This review synthesises bio-inspired coordination mechanisms, communication strategies, and system design considerations for underwater swarm robotics. It examines key marine-specific algorithms, including the Artificial Fish Swarm Algorithm, Whale Optimisation Algorithm, Coral Reef Optimisation, and Marine Predators Algorithm, highlighting their applications in formation control, task allocation, and environmental interaction. The review also analyses communication constraints unique to the underwater domain and emerging acoustic, optical, and hybrid solutions that support cooperative operation. Additionally, it examines hardware and system design advances that enhance system efficiency and scalability. A multi-dimensional classification framework evaluates existing approaches across communication dependency, environmental adaptability, energy efficiency, and swarm scalability. Through this integrated analysis, the review unifies bio-inspired coordination algorithms, communication modalities, and system design approaches. It also identifies converging trends, key challenges, and future research directions for real-world deployment of underwater swarm systems.

</details>


### [379] [An Efficient and Multi-Modal Navigation System with One-Step World Model](https://arxiv.org/abs/2601.12277)
*Wangtian Shen,Ziyang Meng,Jinming Ma,Mingliang Zhou,Diyun Xiang*

Main category: cs.RO

TL;DR: Lightweight navigation world model using one-step generation and 3D U-Net with spatial-temporal attention enables real-time robot navigation with superior efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Current learning-based navigation policies struggle with 3D spatial reasoning and physical dynamics understanding. Existing world models using transformers with multi-step diffusion and autoregressive generation have prohibitive computational latency, making real-time deployment impossible.

Method: Proposes a lightweight navigation world model with one-step generation paradigm and 3D U-Net backbone with efficient spatial-temporal attention. Integrated into optimization-based planning framework using anchor-based initialization for multi-modal goal navigation tasks.

Result: Drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. Extensive closed-loop experiments in simulation and real-world demonstrate superior efficiency and robustness compared to state-of-the-art baselines.

Conclusion: The proposed lightweight navigation world model successfully addresses the computational bottleneck of existing approaches, enabling real-time deployment while maintaining high performance in complex navigation tasks.

Abstract: Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.

</details>


### [380] [OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization](https://arxiv.org/abs/2601.12291)
*Jianhao Jiao,Changkun Liu,Jingwen Yu,Boyi Liu,Qianyi Zhang,Yue Wang,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: OPENNAVMAP is a lightweight, structure-free topometric mapping system using 3D geometric foundation models for on-demand reconstruction, achieving robust multi-session map alignment without pre-built 3D models.


<details>
  <summary>Details</summary>
Motivation: Traditional structure-based mapping methods have high maintenance costs and fail in feature-less environments or under significant viewpoint changes, limiting scalable deployment of robots in real-world environments.

Method: Unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization for coarse-to-fine submap alignment, leveraging 3D geometric foundation models for on-demand reconstruction without requiring pre-built 3D models.

Result: Achieves average translation error of 0.62m on Map-Free benchmark, maintains global consistency across 15km of multi-session data with absolute trajectory error below 3m for map merging, and successfully completes 12 autonomous image-goal navigation tasks.

Conclusion: OPENNAVMAP provides a scalable, maintainable solution for large-scale visual navigation that overcomes limitations of traditional structure-based methods, enabling robust deployment in real-world environments.

Abstract: Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.

</details>


### [381] [From Shallow Waters to Mariana Trench: A Survey of Bio-inspired Underwater Soft Robots](https://arxiv.org/abs/2601.12353)
*Jie Wang,Peng Du,Yiyuan Zhang,Zhexin Xie,Cecilia Laschi*

Main category: cs.RO

TL;DR: Review of underwater bio-inspired soft robots for ocean exploration, covering design considerations, bio-inspirations, environmental factors, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional underwater robots struggle with extreme water pressure, cause noise and ecosystem damage, while bio-inspired soft robots can address these challenges by mimicking aquatic creatures for better pressure resistance, reduced drag, efficient operation, and eco-friendly interaction.

Method: Literature review approach analyzing recent advancements in underwater bio-inspired soft robots, examining design considerations for different functions, bio-inspirations, and environmental factors (pressure, temperature, light, biodiversity).

Result: Bio-inspired soft robots have emerged as a promising field for ocean exploration, capable of withstanding high water pressure, minimizing drag, operating efficiently, and interacting with the environment in an eco-friendly manner.

Conclusion: The paper explores the progression from bio-inspired principles to practical applications and suggests potential directions for developing the next generation of underwater soft robots.

Abstract: Sample Exploring the ocean environment holds profound significance in areas such as resource exploration and ecological protection. Underwater robots struggle with extreme water pressure and often cause noise and damage to the underwater ecosystem, while bio-inspired soft robots draw inspiration from aquatic creatures to address these challenges. These bio-inspired approaches enable robots to withstand high water pressure, minimize drag, operate with efficient manipulation and sensing systems, and interact with the environment in an eco-friendly manner. Consequently, bio-inspired soft robots have emerged as a promising field for ocean exploration. This paper reviews recent advancements in underwater bio-inspired soft robots, analyses their design considerations when facing different desired functions, bio-inspirations, ambient pressure, temperature, light, and biodiversity , and finally explores the progression from bio-inspired principles to practical applications in the field and suggests potential directions for developing the next generation of underwater soft robots.

</details>


### [382] [R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry](https://arxiv.org/abs/2601.12377)
*Haobo Xi,Shiyong Zhang,Qianli Dong,Yunze Tong,Songyang Wu,Jing Yuan,Xuebo Zhang*

Main category: cs.RO

TL;DR: R-VoxelMap improves voxel mapping for LiDAR odometry using recursive plane fitting with outlier handling to enhance localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing voxel mapping methods like VoxelMap suffer from plane parameter deviation due to outliers, over-segmentation of large planes, and incorrect merging of different physical planes, which degrade localization accuracy.

Method: Uses geometry-driven recursive construction with outlier detect-and-reuse pipeline: 1) Fit accurate planes while separating outliers using RANSAC, 2) Propagate outliers to deeper octree levels for recursive processing, 3) Implement point distribution-based validity check to prevent erroneous plane merging.

Result: Extensive experiments on diverse LiDAR(-inertial) SLAM datasets show R-VoxelMap achieves higher accuracy than state-of-the-art approaches with comparable efficiency and memory usage.

Conclusion: R-VoxelMap provides a more accurate voxel mapping solution for LiDAR odometry through recursive plane fitting and robust outlier handling, with code to be made publicly available.

Abstract: This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry. VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes. To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline. Specifically, for each voxel, accurate planes are first fitted while separating outliers using random sample consensus (RANSAC). The remaining outliers are then propagated to deeper octree levels for recursive processing, ensuring a detailed representation of the environment. In addition, a point distribution-based validity check algorithm is devised to prevent erroneous plane merging. Extensive experiments on diverse open-source LiDAR(-inertial) simultaneous localization and mapping (SLAM) datasets validate that our method achieves higher accuracy than other state-of-the-art approaches, with comparable efficiency and memory usage. Code will be available on GitHub.

</details>


### [383] [VR$^2$: A Co-Located Dual-Headset Platform for Touch-Enabled Human-Robot Interaction Research](https://arxiv.org/abs/2601.12395)
*Chao Wang,Anna Belardinelli,Michael Gienger*

Main category: cs.RO

TL;DR: VR2VR is a dual VR-headset platform for touch-rich HRI research that enables physical co-presence between participant and operator while allowing different virtual embodiments and controlled nonverbal communication channels.


<details>
  <summary>Details</summary>
Motivation: Studying touch-rich human-robot interaction is challenging due to high costs of physical robots and limitations of VR-based approaches that often remove physical contact or decouple the agent's body from user's felt touch.

Method: A co-located dual VR-headset platform where participant and hidden operator share physical space but experience different virtual embodiments. Operator's motion and facial signals are mapped to virtual robot in real time, enabling physical touch through calibrated coordinate frames and inverse kinematics for precise contact.

Result: The system enables faithful motion retargeting for limb teleoperation and experimental control by selectively enabling nonverbal channels (head, eyes, facial expressions) while maintaining constant physical interaction, demonstrated through a touch-based Wizard-of-Oz HRI study.

Conclusion: VR2VR lowers barriers for rapidly prototyping and rigorously evaluating embodied, touch-centric robot behaviors by combining physical co-presence with virtual embodiment flexibility and controlled experimental conditions.

Abstract: Touch-rich human-robot interaction (HRI) is difficult to study: building and programming physical robots is costly and slow, while VR-based robot prototypes often remove physical contact or break the tight coupling between an agent's body and the user's felt touch. We present VR2VR, a co-located dual VR-headset platform for HRI research in which a participant and a hidden operator share the same physical space while experiencing different virtual embodiments. The participant sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot's upper-body gestures, head and gaze behaviors, and facial expressions are mapped from the operator's tracked motion and face signals. Because the operator is physically co-present and calibrated into the same coordinate frame, the operator can also physically touch the participant, enabling the participant to perceive robot touch aligned with the robot's hands; finger and hand motion are mapped to the robot using inverse kinematics to support precise contact. Beyond faithful motion retargeting for limb teleoperation, our VR2VR system supports experimental control by retargeting or selectively enabling nonverbal channels (e.g., head only vs. head+eyes vs. head+eyes+facial expressions) while keeping physical interaction constant. We detail the system design, calibration workflow, and safety considerations, and demonstrate the platform through a touch-based Wizard-of-Oz HRI study, illustrating how VR2VR lowers barriers for rapidly prototyping and rigorously evaluating embodied, touch-centric robot behaviors.

</details>


### [384] [Learning Diverse Skills for Behavior Models with Mixture of Experts](https://arxiv.org/abs/2601.12397)
*Wangtian Shen,Jinming Ma,Mingliang Zhou,Ziyang Meng*

Main category: cs.RO

TL;DR: Di-BM uses Mixture of Experts with energy-based models to learn diverse skills in imitation learning, preventing performance degradation in multi-task robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning models perform well on single tasks but degrade in multi-task settings due to interference across tasks causing an averaging effect.

Method: Proposes Di-BM that learns diverse skills using Mixture of Experts, where each expert specializes in sub-regions of observation space using energy-based models for observation distributions, jointly trained with action models.

Result: Di-BM significantly outperforms state-of-the-art baselines on multiple real-world robotic manipulation tasks, and shows superior data efficiency and reusable knowledge when fine-tuned on novel tasks.

Conclusion: Di-BM effectively addresses multi-task interference in imitation learning through specialized experts, demonstrating strong performance and transferability in robotic manipulation.

Abstract: Imitation learning has demonstrated strong performance in robotic manipulation by learning from large-scale human demonstrations. While existing models excel at single-task learning, it is observed in practical applications that their performance degrades in the multi-task setting, where interference across tasks leads to an averaging effect. To address this issue, we propose to learn diverse skills for behavior models with Mixture of Experts, referred to as Di-BM. Di-BM associates each expert with a distinct observation distribution, enabling experts to specialize in sub-regions of the observation space. Specifically, we employ energy-based models to represent expert-specific observation distributions and jointly train them alongside the corresponding action models. Our approach is plug-and-play and can be seamlessly integrated into standard imitation learning methods. Extensive experiments on multiple real-world robotic manipulation tasks demonstrate that Di-BM significantly outperforms state-of-the-art baselines. Moreover, fine-tuning the pretrained Di-BM on novel tasks exhibits superior data efficiency and the reusable of expert-learned knowledge. Code is available at https://github.com/robotnav-bot/Di-BM.

</details>


### [385] [ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models](https://arxiv.org/abs/2601.12428)
*Baorui Peng,Wenyao Zhang,Liang Xu,Zekun Qi,Jiazhao Zhang,Hongsi Liu,Wenjun Zeng,Xin Jin*

Main category: cs.RO

TL;DR: ReWorld: A framework that uses reinforcement learning to align video-based world models with physical realism, task logic, and human preferences for contact-rich manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current video-based world models focus too much on visual quality while ignoring physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, limiting their downstream applicability.

Method: 1) Construct a large-scale video preference dataset (~235K), 2) Train hierarchical reward model capturing multi-dimensional rewards aligned with human preferences, 3) Propose alignment algorithm that post-trains flow-based world models using reward via efficient PPO-style algorithm.

Result: ReWorld significantly improves physical fidelity, logical coherence, embodiment, and visual quality of generated rollouts, outperforming previous methods.

Conclusion: The framework successfully aligns video-based embodied world models with physical realism and task completion capability through reinforcement learning and human preference alignment.

Abstract: Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.

</details>


### [386] [KILO-EKF: Koopman-Inspired Learned Observations Extended Kalman Filter](https://arxiv.org/abs/2601.12463)
*Zi Cong Guo,James R. Forbes,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: KILO-EKF combines EKF prediction with learned Koopman-inspired measurement models, enabling flexible sensor modeling without explicit calibration while maintaining recursive filtering efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional EKFs require explicit parametric sensor models and calibration, which can be challenging for complex or poorly calibrated sensors. There's a need for flexible sensor modeling that retains the efficiency of recursive filtering.

Method: KILO-EKF uses a standard EKF prediction step with a learned Koopman-inspired measurement model. Measurements are lifted into a feature space where they become linear in the state, creating a linear-Gaussian measurement model learned in closed form from groundtruth data without iterative optimization.

Result: On quadrotor localization using IMU, UWB, and laser sensors, KILO-EKF outperformed data-calibrated EKF baselines in accuracy and consistency, and significantly beat EKFs relying on imperfect geometric models, while maintaining real-time inference and fast training.

Conclusion: Koopman-inspired measurement learning provides a scalable alternative to traditional model-based calibration, enabling effective handling of complex sensors while preserving recursive filtering efficiency.

Abstract: We present the Koopman-Inspired Learned Observations Extended Kalman Filter (KILO-EKF), which combines a standard EKF prediction step with a correction step based on a Koopman-inspired measurement model learned from data. By lifting measurements into a feature space where they are linear in the state, KILO-EKF enables flexible modeling of complex or poorly calibrated sensors while retaining the structure and efficiency of recursive filtering. The resulting linear-Gaussian measurement model is learned in closed form from groundtruth training data, without iterative optimization or reliance on an explicit parametric sensor model. At inference, KILO-EKF performs a standard EKF update using Jacobians obtained via the learned lifting. We validate the approach on a real-world quadrotor localization task using an IMU, ultra-wideband (UWB) sensors, and a downward-facing laser. We compare against multiple EKF baselines with varying levels of sensor calibration. KILO-EKF achieves better accuracy and consistency compared to data-calibrated baselines, and significantly outperforms EKFs that rely on imperfect geometric models, while maintaining real-time inference and fast training. These results demonstrate the effectiveness of Koopman-inspired measurement learning as a scalable alternative to traditional model-based calibration.

</details>


### [387] [Language-Based Swarm Perception: Decentralized Person Re-Identification via Natural Language Descriptions](https://arxiv.org/abs/2601.12479)
*Miquel Kegeleirs,Lorenzo Garattoni,Gianpiero Francesca,Mauro Birattari*

Main category: cs.RO

TL;DR: Decentralized person re-identification method for robot swarms using natural language descriptions instead of visual embeddings, enabling interpretable perception and querying.


<details>
  <summary>Details</summary>
Motivation: Traditional visual embeddings are opaque and lack interpretability. The paper aims to create a transparent, explainable person re-identification system for robot swarms using human-readable language representations.

Method: Each robot uses a vision-language model (VLM) to generate textual descriptions of detected individuals. These descriptions are compared and clustered across the swarm without centralized coordination. Language models then distill clusters into representative descriptions for interpretable summaries.

Result: Preliminary experiments show competitive performance in identity consistency and interpretability compared to embedding-based methods, though with current limitations in text similarity and computational load.

Conclusion: Language-based representation enables natural-language querying, enhances transparency, and supports explainable swarm behavior. Future work includes refined similarity metrics, semantic navigation, and extending language-based perception to environmental elements.

Abstract: We introduce a method for decentralized person re-identification in robot swarms that leverages natural language as the primary representational modality. Unlike traditional approaches that rely on opaque visual embeddings -- high-dimensional feature vectors extracted from images -- the proposed method uses human-readable language to represent observations. Each robot locally detects and describes individuals using a vision-language model (VLM), producing textual descriptions of appearance instead of feature vectors. These descriptions are compared and clustered across the swarm without centralized coordination, allowing robots to collaboratively group observations of the same individual. Each cluster is distilled into a representative description by a language model, providing an interpretable, concise summary of the swarm's collective perception. This approach enables natural-language querying, enhances transparency, and supports explainable swarm behavior. Preliminary experiments demonstrate competitive performance in identity consistency and interpretability compared to embedding-based methods, despite current limitations in text similarity and computational load. Ongoing work explores refined similarity metrics, semantic navigation, and the extension of language-based perception to environmental elements. This work prioritizes decentralized perception and communication, while active navigation remains an open direction for future study.

</details>


### [388] [Enabling High-Curvature Navigation in Eversion Robots through Buckle-Inducing Constrictive Bands](https://arxiv.org/abs/2601.12523)
*Cem Suulker,Muhie Al Haimus,Thomas Mack,Mohammad Sheikhsofla,Neri Niccolò Dei,Reza Kashef,Hadi Sadati,Federica Barontini,Fanny Ficuciello,Alberto Arezzo,Bruno Siciliano,Sebastien Ourselin,Kaspar Althoefer*

Main category: cs.RO

TL;DR: Passive buckling bands reduce eversion robot bending stiffness by 91%, enabling navigation through tighter 25mm radius bends without active steering or sacrificing softness.


<details>
  <summary>Details</summary>
Motivation: Existing eversion robot navigation solutions use artificial muscles or active tip-steering, which add complexity and compromise the robots' inherent softness and compliance advantages.

Method: Introduce passive buckling points using inextensible diameter-reducing circumferential bands at regular intervals along the robot body, leveraging natural environment interaction rather than active steering. Develop Cosserat rod-based mathematical model to quantify local stiffness reductions.

Result: Bands reduce tip bending stiffness by up to 91%, enabling consistent traversal of 180° bends with 25mm radius (vs. 35mm for standard robots). Demonstrated feasibility in colon phantom case study.

Conclusion: Passive band approach significantly improves maneuverability without sacrificing softness or increasing mechanical complexity, expanding eversion robot applicability in highly curved pathways for pipe inspection and medical procedures.

Abstract: Tip-growing eversion robots are renowned for their ability to access remote spaces through narrow passages. However, achieving reliable navigation remains a significant challenge. Existing solutions often rely on artificial muscles integrated into the robot body or active tip-steering mechanisms. While effective, these additions introduce structural complexity and compromise the defining advantages of eversion robots: their inherent softness and compliance. In this paper, we propose a passive approach to reduce bending stiffness by purposefully introducing buckling points along the robot's outer wall. We achieve this by integrating inextensible diameter-reducing circumferential bands at regular intervals along the robot body facilitating forward motion through tortuous, obstacle cluttered paths. Rather than relying on active steering, our approach leverages the robot's natural interaction with the environment, allowing for smooth, compliant navigation. We present a Cosserat rod-based mathematical model to quantify this behavior, capturing the local stiffness reductions caused by the constricting bands and their impact on global bending mechanics. Experimental results demonstrate that these bands reduce the robot's stiffness when bent at the tip by up to 91 percent, enabling consistent traversal of 180 degree bends with a bending radius of as low as 25 mm-notably lower than the 35 mm achievable by standard eversion robots under identical conditions. The feasibility of the proposed method is further demonstrated through a case study in a colon phantom. By significantly improving maneuverability without sacrificing softness or increasing mechanical complexity, this approach expands the applicability of eversion robots in highly curved pathways, whether in relation to pipe inspection or medical procedures such as colonoscopy.

</details>


### [389] [RPT*: Global Planning with Probabilistic Terminals for Target Search in Complex Environments](https://arxiv.org/abs/2601.12701)
*Yunpeng Lyu,Chao Cao,Ji Zhang,Howie Choset,Zhongqiang Ren*

Main category: cs.RO

TL;DR: This paper introduces HPP-PT, a variant of Hamiltonian Path Problem where vertices have termination probabilities, and proposes RPT* algorithm with optimality guarantees for efficient target search in robotics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for routing algorithms that handle uncertainty in target search scenarios, where a robot must visit candidate locations to find an object with probabilistic prior knowledge of its location. Existing routing problems don't adequately handle the history-dependent expected cost calculations required for probabilistic termination.

Method: Proposes RPT*, a search-based approach using dynamic programming in a new state space to bypass history dependency, with novel heuristics for computational speed. Extends to HATS system combining RPT* with Bayesian filtering for lifelong search or autonomous exploration for unknown environments.

Result: Experiments in simulation and real robots show the approach naturally balances exploitation and exploration, finding targets more quickly on average than baseline methods.

Conclusion: The paper successfully addresses the HPP-PT problem with an optimal algorithm that handles uncertainty in target search scenarios, demonstrating practical effectiveness in robotic applications through both theoretical guarantees and empirical validation.

Abstract: Routing problems such as Hamiltonian Path Problem (HPP), seeks a path to visit all the vertices in a graph while minimizing the path cost. This paper studies a variant, HPP with Probabilistic Terminals (HPP-PT), where each vertex has a probability representing the likelihood that the robot's path terminates there, and the objective is to minimize the expected path cost. HPP-PT arises in target object search, where a mobile robot must visit all candidate locations to find an object, and prior knowledge of the object's location is expressed as vertex probabilities. While routing problems have been studied for decades, few of them consider uncertainty as required in this work. The challenge lies not only in optimally ordering the vertices, as in standard HPP, but also in handling history dependency: the expected path cost depends on the order in which vertices were previously visited. This makes many existing methods inefficient or inapplicable. To address the challenge, we propose a search-based approach RPT* with solution optimality guarantees, which leverages dynamic programming in a new state space to bypass the history dependency and novel heuristics to speed up the computation. Building on RPT*, we design a Hierarchical Autonomous Target Search (HATS) system that combines RPT* with either Bayesian filtering for lifelong target search with noisy sensors, or autonomous exploration to find targets in unknown environments. Experiments in both simulation and real robot show that our approach can naturally balance between exploitation and exploration, thereby finding targets more quickly on average than baseline methods.

</details>


### [390] [AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation](https://arxiv.org/abs/2601.12742)
*Xuecheng Chen,Zongzhuo Liu,Jianfa Ma,Bang Du,Tiantian Zhang,Xueqian Wang,Boyu Zhou*

Main category: cs.RO

TL;DR: AirHunt is an aerial object navigation system that efficiently locates open-set objects in outdoor environments by fusing VLM semantic reasoning with continuous path planning through a dual-pathway asynchronous architecture.


<details>
  <summary>Details</summary>
Motivation: Prior aerial systems struggle to integrate Vision-Language Models (VLMs) due to frequency mismatch between VLM inference and real-time planning, limited 3D scene understanding in VLMs, and lack of unified mechanisms to balance semantic guidance with motion efficiency in large-scale environments.

Method: AirHunt features: 1) Dual-pathway asynchronous architecture for synergistic interface between VLM reasoning and path planning, 2) Active dual-task reasoning module exploiting geometric and semantic redundancy for selective VLM querying, 3) Semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework.

Result: AirHunt demonstrates higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods across diverse object navigation tasks and environments. Real-world experiments validate its practical capability in complex and challenging environments.

Conclusion: AirHunt successfully addresses the integration challenges of VLMs in aerial systems by enabling efficient open-set object navigation with zero-shot generalization through seamless fusion of semantic reasoning and continuous path planning.

Abstract: Recent advances in large Vision-Language Models (VLMs) have provided rich semantic understanding that empowers drones to search for open-set objects via natural language instructions. However, prior systems struggle to integrate VLMs into practical aerial systems due to orders-of-magnitude frequency mismatch between VLM inference and real-time planning, as well as VLMs' limited 3D scene understanding. They also lack a unified mechanism to balance semantic guidance with motion efficiency in large-scale environments. To address these challenges, we present AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing VLM semantic reasoning with continuous path planning. AirHunt features a dual-pathway asynchronous architecture that establishes a synergistic interface between VLM reasoning and path planning, enabling continuous flight with adaptive semantic guidance that evolves through motion. Moreover, we propose an active dual-task reasoning module that exploits geometric and semantic redundancy to enable selective VLM querying, and a semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework, enabling seamless adaptation to environmental heterogeneity. We evaluate AirHunt across diverse object navigation tasks and environments, demonstrating a higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods. Real-world experiments further validate AirHunt's practical capability in complex and challenging environments. Code and dataset will be made publicly available before publication.

</details>


### [391] [FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation](https://arxiv.org/abs/2601.12790)
*Yang Zhang,Jianming Ma,Liyun Yan,Zhanxiang Cao,Yazhou Zhang,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: FocusNav is a spatial selective attention framework for humanoid robot navigation that adaptively modulates perceptual focus based on navigational intent and real-time stability, improving performance in unstructured dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Robust local navigation in unstructured and dynamic environments is challenging for humanoid robots, requiring balance between long-range navigation targets and immediate motion stability.

Method: FocusNav uses Waypoint-Guided Spatial Cross-Attention (WGSCA) to anchor environmental feature aggregation to predicted collision-free waypoints, and Stability-Aware Selective Gating (SASG) to truncate distal information during instability, prioritizing immediate foothold safety.

Result: Extensive experiments on Unitree G1 humanoid robot show FocusNav significantly improves navigation success rates in challenging scenarios, outperforming baselines in both collision avoidance and motion stability.

Conclusion: FocusNav achieves robust navigation in dynamic and complex environments by adaptively modulating perceptual focus based on navigational intent and real-time stability.

Abstract: Robust local navigation in unstructured and dynamic environments remains a significant challenge for humanoid robots, requiring a delicate balance between long-range navigation targets and immediate motion stability. In this paper, we propose FocusNav, a spatial selective attention framework that adaptively modulates the robot's perceptual field based on navigational intent and real-time stability. FocusNav features a Waypoint-Guided Spatial Cross-Attention (WGSCA) mechanism that anchors environmental feature aggregation to a sequence of predicted collision-free waypoints, ensuring task-relevant perception along the planned trajectory. To enhance robustness in complex terrains, the Stability-Aware Selective Gating (SASG) module autonomously truncates distal information when detecting instability, compelling the policy to prioritize immediate foothold safety. Extensive experiments on the Unitree G1 humanoid robot demonstrate that FocusNav significantly improves navigation success rates in challenging scenarios, outperforming baselines in both collision avoidance and motion stability, achieving robust navigation in dynamic and complex environments.

</details>


### [392] [Contact-Aware Neural Dynamics](https://arxiv.org/abs/2601.12796)
*Changwei Jing,Jai Krishna Bandi,Jianglong Ye,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: Learning contact-aware neural dynamics model to align simulator with real-world using tactile data for better sim-to-real transfer in contact-rich robotic tasks.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap persists in robotic learning, especially for contact-rich tasks with complex, dynamic, discontinuous interactions. Explicit system identification with parameter tuning is insufficient for aligning high-dimensional, state-dependent real-world dynamics.

Method: Proposes implicit sim-to-real alignment framework that treats off-the-shelf simulator as base prior and learns contact-aware neural dynamics model to refine simulated states using real-world observations, particularly tactile contact information from robotic hands.

Result: The learned forward dynamics model improves state prediction accuracy and can effectively predict policy performance and refine policies trained purely in standard simulators.

Conclusion: Offers a scalable, data-driven approach to sim-to-real alignment that handles non-smooth discontinuities in contact-rich tasks through neural dynamics models grounded by real-world tactile data.

Abstract: High-fidelity physics simulation is essential for scalable robotic learning, but the sim-to-real gap persists, especially for tasks involving complex, dynamic, and discontinuous interactions like physical contacts. Explicit system identification, which tunes explicit simulator parameters, is often insufficient to align the intricate, high-dimensional, and state-dependent dynamics of the real world. To overcome this, we propose an implicit sim-to-real alignment framework that learns to directly align the simulator's dynamics with contact information. Our method treats the off-the-shelf simulator as a base prior and learns a contact-aware neural dynamics model to refine simulated states using real-world observations. We show that using tactile contact information from robotic hands can effectively model the non-smooth discontinuities inherent in contact-rich tasks, resulting in a neural dynamics model grounded by real-world data. We demonstrate that this learned forward dynamics model improves state prediction accuracy and can be effectively used to predict policy performance and refine policies trained purely in standard simulators, offering a scalable, data-driven approach to sim-to-real alignment.

</details>


### [393] [FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions](https://arxiv.org/abs/2601.12799)
*Peng Li,Zihan Zhuang,Yangfan Gao,Yi Dong,Sixian Li,Changhao Jiang,Shihan Dou,Zhiheng Xi,Enyu Zhou,Jixuan Huang,Hui Li,Jingjing Gong,Xingjun Ma,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Xipeng Qiu*

Main category: cs.RO

TL;DR: FRoM-W1 is an open-source framework for general humanoid whole-body motion control using natural language, featuring a two-stage approach with language-driven motion generation and reinforcement learning-based control.


<details>
  <summary>Details</summary>
Motivation: Current humanoid robot motions are often hard-coded or specifically trained, limiting versatility. The authors aim to create a general framework for natural language-driven whole-body motion control that works across different humanoid robots.

Method: Two-stage approach: (1) H-GPT: Large-scale language-driven human motion generation model trained on massive human data, enhanced with Chain-of-Thought technique for better instruction understanding. (2) H-ACT: Motion controller pretrained and fine-tuned through reinforcement learning in simulation, with retargeting of human motions to robot-specific actions and simulation-to-reality deployment.

Result: Superior performance on HumanML3D-X benchmark for human whole-body motion generation. Reinforcement learning fine-tuning consistently improves motion tracking accuracy and task success rates on Unitree H1 and G1 robots.

Conclusion: FRoM-W1 successfully enables general humanoid whole-body motion control via natural language, advancing humanoid intelligence development. The framework is open-sourced to promote further research.

Abstract: Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model's generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.

</details>


### [394] [Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning](https://arxiv.org/abs/2601.12894)
*Kangye Ji,Yuan Meng,Zhou Jianbo,Ye Li,Hanyun Cui,Zhi Wang*

Main category: cs.RO

TL;DR: SAG (Sparse ActionGen) accelerates diffusion policy for real-time visuomotor control by adaptively pruning and reusing computations based on rollout dynamics, achieving 4× speedup without performance loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies are powerful for multi-modal action generation but too slow for real-time control due to multi-step denoising. Existing acceleration methods use static schedules that don't adapt to robot-environment dynamics, leading to suboptimal performance.

Method: SAG uses a rollout-adaptive prune-then-reuse mechanism: 1) globally identifies prunable computations, 2) reuses cached activations during action diffusion. It parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation with efficient design, and implements a one-for-all reusing strategy that reuses activations across timesteps and blocks in zig-zag manner.

Result: Extensive experiments on multiple robotic benchmarks show SAG achieves up to 4× generation speedup without sacrificing performance.

Conclusion: SAG enables real-time visuomotor control with diffusion policies by adaptively pruning computations based on rollout dynamics, making diffusion-based action generation practical for real-time robotic applications.

Abstract: Diffusion Policy has dominated action generation due to its strong capabilities for modeling multi-modal action distributions, but its multi-step denoising processes make it impractical for real-time visuomotor control. Existing caching-based acceleration methods typically rely on $\textit{static}$ schedules that fail to adapt to the $\textit{dynamics}$ of robot-environment interactions, thereby leading to suboptimal performance. In this paper, we propose $\underline{\textbf{S}}$parse $\underline{\textbf{A}}$ction$\underline{\textbf{G}}$en ($\textbf{SAG}$) for extremely sparse action generation. To accommodate the iterative interactions, SAG customizes a rollout-adaptive prune-then-reuse mechanism that first identifies prunable computations globally and then reuses cached activations to substitute them during action diffusion. To capture the rollout dynamics, SAG parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation and instantiates it with a highly parameter- and inference-efficient design for real-time prediction. Furthermore, SAG introduces a one-for-all reusing strategy that reuses activations across both timesteps and blocks in a zig-zag manner, minimizing the global redundancy. Extensive experiments on multiple robotic benchmarks demonstrate that SAG achieves up to 4$\times$ generation speedup without sacrificing performance. Project Page: https://sparse-actiongen.github.io/.

</details>


### [395] [PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning](https://arxiv.org/abs/2601.12901)
*Hongchen Li,Tianyu Li,Jiazhi Yang,Haochen Tian,Caojun Wang,Lei Shi,Mingyang Shang,Zengrong Lin,Gaoqiang Wu,Zhihui Hao,Xianpeng Lang,Jia Hu,Hongyang Li*

Main category: cs.RO

TL;DR: PlannerRFT: A reinforcement fine-tuning framework for diffusion-based autonomous driving planners that improves multi-modal trajectory generation and exploration efficiency through dual-branch optimization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based planners with reinforcement fine-tuning struggle to generate multi-modal, scenario-adaptive trajectories, which limits their ability to efficiently exploit informative rewards during fine-tuning.

Method: Proposes PlannerRFT with dual-branch optimization that simultaneously refines trajectory distribution and adaptively guides denoising process toward promising exploration without altering original inference pipeline. Also develops nuMax simulator for 10x faster parallel learning.

Result: Extensive experiments show PlannerRFT achieves state-of-the-art performance with distinct behaviors emerging during learning process.

Conclusion: PlannerRFT provides a sample-efficient reinforcement fine-tuning framework that enhances diffusion-based planners' ability to generate multi-modal, adaptive trajectories while maintaining efficient exploration.

Abstract: Diffusion-based planners have emerged as a promising approach for human-like trajectory generation in autonomous driving. Recent works incorporate reinforcement fine-tuning to enhance the robustness of diffusion planners through reward-oriented optimization in a generation-evaluation loop. However, they struggle to generate multi-modal, scenario-adaptive trajectories, hindering the exploitation efficiency of informative rewards during fine-tuning. To resolve this, we propose PlannerRFT, a sample-efficient reinforcement fine-tuning framework for diffusion-based planners. PlannerRFT adopts a dual-branch optimization that simultaneously refines the trajectory distribution and adaptively guides the denoising process toward more promising exploration, without altering the original inference pipeline. To support parallel learning at scale, we develop nuMax, an optimized simulator that achieves 10 times faster rollout compared to native nuPlan. Extensive experiments shows that PlannerRFT yields state-of-the-art performance with distinct behaviors emerging during the learning process.

</details>


### [396] [Dynamic Hand Gesture Recognition for Robot Manipulator Tasks](https://arxiv.org/abs/2601.12918)
*Dharmendra Sharma,Peeyush Thakur,Sandeep Gupta,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: Unsupervised GMM-based approach for real-time dynamic hand gesture recognition to facilitate human-robot interaction through gesture-assigned robot tasks.


<details>
  <summary>Details</summary>
Motivation: Enable seamless human-robot interaction by allowing robots to recognize dynamic hand gestures that correspond to specific manipulator tasks, despite variations in gesture execution.

Method: Unsupervised model based on Gaussian Mixture Model (GMM) that can recognize multiple dynamic hand gestures with variations in real-time without requiring labeled training data.

Result: High accuracy achieved during both training and real-time testing, demonstrating the effectiveness of the proposed methodology for gesture recognition.

Conclusion: The GMM-based unsupervised approach provides an effective solution for real-time dynamic hand gesture recognition, enabling intuitive human-robot interaction through gesture-controlled robot tasks.

Abstract: This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.

</details>


### [397] [ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation](https://arxiv.org/abs/2601.12925)
*Weize Xie,Yi Ding,Ying He,Leilei Wang,Binwen Bai,Zheyi Zhao,Chenyang Wang,F. Richard Yu*

Main category: cs.RO

TL;DR: ForeDiffusion improves robot manipulation by incorporating future view predictions into diffusion policies, using dual loss optimization to correct trajectory deviations and achieve higher success rates in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion strategies for robot manipulation have two key limitations: they only use short-term observations as conditions, and rely solely on denoising loss which causes error accumulation and grasping deviations, leading to decreased success rates as task complexity increases.

Method: Proposes Foresight-Conditioned Diffusion (ForeDiffusion) that injects predicted future view representations into the diffusion process, making the policy forward-looking. Uses dual loss mechanism combining traditional denoising loss with consistency loss of future observations for unified optimization.

Result: Achieves average success rate of 80% overall, outperforming existing mainstream diffusion methods by 23% in complex tasks on Adroit suite and MetaWorld benchmark, while maintaining more stable performance across all tasks.

Conclusion: ForeDiffusion successfully addresses limitations of current diffusion strategies by incorporating future-conditioning and dual loss optimization, significantly improving robot manipulation performance in complex tasks through forward-looking trajectory correction.

Abstract: Diffusion strategies have advanced visual motor control by progressively denoising high-dimensional action sequences, providing a promising method for robot manipulation. However, as task complexity increases, the success rate of existing baseline models decreases considerably. Analysis indicates that current diffusion strategies are confronted with two limitations. First, these strategies only rely on short-term observations as conditions. Second, the training objective remains limited to a single denoising loss, which leads to error accumulation and causes grasping deviations. To address these limitations, this paper proposes Foresight-Conditioned Diffusion (ForeDiffusion), by injecting the predicted future view representation into the diffusion process. As a result, the policy is guided to be forward-looking, enabling it to correct trajectory deviations. Following this design, ForeDiffusion employs a dual loss mechanism, combining the traditional denoising loss and the consistency loss of future observations, to achieve the unified optimization. Extensive evaluation on the Adroit suite and the MetaWorld benchmark demonstrates that ForeDiffusion achieves an average success rate of 80% for the overall task, significantly outperforming the existing mainstream diffusion methods by 23% in complex tasks, while maintaining more stable performance across the entire tasks.

</details>


### [398] [Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design](https://arxiv.org/abs/2601.12939)
*Kaleem Arshid,Ali Krayani,Lucio Marcenaro,David Martin Gomez,Carlo Regazzoni*

Main category: cs.RO

TL;DR: Active Inference framework enables autonomous UAV swarm trajectory design using probabilistic reasoning and self-learning, outperforming Q-Learning in convergence, stability, and safety.


<details>
  <summary>Details</summary>
Motivation: To develop an intelligent, scalable framework for autonomous UAV swarm control that can handle distributed mission allocation, route ordering, and motion planning in dynamic environments through cognitive grounding and adaptive behavior.

Method: Integrates Active Inference with probabilistic reasoning and self-learning. Uses expert trajectories from Genetic Algorithm with Repulsion Forces (GA-RF) to train hierarchical World Model capturing swarm behavior at mission, route, and motion levels. During operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states.

Result: Simulation results demonstrate faster convergence, higher stability, and safer navigation compared to Q-Learning, showing scalability and cognitive grounding for intelligent UAV swarm control.

Conclusion: The Active Inference-based framework provides an effective approach for autonomous trajectory design in UAV swarms, enabling adaptive responses to dynamic environments through probabilistic reasoning and self-learning mechanisms.

Abstract: This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.

</details>


### [399] [Imitation learning-based spacecraft rendezvous and docking method with Expert Demonstration](https://arxiv.org/abs/2601.12952)
*Shibo Shao,Dong Zhou,Guanghui Sun,Liwen Zhang,Mingxuan Jiang*

Main category: cs.RO

TL;DR: IL-SRD: Imitation Learning-based spacecraft rendezvous & docking control framework that learns from expert demonstrations, uses anchored decoder targets for physically consistent control, and incorporates temporal aggregation to reduce error accumulation in Transformer models.


<details>
  <summary>Details</summary>
Motivation: Existing spacecraft rendezvous/docking methods rely heavily on predefined dynamic models and lack robustness in real on-orbit environments. Need for model-free approaches that reduce dependence on accurate modeling.

Method: Proposes IL-SRD framework with anchored decoder target mechanism (conditions decoder queries on state-related anchors to constrain control generation) and temporal aggregation mechanism (mitigates error accumulation in sequential Transformer predictions).

Result: Achieves accurate and energy-efficient model-free rendezvous and docking control. Robustness evaluations show competitive performance under significant unknown disturbances.

Conclusion: IL-SRD provides a robust, model-free solution for spacecraft rendezvous and docking that reduces dependence on accurate modeling while maintaining performance under disturbances.

Abstract: Existing spacecraft rendezvous and docking control methods largely rely on predefined dynamic models and often exhibit limited robustness in realistic on-orbit environments. To address this issue, this paper proposes an Imitation Learning-based spacecraft rendezvous and docking control framework (IL-SRD) that directly learns control policies from expert demonstrations, thereby reducing dependence on accurate modeling. We propose an anchored decoder target mechanism, which conditions the decoder queries on state-related anchors to explicitly constrain the control generation process. This mechanism enforces physically consistent control evolution and effectively suppresses implausible action deviations in sequential prediction, enabling reliable six-degree-of-freedom (6-DOF) rendezvous and docking control. To further enhance stability, a temporal aggregation mechanism is incorporated to mitigate error accumulation caused by the sequential prediction nature of Transformer-based models, where small inaccuracies at each time step can propagate and amplify over long horizons. Extensive simulation results demonstrate that the proposed IL-SRD framework achieves accurate and energy-efficient model-free rendezvous and docking control. Robustness evaluations further confirm its capability to maintain competitive performance under significant unknown disturbances. The source code is available at https://github.com/Dongzhou-1996/IL-SRD.

</details>


### [400] [Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization](https://arxiv.org/abs/2601.12993)
*Hao Luo,Ye Wang,Wanpeng Zhang,Sipeng Zheng,Ziheng Xi,Chaoyi Xu,Haiweng Xu,Haoqi Yuan,Chi Zhang,Yiqing Wang,Yicheng Feng,Zongqing Lu*

Main category: cs.RO

TL;DR: Being-H0.5 is a Vision-Language-Action model that achieves cross-embodiment generalization by using human interaction data as universal foundation, with novel architectural components for handling diverse robotic platforms.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action models struggle with morphological heterogeneity across different robotic platforms and data scarcity. There's a need for models that can generalize across diverse robot embodiments with varying physical structures and control mechanisms.

Method: 1) Human-centric learning paradigm treating human interaction as universal foundation; 2) UniHand-2.0 pre-training with 35,000+ hours of multimodal data across 30 robot embodiments; 3) Unified Action Space mapping heterogeneous controls; 4) Mixture-of-Transformers with Mixture-of-Flow framework; 5) Manifold-Preserving Gating and Universal Async Chunking for real-world stability.

Result: State-of-the-art results on simulated benchmarks: LIBERO (98.9%) and RoboCasa (53.9%). Strong cross-embodiment capabilities demonstrated on five real robotic platforms, showing robust generalization across different robot morphologies.

Conclusion: Being-H0.5 successfully addresses cross-embodiment generalization challenges through human-centric learning, unified action representation, and novel architectural innovations, enabling robust skill transfer across diverse robotic platforms with limited data requirements.

Abstract: We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.

</details>


### [401] [Static Is Not Enough: A Comparative Study of VR and SpaceMouse in Static and Dynamic Teleoperation Tasks](https://arxiv.org/abs/2601.13042)
*Yijun Zhou,Muhan Hou,Kim Baraka*

Main category: cs.RO

TL;DR: VR controllers outperform SpaceMouse for teleoperation in dynamic tasks, with higher success rates, faster execution, lower workload, and better usability.


<details>
  <summary>Details</summary>
Motivation: Prior teleoperation interface evaluations focused on static tasks, but dynamic tasks require reactive control and impose different interface demands. There's a gap in understanding which interfaces work best for dynamic tasks in imitation learning.

Method: Within-subjects study with 25 participants comparing VR controller vs SpaceMouse across two static and two dynamic tasks. Measured success rate, task duration, cumulative success, NASA-TLX workload, SUS usability, and collected open-ended feedback.

Result: VR showed statistically significant advantages: higher success rates (especially on dynamic tasks), shorter successful execution times across all tasks, earlier successes across attempts, significantly lower workload, and higher usability.

Conclusion: VR controllers are superior to SpaceMouse for teleoperation in dynamic tasks, and the researchers are releasing their open-source VR interface to address the gap in available systems for dynamic task teleoperation.

Abstract: Imitation learning relies on high-quality demonstrations, and teleoperation is a primary way to collect them, making teleoperation interface choice crucial for the data. Prior work mainly focused on static tasks, i.e., discrete, segmented motions, yet demonstrations also include dynamic tasks requiring reactive control. As dynamic tasks impose fundamentally different interface demands, insights from static-task evaluations cannot generalize. To address this gap, we conduct a within-subjects study comparing a VR controller and a SpaceMouse across two static and two dynamic tasks ($N=25$). We assess success rate, task duration, cumulative success, alongside NASA-TLX, SUS, and open-ended feedback. Results show statistically significant advantages for VR: higher success rates, particularly on dynamic tasks, shorter successful execution times across tasks, and earlier successes across attempts, with significantly lower workload and higher usability. As existing VR teleoperation systems are rarely open-source or suited for dynamic tasks, we release our VR interface to fill this gap.

</details>


### [402] [Exploiting Light To Enhance The Endurance and Navigation of Lighter-Than-Air Micro-Drones](https://arxiv.org/abs/2601.13088)
*Harry Huang,Talia Xu,Marco Zúñiga Zamalloa*

Main category: cs.RO

TL;DR: Compact LTA drone uses light for both energy harvesting and navigation, enabling sustainable autonomous operation with solar-powered buoyancy and single-beacon light-seeking navigation.


<details>
  <summary>Details</summary>
Motivation: Micro-UAVs have short endurance and unreliable GPS-denied navigation, while LTA drones offer energy-efficient buoyancy but lack integrated solutions for sustained autonomous operations with simple infrastructure.

Method: Threefold approach: (1) high-fidelity simulation framework for LTA aerodynamics and stable configuration selection, (2) solar cell integration on envelope for net-positive energy, (3) point-and-go navigation with three light-seeking algorithms using single light beacon.

Result: System provides 1 minute of flying time for every 4 minutes of energy harvesting at 80klux illumination, and robust single-beacon navigation up to 7m away in indoor/outdoor environments with moderate winds.

Conclusion: The work demonstrates a plausible path toward persistent, autonomous operation for indoor/outdoor monitoring and provides a practical pathway for translating LTA drone promise into self-sustaining aerial systems.

Abstract: Micro-Unmanned Aerial Vehicles (UAVs) are rapidly expanding into tasks from inventory to environmental sensing, yet their short endurance and unreliable navigation in GPS-denied spaces limit deployment. Lighter-Than-Air (LTA) drones offer an energy-efficient alternative: they use a helium envelope to provide buoyancy, which enables near-zero-power drain during hovering and much longer operation. LTAs are promising, but their design is complex, and they lack integrated solutions to enable sustained autonomous operations and navigation with simple, low-infrastructure.
  We propose a compact, self-sustaining LTA drone that uses light for both energy harvesting and navigation. Our contributions are threefold: (i) a high-fidelity simulation framework to analyze LTA aerodynamics and select a stable, efficient configuration; (ii) a framework to integrate solar cells on the envelope to provide net-positive energy; and (iii) a point-and-go navigation system with three light-seeking algorithms operating on a single light beacon.
  Our LTA-analysis, together with the integrated solar panels, not only saves energy while flying, but also enables sustainable operation: providing 1 minute of flying time for every 4 minutes of energy harvesting, under illuminations of 80klux. We also demonstrate robust single-beacon navigation towards a light source that can be up to 7m away, in indoor and outdoor environments, even with moderate winds. The resulting system indicates a plausible path toward persistent, autonomous operation for indoor and outdoor monitoring. More broadly, this work provides a practical pathway for translating the promise of LTA drones into a persistent, self-sustaining aerial system.

</details>


### [403] [LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System](https://arxiv.org/abs/2601.13096)
*Muhayy Ud Din,Waseem Akram,Ahsan B. Bakht,Irfan Hussain*

Main category: cs.RO

TL;DR: LLM+VLM framework enables autonomous maritime port inspection using aerial/surface robots, replacing manual methods with AI-driven planning and semantic perception.


<details>
  <summary>Details</summary>
Motivation: Existing maritime port inspection methods rely on manual operations and conventional computer vision lacking scalability and contextual understanding, creating need for autonomous, intelligent systems.

Method: Integrated framework combining LLMs for symbolic mission planning (natural language to executable plans with dependency graphs) and VLMs for real-time semantic inspection and compliance assessment, deployed on cooperative UAV-USV platforms.

Result: Validated in extended MBZIRC Maritime Simulator with realistic port infrastructure and real-world robotic trials; lightweight on-board design suitable for resource-constrained platforms.

Conclusion: The LLM+VLM fusion enables context-aware, adaptive monitoring for autonomous maritime inspection, advancing development of intelligent inspection systems with structured reporting and safe coordination.

Abstract: Maritime port inspection plays a critical role in ensuring safety, regulatory compliance, and operational efficiency in complex maritime environments. However, existing inspection methods often rely on manual operations and conventional computer vision techniques that lack scalability and contextual understanding. This study introduces a novel integrated engineering framework that utilizes the synergy between Large Language Models (LLMs) and Vision Language Models (VLMs) to enable autonomous maritime port inspection using cooperative aerial and surface robotic platforms. The proposed framework replaces traditional state-machine mission planners with LLM-driven symbolic planning and improved perception pipelines through VLM-based semantic inspection, enabling context-aware and adaptive monitoring. The LLM module translates natural language mission instructions into executable symbolic plans with dependency graphs that encode operational constraints and ensure safe UAV-USV coordination. Meanwhile, the VLM module performs real-time semantic inspection and compliance assessment, generating structured reports with contextual reasoning. The framework was validated using the extended MBZIRC Maritime Simulator with realistic port infrastructure and further assessed through real-world robotic inspection trials. The lightweight on-board design ensures suitability for resource-constrained maritime platforms, advancing the development of intelligent, autonomous inspection systems. Project resources (code and videos) can be found here: https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection

</details>


### [404] [Helical Tendon-Driven Continuum Robot with Programmable Follow-the-Leader Operation](https://arxiv.org/abs/2601.13177)
*Behnam Moradkhani,Raghav Sankaranarayanan,Pejman Kheradmand,Harshith Jella,Nicholas Ahn,Ajmal Zemmar,Yash Chitalia*

Main category: cs.RO

TL;DR: A steerable robotic tool (ExoNav) for precise spinal cord stimulation lead placement uses Cosserat rod modeling to predict shape from tendon forces, enabling follow-the-leader navigation to ventral/lateral epidural targets.


<details>
  <summary>Details</summary>
Motivation: Current manual steering for spinal cord stimulation lead placement in ventral/lateral epidural space is challenging; a robotic solution is needed for precise navigation to motor neuron targets for functional recovery.

Method: Developed ExoNav steerable robotic tool using Cosserat rod framework to model tendon actuation forces and robot shape, incorporating gravity effects, with simulation for follow-the-leader motion planning.

Result: Experimental RMSE values of 1.76-2.33mm for shape prediction; FTL navigation achieved 3.75mm max RMSE; successful phantom model demonstration reaching lateral/ventral spinal cord targets and dorsal root ganglia.

Conclusion: ExoNav robotic system enables precise navigation to ventral/lateral epidural targets for spinal cord stimulation, showing potential for both motor function recovery and pain management applications.

Abstract: Spinal cord stimulation (SCS) is primarily utilized for pain management and has recently demonstrated efficacy in promoting functional recovery in patients with spinal cord injury. Effective stimulation of motor neurons ideally requires the placement of SCS leads in the ventral or lateral epidural space where the corticospinal and rubrospinal motor fibers are located. This poses significant challenges with the current standard of manual steering. In this study, we present a static modeling approach for the ExoNav, a steerable robotic tool designed to facilitate precise navigation to the ventral and lateral epidural space. Cosserat rod framework is employed to establish the relationship between tendon actuation forces and the robot's overall shape. The effects of gravity, as an example of an external load, are investigated and implemented in the model and simulation. The experimental results indicate RMSE values of 1.76mm, 2.33mm, 2.18mm, and 1.33mm across four tested prototypes. Based on the helical shape of the ExoNav upon actuation, it is capable of performing follow-the-leader (FTL) motion by adding insertion and rotation DoFs to this robotic system, which is shown in simulation and experimentally. The proposed simulation has the capability to calculate optimum tendon tensions to follow the desired FTL paths while gravity-induced robot deformations are present. Three FTL experimental trials are conducted and the end-effector position showed repeatable alignments with the desired path with maximum RMSE value of 3.75mm. Ultimately, a phantom model demonstration is conducted where the teleoperated robot successfully navigated to the lateral and ventral spinal cord targets. Additionally, the user was able to navigate to the dorsal root ganglia, illustrating ExoNav's potential in both motor function recovery and pain management.

</details>


### [405] [Active Informative Planning for UAV-based Weed Mapping using Discrete Gaussian Process Representations](https://arxiv.org/abs/2601.13196)
*Jacob Swindell,Marija Popović,Riccardo Polvara*

Main category: cs.RO

TL;DR: Different discrete Gaussian process representations significantly impact UAV weed mapping performance, affecting exploration behavior, efficiency, and computational load in informative path planning.


<details>
  <summary>Details</summary>
Motivation: Accurate weed mapping is crucial for precision farming, but traditional UAV methods use rigid flight paths and offline processing. While Gaussian processes provide continuous weed distribution models with uncertainty, they must be discretized for practical autonomous planning, and the impact of different discretization choices remains poorly understood.

Method: Implemented a receding-horizon informative path planning strategy for UAVs with downward-facing cameras, selecting sampling locations based on map uncertainty, travel cost, and coverage penalties. Investigated multiple discretization strategies for representing GP posteriors and used induced map partitions to generate candidate viewpoints for planning.

Result: Experiments on real-world weed distributions show that representation choice significantly affects exploration behavior and efficiency. Different discretization strategies lead to varying planning dynamics, coverage efficiency, and computational load.

Conclusion: Discretization is not merely a representational detail but a key design choice that shapes planning dynamics, coverage efficiency, and computational load in online UAV weed mapping, with significant implications for practical implementation.

Abstract: Accurate agricultural weed mapping using unmanned aerial vehicles (UAVs) is crucial for precision farming. While traditional methods rely on rigid, pre-defined flight paths and intensive offline processing, informative path planning (IPP) offers a way to collect data adaptively where it is most needed. Gaussian process (GP) mapping provides a continuous model of weed distribution with built-in uncertainty. However, GPs must be discretised for practical use in autonomous planning. Many discretisation techniques exist, but the impact of discrete representation choice remains poorly understood. This paper investigates how different discrete GP representations influence both mapping quality and mission-level performance in UAV-based weed mapping. Considering a UAV equipped with a downward-facing camera, we implement a receding-horizon IPP strategy that selects sampling locations based on the map uncertainty, travel cost, and coverage penalties. We investigate multiple discretisation strategies for representing the GP posterior and use their induced map partitions to generate candidate viewpoints for planning. Experiments on real-world weed distributions show that representation choice significantly affects exploration behaviour and efficiency. Overall, our results demonstrate that discretisation is not only a representational detail but a key design choice that shapes planning dynamics, coverage efficiency, and computational load in online UAV weed mapping.

</details>


### [406] [MATTERIX: toward a digital twin for robotics-assisted chemistry laboratory automation](https://arxiv.org/abs/2601.13232)
*Kourosh Darvish,Arjun Sohal,Abhijoy Mandal,Hatem Fakhruldeen,Nikola Radulov,Zhengxue Zhou,Satheeshkumar Veeramani,Joshua Choi,Sijie Han,Brayden Zhang,Jeeyeoun Chae,Alex Wright,Yijie Wang,Hossein Darvish,Yuchi Zhao,Gary Tom,Han Hao,Miroslav Bogdanovic,Gabriella Pizzuto,Andrew I. Cooper,Alán Aspuru-Guzik,Florian Shkurti,Animesh Garg*

Main category: cs.RO

TL;DR: MATTERIX is a GPU-accelerated robotic simulation framework that creates high-fidelity digital twins of chemistry labs to accelerate workflow development through simulation rather than physical experiments.


<details>
  <summary>Details</summary>
Motivation: Accelerating materials discovery is critical for global challenges, but current laboratory workflow development relies heavily on real-world experimental trials, which hinders scalability due to numerous physical make-and-test iterations.

Method: MATTERIX integrates realistic physics simulation and photorealistic rendering with a modular GPU-accelerated semantics engine that models logical states and continuous behaviors. It uses open-source asset libraries, hierarchical plan definition, and a modular skill library with learning-based methods to simulate chemistry workflows across different abstraction levels.

Result: The framework demonstrates sim-to-real transfer in robotic chemistry setups, reducing reliance on costly real-world experiments and enabling testing of hypothetical automated workflows in silico.

Conclusion: MATTERIX provides a scalable solution for accelerating chemistry laboratory workflow development through high-fidelity digital twins, potentially transforming materials discovery by reducing physical experimentation requirements.

Abstract: Accelerated materials discovery is critical for addressing global challenges. However, developing new laboratory workflows relies heavily on real-world experimental trials, and this can hinder scalability because of the need for numerous physical make-and-test iterations. Here we present MATTERIX, a multiscale, graphics processing unit-accelerated robotic simulation framework designed to create high-fidelity digital twins of chemistry laboratories, thus accelerating workflow development. This multiscale digital twin simulates robotic physical manipulation, powder and liquid dynamics, device functionalities, heat transfer and basic chemical reaction kinetics. This is enabled by integrating realistic physics simulation and photorealistic rendering with a modular graphics processing unit-accelerated semantics engine, which models logical states and continuous behaviors to simulate chemistry workflows across different levels of abstraction. MATTERIX streamlines the creation of digital twin environments through open-source asset libraries and interfaces, while enabling flexible workflow design via hierarchical plan definition and a modular skill library that incorporates learning-based methods. Our approach demonstrates sim-to-real transfer in robotic chemistry setups, reducing reliance on costly real-world experiments and enabling the testing of hypothetical automated workflows in silico. The project website is available at https://accelerationconsortium.github.io/Matterix/ .

</details>


### [407] [Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation](https://arxiv.org/abs/2601.13250)
*Ante Marić,Giammarco Caroleo,Alessandro Albini,Julius Jankowski,Perla Maiolino,Sylvain Calinon*

Main category: cs.RO

TL;DR: Learning inverse tactile sensor model using denoising diffusion for object pose estimation, integrated with particle filtering for multimodal belief tracking without visual data.


<details>
  <summary>Details</summary>
Motivation: Tactile sensing is promising for object pose estimation in visually-limited manipulation settings, but efficiently leveraging tactile data is challenging due to partial observability where single observations correspond to multiple possible contact configurations.

Method: Learn inverse tactile sensor model using denoising diffusion conditioned on tactile observations from distributed sensor, trained in simulation using geometric sensor model based on signed distance fields. Enforce contact constraints during inference via single-step projection using SDF distance/gradient information. Integrate inverse model with particle filter through proposal scheme combining generated hypotheses with prior belief particles.

Result: Validated in simulated and real-world planar pose estimation settings without visual data or tight initial pose priors. Evaluated robustness to unmodeled contact and sensor dynamics for pose tracking in box-pushing scenario. Improves sampling efficiency and estimation accuracy compared to local sampling baselines while preserving multimodal beliefs across objects with varying tactile discriminability.

Conclusion: The proposed approach successfully addresses challenges of tactile-based pose estimation by learning an inverse sensor model with diffusion, enabling efficient multimodal belief tracking in visually-limited manipulation scenarios.

Abstract: Tactile sensing provides a promising sensing modality for object pose estimation in manipulation settings where visual information is limited due to occlusion or environmental effects. However, efficiently leveraging tactile data for estimation remains a challenge due to partial observability, with single observations corresponding to multiple possible contact configurations. This limits conventional estimation approaches largely tailored to vision. We propose to address these challenges by learning an inverse tactile sensor model using denoising diffusion. The model is conditioned on tactile observations from a distributed tactile sensor and trained in simulation using a geometric sensor model based on signed distance fields. Contact constraints are enforced during inference through single-step projection using distance and gradient information from the signed distance field. For online pose estimation, we integrate the inverse model with a particle filter through a proposal scheme that combines generated hypotheses with particles from the prior belief. Our approach is validated in simulated and real-world planar pose estimation settings, without access to visual data or tight initial pose priors. We further evaluate robustness to unmodeled contact and sensor dynamics for pose tracking in a box-pushing scenario. Compared to local sampling baselines, the inverse sensor model improves sampling efficiency and estimation accuracy while preserving multimodal beliefs across objects with varying tactile discriminability.

</details>


### [408] [Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints](https://arxiv.org/abs/2601.13252)
*Mahmud S. Zango,Jianglin Lan*

Main category: cs.RO

TL;DR: Review of nano-UAV autonomous navigation under extreme SWaP constraints (<50g, <100mW), covering sensing, computing, and control architectures, with focus on Edge AI, hardware-software co-design, and gaps in long-term endurance and sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Nano-UAVs face extreme Size, Weight, and Power (SWaP) constraints fundamentally different from standard robotics, requiring specialized approaches for autonomous navigation in GPS-denied environments.

Method: Synthesis and critical analysis of state-of-the-art in sensing, computing, and control architectures for sub-100mW systems, covering transition from geometry-based methods to Edge AI paradigms including quantized DNNs, neuromorphic event-based control, optical flow, optimized SLAM, and learning-based flight control.

Result: Significant progress in visual navigation and relative pose estimation, but persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and sim-to-real transfer of reinforcement learning policies.

Conclusion: Roadmap advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.

Abstract: Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging "Edge AI" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the "Sim-to-Real" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.

</details>


### [409] [CLEAR: A Semantic-Geometric Terrain Abstraction for Large-Scale Unstructured Environments](https://arxiv.org/abs/2601.13361)
*Pranay Meshram,Charuvahan Adhivarahan,Ehsan Tarkesh Esfahani,Souma Chowdhury,Chen Wang,Karthik Dantu*

Main category: cs.RO

TL;DR: CLEAR is a terrain abstraction method that combines boundary-aware spatial decomposition with recursive plane fitting to create convex, semantically aligned regions for efficient long-horizon navigation in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: Existing terrain abstraction methods (grids, quadtrees) fail to scale to tens of km² while preserving both geometric structure and landcover semantics needed for traversability-aware planning, leading to infeasible or unreliable paths for autonomous ground vehicles operating over 10+ km² under real-time constraints.

Method: CLEAR couples boundary-aware spatial decomposition with recursive plane fitting to produce convex, semantically aligned regions that are encoded as a terrain-aware graph representation.

Result: Evaluated on maps spanning 9-100 km² using physics-based simulation, CLEAR achieves up to 10x faster planning than raw grids with only 6.7% cost overhead, and delivers 6-9% shorter, more reliable paths than other abstraction baselines.

Conclusion: CLEAR demonstrates scalability and utility for long-range navigation in applications such as disaster response, defense, and planetary exploration by providing efficient terrain abstractions that preserve both semantic and geometric structure.

Abstract: Long-horizon navigation in unstructured environments demands terrain abstractions that scale to tens of km$^2$ while preserving semantic and geometric structure, a combination existing methods fail to achieve. Grids scale poorly; quadtrees misalign with terrain boundaries; neither encodes landcover semantics essential for traversability-aware planning. This yields infeasible or unreliable paths for autonomous ground vehicles operating over 10+ km$^2$ under real-time constraints. CLEAR (Connected Landcover Elevation Abstract Representation) couples boundary-aware spatial decomposition with recursive plane fitting to produce convex, semantically aligned regions encoded as a terrain-aware graph. Evaluated on maps spanning 9-100~km$^2$ using a physics-based simulator, CLEAR achieves up to 10x faster planning than raw grids with only 6.7% cost overhead and delivers 6-9% shorter, more reliable paths than other abstraction baselines. These results highlight CLEAR's scalability and utility for long-range navigation in applications such as disaster response, defense, and planetary exploration.

</details>


### [410] [Robustness and Resilience Evaluation of Eco-Driving Strategies at Signalized Intersections](https://arxiv.org/abs/2601.13389)
*Zhaohui Liang,Chengyuan Ma,Keke Long,Xiaopeng Li*

Main category: cs.RO

TL;DR: Framework evaluates eco-driving controllers using robustness and resilience metrics, revealing tradeoffs between optimization-based and analytical approaches in real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Existing eco-driving evaluations rely on simplified simulations with assumptions, lacking comprehensive assessment of controller performance under real-world variability and disturbances.

Method: Developed unified framework with formal indicators to quantify performance degradation from internal execution variability and external environmental disturbances, applied through real-world vehicle experiments.

Result: Optimization-based controllers offer more consistent performance across varying disturbance levels, while analytical controllers perform comparably under nominal conditions but show greater sensitivity to execution and timing variability.

Conclusion: The framework reveals key tradeoffs between tracking accuracy and adaptability, providing systematic evaluation of eco-driving controllers' robustness and resilience in real-world conditions.

Abstract: Eco-driving strategies have demonstrated substantial potential for improving energy efficiency and reducing emissions, especially at signalized intersections. However, evaluations of eco-driving methods typically rely on simplified simulation or experimental conditions, where certain assumptions are made to manage complexity and experimental control. This study introduces a unified framework to evaluate eco-driving strategies through the lens of two complementary criteria: control robustness and environmental resilience. We define formal indicators that quantify performance degradation caused by internal execution variability and external environmental disturbances, respectively. These indicators are then applied to assess multiple eco-driving controllers through real-world vehicle experiments. The results reveal key tradeoffs between tracking accuracy and adaptability, showing that optimization-based controllers offer more consistent performance across varying disturbance levels, while analytical controllers may perform comparably under nominal conditions but exhibit greater sensitivity to execution and timing variability.

</details>


### [411] [Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization](https://arxiv.org/abs/2601.13451)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

TL;DR: A novel robotic vision navigation framework combining Hybrid Neural Networks (HNNs) with SNN-based filtering for enhanced obstacle detection and localization in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To develop a neuromorphic navigation system that can operate effectively in unpredictable and dynamic environments by leveraging the complementary strengths of ANNs and SNNs for both accurate environmental understanding and energy-efficient real-time processing.

Method: Dual-pathway architecture: ANN processes static spatial features at low frequency, while SNN handles dynamic event-based sensor data in real-time. Incorporates a pre-developed SNN-based filter that directly uses spike-encoded inputs for localization and state estimation, with anomaly validation using ANN contextual information.

Result: The proposed method achieves acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, operating at a fraction of the resource cost of conventional approaches.

Conclusion: This framework represents a significant advancement in neuromorphic navigation systems, enabling robots to operate effectively in unpredictable and dynamic environments through the synergistic integration of ANN and SNN technologies.

Abstract: This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.

</details>


### [412] [The OncoReach Stylet for Brachytherapy: Design Evaluation and Pilot Study](https://arxiv.org/abs/2601.13529)
*Pejman Kheradmand,Kent K. Yamamoto,Emma Webster,Keith Sowards,Gianna Hatheway,Katharine L. Jackson,Sabino Zani,Julie A. Raffi,Diandra N. Ayala-Peacock,Scott R. Silva,Joanna Deaton Bertram,Yash Chitalia*

Main category: cs.RO

TL;DR: A steerable stylet (OncoReach) for cervical cancer brachytherapy that enables curved needle paths to reach lateral tumor targets from medial entry points.


<details>
  <summary>Details</summary>
Motivation: Current interstitial brachytherapy for cervical cancer uses straight needles, limiting surgical planning to linear paths and restricting access to lateral tumor targets from less-invasive medial entry points.

Method: Developed a handheld, tendon-driven steerable stylet compatible with standard ISBT needles; evaluated design parameters (needle gauge, spherical joint count/placement, asymmetric disk design); used free space experiments and Cosserat rod modeling; integrated best configuration into reusable prototype; tested with patient-derived phantom model.

Result: Successfully demonstrated ability to steer from medial entry points to reach lateral-most targets in phantom experiments, showing improved target accessibility compared to straight needles.

Conclusion: The OncoReach steerable stylet enables curved needle paths for cervical cancer brachytherapy, potentially improving treatment outcomes by allowing access to lateral tumor targets from less-invasive entry points.

Abstract: Cervical cancer accounts for a significant portion of the global cancer burden among women. Interstitial brachytherapy (ISBT) is a standard procedure for treating cervical cancer; it involves placing a radioactive source through a straight hollow needle within or in close proximity to the tumor and surrounding tissue. However, the use of straight needles limits surgical planning to a linear needle path. We present the OncoReach stylet, a handheld, tendon-driven steerable stylet designed for compatibility with standard ISBT 15- and 13-gauge needles. Building upon our prior work, we evaluated design parameters like needle gauge, spherical joint count and spherical joint placement, including an asymmetric disk design to identify a configuration that maximizes bending compliance while retaining axial stiffness. Free space experiments quantified tip deflection across configurations, and a two-tube Cosserat rod model accurately predicted the centerline shape of the needle for most trials. The best performing configuration was integrated into a reusable handheld prototype that enables manual actuation. A patient-derived, multi-composite phantom model of the uterus and pelvis was developed to conduct a pilot study of the OncoReach steerable stylet with one expert user. Results showed the ability to steer from less-invasive, medial entry points to reach the lateral-most targets, underscoring the significance of steerable stylets.

</details>


### [413] [LogicEnvGen: Task-Logic Driven Generation of Diverse Simulated Environments for Embodied AI](https://arxiv.org/abs/2601.13556)
*Jianan Wang,Siyang Zhang,Bin Li,Juan Chen,Jingtao Qi,Zhuo Zhang,Chen Qian*

Main category: cs.RO

TL;DR: LogicEnvGen uses LLMs to generate logically diverse simulated environments as test cases for embodied AI agents, improving fault detection by 4-68%.


<details>
  <summary>Details</summary>
Motivation: Existing environment generation methods focus too much on visual realism while overlooking logical diversity from a testing perspective, limiting comprehensive evaluation of agent adaptability and planning robustness.

Method: LogicEnvGen uses LLMs in a top-down paradigm: analyzes agent task execution logic to construct decision-tree behavior plans, synthesizes logical trajectories, refines them with heuristic algorithms, and instantiates concrete environments with constraint solving for physical plausibility.

Result: LogicEnvGen achieves 1.04-2.61x greater logical diversity than baselines and improves agent fault detection performance by 4.00%-68.00%.

Conclusion: LogicEnvGen effectively bridges the gap in logical diversity for simulated environment generation, providing more comprehensive testing for embodied AI agents through logically diverse test cases.

Abstract: Simulated environments play an essential role in embodied AI, functionally analogous to test cases in software engineering. However, existing environment generation methods often emphasize visual realism (e.g., object diversity and layout coherence), overlooking a crucial aspect: logical diversity from the testing perspective. This limits the comprehensive evaluation of agent adaptability and planning robustness in distinct simulated environments. To bridge this gap, we propose LogicEnvGen, a novel method driven by Large Language Models (LLMs) that adopts a top-down paradigm to generate logically diverse simulated environments as test cases for agents. Given an agent task, LogicEnvGen first analyzes its execution logic to construct decision-tree-structured behavior plans and then synthesizes a set of logical trajectories. Subsequently, it adopts a heuristic algorithm to refine the trajectory set, reducing redundant simulation. For each logical trajectory, which represents a potential task situation, LogicEnvGen correspondingly instantiates a concrete environment. Notably, it employs constraint solving for physical plausibility. Furthermore, we introduce LogicEnvEval, a novel benchmark comprising four quantitative metrics for environment evaluation. Experimental results verify the lack of logical diversity in baselines and demonstrate that LogicEnvGen achieves 1.04-2.61x greater diversity, significantly improving the performance in revealing agent faults by 4.00%-68.00%.

</details>


### [414] [Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction](https://arxiv.org/abs/2601.13574)
*Guanyu Xu,Jiaqi Wang,Dezhong Tong,Xiaonan Huang*

Main category: cs.RO

TL;DR: A soft optical waveguide membrane sensor reconstructs 3D surface geometry by measuring deformation-dependent light signals, achieving real-time shape perception with 1.3mm average error.


<details>
  <summary>Details</summary>
Motivation: Vision-based 3D reconstruction fails under low light/occlusion, and conventional shape-sensing membranes have issues with complexity, limited compliance, and electromagnetic interference.

Method: Soft silicone membrane with optical waveguide sensing using edge-mounted LEDs and distributed photodiodes connected by liquid-metal traces; data-driven model decodes light intensity to reconstruct 3D geometry.

Result: 140mm square membrane achieves 90Hz real-time reconstruction with 1.3mm average Chamfer distance error, handling up to 25mm indentations while maintaining accuracy.

Conclusion: The optical waveguide membrane provides scalable, robust, low-profile global shape perception for deformable robotic systems, overcoming limitations of vision and conventional sensing methods.

Abstract: Reconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.

</details>


### [415] [A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint](https://arxiv.org/abs/2601.13639)
*Deyun Qin,Zezhi Liu,Hanqian Luo,Xiao Liang,Yongchun Fang*

Main category: cs.RO

TL;DR: One-shot multimodal active perception framework for robotic manipulation that directly predicts optimal camera viewpoints using cross-attention fusion of multimodal features, achieving significant grasp success improvements without iterative optimization.


<details>
  <summary>Details</summary>
Motivation: Existing active perception methods rely on iterative optimization (high time/motion costs) and are tightly coupled with task-specific objectives, limiting transferability. Need for general, efficient active perception for robotic manipulation.

Method: Proposes a general one-shot multimodal active perception framework with: 1) data collection pipeline using systematic viewpoint sampling and domain randomization, 2) optimal viewpoint prediction network using cross-attention to align/fuse multimodal features and directly predict camera pose adjustments, 3) decoupled viewpoint quality evaluation supporting heterogeneous tasks.

Result: Framework instantiated in robotic grasping under viewpoint-constrained environments. Active perception guided by framework significantly improves grasp success rates. Real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning.

Conclusion: Proposed general one-shot multimodal active perception framework effectively improves robotic manipulation performance, eliminates iterative optimization overhead, supports heterogeneous tasks, and enables seamless sim-to-real transfer, demonstrating strong practical value for vision-based robotic manipulation.

Abstract: Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.

</details>


### [416] [Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2601.13657)
*Myong-Yol Choi,Hankyoul Ko,Hanse Cho,Changseung Kim,Seunghwan Kim,Jaemin Seo,Hyondong Oh*

Main category: cs.RO

TL;DR: DRL-based controller enables UAV swarm navigation without communication using implicit leader-follower framework where only leader knows goal, followers use LiDAR perception to follow leader and avoid obstacles.


<details>
  <summary>Details</summary>
Motivation: Enable robust collective navigation of UAV swarms in communication-denied environments with complex obstacles, inspired by biological swarms where informed individuals guide groups without explicit communication.

Method: Implicit leader-follower framework with DRL controller trained in GPU-accelerated Nvidia Isaac Sim. Followers use LiDAR point clustering and extended Kalman filter for neighbor tracking, learning emergent behaviors (flocking + obstacle avoidance) from local perception only.

Result: Successfully demonstrated collective navigation with five UAVs in diverse indoor/outdoor environments without communication or external localization, confirming robustness and sim-to-real transfer.

Conclusion: DRL-based approach enables communication-free UAV swarm navigation using only local perception, with proven sim-to-real transfer and robust performance in complex environments.

Abstract: This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.

</details>


### [417] [SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation](https://arxiv.org/abs/2601.13732)
*Andreas Wiedholz,Rafael Paintner,Julian Gleißner,Alwin Hoffmann,Tobias Huber*

Main category: cs.RO

TL;DR: SUNSET is a ROS2-based exemplar for evaluating architecture-based self-adaptation in robotic systems facing multiple concurrent uncertainties with ambiguous root causes.


<details>
  <summary>Details</summary>
Motivation: Robots increasingly operate in dynamic environments with complex software systems, facing uncertainties where symptoms are observable but root causes are ambiguous, and multiple uncertainties can appear concurrently.

Method: SUNSET implements a sensor fusion semantic-segmentation pipeline with a trained ML model, where input preprocessing can be perturbed to induce realistic performance degradations. It exposes five observable symptoms with different possible root causes and supports concurrent uncertainties for self-healing and self-optimization.

Result: The exemplar includes the segmentation pipeline, trained ML model, uncertainty-injection scripts, baseline controller, and comprehensive documentation for reproducible studies and fair comparison.

Conclusion: SUNSET enables rigorous, repeatable evaluation of architecture-based self-adaptation in robotic systems operating amid uncertainties, facilitating research on self-adaptive approaches for complex robotic software.

Abstract: The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison.

</details>


### [418] [RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure](https://arxiv.org/abs/2601.13737)
*Joon Lee,Jeongyoon Han,Doyoung Kim,Seokhwan Jeong*

Main category: cs.RO

TL;DR: Biomimetic robotic hand with flexible palm using Nitinol wires and silicone skin achieves human-like flexibility, 2x payload, and 3x contact area vs rigid designs.


<details>
  <summary>Details</summary>
Motivation: To create a more dexterous, compliant, and anthropomorphic robotic hand for prosthetic and service-robot applications by replicating human hand anatomy and flexibility.

Method: Design replicates carpometacarpal (CMC) joints with full carpal-to-metacarpal anatomy, uses superelastic Nitinol wires throughout skeletal framework for joint restoration and support, implements tendon-driven fingers, and adds flexible silicone skin for increased friction and contact area.

Result: Palm deforms up to 28% (matching human hand flexibility), achieves >2x payload capacity and 3x contact area compared to rigid palm design, enabling stable grasps for diverse objects.

Conclusion: The RIM Hand offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.

Abstract: This paper presents the flexible RIM Hand, a biomimetic robotic hand that precisely replicates the carpometacarpal (CMC) joints and employs superelastic Nitinol wires throughout its skeletal framework. By modeling the full carpal-to-metacarpal anatomy, the design enables realistic palm deformation through tendon-driven fingers while enhancing joint restoration and supports skeletal structure with Nitinol-based dorsal extensors. A flexible silicone skin further increases contact friction and contact area, enabling stable grasps for diverse objects. Experiments show that the palm can deform up to 28%, matching human hand flexibility, while achieving more than twice the payload capacity and three times the contact area compared to a rigid palm design. The RIM Hand thus offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.

</details>


### [419] [Sample Efficient Learning of Body-Environment Interaction of an Under-Actuated System](https://arxiv.org/abs/2601.13777)
*Zvi Chapnik,Yizhar Or,Shai Revzen*

Main category: cs.RO

TL;DR: Comparison of four methods for learning motility maps from robot motion data shows trade-off between simple methods (better with small datasets) and sophisticated methods (better with large datasets).


<details>
  <summary>Details</summary>
Motivation: Geometric mechanics helps understand how biological/robotic systems use shape changes to move through environmental interactions. Need to compare methods for learning motility maps from real robot data with under-actuated degrees of freedom and complex substrate interactions.

Method: Tested four modeling approaches on a physical robot designed specifically for testing, with under-actuated degrees of freedom and hard-to-model substrate interactions. Compared methods' ability to predict body velocity from shape change across same gait, different gaits, and different speeds.

Result: Found trade-off: simpler methods perform better with small training datasets, while more sophisticated methods excel when more training data is available.

Conclusion: The choice of motility map learning method depends on available training data size, with simpler methods preferred for limited data and sophisticated methods for abundant data.

Abstract: Geometric mechanics provides valuable insights into how biological and robotic systems use changes in shape to move by mechanically interacting with their environment. In high-friction environments it provides that the entire interaction is captured by the ``motility map''. Here we compare methods for learning the motility map from motion tracking data of a physical robot created specifically to test these methods by having under-actuated degrees of freedom and a hard to model interaction with its substrate. We compared four modeling approaches in terms of their ability to predict body velocity from shape change within the same gait, across gaits, and across speeds. Our results show a trade-off between simpler methods which are superior on small training datasets, and more sophisticated methods, which are superior when more training data is available.

</details>


### [420] [HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction](https://arxiv.org/abs/2601.13801)
*Yuhua Jin,Nikita Kuzmin,Georgii Demianchuk,Mariya Lezina,Fawad Mehboob,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HoverAI is an embodied aerial agent that combines drone mobility with visual projection and conversational AI to create socially responsive drones that can interact with humans through adaptive avatars and real-time dialogue.


<details>
  <summary>Details</summary>
Motivation: Current drones operating in human-occupied spaces lack sufficient communication mechanisms, creating uncertainty about their intentions and limiting their ability to interact effectively with people.

Method: Integrates drone mobility, MEMS laser projector, onboard semi-rigid screen, and RGB camera with a multimodal pipeline combining voice activity detection, speech recognition (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2).

Result: High accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181).

Conclusion: HoverAI introduces a new class of spatially-aware, socially responsive embodied agents that unite aerial robotics with adaptive conversational AI and self-contained visual output for applications in guidance, assistance, and human-centered interaction.

Abstract: Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.

</details>


### [421] [DroneVLA: VLA based Aerial Manipulation](https://arxiv.org/abs/2601.13809)
*Fawad Mehboob,Monijesu James,Amir Habel,Jeffrin Sam,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: Aerial manipulation system that interprets natural language commands to retrieve and deliver objects using vision-language-action models and human-centric control for safe handovers.


<details>
  <summary>Details</summary>
Motivation: As aerial platforms evolve from passive observers to active manipulators, there's a need for intuitive interfaces that allow non-expert users to command these systems naturally through high-level natural language instructions.

Method: Integrates MediaPipe with Grounding DINO and a Vision-Language-Action (VLA) model on a custom drone with 1-DOF gripper and RGB-D camera. VLA interprets user prompts and generates prioritized task queues, while Grounding DINO and dynamic A* planning handle navigation. Human-centric controller uses MediaPipe for real-time pose estimation and visual servoing for safe handovers.

Result: Real-world experiments showed successful localization and navigation with 0.164m max error, 0.070m mean Euclidean error, and 0.084m root-mean squared error, demonstrating feasibility of VLA for aerial manipulation operations.

Conclusion: The system successfully demonstrates natural language-controlled aerial manipulation with safe human interaction, highlighting the potential of vision-language-action models for intuitive drone control and object delivery tasks.

Abstract: As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.

</details>


### [422] [GuideTouch: An Obstacle Avoidance Device for Visually Impaired](https://arxiv.org/abs/2601.13813)
*Timofei Kozlov,Artem Trandofilov,Georgii Gazaryan,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: GuideTouch is a wearable device for visually impaired individuals that uses ToF sensors and haptic feedback to detect head-level obstacles and provide directional cues for safe navigation.


<details>
  <summary>Details</summary>
Motivation: Traditional mobility aids fail to detect head-level obstacles, creating safety risks for visually impaired individuals during navigation.

Method: A compact wearable device with vertically aligned ToF sensors for 3D perception and 4 vibrotactile actuators for directional haptic feedback, plus self-cleaning optical covers and sound alarms.

Result: 92.9% average recognition accuracy for directional patterns in 22 participants, and 93.75% accuracy with 14 visually impaired users for primary directional cues.

Conclusion: GuideTouch enables intuitive spatial perception and could significantly improve safety, confidence, and autonomy for visually impaired users during independent navigation.

Abstract: Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.

</details>


### [423] [Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework](https://arxiv.org/abs/2601.13945)
*Yixuan Deng,Tongrun Wu,Donghao Wu,Zeyu Wei,Jiayuan Wang,Zhenglong Sun,Yuqing Tang,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: ANCHOR is a modular framework that provides explicit system-level primitives for decoupling and robustness in Embodied AI systems, replacing ad-hoc integration with explicit contracts for controlled degradation and self-healing recovery.


<details>
  <summary>Details</summary>
Motivation: Current Embodied AI deployments suffer from partial decoupling where shared context and feedback semantics are implicit, leading to interface drift, cross-module interference, and brittle recovery at scale as systems evolve rapidly in real-world deployments.

Method: ANCHOR separates Canonical Records (evolvable contracts for standardized shared state) from a communication bus for many-to-many dissemination and feedback-oriented coordination, creating an inspectable end-to-end loop.

Result: Validated closed-loop feasibility on de-identified workflows, characterized latency distributions under varying payload sizes and publish rates, and demonstrated automatic stream resumption after hard crashes and restarts even with shared-memory loss.

Conclusion: ANCHOR transforms ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.

Abstract: As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.

</details>


### [424] [Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects](https://arxiv.org/abs/2601.13979)
*Raffaele Mazza,Ciro Natale,Pietro Falco*

Main category: cs.RO

TL;DR: Novel cross-modal visuo-tactile framework for 3D shape reconstruction of cables under severe visual occlusions, integrating foundation-model-based vision with adaptive tactile exploration.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based methods for deformable linear object (DLO) reconstruction degrade under challenging conditions like varying illumination, background clutter, and partial visibility, especially for occluded cables.

Method: Integrates foundation-model-based visual perception (SAM for segmentation, Florence for semantic refinement) with adaptive tactile exploration. Visual pipeline includes skeletonization, endpoint detection, and point-cloud extraction. Tactile sensor explores occluded segments, with data merged through Euclidean clustering and topology-preserving fusion. B-spline interpolation with endpoint-guided point sorting yields smooth reconstruction.

Result: Experimental validation with robotic manipulator shows accurate reconstruction of both simple and highly curved single/multiple cable configurations, even with large occluded portions.

Conclusion: Demonstrates potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects, particularly in overcoming visual occlusion challenges.

Abstract: This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.

</details>


### [425] [Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior](https://arxiv.org/abs/2601.14000)
*Junwoo Chang,Joseph Park,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.RO

TL;DR: GISD embeds group symmetries into skill discovery via a group-invariant Wasserstein dependency measure, leading to more efficient exploration and better downstream task learning.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised skill discovery methods ignore geometric symmetries in physical environments, causing redundant behaviors and sample inefficiency. The paper aims to leverage these symmetries to discover more efficient and generalizable skills.

Method: Introduces Group-Invariant Skill Discovery (GISD) with theoretical guarantee: in group-symmetric environments, optimal solution consists of equivariant policy and group-invariant scoring function. Formulates Group-Invariant Wasserstein dependency measure, parameterizes scoring function using group Fourier representation, and defines intrinsic reward via alignment of equivariant latent features.

Result: Experiments on state-based and pixel-based locomotion benchmarks show GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to strong baselines.

Conclusion: Explicitly embedding group structure into skill discovery objectives leads to more systematic generalization under transformations, reduces redundancy, and improves sample efficiency in unsupervised skill acquisition.

Abstract: Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.

</details>


### [426] [Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems](https://arxiv.org/abs/2601.14091)
*Hossein Naderi,Alireza Shojaei,Lifu Huang,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: Foundation models enable cost-effective multi-agent teams for construction robot task planning, outperforming GPT-4o while being 10x cheaper.


<details>
  <summary>Details</summary>
Motivation: Robots face challenges in construction due to high costs and difficulty adapting to dynamic tasks, needing more adaptable and generalizable planning approaches.

Method: Proposed four models using lightweight open-source LLMs/VLMs: one single agent and three multi-agent teams that collaborate to create robot action plans, evaluated across three construction roles (Painter, Safety Inspector, Floor Tiling).

Result: Four-agent team outperformed state-of-the-art GPT-4o in most metrics while being ten times more cost-effective; three and four-agent teams showed improved generalizability.

Conclusion: Multi-agent AI teams using foundation models enhance construction robot adaptability and generalizability, supporting future research in diverse unstructured environments beyond construction.

Abstract: Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.

</details>


### [427] [Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning](https://arxiv.org/abs/2601.14104)
*Tairan Huang,Qingqing Ye,Yulin Jin,Jiawei Lian,Yi Wang,Haibo Hu*

Main category: cs.RO

TL;DR: DGBA is a diffusion-guided backdoor attack framework for real-world RL that uses printable visual patch triggers and advantage-based poisoning to bypass safety constraints in robotic systems.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks for RL are validated only in simulation, but real-world robotic systems have safety-constrained control pipelines (velocity limiting, action smoothing, collision avoidance) that suppress abnormal actions and attenuate conventional attacks.

Method: 1) Design small printable visual patch triggers placed on the floor, generated using conditional diffusion model for diverse appearances under real-world variations. 2) Treat robot control stack as black-box system. 3) Introduce advantage-based poisoning strategy that injects triggers only at decision-critical training states.

Result: Evaluated on TurtleBot3 mobile robot, demonstrating reliable activation of targeted attacks while preserving normal task performance.

Conclusion: DGBA successfully addresses the previously overlooked problem of safety constraints attenuating backdoor attacks in real-world RL systems, enabling effective attacks in physical robotic deployments.

Abstract: Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.

</details>


### [428] [SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media](https://arxiv.org/abs/2601.14128)
*Shoujie Li,Changqing Guo,Junhao Gong,Chenxin Liang,Wenhua Ding,Wenbo Ding*

Main category: cs.RO

TL;DR: SandWorm robot with SWTac sensor for granular media perception: biomimetic screw-peristaltic locomotion + event-based visuotactile sensing with vibration isolation achieves 0.2mm texture resolution and 98% stone classification.


<details>
  <summary>Details</summary>
Motivation: Perception in granular media is challenging due to unpredictable particle dynamics, requiring specialized sensing and locomotion capabilities for effective operation in such environments.

Method: Developed SandWorm (biomimetic screw-actuated robot with peristaltic motion) and SWTac (event-based visuotactile sensor with spring-isolated event camera). Used IMU-guided temporal filter, systematic sensor optimization, and U-Net for contact surface estimation.

Result: SWTac achieves 0.2 mm texture resolution, 98% stone classification accuracy, 0.15 N force estimation error. SandWorm reaches 12.5 mm/s locomotion, 90% success rate in granular media tasks including pipeline dredging and subsurface exploration.

Conclusion: The integrated SandWorm-SWTac system demonstrates practical performance in challenging granular media environments through biomimetic locomotion and advanced event-based tactile sensing with vibration isolation.

Abstract: Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.

</details>


### [429] [TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers](https://arxiv.org/abs/2601.14133)
*Bin Yu,Shijie Lian,Xiaopeng Lin,Yuliang Wei,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Xinming Wang,Bailing Wang,Cong Huang,Kai Chen*

Main category: cs.RO

TL;DR: TwinBrainVLA introduces a dual-VLM architecture with frozen "Left Brain" for general semantic understanding and trainable "Right Brain" for embodied proprioception, using Asymmetric Mixture-of-Transformers to prevent catastrophic forgetting while achieving superior robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Standard VLA models face a critical tension between maintaining high-level semantic understanding and learning low-level sensorimotor skills, leading to catastrophic forgetting of open-world capabilities when fine-tuning monolithic VLM backbones for robotic control.

Method: TwinBrainVLA coordinates a frozen generalist VLM ("Left Brain") with a trainable specialist VLM ("Right Brain") via Asymmetric Mixture-of-Transformers (AsyMoT). The Right Brain dynamically queries semantic knowledge from the frozen Left Brain and fuses it with proprioceptive states to condition a Flow-Matching Action Expert for continuous control generation.

Result: Extensive experiments on SimplerEnv and RoboCasa benchmarks show TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM.

Conclusion: TwinBrainVLA offers a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity by resolving the conflict between maintaining general capabilities and learning specialized control skills.

Abstract: Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to "catastrophic forgetting" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen "Left Brain", which retains robust general visual reasoning, with a trainable "Right Brain", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.

</details>
