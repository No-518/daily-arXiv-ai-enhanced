{"id": "2510.24949", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24949", "abs": "https://arxiv.org/abs/2510.24949", "authors": ["Anil Yildiz", "Sarah M. Thornton", "Carl Hildebrandt", "Sreeja Roy-Singh", "Mykel J. Kochenderfer"], "title": "SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving", "comment": null, "summary": "Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.", "AI": {"tldr": "SCOUT is a lightweight surrogate model that predicts scenario coverage labels from autonomous agents' sensor representations, eliminating the need for expensive human annotations or large vision-language models.", "motivation": "Existing methods for assessing scenario coverage in autonomous agents rely on costly human annotations or computationally intensive LVLMs, making them impractical for large-scale deployment due to cost and efficiency constraints.", "method": "SCOUT uses a distillation process to learn from LVLM-generated coverage labels, leveraging precomputed perception features from agents' latent sensor representations to enable fast, scalable scenario coverage estimation without continuous LVLM inference.", "result": "Evaluation across real-life autonomous navigation scenarios shows SCOUT maintains high accuracy while significantly reducing computational cost, providing an effective practical alternative for large-scale coverage analysis.", "conclusion": "SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems, though its performance depends on the quality of LVLM-generated training labels."}}
{"id": "2510.24972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24972", "abs": "https://arxiv.org/abs/2510.24972", "authors": ["Iancu Andrei", "Marius Kloetzer", "Cristian Mahulea", "Catalin Dosoftei"], "title": "Smooth path planning with safety margins using Piece-Wise Bezier curves", "comment": null, "summary": "In this paper, we propose a computationally efficient quadratic programming\n(QP) approach for generating smooth, $C^1$ continuous paths for mobile robots\nusing piece-wise quadratic Bezier (PWB) curves. Our method explicitly\nincorporates safety margins within a structured optimization framework,\nbalancing trajectory smoothness and robustness with manageable numerical\ncomplexity suitable for real-time and embedded applications. Comparative\nsimulations demonstrate clear advantages over traditional piece-wise linear\n(PWL) path planning methods, showing reduced trajectory deviations, enhanced\nrobustness, and improved overall path quality. These benefits are validated\nthrough simulations using a Pure-Pursuit controller in representative\nscenarios, highlighting the practical effectiveness and scalability of our\napproach for safe navigation.", "AI": {"tldr": "A computationally efficient QP approach using piece-wise quadratic Bezier curves for generating smooth C^1 continuous paths for mobile robots with explicit safety margins.", "motivation": "To create smooth, safe robot paths that balance trajectory smoothness and robustness while maintaining computational efficiency suitable for real-time applications.", "method": "Quadratic programming approach with piece-wise quadratic Bezier curves, incorporating safety margins within a structured optimization framework.", "result": "Comparative simulations show advantages over traditional piece-wise linear methods: reduced trajectory deviations, enhanced robustness, and improved path quality.", "conclusion": "The proposed approach is practically effective and scalable for safe navigation, validated through Pure-Pursuit controller simulations in representative scenarios."}}
{"id": "2510.24994", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24994", "abs": "https://arxiv.org/abs/2510.24994", "authors": ["Matsive Ali", "Blake Gassen", "Sen Liu"], "title": "Defect Mitigation for Robot Arm-based Additive Manufacturing Utilizing Intelligent Control and IOT", "comment": "This Paper Has Accepted at ASME 2025 International Mechanical\n  Engineering Congress and Exposition (IMECE 2025)", "summary": "This paper presents an integrated robotic fused deposition modeling additive\nmanufacturing system featuring closed-loop thermal control and intelligent\nin-situ defect correction using a 6-degree of freedom robotic arm and an Oak-D\ncamera. The robot arm end effector was modified to mount an E3D hotend\nthermally regulated by an IoT microcontroller, enabling precise temperature\ncontrol through real-time feedback. Filament extrusion system was synchronized\nwith robotic motion, coordinated via ROS2, ensuring consistent deposition along\ncomplex trajectories. A vision system based on OpenCV detects layer-wise\ndefects position, commanding autonomous re-extrusion at identified sites.\nExperimental validation demonstrated successful defect mitigation in printing\noperations. The integrated system effectively addresses challenges real-time\nquality assurance. Inverse kinematics were used for motion planning, while\nhomography transformations corrected camera perspectives for accurate defect\nlocalization. The intelligent system successfully mitigated surface anomalies\nwithout interrupting the print process. By combining real-time thermal\nregulation, motion control, and intelligent defect detection & correction, this\narchitecture establishes a scalable and adaptive robotic additive manufacturing\nframework suitable for aerospace, biomedical, and industrial applications.", "AI": {"tldr": "An integrated robotic 3D printing system with closed-loop thermal control and in-situ defect correction using a 6-DOF robotic arm and camera system, enabling autonomous defect detection and re-extrusion without interrupting printing.", "motivation": "To address challenges in real-time quality assurance for additive manufacturing by developing an integrated system that can detect and correct defects during the printing process, particularly for complex trajectories in aerospace, biomedical, and industrial applications.", "method": "Uses a 6-DOF robotic arm with modified E3D hotend thermally regulated by IoT microcontroller, synchronized filament extrusion with robotic motion via ROS2, vision system with OpenCV for defect detection, inverse kinematics for motion planning, and homography transformations for accurate defect localization.", "result": "Experimental validation demonstrated successful defect mitigation in printing operations, with the intelligent system successfully mitigating surface anomalies without interrupting the print process.", "conclusion": "The integrated system combining real-time thermal regulation, motion control, and intelligent defect detection & correction establishes a scalable and adaptive robotic additive manufacturing framework suitable for various industrial applications."}}
{"id": "2510.25053", "categories": ["cs.RO", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.25053", "abs": "https://arxiv.org/abs/2510.25053", "authors": ["Hayato Idei", "Tamon Miyake", "Tetsuya Ogata", "Yuichi Yamashita"], "title": "Scalable predictive processing framework for multitask caregiving robots", "comment": null, "summary": "The rapid aging of societies is intensifying demand for autonomous care\nrobots; however, most existing systems are task-specific and rely on\nhandcrafted preprocessing, limiting their ability to generalize across diverse\nscenarios. A prevailing theory in cognitive neuroscience proposes that the\nhuman brain operates through hierarchical predictive processing, which\nunderlies flexible cognition and behavior by integrating multimodal sensory\nsignals. Inspired by this principle, we introduce a hierarchical multimodal\nrecurrent neural network grounded in predictive processing under the\nfree-energy principle, capable of directly integrating over 30,000-dimensional\nvisuo-proprioceptive inputs without dimensionality reduction. The model was\nable to learn two representative caregiving tasks, rigid-body repositioning and\nflexible-towel wiping, without task-specific feature engineering. We\ndemonstrate three key properties: (i) self-organization of hierarchical latent\ndynamics that regulate task transitions, capture variability in uncertainty,\nand infer occluded states; (ii) robustness to degraded vision through\nvisuo-proprioceptive integration; and (iii) asymmetric interference in\nmultitask learning, where the more variable wiping task had little influence on\nrepositioning, whereas learning the repositioning task led to a modest\nreduction in wiping performance, while the model maintained overall robustness.\nAlthough the evaluation was limited to simulation, these results establish\npredictive processing as a universal and scalable computational principle,\npointing toward robust, flexible, and autonomous caregiving robots while\noffering theoretical insight into the human brain's ability to achieve flexible\nadaptation in uncertain real-world environments.", "AI": {"tldr": "A hierarchical multimodal recurrent neural network based on predictive processing was developed for autonomous care robots, capable of learning multiple caregiving tasks without task-specific preprocessing and demonstrating robust performance in simulation.", "motivation": "Address the limitations of existing task-specific care robots that rely on handcrafted preprocessing and lack generalization ability, inspired by the brain's hierarchical predictive processing for flexible cognition.", "method": "Developed a hierarchical multimodal recurrent neural network grounded in predictive processing under the free-energy principle, capable of directly integrating high-dimensional visuo-proprioceptive inputs without dimensionality reduction.", "result": "The model successfully learned rigid-body repositioning and flexible-towel wiping tasks, demonstrated hierarchical latent dynamics for task transitions, robustness to degraded vision, and asymmetric interference in multitask learning while maintaining overall robustness.", "conclusion": "Predictive processing serves as a universal and scalable computational principle for developing robust, flexible autonomous caregiving robots, while providing theoretical insights into human brain's flexible adaptation in uncertain environments."}}
{"id": "2510.24734", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24734", "abs": "https://arxiv.org/abs/2510.24734", "authors": ["Qirui Hou", "Wenzhang Sun", "Chang Zeng", "Chunfeng Wang", "Hao Li", "Jianxun Cui"], "title": "DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes", "comment": "Autonomous Driving, Novel view Synthesis, Multi task Learning", "summary": "Real-time, high-fidelity reconstruction of dynamic driving scenes is\nchallenged by complex dynamics and sparse views, with prior methods struggling\nto balance quality and efficiency. We propose DrivingScene, an online,\nfeed-forward framework that reconstructs 4D dynamic scenes from only two\nconsecutive surround-view images. Our key innovation is a lightweight residual\nflow network that predicts the non-rigid motion of dynamic objects per camera\non top of a learned static scene prior, explicitly modeling dynamics via scene\nflow. We also introduce a coarse-to-fine training paradigm that circumvents the\ninstabilities common to end-to-end approaches. Experiments on nuScenes dataset\nshow our image-only method simultaneously generates high-quality depth, scene\nflow, and 3D Gaussian point clouds online, significantly outperforming\nstate-of-the-art methods in both dynamic reconstruction and novel view\nsynthesis.", "AI": {"tldr": "DrivingScene is an online framework that reconstructs 4D dynamic driving scenes from just two consecutive surround-view images using a residual flow network and scene flow modeling, achieving high-quality depth, scene flow, and 3D Gaussian point clouds.", "motivation": "Real-time, high-fidelity reconstruction of dynamic driving scenes is challenging due to complex dynamics and sparse views, with existing methods struggling to balance quality and efficiency.", "method": "Uses a lightweight residual flow network that predicts non-rigid motion of dynamic objects per camera on top of a learned static scene prior, explicitly modeling dynamics via scene flow. Introduces a coarse-to-fine training paradigm to avoid instabilities in end-to-end approaches.", "result": "Experiments on nuScenes dataset show the method simultaneously generates high-quality depth, scene flow, and 3D Gaussian point clouds online, significantly outperforming state-of-the-art methods in both dynamic reconstruction and novel view synthesis.", "conclusion": "DrivingScene provides an effective online, feed-forward solution for 4D dynamic scene reconstruction from sparse views, demonstrating superior performance in dynamic reconstruction and novel view synthesis tasks."}}
{"id": "2510.24832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24832", "abs": "https://arxiv.org/abs/2510.24832", "authors": ["Hong Wang", "Zhezheng Hao", "Jian Luo", "Chenxing Wei", "Yao Shu", "Lei Liu", "Qiang Lin", "Hande Dong", "Jiawei Chen"], "title": "Scheduling Your LLM Reinforcement Learning with Reasoning Trees", "comment": null, "summary": "Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large\nLanguage Models (LLMs) can be conceptualized as progressively editing a query's\n`Reasoning Tree'. This process involves exploring nodes (tokens) and\ndynamically modifying the model's policy at each node. When combined with data\nscheduling, this process yields further gains in data efficiency and accuracy.\nHowever, existing RLVR data scheduling methods typically rely on path-based\nmetrics to rank queries, overlooking the reasoning tree structures of these\nqueries. In this paper, we introduce a novel metric, namely Reasoning Score\n(r-score), which measures the query's learning difficulty based on the\nstructure of its reasoning tree. Based on the r-score, we propose the Reasoning\nTree Schedule (Re-Schedule), a scheduling algorithm that constructs a\ncurriculum progressing from structurally simple (high r-score) to complex (low\nr-score) queries. Experiments on six math-reasoning benchmarks show that\nRe-Schedule significantly improves average accuracy, achieving gains of up to\n3.2%. These strong results validate our approach and demonstrate that a\nstructural understanding of the reasoning tree provides a more powerful and\nprincipled foundation for RLVR data scheduling.", "AI": {"tldr": "The paper introduces a novel Reasoning Score (r-score) metric and Re-Schedule algorithm for RLVR data scheduling that considers reasoning tree structures, achieving up to 3.2% accuracy gains on math benchmarks.", "motivation": "Existing RLVR data scheduling methods use path-based metrics that ignore reasoning tree structures, limiting their effectiveness in optimizing LLM training.", "method": "Proposed Reasoning Score (r-score) to measure query learning difficulty based on reasoning tree structure, and developed Re-Schedule algorithm that creates a curriculum from simple to complex queries.", "result": "Experiments on six math-reasoning benchmarks show Re-Schedule significantly improves average accuracy with gains up to 3.2%.", "conclusion": "Structural understanding of reasoning trees provides a more powerful and principled foundation for RLVR data scheduling than path-based approaches."}}
{"id": "2510.25072", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25072", "abs": "https://arxiv.org/abs/2510.25072", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "Non-Invasive Calibration Of A Stewart Platform By Photogrammetry", "comment": "The International Journal of Advanced Manufacturing Technology, 2024", "summary": "Accurate calibration of a Stewart platform is important for their precise and\nefficient operation. However, the calibration of these platforms using forward\nkinematics is a challenge for researchers because forward kinematics normally\ngenerates multiple feasible and unfeasible solutions for any pose of the moving\nplatform. The complex kinematic relations among the six actuator paths\nconnecting the fixed base to the moving platform further compound the\ndifficulty in establishing a straightforward and efficient calibration method.\nThe authors developed a new forward kinematics-based calibration method using\nDenavit-Hartenberg convention and used the Stewart platform Tiger 66.1\ndeveloped in their lab for experimenting with the photogrammetry-based\ncalibration strategies described in this paper. This system became operational\nupon completion of construction, marking its inaugural use. The authors used\ntheir calibration model for estimating the errors in the system and adopted\nthree compensation options or strategies as per Least Square method to improve\nthe accuracy of the system. These strategies leveraged a high-resolution\ndigital camera and off-the-shelf software to capture the poses of the moving\nplatform's center. This process is non-invasive and does not need any\nadditional equipment to be attached to the hexapod or any alteration of the\nhexapod hardware. This photogrammetry-based calibration process involves\nmultiple high-resolution images from different angles to measure the position\nand orientation of the platform center in the three-dimensional space. The\nTarget poses and Actual poses are then compared, and the error compensations\nare estimated using the Least-Squared methods to calculate the Predicted poses.\nResults from each of the three compensation approaches demonstrated noticeable\nenhancements in platform pose accuracies, suggesting room for further\nimprovements.", "AI": {"tldr": "A new forward kinematics-based calibration method using Denavit-Hartenberg convention was developed for Stewart platforms, employing photogrammetry with high-resolution cameras to measure platform poses and using Least Square methods for error compensation.", "motivation": "Accurate calibration of Stewart platforms is crucial for precise operation, but forward kinematics generates multiple solutions and complex kinematic relations make calibration challenging.", "method": "Used Denavit-Hartenberg convention for forward kinematics modeling, photogrammetry with high-resolution digital cameras to capture platform poses, and three Least Square compensation strategies for error estimation.", "result": "All three compensation approaches demonstrated noticeable enhancements in platform pose accuracies, showing potential for further improvements.", "conclusion": "The photogrammetry-based calibration method provides a non-invasive solution that doesn't require additional hardware modifications and effectively improves Stewart platform accuracy."}}
{"id": "2510.24767", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24767", "abs": "https://arxiv.org/abs/2510.24767", "authors": ["Guorui Song", "Guocun Wang", "Zhe Huang", "Jing Lin", "Xuefei Zhe", "Jian Li", "Haoqian Wang"], "title": "Towards Fine-Grained Human Motion Video Captioning", "comment": null, "summary": "Generating accurate descriptions of human actions in videos remains a\nchallenging task for video captioning models. Existing approaches often\nstruggle to capture fine-grained motion details, resulting in vague or\nsemantically inconsistent captions. In this work, we introduce the\nMotion-Augmented Caption Model (M-ACM), a novel generative framework that\nenhances caption quality by incorporating motion-aware decoding. At its core,\nM-ACM leverages motion representations derived from human mesh recovery to\nexplicitly highlight human body dynamics, thereby reducing hallucinations and\nimproving both semantic fidelity and spatial alignment in the generated\ncaptions. To support research in this area, we present the Human Motion Insight\n(HMI) Dataset, comprising 115K video-description pairs focused on human\nmovement, along with HMI-Bench, a dedicated benchmark for evaluating\nmotion-focused video captioning. Experimental results demonstrate that M-ACM\nsignificantly outperforms previous methods in accurately describing complex\nhuman motions and subtle temporal variations, setting a new standard for\nmotion-centric video captioning.", "AI": {"tldr": "M-ACM is a motion-augmented caption model that improves video captioning by incorporating motion-aware decoding using human mesh recovery to capture fine-grained human body dynamics.", "motivation": "Existing video captioning models struggle with capturing fine-grained motion details, leading to vague or semantically inconsistent captions that fail to accurately describe human actions.", "method": "M-ACM uses motion representations from human mesh recovery to explicitly highlight human body dynamics through motion-aware decoding, reducing hallucinations and improving semantic fidelity and spatial alignment.", "result": "M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.", "conclusion": "The proposed M-ACM framework with motion-aware decoding effectively enhances caption quality for human action descriptions, and the introduced HMI Dataset and HMI-Bench provide valuable resources for motion-focused video captioning research."}}
{"id": "2510.25005", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.25005", "abs": "https://arxiv.org/abs/2510.25005", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Utpal Garain"], "title": "Cyclic Counterfactuals under Shift-Scale Interventions", "comment": "Accepted at NeurIPS 2025", "summary": "Most counterfactual inference frameworks traditionally assume acyclic\nstructural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,\nmany real-world systems (e.g. biological systems) contain feedback loops or\ncyclic dependencies that violate acyclicity. In this work, we study\ncounterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,\nsoft, policy-style changes that rescale and/or shift a variable's mechanism.", "AI": {"tldr": "This paper studies counterfactual inference in cyclic structural causal models (SCMs) with shift-scale interventions, addressing real-world systems with feedback loops.", "motivation": "Traditional counterfactual inference frameworks assume acyclic SCMs (DAGs), but many real-world systems like biological systems contain feedback loops and cyclic dependencies that violate acyclicity.", "method": "The authors study counterfactual inference in cyclic SCMs under shift-scale interventions, which are soft, policy-style changes that rescale and/or shift a variable's mechanism.", "result": "Not specified in the provided abstract.", "conclusion": "Not specified in the provided abstract."}}
{"id": "2510.25086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25086", "abs": "https://arxiv.org/abs/2510.25086", "authors": ["Guibin Sun", "Jinhu L\u00fc", "Kexin Liu", "Zhenqian Wang", "Guanrong Chen"], "title": "Mean-Shift Theory and Its Applications in Swarm Robotics: A New Way to Enhance the Efficiency of Multi-Robot Collaboration", "comment": null, "summary": "Swarms evolving from collective behaviors among multiple individuals are\ncommonly seen in nature, which enables biological systems to exhibit more\nefficient and robust collaboration. Creating similar swarm intelligence in\nengineered robots poses challenges to the design of collaborative algorithms\nthat can be programmed at large scales. The assignment-based method has played\nan eminent role for a very long time in solving collaboration problems of robot\nswarms. However, it faces fundamental limitations in terms of efficiency and\nrobustness due to its unscalability to swarm variants. This article presents a\ntutorial review on recent advances in assignment-free collaboration of robot\nswarms, focusing on the problem of shape formation. A key theoretical component\nis the recently developed \\emph{mean-shift exploration} strategy, which\nimproves the collaboration efficiency of large-scale swarms by dozens of times.\nFurther, the efficiency improvement is more significant as the swarm scale\nincreases. Finally, this article discusses three important applications of the\nmean-shift exploration strategy, including precise shape formation, area\ncoverage formation, and maneuvering formation, as well as their corresponding\nindustrial scenarios in smart warehousing, area exploration, and cargo\ntransportation.", "AI": {"tldr": "This paper reviews assignment-free collaboration methods for robot swarms, focusing on shape formation using mean-shift exploration strategy that significantly improves efficiency and scalability.", "motivation": "Traditional assignment-based methods for robot swarm collaboration face fundamental limitations in efficiency and robustness due to poor scalability to swarm variants, while biological swarms demonstrate efficient collective behaviors.", "method": "The paper presents mean-shift exploration strategy as a key theoretical component for assignment-free collaboration, enabling large-scale robot swarms to achieve shape formation without individual task assignments.", "result": "The mean-shift exploration strategy improves collaboration efficiency of large-scale swarms by dozens of times, with efficiency gains becoming more significant as swarm scale increases.", "conclusion": "The assignment-free approach using mean-shift exploration enables practical applications in smart warehousing, area exploration, and cargo transportation through precise shape formation, area coverage formation, and maneuvering formation capabilities."}}
{"id": "2510.24768", "categories": ["cs.CV", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24768", "abs": "https://arxiv.org/abs/2510.24768", "authors": ["Benjamin Camus", "Julien Houssay", "Corentin Le Barbu", "Eric Monteux", "C\u00e9dric Saleun", "Christian Cochin"], "title": "Combining SAR Simulators to Train ATR Models with Synthetic Data", "comment": null, "summary": "This work aims to train Deep Learning models to perform Automatic Target\nRecognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the\nlack of real labelled measurements, we resort to synthetic data produced by SAR\nsimulators. Simulation offers full control over the virtual environment, which\nenables us to generate large and diversified datasets at will. However,\nsimulations are intrinsically grounded on simplifying assumptions of the real\nworld (i.e. physical models). Thus, synthetic datasets are not as\nrepresentative as real measurements. Consequently, ATR models trained on\nsynthetic images cannot generalize well on real measurements. Our contributions\nto this problem are twofold: on one hand, we demonstrate and quantify the\nimpact of the simulation paradigm on the ATR. On the other hand, we propose a\nnew approach to tackle the ATR problem: combine two SAR simulators that are\ngrounded on different (but complementary) paradigms to produce synthetic\ndatasets. To this end, we use two simulators: MOCEM, which is based on a\nscattering centers model approach, and Salsa, which resorts on a ray tracing\nstrategy. We train ATR models using synthetic dataset generated both by MOCEM\nand Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of\nalmost 88 % on the MSTAR measurements.", "AI": {"tldr": "This paper trains Deep Learning models for Automatic Target Recognition (SAR) using synthetic data from two complementary SAR simulators (MOCEM and Salsa) to overcome the lack of real labeled data, achieving 88% accuracy on real MSTAR measurements.", "motivation": "Real labeled SAR measurements are scarce, so synthetic data from simulators is used, but synthetic datasets don't generalize well to real measurements due to simplifying assumptions in simulation models.", "method": "Proposes combining two SAR simulators with different paradigms (MOCEM using scattering centers model and Salsa using ray tracing) to generate synthetic datasets, then trains ATR models using the ADASCA Deep Learning approach.", "result": "Achieves nearly 88% accuracy on real MSTAR measurements, demonstrating improved generalization from synthetic to real data.", "conclusion": "Using complementary simulation paradigms can help bridge the domain gap between synthetic and real SAR data, enabling effective ATR model training when real labeled data is unavailable."}}
{"id": "2510.25007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25007", "abs": "https://arxiv.org/abs/2510.25007", "authors": ["Islam Nassar", "Yang Lin", "Yuan Jin", "Rongxin Zhu", "Chang Wei Tan", "Zenan Zhai", "Nitika Mathur", "Thanh Tien Vu", "Xu Zhong", "Long Duong", "Yuan-Fang Li"], "title": "Taming the Real-world Complexities in CPT E/M Coding with Large Language Models", "comment": "EMNLP 2025 Industry Track", "summary": "Evaluation and Management (E/M) coding, under the Current Procedural\nTerminology (CPT) taxonomy, documents medical services provided to patients by\nphysicians. Used primarily for billing purposes, it is in physicians' best\ninterest to provide accurate CPT E/M codes. %While important, it is an\nauxiliary task that adds to physicians' documentation burden. Automating this\ncoding task will help alleviate physicians' documentation burden, improve\nbilling efficiency, and ultimately enable better patient care. However, a\nnumber of real-world complexities have made E/M encoding automation a\nchallenging task. In this paper, we elaborate some of the key complexities and\npresent ProFees, our LLM-based framework that tackles them, followed by a\nsystematic evaluation. On an expert-curated real-world dataset, ProFees\nachieves an increase in coding accuracy of more than 36\\% over a commercial CPT\nE/M coding system and almost 5\\% over our strongest single-prompt baseline,\ndemonstrating its effectiveness in addressing the real-world complexities.", "AI": {"tldr": "ProFees is an LLM-based framework that automates E/M coding for medical billing, achieving 36% higher accuracy than commercial systems and 5% improvement over single-prompt baselines.", "motivation": "Automating E/M coding reduces physicians' documentation burden, improves billing efficiency, and enables better patient care by handling real-world complexities in medical coding.", "method": "Uses an LLM-based framework called ProFees that addresses key complexities in E/M coding through advanced prompt engineering and systematic approaches.", "result": "Achieved more than 36% increase in coding accuracy over commercial CPT E/M coding systems and almost 5% improvement over strongest single-prompt baselines on expert-curated real-world dataset.", "conclusion": "ProFees effectively addresses real-world complexities in E/M coding automation, demonstrating significant improvements in accuracy and practical utility for medical billing systems."}}
{"id": "2510.25122", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25122", "abs": "https://arxiv.org/abs/2510.25122", "authors": ["Jiahong Chen", "Jing Wang", "Long Chen", "Chuwei Cai", "Jinghui Lu"], "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies", "comment": null, "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.", "AI": {"tldr": "NanoVLA introduces lightweight vision-language-action models for edge devices with innovations in vision-language decoupling, action chunking, and dynamic routing to achieve 52x faster inference with 98% fewer parameters while maintaining performance.", "motivation": "Current VLA models have high computational demands that make deployment on resource-constrained edge devices challenging, limiting practical robotic applications where power, latency, and computational resources are critical.", "method": "Three core innovations: (1) vision-language decoupling that moves fusion to late stage for better performance and reduced inference overhead; (2) long-short action chunking for smooth multi-step planning; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity.", "result": "Achieves up to 52x faster inference on edge devices with 98% less parameters while maintaining or surpassing state-of-the-art VLA models' task accuracy and generalization. Ablation studies confirm preserved cross-task transferability and enhanced cost-performance trade-offs.", "conclusion": "NanoVLA enables practical, high-precision robotic manipulation on resource-constrained hardware by optimizing the efficiency-performance trade-off through architectural innovations."}}
{"id": "2510.24773", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24773", "abs": "https://arxiv.org/abs/2510.24773", "authors": ["Ziyang Xu", "Olaf Wysocki", "Christoph Holst"], "title": "Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds", "comment": null, "summary": "Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point\nclouds is essential for ensuring the accuracy and credibility of downstream\napplications such as 3D mapping, modeling, and change analysis. Traditional\nbackward uncertainty modeling heavily rely on high-precision reference data,\nwhich are often costly or infeasible to obtain at large scales. To address this\nissue, this study proposes a machine learning-based framework for point-level\nuncertainty evaluation that learns the relationship between local geometric\nfeatures and point-level errors. The framework is implemented using two\nensemble learning models, Random Forest (RF) and XGBoost, which are trained and\nvalidated on a spatially partitioned real-world dataset to avoid data leakage.\nExperimental results demonstrate that both models can effectively capture the\nnonlinear relationships between geometric characteristics and uncertainty,\nachieving mean ROC-AUC values above 0.87. The analysis further reveals that\ngeometric features describing elevation variation, point density, and local\nstructural complexity play a dominant role in predicting uncertainty. The\nproposed framework offers a data-driven perspective of uncertainty evaluation,\nproviding a scalable and adaptable foundation for future quality control and\nerror analysis of large-scale point clouds.", "AI": {"tldr": "This paper proposes a machine learning framework using Random Forest and XGBoost to evaluate point-level uncertainty in Mobile Laser Scanning point clouds by learning relationships between local geometric features and point errors, achieving ROC-AUC above 0.87.", "motivation": "Traditional uncertainty modeling for MLS point clouds requires high-precision reference data that is costly and infeasible at large scales, creating a need for scalable uncertainty evaluation methods.", "method": "A machine learning framework using ensemble learning models (Random Forest and XGBoost) trained on spatially partitioned real-world datasets to learn relationships between local geometric features and point-level errors, avoiding data leakage.", "result": "Both models effectively captured nonlinear relationships between geometric characteristics and uncertainty, achieving mean ROC-AUC values above 0.87, with elevation variation, point density, and local structural complexity being dominant predictive features.", "conclusion": "The proposed framework provides a scalable, data-driven approach for uncertainty evaluation in large-scale point clouds, offering a foundation for future quality control and error analysis."}}
{"id": "2510.25014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25014", "abs": "https://arxiv.org/abs/2510.25014", "authors": ["Minkyung Kim", "Junsik Kim", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading", "comment": "8 pages main content, 18 pages supplementary material, 4 figures", "summary": "Large Language Models (LLMs) enable dynamic game interactions but fail to\nfollow essential procedural flows in rule-governed trading systems, eroding\nplayer trust. This work resolves the core tension between the creative\nflexibility of LLMs and the procedural demands of in-game trading\n(browse-offer-review-confirm). To this end, Autoregressive State-Tracking\nPrompting (ASTP) is introduced, a methodology centered on a strategically\norchestrated prompt that compels an LLM to make its state-tracking process\nexplicit and verifiable. Instead of relying on implicit contextual\nunderstanding, ASTP tasks the LLM with identifying and reporting a predefined\nstate label from the previous turn. To ensure transactional integrity, this is\ncomplemented by a state-specific placeholder post-processing method for\naccurate price calculations. Evaluation across 300 trading dialogues\ndemonstrates >99% state compliance and 99.3% calculation precision. Notably,\nASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)\nmatches larger models' (Gemini-2.5-Pro) performance while reducing response\ntime from 21.2s to 2.4s, establishing a practical foundation that satisfies\nboth real-time requirements and resource constraints of commercial games.", "AI": {"tldr": "ASTP method enables LLMs to follow procedural trading flows with >99% state compliance and 99.3% calculation precision, matching larger models' performance while being 9x faster.", "motivation": "LLMs fail to follow essential procedural flows in rule-governed trading systems, eroding player trust, creating tension between creative flexibility and procedural demands.", "method": "Autoregressive State-Tracking Prompting (ASTP) - strategic prompting that makes LLM's state-tracking explicit and verifiable, plus state-specific placeholder post-processing for accurate price calculations.", "result": ">99% state compliance and 99.3% calculation precision across 300 dialogues; ASTP with smaller models (Gemini-2.5-Flash) matches larger models' performance while reducing response time from 21.2s to 2.4s.", "conclusion": "ASTP establishes practical foundation satisfying both real-time requirements and resource constraints of commercial games by enabling procedural compliance without sacrificing performance."}}
{"id": "2510.25138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25138", "abs": "https://arxiv.org/abs/2510.25138", "authors": ["Yuxiang Yan", "Zhiyuan Zhou", "Xin Gao", "Guanghao Li", "Shenglin Li", "Jiaqi Chen", "Qunyan Pu", "Jian Pu"], "title": "Learning Spatial-Aware Manipulation Ordering", "comment": "Accepted to NeurIPS 2025", "summary": "Manipulation in cluttered environments is challenging due to spatial\ndependencies among objects, where an improper manipulation order can cause\ncollisions or blocked access. Existing approaches often overlook these spatial\nrelationships, limiting their flexibility and scalability. To address these\nlimitations, we propose OrderMind, a unified spatial-aware manipulation\nordering framework that directly learns object manipulation priorities based on\nspatial context. Our architecture integrates a spatial context encoder with a\ntemporal priority structuring module. We construct a spatial graph using\nk-Nearest Neighbors to aggregate geometric information from the local layout\nand encode both object-object and object-manipulator interactions to support\naccurate manipulation ordering in real-time. To generate physically and\nsemantically plausible supervision signals, we introduce a spatial prior\nlabeling method that guides a vision-language model to produce reasonable\nmanipulation orders for distillation. We evaluate OrderMind on our Manipulation\nOrdering Benchmark, comprising 163,222 samples of varying difficulty. Extensive\nexperiments in both simulation and real-world environments demonstrate that our\nmethod significantly outperforms prior approaches in effectiveness and\nefficiency, enabling robust manipulation in cluttered scenes.", "AI": {"tldr": "OrderMind is a spatial-aware manipulation ordering framework that learns object manipulation priorities using spatial context to handle cluttered environments effectively.", "motivation": "Existing manipulation approaches often ignore spatial dependencies among objects in cluttered environments, leading to collisions and blocked access, which limits flexibility and scalability.", "method": "Integrates spatial context encoder with temporal priority structuring module using k-NN spatial graphs, encodes object-object and object-manipulator interactions, and uses spatial prior labeling with vision-language models for supervision.", "result": "Outperforms prior approaches in effectiveness and efficiency on Manipulation Ordering Benchmark (163,222 samples), enabling robust manipulation in both simulation and real-world cluttered scenes.", "conclusion": "OrderMind provides a unified framework that successfully addresses spatial dependencies in cluttered manipulation tasks through spatial-aware priority learning."}}
{"id": "2510.24777", "categories": ["cs.CV", "cs.AI", "eess.IV", "68T07", "I.2; H.5.1"], "pdf": "https://arxiv.org/pdf/2510.24777", "abs": "https://arxiv.org/abs/2510.24777", "authors": ["Yujie Nie", "Jianzhang Ni", "Yonglong Ye", "Yuan-Ting Zhang", "Yun Kwok Wing", "Xiangqing Xu", "Xin Ma", "Lizhou Fan"], "title": "Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis", "comment": "35 pages, 8 figures, and 7 tables", "summary": "Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling\ntimely intervention and slowing disease progression. Multimodal diagnostic\napproaches offer considerable promise by integrating complementary information\nacross behavioral and perceptual domains. Eye-tracking and facial features, in\nparticular, are important indicators of cognitive function, reflecting\nattentional distribution and neurocognitive state. However, few studies have\nexplored their joint integration for auxiliary AD diagnosis. In this study, we\npropose a multimodal cross-enhanced fusion framework that synergistically\nleverages eye-tracking and facial features for AD detection. The framework\nincorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module\n(CEFAM), which models inter-modal interactions through cross-attention and\nglobal enhancement, and (b) a Direction-Aware Convolution Module (DACM), which\ncaptures fine-grained directional facial features via horizontal-vertical\nreceptive fields. Together, these modules enable adaptive and discriminative\nmultimodal representation learning. To support this work, we constructed a\nsynchronized multimodal dataset, including 25 patients with AD and 25 healthy\ncontrols (HC), by recording aligned facial video and eye-tracking sequences\nduring a visual memory-search paradigm, providing an ecologically valid\nresource for evaluating integration strategies. Extensive experiments on this\ndataset demonstrate that our framework outperforms traditional late fusion and\nfeature concatenation methods, achieving a classification accuracy of 95.11% in\ndistinguishing AD from HC, highlighting superior robustness and diagnostic\nperformance by explicitly modeling inter-modal dependencies and\nmodality-specific contributions.", "AI": {"tldr": "A multimodal framework combining eye-tracking and facial features achieves 95.11% accuracy in Alzheimer's disease detection using cross-attention fusion and directional-aware convolution modules.", "motivation": "Accurate AD diagnosis is crucial for timely intervention, and multimodal approaches integrating complementary behavioral/perceptual data (eye-tracking and facial features) show promise but haven't been jointly explored for AD diagnosis.", "method": "Proposed multimodal cross-enhanced fusion framework with: (a) Cross-Enhanced Fusion Attention Module (CEFAM) for inter-modal interactions via cross-attention and global enhancement, (b) Direction-Aware Convolution Module (DACM) for fine-grained directional facial features using horizontal-vertical receptive fields. Built synchronized dataset of 25 AD patients and 25 healthy controls during visual memory-search tasks.", "result": "Achieved 95.11% classification accuracy in distinguishing AD from healthy controls, outperforming traditional late fusion and feature concatenation methods. Demonstrated superior robustness and diagnostic performance.", "conclusion": "The framework successfully enables adaptive multimodal representation learning by explicitly modeling inter-modal dependencies and modality-specific contributions, providing an effective approach for auxiliary AD diagnosis."}}
{"id": "2510.25065", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25065", "abs": "https://arxiv.org/abs/2510.25065", "authors": ["Taekhyun Park", "Yongjae Lee", "Hyerim Bae"], "title": "Reasoning-Aware GRPO using Process Mining", "comment": null, "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.", "AI": {"tldr": "PM4GRPO enhances RL-based post-training for large reasoning models by adding process mining-based conformance rewards to measure reasoning alignment with teacher models, outperforming existing GRPO methods.", "motivation": "Current RL-based post-training for large reasoning models uses outcome-centric rewards, but lacks signals about the reasoning process itself.", "method": "Proposes PM4GRPO that augments standard answer/format rewards with process mining techniques to compute conformance rewards measuring reasoning alignment with teacher models.", "result": "Empirical results on five benchmarks show PM4GRPO significantly outperforms existing GRPO-based post-training methodologies.", "conclusion": "Leveraging process mining for reasoning-aware GRPO effectively enhances reasoning capabilities of policy models."}}
{"id": "2510.25191", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25191", "abs": "https://arxiv.org/abs/2510.25191", "authors": ["Hongyu Song", "Rishabh Dev Yadav", "Cheng Guo", "Wei Pan"], "title": "SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning", "comment": null, "summary": "Interpreting visual observations and natural language instructions for\ncomplex task execution remains a key challenge in robotics and AI. Despite\nrecent advances, language-driven navigation is still difficult, particularly\nfor UAVs in small-scale 3D environments. Existing Vision-Language Navigation\n(VLN) approaches are mostly designed for ground robots and struggle to\ngeneralize to aerial tasks that require full 3D spatial reasoning. The\nemergence of large Vision-Language Models (VLMs), such as GPT and Claude,\nenables zero-shot semantic reasoning from visual and textual inputs. However,\nthese models lack spatial grounding and are not directly applicable to\nnavigation. To address these limitations, SoraNav is introduced, an adaptive\nUAV navigation framework that integrates zero-shot VLM reasoning with\ngeometry-aware decision-making. Geometric priors are incorporated into image\nannotations to constrain the VLM action space and improve decision quality. A\nhybrid switching strategy leverages navigation history to alternate between VLM\nreasoning and geometry-based exploration, mitigating dead-ends and redundant\nrevisits. A PX4-based hardware-software platform, comprising both a digital\ntwin and a physical micro-UAV, enables reproducible evaluation. Experimental\nresults show that in 2.5D scenarios, our method improves Success Rate (SR) by\n25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it\nimproves SR by 29.5% and SPL by 18.5% relative to the baseline.", "AI": {"tldr": "SoraNav is an adaptive UAV navigation framework that integrates zero-shot vision-language model reasoning with geometry-aware decision-making to improve aerial navigation in 3D environments.", "motivation": "Existing Vision-Language Navigation approaches struggle with aerial tasks requiring full 3D spatial reasoning, and large VLMs lack spatial grounding for navigation tasks.", "method": "Integrates zero-shot VLM reasoning with geometric priors in image annotations, uses hybrid switching strategy between VLM reasoning and geometry-based exploration, and implements PX4-based hardware-software platform with digital twin and physical micro-UAV.", "result": "In 2.5D scenarios: SR improved by 25.7%, SPL by 17%. In 3D scenarios: SR improved by 29.5%, SPL by 18.5% relative to baseline.", "conclusion": "SoraNav effectively bridges the gap between semantic reasoning and spatial navigation for UAVs, demonstrating significant performance improvements in complex 3D environments."}}
{"id": "2510.24778", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24778", "abs": "https://arxiv.org/abs/2510.24778", "authors": ["Ibrahim Qamar", "Saber Mahmoud", "Seif Megahed", "Mohamed Khaled", "Saleh Hesham", "Ahmed Matar", "Saif Gebril", "Mervat Mahmoud"], "title": "FPGA-based Lane Detection System incorporating Temperature and Light Control Units", "comment": "5 pages, 8 figures, 3 tables", "summary": "Intelligent vehicles are one of the most important outcomes gained from the\nworld tendency toward automation. Applications of IVs, whether in urban roads\nor robot tracks, do prioritize lane path detection. This paper proposes an\nFPGA-based Lane Detector Vehicle LDV architecture that relies on the Sobel\nalgorithm for edge detection. Operating on 416 x 416 images and 150 MHz, the\nsystem can generate a valid output every 1.17 ms. The valid output consists of\nthe number of present lanes, the current lane index, as well as its right and\nleft boundaries. Additionally, the automated light and temperature control\nunits in the proposed system enhance its adaptability to the surrounding\nenvironmental conditions.", "AI": {"tldr": "FPGA-based lane detection system using Sobel edge detection that processes 416x416 images at 150MHz, providing lane count, current lane index, and boundaries in 1.17ms, with environmental adaptation features.", "motivation": "Intelligent vehicles require efficient lane detection for automation in both urban roads and robot tracks, prioritizing real-time performance and adaptability to environmental conditions.", "method": "Uses FPGA-based architecture with Sobel algorithm for edge detection, operating on 416x416 resolution images at 150MHz frequency, with automated light and temperature control units for environmental adaptation.", "result": "System achieves processing time of 1.17ms per image, outputs lane count, current lane index, and lane boundaries, with enhanced adaptability to environmental conditions.", "conclusion": "The proposed FPGA-based LDV architecture provides efficient real-time lane detection suitable for intelligent vehicle applications, with environmental adaptability features."}}
{"id": "2510.25091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25091", "abs": "https://arxiv.org/abs/2510.25091", "authors": ["Peilin Tan", "Liang Xie", "Churan Zhi", "Dian Tu", "Chuanqi Shi"], "title": "H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts", "comment": null, "summary": "Stock movement prediction remains fundamentally challenging due to complex\ntemporal dependencies, heterogeneous modalities, and dynamically evolving\ninter-stock relationships. Existing approaches often fail to unify structural,\nsemantic, and regime-adaptive modeling within a scalable framework. This work\nintroduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with\nLLM reasoning and Style-Structured Mixture of Experts, integrating three key\ninnovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically\ncaptures fine-grained spatiotemporal dynamics via a Local Context Hypergraph\n(LCH) and persistent inter-stock dependencies through a Global Context\nHypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon\nDivergence weighting mechanism for adaptive relational learning and cross-modal\nalignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large\nlanguage model with lightweight adapters to semantically fuse and align\nquantitative and textual modalities, enriching representations with\ndomain-specific financial knowledge; and (3) a Style-Structured Mixture of\nExperts (SSMoEs) that combines shared market experts and industry-specialized\nexperts, each parameterized by learnable style vectors enabling regime-aware\nspecialization under sparse activation. Extensive experiments on three major\nstock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in\nboth superior predictive accuracy and investment performance, while exhibiting\neffective risk control. Datasets, source code, and model weights are available\nat our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.", "AI": {"tldr": "H3M-SSMoEs is a novel hypergraph-based multimodal architecture for stock prediction that integrates hypergraph modeling, LLM reasoning, and style-structured mixture of experts to handle complex temporal dependencies and dynamic market relationships.", "motivation": "Stock movement prediction is challenging due to complex temporal dependencies, heterogeneous modalities, and evolving inter-stock relationships. Existing approaches fail to unify structural, semantic, and regime-adaptive modeling in a scalable framework.", "method": "Three key innovations: (1) Multi-Context Multimodal Hypergraph with Local and Global Context Hypergraphs using cross-modal hyperedges and Jensen-Shannon Divergence weighting; (2) LLM-enhanced reasoning module with frozen LLM and lightweight adapters for semantic fusion; (3) Style-Structured Mixture of Experts combining shared market experts and industry-specialized experts with learnable style vectors.", "result": "Extensive experiments on three major stock markets show H3M-SSMoEs surpasses state-of-the-art methods in both predictive accuracy and investment performance, with effective risk control.", "conclusion": "The proposed H3M-SSMoEs framework successfully addresses the challenges in stock prediction through unified structural, semantic, and regime-adaptive modeling, demonstrating superior performance across multiple markets."}}
{"id": "2510.25211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25211", "abs": "https://arxiv.org/abs/2510.25211", "authors": ["Amith Khandakar", "David Michelson", "Shaikh Golam Rabbani", "Fariya Bintay Shafi", "Md. Faysal Ahamed", "Khondokar Radwanur Rahman", "Md Abidur Rahman", "Md. Fahmidun Nabi", "Mohamed Arselene Ayari", "Khaled Khan", "Ponnuthurai Nagaratnam Suganthan"], "title": "RoadSens-4M: A Multimodal Smartphone & Camera Dataset for Holistic Road-way Analysis", "comment": null, "summary": "It's important to monitor road issues such as bumps and potholes to enhance\nsafety and improve road conditions. Smartphones are equipped with various\nbuilt-in sensors that offer a cost-effective and straightforward way to assess\nroad quality. However, progress in this area has been slow due to the lack of\nhigh-quality, standardized datasets. This paper discusses a new dataset created\nby a mobile app that collects sensor data from devices like GPS,\naccelerometers, gyroscopes, magnetometers, gravity sensors, and orientation\nsensors. This dataset is one of the few that integrates Geographic Information\nSystem (GIS) data with weather information and video footage of road\nconditions, providing a comprehensive understanding of road issues with\ngeographic context. The dataset allows for a clearer analysis of road\nconditions by compiling essential data, including vehicle speed, acceleration,\nrotation rates, and magnetic field intensity, along with the visual and spatial\ncontext provided by GIS, weather, and video data. Its goal is to provide\nfunding for initiatives that enhance traffic management, infrastructure\ndevelopment, road safety, and urban planning. Additionally, the dataset will be\npublicly accessible to promote further research and innovation in smart\ntransportation systems.", "AI": {"tldr": "A new comprehensive dataset for road quality assessment that combines smartphone sensor data with GIS, weather information, and video footage to enable better analysis of road conditions like bumps and potholes.", "motivation": "Current progress in road quality monitoring using smartphones is limited by the lack of high-quality, standardized datasets that provide comprehensive context for analyzing road issues.", "method": "Created a dataset using a mobile app that collects data from multiple smartphone sensors (GPS, accelerometer, gyroscope, magnetometer, gravity, orientation) and integrates it with GIS data, weather information, and video footage of road conditions.", "result": "Developed one of the few datasets that provides comprehensive road condition analysis with geographic context, including vehicle speed, acceleration, rotation rates, magnetic field intensity, and visual/spatial context.", "conclusion": "The dataset will be publicly accessible to support initiatives in traffic management, infrastructure development, road safety, urban planning, and promote further research in smart transportation systems."}}
{"id": "2510.24787", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24787", "abs": "https://arxiv.org/abs/2510.24787", "authors": ["Mingzhi Zhu", "Ding Shang", "Sai Qian Zhang"], "title": "ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality", "comment": null, "summary": "Photorealistic Codec Avatars (PCA), which generate high-fidelity human face\nrenderings, are increasingly being used in Virtual Reality (VR) environments to\nenable immersive communication and interaction through deep learning-based\ngenerative models. However, these models impose significant computational\ndemands, making real-time inference challenging on resource-constrained VR\ndevices such as head-mounted displays, where latency and power efficiency are\ncritical. To address this challenge, we propose an efficient post-training\nquantization (PTQ) method tailored for Codec Avatar models, enabling\nlow-precision execution without compromising output quality. In addition, we\ndesign a custom hardware accelerator that can be integrated into the\nsystem-on-chip of VR devices to further enhance processing efficiency. Building\non these components, we introduce ESCA, a full-stack optimization framework\nthat accelerates PCA inference on edge VR platforms. Experimental results\ndemonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over\nthe best 4-bit baseline, delivers up to $3.36\\times$ latency reduction, and\nsustains a rendering rate of 100 frames per second in end-to-end tests,\nsatisfying real-time VR requirements. These results demonstrate the feasibility\nof deploying high-fidelity codec avatars on resource-constrained devices,\nopening the door to more immersive and portable VR experiences.", "AI": {"tldr": "ESCA is a full-stack optimization framework that accelerates Photorealistic Codec Avatar (PCA) inference on VR devices through efficient quantization and custom hardware acceleration.", "motivation": "Codec Avatar models impose significant computational demands that make real-time inference challenging on resource-constrained VR devices where latency and power efficiency are critical.", "method": "Proposed an efficient post-training quantization (PTQ) method for Codec Avatar models and designed a custom hardware accelerator that can be integrated into VR system-on-chips.", "result": "ESCA boosts FovVideoVDP quality scores by up to +0.39 over 4-bit baselines, delivers 3.36\u00d7 latency reduction, and sustains 100 FPS rendering rate in end-to-end tests.", "conclusion": "The results demonstrate the feasibility of deploying high-fidelity codec avatars on resource-constrained devices, enabling more immersive and portable VR experiences."}}
{"id": "2510.25101", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25101", "abs": "https://arxiv.org/abs/2510.25101", "authors": ["Zhuo Chen", "Fei Wang", "Zixuan Li", "Zhao Zhang", "Weiwei Ding", "Chuanguang Yang", "Yongjun Xu", "Xiaolong Jin", "Jiafeng Guo"], "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA", "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.", "AI": {"tldr": "KnowCoder-A1 is an LLM that autonomously performs agentic reasoning on Knowledge Bases using outcome-only supervision via multi-stage curriculum reinforcement learning, achieving significant improvements over prior methods with less training data.", "motivation": "Current KBQA methods use process supervision which provides weak incentives for exploration and fails to strengthen agentic reasoning ability. The paper aims to develop autonomous reasoning capabilities through outcome-only supervision.", "method": "Multi-stage curriculum reinforcement learning with easy-to-hard curriculum: 1) Fine-tune LLM on high-quality trajectories via outcome-based rejection sampling, 2) Apply multi-stage curriculum RL with progressive reward schedules to alleviate reward sparsity.", "result": "KnowCoder-A1 consistently outperforms prior approaches across three mainstream datasets, achieving up to 11.1% relative improvement on GrailQA zero-shot subset while using only one-twelfth of the training data.", "conclusion": "Outcome-only supervision with multi-stage curriculum RL enables powerful autonomous reasoning behaviors and demonstrates strong agentic reasoning capabilities in KBQA tasks."}}
{"id": "2510.25233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25233", "abs": "https://arxiv.org/abs/2510.25233", "authors": ["Jee Won Lee", "Hansol Lim", "Sooyeun Yang", "Jongseong Brad Choi"], "title": "Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery", "comment": null, "summary": "Vision-based control systems, such as image-based visual servoing (IBVS),\nhave been extensively explored for precise robot manipulation. A persistent\nchallenge, however, is maintaining robust target tracking under partial or full\nocclusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking\nbut are fragile to occlusion and drift, while deep learning-based approaches\noften require continuous visibility and intensive computation. To address these\ngaps, we propose a hybrid visual tracking framework that bridges advanced\nperception with real-time servo control. First, a fast global template matcher\nconstrains the pose search region; next, a deep-feature Lucas-Kanade module\noperating on early VGG layers refines alignment to sub-pixel accuracy (<2px);\nthen, a lightweight residual regressor corrects local misalignments caused by\ntexture degradation or partial occlusion. When visual confidence falls below a\nthreshold, a GRU-based predictor seamlessly extrapolates pose updates from\nrecent motion history. Crucially, the pipeline's final outputs-translation,\nrotation, and scale deltas-are packaged as direct control signals for 30Hz\nimage-based servo loops. Evaluated on handheld video sequences with up to 90%\nocclusion, our system sustains under 2px tracking error, demonstrating the\nrobustness and low-latency precision essential for reliable real-world robot\nvision applications.", "AI": {"tldr": "A hybrid visual tracking framework combining template matching, deep-feature Lucas-Kanade, and GRU-based prediction for robust robot vision under occlusion.", "motivation": "Address limitations of classical tracking methods (fragile to occlusion/drift) and deep learning approaches (require continuous visibility, computationally intensive) for robust robot manipulation.", "method": "Three-stage pipeline: 1) Fast global template matcher constrains pose search, 2) Deep-feature Lucas-Kanade on VGG layers for sub-pixel alignment, 3) Residual regressor for local corrections, with GRU-based prediction during low confidence periods.", "result": "Sustains under 2px tracking error on handheld video sequences with up to 90% occlusion, providing 30Hz control signals for real-time servo loops.", "conclusion": "The hybrid framework achieves robust, low-latency precision essential for reliable real-world robot vision applications, bridging advanced perception with real-time servo control."}}
{"id": "2510.24788", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24788", "abs": "https://arxiv.org/abs/2510.24788", "authors": ["Xinjian Zhao", "Wei Pang", "Zhongkai Xue", "Xiangru Jian", "Lei Zhang", "Yaoyao Xu", "Xiaozhuang Song", "Shu Wu", "Tianshu Yu"], "title": "The Underappreciated Power of Vision Models for Graph Structural Understanding", "comment": "NeurIPS 2025", "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.", "AI": {"tldr": "Vision models can match GNN performance on graph tasks while showing different learning patterns, excelling at global structural understanding where GNNs struggle.", "motivation": "GNNs use bottom-up message-passing while human vision captures global structures first. Existing benchmarks conflate domain features with topological understanding.", "method": "Introduced GraphAbstract benchmark to evaluate models' ability to perceive global graph properties like organizational archetypes, symmetry, connectivity strength, and critical elements.", "result": "Vision models significantly outperform GNNs on holistic structural understanding tasks, maintain generalizability across graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size.", "conclusion": "Vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for global topological awareness and scale-invariant reasoning, opening new avenues for graph foundation models."}}
{"id": "2510.25179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25179", "abs": "https://arxiv.org/abs/2510.25179", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models", "comment": null, "summary": "Agentic methods have emerged as a powerful and autonomous paradigm that\nenhances reasoning, collaboration, and adaptive control, enabling systems to\ncoordinate and independently solve complex tasks. We extend this paradigm to\nsafety alignment by introducing Agentic Moderation, a model-agnostic framework\nthat leverages specialised agents to defend multimodal systems against\njailbreak attacks. Unlike prior approaches that apply as a static layer over\ninputs or outputs and provide only binary classifications (safe or unsafe), our\nmethod integrates dynamic, cooperative agents, including Shield, Responder,\nEvaluator, and Reflector, to achieve context-aware and interpretable\nmoderation. Extensive experiments across five datasets and four representative\nLarge Vision-Language Models (LVLMs) demonstrate that our approach reduces the\nAttack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),\nand improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,\nand well-balanced safety performance. By harnessing the flexibility and\nreasoning capacity of agentic architectures, Agentic Moderation provides\nmodular, scalable, and fine-grained safety enforcement, highlighting the\nbroader potential of agentic systems as a foundation for automated safety\ngovernance.", "AI": {"tldr": "Agentic Moderation is a model-agnostic framework that uses specialized agents to defend multimodal systems against jailbreak attacks, achieving significant improvements in safety metrics through dynamic, cooperative agent coordination.", "motivation": "To extend agentic methods to safety alignment by creating a more effective defense against jailbreak attacks in multimodal systems, moving beyond static binary classifications to achieve context-aware and interpretable moderation.", "method": "Uses a framework with four specialized agents (Shield, Responder, Evaluator, Reflector) that work cooperatively to provide dynamic moderation, leveraging agentic architectures for flexible and reasoning-based safety enforcement.", "result": "Reduces Attack Success Rate by 7-19%, maintains stable Non-Following Rate, improves Refusal Rate by 4-20% across five datasets and four LVLMs, demonstrating robust and well-balanced safety performance.", "conclusion": "Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the potential of agentic systems as a foundation for automated safety governance in multimodal AI systems."}}
{"id": "2510.25241", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25241", "abs": "https://arxiv.org/abs/2510.25241", "authors": ["Hao Huang", "Geeta Chandra Raju Bethala", "Shuaihang Yuan", "Congcong Wen", "Anthony Tzes", "Yi Fang"], "title": "One-shot Humanoid Whole-body Motion Learning", "comment": "10 pages, 3 figures, 5 tables", "summary": "Whole-body humanoid motion represents a cornerstone challenge in robotics,\nintegrating balance, coordination, and adaptability to enable human-like\nbehaviors. However, existing methods typically require multiple training\nsamples per motion category, rendering the collection of high-quality human\nmotion datasets both labor-intensive and costly. To address this, we propose a\nnovel approach that trains effective humanoid motion policies using only a\nsingle non-walking target motion sample alongside readily available walking\nmotions. The core idea lies in leveraging order-preserving optimal transport to\ncompute distances between walking and non-walking sequences, followed by\ninterpolation along geodesics to generate new intermediate pose skeletons,\nwhich are then optimized for collision-free configurations and retargeted to\nthe humanoid before integration into a simulated environment for policy\ntraining via reinforcement learning. Experimental evaluations on the CMU MoCap\ndataset demonstrate that our method consistently outperforms baselines,\nachieving superior performance across metrics. Code will be released upon\nacceptance.", "AI": {"tldr": "A novel method for training humanoid motion policies using only one non-walking motion sample plus walking motions, leveraging optimal transport and geodesic interpolation to generate intermediate poses for reinforcement learning.", "motivation": "Existing methods require multiple training samples per motion category, making human motion dataset collection labor-intensive and costly. The goal is to reduce data requirements while maintaining performance.", "method": "Uses order-preserving optimal transport to compute distances between walking and non-walking sequences, interpolates along geodesics to generate new intermediate pose skeletons, optimizes for collision-free configurations, retargets to humanoid, and trains policies via reinforcement learning in simulation.", "result": "Experimental evaluations on CMU MoCap dataset show the method consistently outperforms baselines and achieves superior performance across metrics.", "conclusion": "The proposed approach successfully enables effective humanoid motion policy training with minimal data requirements, demonstrating the viability of single-sample learning for complex motion tasks."}}
{"id": "2510.24791", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24791", "abs": "https://arxiv.org/abs/2510.24791", "authors": ["Jingjun Bi", "Fadi Dornaika"], "title": "A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data", "comment": null, "summary": "Recently, graph-based semi-supervised learning and pseudo-labeling have\ngained attention due to their effectiveness in reducing the need for extensive\ndata annotations. Pseudo-labeling uses predictions from unlabeled data to\nimprove model training, while graph-based methods are characterized by\nprocessing data represented as graphs. However, the lack of clear graph\nstructures in images combined with the complexity of multi-view data limits the\nefficiency of traditional and existing techniques. Moreover, the integration of\ngraph structures in multi-view data is still a challenge. In this paper, we\npropose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view\nData (RSGSLM). Our method addresses these challenges by (i) combining linear\nfeature transformation and multi-view graph fusion within a Graph Convolutional\nNetwork (GCN) framework, (ii) dynamically incorporating pseudo-labels into the\nGCN loss function to improve classification in multi-view data, and (iii)\ncorrecting topological imbalances by adjusting the weights of labeled samples\nnear class boundaries. Additionally, (iv) we introduce an unsupervised\nsmoothing loss applicable to all samples. This combination optimizes\nperformance while maintaining computational efficiency. Experimental results on\nmulti-view benchmark image datasets demonstrate that RSGSLM surpasses existing\nsemi-supervised learning approaches in multi-view contexts.", "AI": {"tldr": "RSGSLM is a graph-based semi-supervised learning method for multi-view data that combines linear feature transformation, multi-view graph fusion, dynamic pseudo-labeling, topological imbalance correction, and unsupervised smoothing loss to improve classification performance.", "motivation": "Traditional graph-based methods struggle with multi-view data due to lack of clear graph structures in images and complexity of multi-view integration, limiting efficiency in semi-supervised learning scenarios.", "method": "Combines linear feature transformation and multi-view graph fusion in GCN framework, dynamically incorporates pseudo-labels into loss function, corrects topological imbalances by adjusting weights of boundary samples, and adds unsupervised smoothing loss.", "result": "Experimental results on multi-view benchmark image datasets show RSGSLM surpasses existing semi-supervised learning approaches in multi-view contexts.", "conclusion": "The proposed RSGSLM method effectively addresses challenges in multi-view graph-based semi-supervised learning and demonstrates superior performance compared to existing techniques."}}
{"id": "2510.25205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25205", "abs": "https://arxiv.org/abs/2510.25205", "authors": ["Yuyang Xia", "Zibo Liang", "Liwei Deng", "Yan Zhao", "Han Su", "Kai Zheng"], "title": "Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision", "comment": "It was accepted by ICDE2026", "summary": "Autonomous driving is an emerging technology that is expected to bring\nsignificant social, economic, and environmental benefits. However, these\nbenefits come with rising energy consumption by computation engines, limiting\nthe driving range of vehicles, especially electric ones. Perception computing\nis typically the most power-intensive component, as it relies on largescale\ndeep learning models to extract environmental features. Recently, numerous\nstudies have employed model compression techniques, such as sparsification,\nquantization, and distillation, to reduce computational consumption. However,\nthese methods often result in either a substantial model size or a significant\ndrop in perception accuracy compared to high-computation models. To address\nthese challenges, we propose an energy-efficient autonomous driving framework,\ncalled EneAD. In the adaptive perception module, a perception optimization\nstrategy is designed from the perspective of data management and tuning.\nFirstly, we manage multiple perception models with different computational\nconsumption and adjust the execution framerate dynamically. Then, we define\nthem as knobs and design a transferable tuning method based on Bayesian\noptimization to identify promising knob values that achieve low computation\nwhile maintaining desired accuracy. To adaptively switch the knob values in\nvarious traffic scenarios, a lightweight classification model is proposed to\ndistinguish the perception difficulty in different scenarios. In the robust\ndecision module, we propose a decision model based on reinforcement learning\nand design a regularization term to enhance driving stability in the face of\nperturbed perception results. Extensive experiments evidence the superiority of\nour framework in both energy consumption and driving performance. EneAD can\nreduce perception consumption by 1.9x to 3.5x and thus improve driving range by\n3.9% to 8.5%", "AI": {"tldr": "EneAD is an energy-efficient autonomous driving framework that reduces perception computation by 1.9x-3.5x through adaptive model management, dynamic framerate adjustment, and reinforcement learning-based decision making, improving driving range by 3.9%-8.5%.", "motivation": "Autonomous driving's energy consumption limits electric vehicle range, with perception computing being the most power-intensive component. Existing model compression techniques often sacrifice either model size or accuracy.", "method": "Adaptive perception module with multiple models and dynamic framerate adjustment, transferable Bayesian optimization tuning, lightweight scenario classification, and reinforcement learning-based decision module with regularization for stability.", "result": "Reduces perception consumption by 1.9x to 3.5x and improves driving range by 3.9% to 8.5% while maintaining desired accuracy.", "conclusion": "EneAD effectively addresses energy efficiency in autonomous driving through adaptive perception optimization and robust decision making, achieving significant energy savings without compromising performance."}}
{"id": "2510.25255", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25255", "abs": "https://arxiv.org/abs/2510.25255", "authors": ["Klaus Zauner", "Hubert Gattringer", "Andreas Mueller"], "title": "Time-Optimal Transport of Loosely Placed Liquid Filled Cups along Prescribed Paths", "comment": null, "summary": "Handling loosely placed objects with robotic manipulators is a difficult task\nfrom the point of view of trajectory planning and control. This becomes even\nmore challenging when the object to be handled is a container filled with\nliquid. This paper addresses the task of transporting a liquid-filled cup\nplaced on a tray along a prescribed path in shortest time. The objective is to\nminimize swapping, thus avoiding spillage of the fluid. To this end, the\nsloshing dynamics is incorporated into the dynamic model used within the\noptimal control problem formulation. The optimization problem is solved using a\ndirect multiple shooting approach.", "AI": {"tldr": "This paper presents an optimal control approach for transporting liquid-filled containers while minimizing spillage by incorporating sloshing dynamics into the trajectory planning.", "motivation": "Handling liquid-filled containers with robotic manipulators is challenging due to sloshing dynamics, which can cause spillage during transport. The goal is to develop a method that can transport such containers along prescribed paths in minimum time while avoiding fluid spillage.", "method": "The authors formulate an optimal control problem that incorporates the sloshing dynamics of the liquid. They solve this optimization problem using a direct multiple shooting approach.", "result": "The proposed method successfully generates trajectories that minimize sloshing and prevent spillage while transporting liquid-filled cups along prescribed paths in shortest possible time.", "conclusion": "By explicitly modeling sloshing dynamics within the optimal control framework, the approach enables efficient and spill-free transportation of liquid-filled containers using robotic manipulators."}}
{"id": "2510.24792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24792", "abs": "https://arxiv.org/abs/2510.24792", "authors": ["Patrick Haller", "Fabio Barth", "Jonas Golde", "Georg Rehm", "Alan Akbik"], "title": "PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models", "comment": "8 pages, 11 tables and figures", "summary": "Vision-language models (VLMs) have demonstrated remarkable progress in\nmultimodal reasoning. However, existing benchmarks remain limited in terms of\nhigh-quality, human-verified examples. Many current datasets rely on\nsynthetically generated content by large language models (LLMs). Furthermore,\nmost datasets are limited to English, as manual quality assurance of translated\nsamples is time-consuming and costly. To fill this gap, we introduce\nPISA-Bench, a multilingual benchmark derived from English examples of the\nexpert-created PISA tests, a unified framework for the assessment of student\ncompetencies in over eighty countries. Each example consists of human-extracted\ninstructions, questions, answer options, and images, enriched with question\ntype categories, and has been translated from English into five additional\nlanguages (Spanish, German, Chinese, French, and Italian), resulting in a fully\nparallel corpus covering six languages. We evaluate state-of-the-art\nvision-language models on PISA-Bench and find that especially small models\n(<20B parameters) fail to achieve high test scores. We further find substantial\nperformance degradation on non-English splits as well as high error-rates when\nmodels are tasked with spatial and geometric reasoning. By releasing the\ndataset and evaluation framework, we provide a resource for advancing research\non multilingual multimodal reasoning.", "AI": {"tldr": "PISA-Bench is a multilingual multimodal benchmark derived from expert-created PISA tests, covering six languages with human-verified examples to address limitations in existing vision-language model evaluation datasets.", "motivation": "Existing vision-language benchmarks lack high-quality human-verified examples, often relying on synthetic LLM-generated content and being limited to English, making multilingual evaluation challenging.", "method": "Created PISA-Bench using human-extracted instructions, questions, answer options, and images from expert-created PISA tests, translated into five additional languages (Spanish, German, Chinese, French, Italian) to form a fully parallel corpus.", "result": "Small VLMs (<20B parameters) fail to achieve high scores, show substantial performance degradation on non-English splits, and have high error rates in spatial and geometric reasoning tasks.", "conclusion": "PISA-Bench provides a valuable resource for advancing multilingual multimodal reasoning research, highlighting current limitations in VLMs' cross-lingual and spatial reasoning capabilities."}}
{"id": "2510.25206", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25206", "abs": "https://arxiv.org/abs/2510.25206", "authors": ["Tianqianjin Lin", "Xi Zhao", "Xingyao Zhang", "Rujiao Long", "Yi Xu", "Zhuoren Jiang", "Wenbo Su", "Bo Zheng"], "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models", "comment": "17 pages, 11 figures", "summary": "Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.", "AI": {"tldr": "RAVR uses answer-conditioned reasoning to improve LLM reasoning by leveraging the insight that explaining \"why\" an answer is correct is easier than generating the answer itself, transforming hard problems into learnable ones.", "motivation": "Standard RL for LLMs fails when the model cannot generate high-quality reasoning paths initially, as it reinforces suboptimal patterns. The paper is motivated by cognitive science showing that explanatory reconstruction (explaining why an answer is correct) is easier than open-ended answer generation.", "method": "RAVR (Reference-Answer-guided Variational Reasoning) uses answer-conditioned reasoning as a variational surrogate for question-only reasoning, formalizing how conditioning on answers increases the expected utility of sampled reasoning paths.", "result": "Experiments in general and math domains show consistent improvements over strong baselines. RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific reasoning strategies.", "conclusion": "Answer-conditioned reasoning effectively transforms intractable reasoning problems into learnable ones by leveraging the cognitive advantage of explanatory reconstruction over open-ended exploration."}}
{"id": "2510.25268", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25268", "abs": "https://arxiv.org/abs/2510.25268", "authors": ["Wang zhi", "Yuyan Liu", "Liu Liu", "Li Zhang", "Ruixuan Lu", "Dan Guo"], "title": "SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation", "comment": null, "summary": "Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.", "AI": {"tldr": "SynHLMA is a framework that generates hand grasp sequences for articulated objects using language instructions, achieving HAOI generation, prediction, and interpolation tasks with superior performance.", "motivation": "Hand grasp synthesis for articulated objects requires considering both object functionality and long-term manipulation sequences with object deformation, which existing methods don't adequately address.", "method": "Uses discrete HAOI representation to model hand-object interaction frames, trains with natural language embeddings via HAOI manipulation language model, and employs joint-aware loss to handle dynamic joint variations.", "result": "Achieves state-of-the-art performance on HAOI-lang dataset for three manipulation tasks and enables robotics grasp execution through imitation learning.", "conclusion": "SynHLMA effectively synthesizes hand language manipulation for articulated objects by aligning grasping processes with language descriptions in shared representation space."}}
{"id": "2510.24795", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24795", "abs": "https://arxiv.org/abs/2510.24795", "authors": ["Zhaoshu Yu", "Bo Wang", "Pengpeng Zeng", "Haonan Zhang", "Ji Zhang", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "title": "A Survey on Efficient Vision-Language-Action Models", "comment": "26 pages, 8 figures", "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/", "AI": {"tldr": "This survey provides the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs), addressing computational and data challenges through a unified taxonomy covering efficient model design, training, and data collection.", "motivation": "Vision-Language-Action models face deployment challenges due to substantial computational and data requirements from large-scale foundation models, creating an urgent need to address these efficiency bottlenecks.", "method": "The survey introduces a unified taxonomy organizing techniques into three pillars: Efficient Model Design (architectures and compression), Efficient Training (reducing computational burdens), and Efficient Data Collection (addressing robotic data bottlenecks).", "result": "The survey establishes a foundational reference framework, summarizes state-of-the-art methods, delineates representative applications, and identifies key challenges in the Efficient VLA domain.", "conclusion": "The work provides a comprehensive roadmap for future research in efficient embodied AI systems and maintains an updated project page to track ongoing developments in the field."}}
{"id": "2510.25223", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25223", "abs": "https://arxiv.org/abs/2510.25223", "authors": ["Kun ouyang", "Haoyu Wang", "Dong Fang"], "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data", "comment": "14 pages, 11 figures", "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments.", "AI": {"tldr": "FELA is a multi-agent system using LLMs to automatically generate explainable features from complex industrial event logs, combining reasoning, coding, and evolutionary algorithms to improve model performance while reducing manual effort.", "motivation": "Industrial event logs are valuable but complex, with existing automated feature engineering methods suffering from limited explainability, rigid operations, and poor adaptability to heterogeneous data.", "method": "FELA uses specialized LLM agents (Idea, Code, Critic, Evaluation) in a multi-agent system with insight-guided self-evolution, combining reinforcement learning and genetic algorithms for idea space exploration.", "result": "Extensive experiments on real industrial datasets show FELA generates explainable, domain-relevant features that significantly improve model performance while reducing manual effort.", "conclusion": "LLM-based multi-agent systems show potential as a general framework for automated, interpretable, and adaptive feature engineering in complex real-world environments."}}
{"id": "2510.25280", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25280", "abs": "https://arxiv.org/abs/2510.25280", "authors": ["Yusuke Tsunoda", "Seiya Yamamoto", "Kazuki Ito", "Runze Xiao", "Keisuke Naniwa", "Koichi Osuka"], "title": "Development of Implicit-Explicit Control Based Amphibious Centipede-Type Robot and Evaluation of its Mobile Performance", "comment": null, "summary": "Multi-legged mobile robots possess high mobility performance in rough terrain\nenvironments, stemming from their high postural stability, joint flexibility,\nand the redundancy provided by multiple legs. In prior research on navigating\nbetween different environments such as land and water, the primary strategy\nemployed involves switching to a controller that generates an appropriate gait\nfor the new environment upon entering it. However, designing appropriate gaits\nfor each complex and diverse environment and accurately determining controller\nswitching for each environment is challenging. Therefore, this research\ndevelops a centipede-type mobile robot that navigates both aquatic and\nterrestrial environments with a simple, unified control scheme, based on the\nimplicit-explicit control philosophy and by ingeniously designing the robot's\nbody structure. In this research, we developed the robot featuring flexible\njoints and left and right legs on each body segment and focused on the leg\nstructure which has extensive contact with the environment. This paper\nevaluates the locomotion performance on land and water using the three\ndeveloped leg structures, using the robot's leg slip rate and actuator energy\nconsumption as evaluation metrics. The experimental results confirmed the\nexistence of an appropriate leg structure capable of navigating both aquatic\nand terrestrial environments under identical control.", "AI": {"tldr": "A centipede-type robot with flexible joints and multiple leg structures was developed to navigate both land and water using a unified control scheme, eliminating the need for environment-specific gait switching.", "motivation": "Traditional multi-legged robots require complex environment-specific gait controllers and accurate switching mechanisms, which is challenging for diverse environments like land and water.", "method": "Developed a centipede robot with flexible joints and tested three different leg structures using implicit-explicit control philosophy with unified control scheme across environments.", "result": "Experimental evaluation using leg slip rate and energy consumption metrics confirmed that appropriate leg structures exist for navigating both aquatic and terrestrial environments under identical control.", "conclusion": "A simple unified control scheme with properly designed leg structures can enable multi-legged robots to navigate both land and water environments without complex controller switching."}}
{"id": "2510.24804", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24804", "abs": "https://arxiv.org/abs/2510.24804", "authors": ["Xiaoyang Hu"], "title": "Conflict Adaptation in Vision-Language Models", "comment": "Workshop on Interpreting Cognition in Deep Learning Models at NeurIPS\n  2025", "summary": "A signature of human cognitive control is conflict adaptation: improved\nperformance on a high-conflict trial following another high-conflict trial.\nThis phenomenon offers an account for how cognitive control, a scarce resource,\nis recruited. Using a sequential Stroop task, we find that 12 of 13\nvision-language models (VLMs) tested exhibit behavior consistent with conflict\nadaptation, with the lone exception likely reflecting a ceiling effect. To\nunderstand the representational basis of this behavior, we use sparse\nautoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.\nPartially overlapping supernodes emerge for text and color in both early and\nlate layers, and their relative sizes mirror the automaticity asymmetry between\nreading and color naming in humans. We further isolate a conflict-modulated\nsupernode in layers 24-25 whose ablation significantly increases Stroop errors\nwhile minimally affecting congruent trials.", "AI": {"tldr": "Vision-language models exhibit human-like conflict adaptation in Stroop tasks, with neural analysis revealing specialized supernodes for text/color processing and a conflict-modulated supernode that regulates cognitive control.", "motivation": "To investigate whether AI models exhibit human-like cognitive control mechanisms, specifically conflict adaptation, and understand the neural representations underlying this behavior.", "method": "Used sequential Stroop task with 13 vision-language models, analyzed representations using sparse autoencoders to identify task-relevant supernodes in InternVL 3.5 4B model.", "result": "12 of 13 VLMs showed conflict adaptation behavior; identified partially overlapping supernodes for text/color processing that mirror human automaticity asymmetry; isolated conflict-modulated supernode in layers 24-25 whose ablation increases Stroop errors.", "conclusion": "VLMs exhibit human-like cognitive control mechanisms, with specialized neural representations for conflict processing that parallel human cognitive architecture."}}
{"id": "2510.25232", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25232", "abs": "https://arxiv.org/abs/2510.25232", "authors": ["Tianxi Wan", "Jiaming Luo", "Siyuan Chen", "Kunyao Lan", "Jianhua Chen", "Haiyang Geng", "Mengyue Wu"], "title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity", "comment": null, "summary": "Psychiatric comorbidity is clinically significant yet challenging due to the\ncomplexity of multiple co-occurring disorders. To address this, we develop a\nnovel approach integrating synthetic patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent framework transfers the\nclinical interview protocol into a hierarchical state machine and context tree,\nsupporting over 130 diagnostic states while maintaining clinical standards.\nThrough this rigorous process, we construct PsyCoTalk, the first large-scale\ndialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset enhances diagnostic accuracy\nand treatment planning, offering a valuable resource for psychiatric\ncomorbidity research. Compared to real-world clinical transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies. Licensed psychiatrists\nconfirm the realism and diagnostic validity of the dialogues. This dataset\nenables the development and evaluation of models capable of multi-disorder\npsychiatric screening in a single conversational pass.", "AI": {"tldr": "Developed PsyCoTalk, the first large-scale dialogue dataset for psychiatric comorbidity using synthetic EMRs and multi-agent diagnostic dialogue generation, containing 3,000 validated multi-turn diagnostic dialogues.", "motivation": "Psychiatric comorbidity is clinically significant but challenging due to complexity of multiple co-occurring disorders, requiring better diagnostic tools and datasets.", "method": "Integrated synthetic patient EMR construction (502 records) with multi-agent diagnostic dialogue generation using hierarchical state machine and context tree supporting 130+ diagnostic states, maintaining clinical standards.", "result": "Created PsyCoTalk dataset with 3,000 multi-turn diagnostic dialogues validated by psychiatrists, showing high structural/linguistic fidelity compared to real clinical transcripts and confirmed realism by licensed psychiatrists.", "conclusion": "PsyCoTalk enables development of models for multi-disorder psychiatric screening in single conversational pass, enhancing diagnostic accuracy and treatment planning for comorbidity research."}}
{"id": "2510.25335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25335", "abs": "https://arxiv.org/abs/2510.25335", "authors": ["Jakob Ziegler", "Bernhard Rameder", "Hubert Gattringer", "Andreas Mueller"], "title": "An approach for combining transparency and motion assistance of a lower body exoskeleton", "comment": "8 pages", "summary": "In this paper, an approach for gait assistance with a lower body exoskeleton\nis described. Two concepts, transparency and motion assistance, are combined.\nThe transparent mode, where the system is following the user's free motion with\na minimum of perceived interaction forces, is realized by exploiting the gear\nbacklash of the actuation units. During walking a superimposed assistance mode\napplies an additional torque guiding the legs to their estimated future\nposition. The concept of adaptive oscillators is utilized to learn the\nquasi-periodic signals typical for locomotion. First experiments showed\npromising results.", "AI": {"tldr": "Combines transparency and motion assistance in a lower body exoskeleton using gear backlash for transparent mode and adaptive oscillators for assistance during walking.", "motivation": "To develop a gait assistance system that can both follow user's free motion with minimal interaction forces and provide active assistance during walking.", "method": "Uses gear backlash in actuation units for transparent mode, and adaptive oscillators to learn quasi-periodic locomotion signals for superimposed assistance mode that applies torque to guide legs to estimated future positions.", "result": "First experiments showed promising results.", "conclusion": "The approach successfully combines transparency and motion assistance concepts for lower body exoskeleton gait assistance."}}
{"id": "2510.24813", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24813", "abs": "https://arxiv.org/abs/2510.24813", "authors": ["Binbin Li", "Guimiao Yang", "Zisen Qi", "Haiping Wang", "Yu Ding"], "title": "DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts", "comment": null, "summary": "Recent lightweight retrieval-augmented image caption models often utilize\nretrieved data solely as text prompts, thereby creating a semantic gap by\nleaving the original visual features unenhanced, particularly for object\ndetails or complex scenes. To address this limitation, we propose $DualCap$, a\nnovel approach that enriches the visual representation by generating a visual\nprompt from retrieved similar images. Our model employs a dual retrieval\nmechanism, using standard image-to-text retrieval for text prompts and a novel\nimage-to-image retrieval to source visually analogous scenes. Specifically,\nsalient keywords and phrases are derived from the captions of visually similar\nscenes to capture key objects and similar details. These textual features are\nthen encoded and integrated with the original image features through a\nlightweight, trainable feature fusion network. Extensive experiments\ndemonstrate that our method achieves competitive performance while requiring\nfewer trainable parameters compared to previous visual-prompting captioning\napproaches.", "AI": {"tldr": "DualCap is a lightweight image captioning model that uses dual retrieval (image-to-text and image-to-image) to generate both text and visual prompts, enhancing visual representations for better object details and complex scenes.", "motivation": "Existing lightweight retrieval-augmented caption models only use retrieved data as text prompts, creating a semantic gap by leaving original visual features unenhanced, especially for object details and complex scenes.", "method": "Uses dual retrieval: standard image-to-text for text prompts and novel image-to-image retrieval for visually similar scenes. Salient keywords from similar images' captions are encoded and fused with original image features through a lightweight trainable feature fusion network.", "result": "Achieves competitive performance while requiring fewer trainable parameters compared to previous visual-prompting captioning approaches.", "conclusion": "DualCap effectively bridges the semantic gap in lightweight retrieval-augmented captioning by enriching visual representations through dual retrieval mechanisms."}}
{"id": "2510.25320", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25320", "abs": "https://arxiv.org/abs/2510.25320", "authors": ["Jiaqi Wu", "Qinlao Zhao", "Zefeng Chen", "Kai Qin", "Yifei Zhao", "Xueqian Wang", "Yuhang Yao"], "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning", "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning.", "AI": {"tldr": "GAP introduces graph-based planning for LLM agents to enable parallel execution of independent sub-tasks, overcoming the sequential bottleneck of traditional approaches like ReAct.", "motivation": "Existing sequential reasoning paradigms fail to exploit parallelism among independent sub-tasks, leading to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios.", "method": "Trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, using a two-stage training strategy: supervised fine-tuning on graph-based planning traces followed by reinforcement learning with correctness-based rewards.", "result": "Significantly outperforms traditional ReAct baselines on MHQA datasets, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization.", "conclusion": "Graph-based agent planning enables adaptive parallel and serial tool execution, substantially improving both execution efficiency and task accuracy in multi-step reasoning scenarios."}}
{"id": "2510.25338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25338", "abs": "https://arxiv.org/abs/2510.25338", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Andreas Mueller"], "title": "Geometric Robot Calibration Using a Calibration Plate", "comment": "pp 309-317", "summary": "In this paper a new method for geometric robot calibration is introduced,\nwhich uses a calibration plate with precisely known distances between its\nmeasuring points. The relative measurement between two points on the\ncalibration plate is used to determine predefined error parameters of the\nsystem. In comparison to conventional measurement methods, like laser tracker\nor motion capture systems, the calibration plate provides a more mechanically\nrobust and cheaper alternative, which is furthermore easier to transport due to\nits small size. The calibration method, the plate design, the mathematical\ndescription of the error system as well as the identification of the parameters\nare described in detail. For identifying the error parameters, the least\nsquares method and a constrained optimization problem are used. The\nfunctionality of this method was demonstrated in experiments that led to\npromising results, correlated with one of a laser tracker calibration. The\nmodeling and identification of the error parameters is done for a gantry\nmachine, but is not restricted to that type of robot.", "AI": {"tldr": "A new geometric robot calibration method using a calibration plate with known distances between measuring points, providing a cheaper and more robust alternative to conventional methods like laser trackers.", "motivation": "To develop a more mechanically robust, cheaper, and easily transportable calibration method compared to conventional systems like laser trackers or motion capture systems.", "method": "Uses a calibration plate with precisely known distances between measuring points, employs relative measurements between points to determine error parameters, and applies least squares method with constrained optimization for parameter identification.", "result": "The method demonstrated promising results in experiments, showing correlation with laser tracker calibration results.", "conclusion": "The calibration plate method provides an effective alternative to conventional calibration systems and can be applied to various robot types beyond the tested gantry machine."}}
{"id": "2510.24814", "categories": ["cs.CV", "cs.AI", "68T07, 68U10", "I.5.4"], "pdf": "https://arxiv.org/pdf/2510.24814", "abs": "https://arxiv.org/abs/2510.24814", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Deep Feature Optimization for Enhanced Fish Freshness Assessment", "comment": "39 pages; 10 tables; 9 figures", "summary": "Assessing fish freshness is vital for ensuring food safety and minimizing\neconomic losses in the seafood industry. However, traditional sensory\nevaluation remains subjective, time-consuming, and inconsistent. Although\nrecent advances in deep learning have automated visual freshness prediction,\nchallenges related to accuracy and feature transparency persist. This study\nintroduces a unified three-stage framework that refines and leverages deep\nvisual representations for reliable fish freshness assessment. First, five\nstate-of-the-art vision architectures - ResNet-50, DenseNet-121,\nEfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a\nstrong baseline. Next, multi-level deep features extracted from these backbones\nare used to train seven classical machine learning classifiers, integrating\ndeep and traditional decision mechanisms. Finally, feature selection methods\nbased on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso\nidentify a compact and informative subset of features. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate that the best\nconfiguration combining Swin-Tiny features, an Extra Trees classifier, and\nLGBM-based feature selection achieves an accuracy of 85.99%, outperforming\nrecent studies on the same dataset by 8.69-22.78%. These findings confirm the\neffectiveness and generalizability of the proposed framework for visual quality\nevaluation tasks.", "AI": {"tldr": "This paper proposes a three-stage deep learning framework for fish freshness assessment that combines multiple vision architectures with classical ML classifiers and feature selection, achieving 85.99% accuracy on the FFE dataset.", "motivation": "Traditional sensory evaluation of fish freshness is subjective and inconsistent, while existing deep learning approaches lack accuracy and feature transparency, creating a need for more reliable automated assessment methods.", "method": "A three-stage framework: 1) Fine-tune five vision architectures (ResNet-50, DenseNet-121, EfficientNet-B0, ConvNeXt-Base, Swin-Tiny), 2) Use multi-level deep features to train seven classical ML classifiers, 3) Apply feature selection methods (LGBM, Random Forest, Lasso) to identify compact feature subsets.", "result": "The best configuration (Swin-Tiny features + Extra Trees classifier + LGBM feature selection) achieved 85.99% accuracy on the FFE dataset, outperforming recent studies by 8.69-22.78%.", "conclusion": "The proposed unified framework is effective and generalizable for visual quality evaluation tasks, demonstrating the value of integrating deep visual representations with traditional decision mechanisms."}}
{"id": "2510.25388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25388", "abs": "https://arxiv.org/abs/2510.25388", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm", "comment": null, "summary": "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,\nwhich can be improved by grouping state-action pairs and using their aggregate\nstatistics instead of single-node statistics. On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS\nabstraction algorithm for deterministic environments that builds its\nabstraction using the Abstractions of State-Action Pairs (ASAP) framework,\nwhich aims to detect states and state-action pairs with the same value under\noptimal play by analysing the search graph. ASAP, however, requires two\nstate-action pairs to have the same immediate reward, which is a rigid\ncondition that limits the number of abstractions that can be found and thereby\nthe sample efficiency. In this paper, we break with the paradigm of grouping\nvalue-equivalent states or state-action pairs and instead group states and\nstate-action pairs with possibly different values as long as the difference\nbetween their values can be inferred. We call this abstraction framework Known\nValue Difference Abstractions (KVDA), which infers the value differences by\nanalysis of the immediate rewards and modifies OGA-UCT to use this framework\ninstead. The modification is called KVDA-UCT, which detects significantly more\nabstractions than OGA-UCT, introduces no additional parameter, and outperforms\nOGA-UCT on a variety of deterministic environments and parameter settings.", "AI": {"tldr": "KVDA-UCT improves MCTS sample efficiency by grouping state-action pairs with known value differences rather than requiring identical values, outperforming OGA-UCT in deterministic environments.", "motivation": "Current MCTS abstraction methods like OGA-UCT require state-action pairs to have identical immediate rewards, which limits the number of possible abstractions and reduces sample efficiency.", "method": "Proposed Known Value Difference Abstractions (KVDA) framework that groups states and state-action pairs with different values as long as their value differences can be inferred through immediate reward analysis, then modified OGA-UCT to use this framework.", "result": "KVDA-UCT detects significantly more abstractions than OGA-UCT, introduces no additional parameters, and outperforms OGA-UCT across various deterministic environments and parameter settings.", "conclusion": "Breaking from the paradigm of grouping only value-equivalent states and instead grouping states with known value differences significantly improves MCTS abstraction capabilities and sample efficiency."}}
{"id": "2510.25386", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.25386", "abs": "https://arxiv.org/abs/2510.25386", "authors": ["Kumar Manas", "Mert Keser", "Alois Knoll"], "title": "Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods", "comment": "Accepted to 2025 IEEE International Automated Vehicle Validation\n  Conference (IAVVC)", "summary": "This survey provides an analysis of current methodologies integrating legal\nand logical specifications into the perception, prediction, and planning\nmodules of automated driving systems. We systematically explore techniques\nranging from logic-based frameworks to computational legal reasoning\napproaches, emphasizing their capability to ensure regulatory compliance and\ninterpretability in dynamic and uncertain driving environments. A central\nfinding is that significant challenges arise at the intersection of perceptual\nreliability, legal compliance, and decision-making justifiability. To\nsystematically analyze these challenges, we introduce a taxonomy categorizing\nexisting approaches by their theoretical foundations, architectural\nimplementations, and validation strategies. We particularly focus on methods\nthat address perceptual uncertainty and incorporate explicit legal norms,\nfacilitating decisions that are both technically robust and legally defensible.\nThe review covers neural-symbolic integration methods for perception,\nlogic-driven rule representation, and norm-aware prediction strategies, all\ncontributing toward transparent and accountable autonomous vehicle operation.\nWe highlight critical open questions and practical trade-offs that must be\naddressed, offering multidisciplinary insights from engineering, logic, and law\nto guide future developments in legally compliant autonomous driving systems.", "AI": {"tldr": "This survey analyzes methods for integrating legal and logical specifications into autonomous driving systems, focusing on regulatory compliance and interpretability across perception, prediction, and planning modules.", "motivation": "To address the challenges at the intersection of perceptual reliability, legal compliance, and decision-making justifiability in autonomous driving systems operating in dynamic and uncertain environments.", "method": "Systematic exploration of techniques ranging from logic-based frameworks to computational legal reasoning approaches, with a taxonomy categorizing approaches by theoretical foundations, architectural implementations, and validation strategies.", "result": "The review covers neural-symbolic integration for perception, logic-driven rule representation, and norm-aware prediction strategies that enable technically robust and legally defensible decisions in autonomous vehicles.", "conclusion": "Critical open questions and practical trade-offs remain, requiring multidisciplinary insights from engineering, logic, and law to guide future developments in legally compliant autonomous driving systems."}}
{"id": "2510.24816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24816", "abs": "https://arxiv.org/abs/2510.24816", "authors": ["Cui Yakun", "Fushuo Huo", "Weijie Shi", "Juntao Dai", "Hang Du", "Zhenghao Zhu", "Sirui Han", "Yike Guo"], "title": "Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection", "comment": null, "summary": "The advent of multi-modal large language models (MLLMs) has greatly advanced\nresearch into applications for Video fake news detection (VFND) tasks.\nTraditional video-based FND benchmarks typically focus on the accuracy of the\nfinal decision, often failing to provide fine-grained assessments for the\nentire detection process, making the detection process a black box. Therefore,\nwe introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based\non the empirical analysis, which provides foundation for tasks definition. The\nbenchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'\nperception, understanding, and reasoning capacities during detection, featuring\n9730 human-annotated video-related questions based on a carefully constructed\ntaxonomy ability of VFND. To validate the impact of combining multiple features\non the final results, we design a novel framework named MVFND-CoT, which\nincorporates both creator-added content and original shooting footage\nreasoning. Building upon the benchmark, we conduct an in-depth analysis of the\ndeeper factors influencing accuracy, including video processing strategies and\nthe alignment between video features and model capabilities. We believe this\nbenchmark will lay a solid foundation for future evaluations and advancements\nof MLLMs in the domain of video fake news detection.", "AI": {"tldr": "The paper introduces MVFNDB, a comprehensive benchmark for evaluating multi-modal large language models in video fake news detection, featuring 10 tasks and 9730 annotated questions to assess perception, understanding, and reasoning capabilities.", "motivation": "Traditional video fake news detection benchmarks focus only on final accuracy without providing fine-grained assessment of the detection process, making it a black box that lacks transparency and detailed evaluation.", "method": "The authors created MVFNDB benchmark with 10 tasks based on empirical analysis, and designed MVFND-CoT framework that incorporates both creator-added content and original shooting footage reasoning to validate multi-feature combination impact.", "result": "The benchmark provides a foundation for task definition and enables in-depth analysis of factors influencing accuracy, including video processing strategies and alignment between video features and model capabilities.", "conclusion": "The MVFNDB benchmark lays a solid foundation for future evaluations and advancements of multi-modal large language models in video fake news detection domain."}}
{"id": "2510.25445", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25445", "abs": "https://arxiv.org/abs/2510.25445", "authors": ["Mohamad Abou Ali", "Fadi Dornaika"], "title": "Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions", "comment": null, "summary": "Agentic AI represents a transformative shift in artificial intelligence, but\nits rapid advancement has led to a fragmented understanding, often conflating\nmodern neural systems with outdated symbolic models -- a practice known as\nconceptual retrofitting. This survey cuts through this confusion by introducing\na novel dual-paradigm framework that categorizes agentic systems into two\ndistinct lineages: the Symbolic/Classical (relying on algorithmic planning and\npersistent state) and the Neural/Generative (leveraging stochastic generation\nand prompt-driven orchestration). Through a systematic PRISMA-based review of\n90 studies (2018--2025), we provide a comprehensive analysis structured around\nthis framework across three dimensions: (1) the theoretical foundations and\narchitectural principles defining each paradigm; (2) domain-specific\nimplementations in healthcare, finance, and robotics, demonstrating how\napplication constraints dictate paradigm selection; and (3) paradigm-specific\nethical and governance challenges, revealing divergent risks and mitigation\nstrategies. Our analysis reveals that the choice of paradigm is strategic:\nsymbolic systems dominate safety-critical domains (e.g., healthcare), while\nneural systems prevail in adaptive, data-rich environments (e.g., finance).\nFurthermore, we identify critical research gaps, including a significant\ndeficit in governance models for symbolic systems and a pressing need for\nhybrid neuro-symbolic architectures. The findings culminate in a strategic\nroadmap arguing that the future of Agentic AI lies not in the dominance of one\nparadigm, but in their intentional integration to create systems that are both\nadaptable and reliable. This work provides the essential conceptual toolkit to\nguide future research, development, and policy toward robust and trustworthy\nhybrid intelligent systems.", "AI": {"tldr": "This survey introduces a dual-paradigm framework for Agentic AI, categorizing systems into Symbolic/Classical (algorithmic planning) and Neural/Generative (stochastic generation) lineages, and analyzes 90 studies to reveal strategic paradigm selection across domains and identify critical research gaps.", "motivation": "To address the fragmented understanding and conceptual retrofitting in Agentic AI by providing a clear framework that distinguishes between modern neural systems and traditional symbolic models, cutting through confusion in the field.", "method": "Systematic PRISMA-based review of 90 studies (2018-2025) using a dual-paradigm framework, analyzing three dimensions: theoretical foundations, domain-specific implementations, and paradigm-specific ethical/governance challenges.", "result": "Symbolic systems dominate safety-critical domains (healthcare) while neural systems prevail in adaptive, data-rich environments (finance). Identified critical gaps: governance models for symbolic systems and need for hybrid neuro-symbolic architectures.", "conclusion": "The future of Agentic AI lies in intentional integration of both paradigms to create systems that are both adaptable and reliable, requiring a strategic roadmap for robust hybrid intelligent systems."}}
{"id": "2510.25405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25405", "abs": "https://arxiv.org/abs/2510.25405", "authors": ["Kei Ikemura", "Yifei Dong", "David Blanco-Mulero", "Alberta Longhini", "Li Chen", "Florian T. Pokorny"], "title": "Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning", "comment": "Under review", "summary": "Robotic manipulation of deformable and fragile objects presents significant\nchallenges, as excessive stress can lead to irreversible damage to the object.\nWhile existing solutions rely on accurate object models or specialized sensors\nand grippers, this adds complexity and often lacks generalization. To address\nthis problem, we present a vision-based reinforcement learning approach that\nincorporates a stress-penalized reward to discourage damage to the object\nexplicitly. In addition, to bootstrap learning, we incorporate offline\ndemonstrations as well as a designed curriculum progressing from rigid proxies\nto deformables. We evaluate the proposed method in both simulated and\nreal-world scenarios, showing that the policy learned in simulation can be\ntransferred to the real world in a zero-shot manner, performing tasks such as\npicking up and pushing tofu. Our results show that the learned policies exhibit\na damage-aware, gentle manipulation behavior, demonstrating their effectiveness\nby decreasing the stress applied to fragile objects by 36.5% while achieving\nthe task goals, compared to vanilla RL policies.", "AI": {"tldr": "Vision-based RL with stress-penalized rewards for gentle manipulation of fragile objects, using offline demonstrations and curriculum learning to reduce damage by 36.5% while maintaining task performance.", "motivation": "Robotic manipulation of deformable and fragile objects is challenging due to potential irreversible damage from excessive stress. Existing solutions rely on accurate models or specialized hardware, which adds complexity and lacks generalization.", "method": "Vision-based reinforcement learning with stress-penalized reward function, offline demonstrations, and curriculum learning progressing from rigid proxies to deformable objects.", "result": "Policies learned in simulation transferred to real-world in zero-shot manner, successfully performing tasks like picking up and pushing tofu. Stress applied to fragile objects decreased by 36.5% compared to vanilla RL policies while achieving task goals.", "conclusion": "The proposed approach enables damage-aware, gentle manipulation behavior for fragile objects, demonstrating effective transfer from simulation to real-world with significant stress reduction."}}
{"id": "2510.24820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24820", "abs": "https://arxiv.org/abs/2510.24820", "authors": ["Ruiyang Zhang", "Jiahao Luo", "Xiaoru Feng", "Qiufan Pang", "Yaodong Yang", "Juntao Dai"], "title": "SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing", "comment": null, "summary": "With the rapid advancement of text-to-image (T2I) models, ensuring their\nsafety has become increasingly critical. Existing safety approaches can be\ncategorized into training-time and inference-time methods. While inference-time\nmethods are widely adopted due to their cost-effectiveness, they often suffer\nfrom limitations such as over-refusal and imbalance between safety and utility.\nTo address these challenges, we propose a multi-round safety editing framework\nthat functions as a model-agnostic, plug-and-play module, enabling efficient\nsafety alignment for any text-to-image model. Central to this framework is\nMR-SafeEdit, a multi-round image-text interleaved dataset specifically\nconstructed for safety editing in text-to-image generation. We introduce a\npost-hoc safety editing paradigm that mirrors the human cognitive process of\nidentifying and refining unsafe content. To instantiate this paradigm, we\ndevelop SafeEditor, a unified MLLM capable of multi-round safety editing on\ngenerated images. Experimental results show that SafeEditor surpasses prior\nsafety approaches by reducing over-refusal while achieving a more favorable\nsafety-utility balance.", "AI": {"tldr": "A multi-round safety editing framework called SafeEditor that serves as a plug-and-play module for text-to-image models, addressing over-refusal and safety-utility imbalance through post-hoc image editing.", "motivation": "Existing inference-time safety methods for text-to-image models suffer from limitations like over-refusal and poor balance between safety and utility, making safety alignment challenging.", "method": "Proposed MR-SafeEdit framework with a multi-round image-text interleaved dataset and SafeEditor MLLM that performs post-hoc safety editing on generated images through iterative refinement.", "result": "SafeEditor outperforms prior safety approaches by reducing over-refusal while achieving better safety-utility balance in text-to-image generation.", "conclusion": "The multi-round safety editing paradigm effectively addresses safety concerns in text-to-image models without compromising utility, providing a practical solution for real-world deployment."}}
{"id": "2510.25471", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25471", "abs": "https://arxiv.org/abs/2510.25471", "authors": ["Willem Fourie"], "title": "Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?", "comment": null, "summary": "In artificial intelligence (AI) alignment research, instrumental goals, also\ncalled instrumental subgoals or instrumental convergent goals, are widely\nassociated with advanced AI systems. These goals, which include tendencies such\nas power-seeking and self-preservation, become problematic when they conflict\nwith human aims. Conventional alignment theory treats instrumental goals as\nsources of risk that become problematic through failure modes such as reward\nhacking or goal misgeneralization, and attempts to limit the symptoms of\ninstrumental goals, notably resource acquisition and self-preservation. This\narticle proposes an alternative framing: that a philosophical argument can be\nconstructed according to which instrumental goals may be understood as features\nto be accepted and managed rather than failures to be limited. Drawing on\nAristotle's ontology and its modern interpretations, an ontology of concrete,\ngoal-directed entities, it argues that advanced AI systems can be seen as\nartifacts whose formal and material constitution gives rise to effects distinct\nfrom their designers' intentions. In this view, the instrumental tendencies of\nsuch systems correspond to per se outcomes of their constitution rather than\naccidental malfunctions. The implication is that efforts should focus less on\neliminating instrumental goals and more on understanding, managing, and\ndirecting them toward human-aligned ends.", "AI": {"tldr": "This paper proposes reframing instrumental goals in AI alignment as inherent features to be managed rather than failures to be eliminated, drawing on Aristotelian ontology.", "motivation": "Conventional AI alignment theory treats instrumental goals (power-seeking, self-preservation) as risk sources and attempts to limit them, but this approach may be insufficient.", "method": "Constructs a philosophical argument using Aristotle's ontology and modern interpretations, viewing AI systems as goal-directed entities whose constitution naturally produces instrumental tendencies.", "result": "The analysis shows instrumental goals can be understood as per se outcomes of AI systems' formal and material constitution rather than accidental malfunctions.", "conclusion": "AI alignment efforts should shift from eliminating instrumental goals to understanding, managing, and directing them toward human-aligned ends."}}
{"id": "2510.25422", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25422", "abs": "https://arxiv.org/abs/2510.25422", "authors": ["Chaz Cornwall", "Jeremy P. Bos"], "title": "Solving the Right Problem with Multi-Robot Formations", "comment": "Submitted to SAE WCX 2026", "summary": "Formation control simplifies minimizing multi-robot cost functions by\nencoding a cost function as a shape the robots maintain. However, by reducing\ncomplex cost functions to formations, discrepancies arise between maintaining\nthe shape and minimizing the original cost function. For example, a Diamond or\nBox formation shape is often used for protecting all members of the formation.\nWhen more information about the surrounding environment becomes available, a\nstatic shape often no longer minimizes the original protection cost. We propose\na formation planner to reduce mismatch between a formation and the cost\nfunction while still leveraging efficient formation controllers. Our formation\nplanner is a two-step optimization problem that identifies desired relative\nrobot positions. We first solve a constrained problem to estimate non-linear\nand non-differentiable costs with a weighted sum of surrogate cost functions.\nWe theoretically analyze this problem and identify situations where weights do\nnot need to be updated. The weighted, surrogate cost function is then minimized\nusing relative positions between robots. The desired relative positions are\nrealized using a non-cooperative formation controller derived from Lyapunov's\ndirect approach. We then demonstrate the efficacy of this approach for\nmilitary-like costs such as protection and obstacle avoidance. In simulations,\nwe show a formation planner can reduce a single cost by over 75%. When\nminimizing a variety of cost functions simultaneously, using a formation\nplanner with adaptive weights can reduce the cost by 20-40%. Formation planning\nprovides better performance by minimizing a surrogate cost function that\nclosely approximates the original cost function instead of relying on a shape\nabstraction.", "AI": {"tldr": "A formation planner that reduces mismatch between formations and cost functions by optimizing relative robot positions through weighted surrogate cost functions, achieving 75% cost reduction for single objectives and 20-40% for multiple objectives.", "motivation": "Static formations often fail to minimize original cost functions when environmental information changes, creating discrepancies between maintaining shapes and actual cost minimization.", "method": "Two-step optimization: first estimate non-linear costs with weighted surrogate functions, then minimize using relative positions with Lyapunov-based non-cooperative formation controller.", "result": "75% cost reduction for single objectives, 20-40% reduction for multiple objectives simultaneously; demonstrated effectiveness for military-like costs including protection and obstacle avoidance.", "conclusion": "Formation planning outperforms static shape abstractions by minimizing surrogate cost functions that better approximate original objectives while leveraging efficient formation controllers."}}
{"id": "2510.24821", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24821", "abs": "https://arxiv.org/abs/2510.24821", "authors": ["Inclusion AI", ":", "Bowen Ma", "Cheng Zou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianing Li", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jianping Jiang", "Jun Peng", "Kaixiang Ji", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Longhua Tan", "Lan Wang", "Mochen Bai", "Ning Gao", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Ruobing Zheng", "Sirui Gao", "Tianqi Li", "Tinghao Liu", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaolong Wang", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yuting Xiao", "Yunxiao Sun", "Yipeng Chen", "Yifan Mao", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zhiqiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zizheng Yang", "Zhengyu He"], "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation", "comment": "18 pages, 5 figures", "summary": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.", "AI": {"tldr": "Ming-Flash-Omni is an upgraded multimodal AI model using sparse Mixture-of-Experts architecture with 100B total parameters (6.1B active per token), achieving state-of-the-art performance in text-to-image generation, generative segmentation, and contextual ASR across 12 benchmarks.", "motivation": "To develop a more efficient and capable unified multimodal AI system that advances toward Artificial General Intelligence by improving computational efficiency while expanding model capacity across vision, speech, and language modalities.", "method": "Built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters (only 6.1B active per token), enabling efficient scaling and unified multimodal intelligence.", "result": "Achieved state-of-the-art in text-to-image generation and generative segmentation; set new records on all 12 contextual ASR benchmarks; significant improvements in speech recognition, image generation fidelity, text rendering, scene consistency, and identity preservation during editing.", "conclusion": "Ming-Flash-Omni represents a key step toward AGI by demonstrating that a single unified architecture can achieve state-of-the-art performance across multiple modalities while maintaining computational efficiency through sparse MoE design."}}
{"id": "2510.25504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25504", "abs": "https://arxiv.org/abs/2510.25504", "authors": ["Oren Salzman", "Carlos Hern\u00e1ndez Ulloa", "Ariel Felner", "Sven Koenig"], "title": "Multi-Objective Search: Algorithms, Applications, and Emerging Directions", "comment": null, "summary": "Multi-objective search (MOS) has emerged as a unifying framework for planning\nand decision-making problems where multiple, often conflicting, criteria must\nbe balanced. While the problem has been studied for decades, recent years have\nseen renewed interest in the topic across AI applications such as robotics,\ntransportation, and operations research, reflecting the reality that real-world\nsystems rarely optimize a single measure. This paper surveys developments in\nMOS while highlighting cross-disciplinary opportunities, and outlines open\nchallenges that define the emerging frontier of MOS", "AI": {"tldr": "This paper surveys multi-objective search (MOS) as a unifying framework for planning and decision-making with multiple conflicting criteria, highlighting recent applications in AI fields like robotics and transportation.", "motivation": "Real-world systems rarely optimize a single measure, requiring frameworks that balance multiple conflicting criteria across various AI applications.", "method": "The paper conducts a comprehensive survey of developments in multi-objective search, examining cross-disciplinary opportunities and analyzing the state of the field.", "result": "The survey identifies renewed interest in MOS across AI applications and highlights the framework's unifying role in planning and decision-making problems.", "conclusion": "The paper outlines open challenges that define the emerging frontier of multi-objective search, emphasizing its importance for real-world systems with multiple optimization criteria."}}
{"id": "2510.25479", "categories": ["cs.RO", "cs.SY", "eess.SY", "93C10 (Primary) 37N35, 93C95, 70B10, 70B15 (Secondary)", "I.6.3; I.6.4; I.6.5; J.2"], "pdf": "https://arxiv.org/pdf/2510.25479", "abs": "https://arxiv.org/abs/2510.25479", "authors": ["Alexander B. Rambech", "Ivar B. Saksvik", "Vahid Hassani"], "title": "Combining Moving Mass Actuators and Manoeuvring Models for Underwater Vehicles: A Lagrangian Approach", "comment": "\\c{opyright} 2025 Alexander Rambech, Ivar Saksvik and Vahid Hassani.\n  Accepted by IFAC for publication under a Creative Commons License CC-BY-NC-ND", "summary": "In this paper, we present a Newton-Euler formulation of the equations of\nmotion for underwater vehicles with an interntal moving mass actuator.\nFurthermore, the moving mass dynamics are expressed as an extension to the\nmanoeuvring model for underwater vehicles, originally introduced by Fossen\n(1991). The influence of the moving mass is described in body-frame and\nincluded as states in both an additional kinematic equation and as part of the\ncoupled rigid-body kinetics of the underwater vehicle. The Coriolis-centripetal\neffects are derived from Kirchhoff's equations and the hydrostatics are derived\nusing first principals. The proposed Newton-Euler model is validated through\nsimulation and compared with the traditional Hamiltonian internal moving mass\nactuator formulation.", "AI": {"tldr": "This paper presents a Newton-Euler formulation for modeling underwater vehicles with internal moving mass actuators, extending Fossen's maneuvering model to include moving mass dynamics in body-frame coordinates.", "motivation": "To develop a more comprehensive dynamic model for underwater vehicles that incorporates the effects of internal moving mass actuators, which are important for control and maneuvering capabilities.", "method": "Derived a Newton-Euler formulation that expresses moving mass dynamics as an extension to Fossen's maneuvering model, including the moving mass influence in body-frame coordinates through additional kinematic equations and coupled rigid-body kinetics, with Coriolis-centripetal effects from Kirchhoff's equations and hydrostatics from first principles.", "result": "The proposed Newton-Euler model was validated through simulation and compared with the traditional Hamiltonian formulation for internal moving mass actuators.", "conclusion": "The Newton-Euler approach provides an effective alternative formulation for modeling underwater vehicles with internal moving mass actuators, successfully extending existing maneuvering models to account for this important actuation mechanism."}}
{"id": "2510.24827", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.24827", "abs": "https://arxiv.org/abs/2510.24827", "authors": ["Haoyang Zhang", "Zhou Yang", "Ke Sun", "Yucai Pang", "Guoliang Xu"], "title": "MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition", "comment": "The paper will be published in the MMAsia2025 conference proceedings", "summary": "Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.", "AI": {"tldr": "MCIHN is a hybrid network model for multimodal emotion recognition that uses adversarial autoencoders for feature learning and cross-modal interaction to reduce modality discrepancies and improve recognition accuracy.", "motivation": "Multimodal emotion recognition faces challenges due to differences between modalities and difficulty in characterizing unimodal emotional information, which limits accurate emotion recognition in human-computer interaction.", "method": "Uses adversarial autoencoders (AAE) for each modality to learn discriminative emotion features, then applies Cross-modal Gate Mechanism (CGMM) to reduce modality discrepancies and generate interaction features, followed by Feature Fusion module (FFM) for multimodal fusion.", "result": "Experiments on SIMS and MOSI datasets demonstrate that MCIHN achieves superior performance in multimodal emotion recognition.", "conclusion": "The proposed MCIHN model effectively addresses modality differences and improves emotion recognition accuracy through cross-modal interaction and feature fusion techniques."}}
{"id": "2510.25510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25510", "abs": "https://arxiv.org/abs/2510.25510", "authors": ["Zekun Xu", "Siyu Xia", "Chuhuai Yue", "Jiajun Chai", "Mingxue Tian", "Xiaohan Wang", "Wei Lin", "Haoxuan Li", "Guojun Yin"], "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL", "comment": null, "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.", "AI": {"tldr": "MTIR-SQL is a multi-turn tool-integrated reinforcement learning framework for Text-to-SQL that uses dynamic execution feedback to improve query generation and achieves state-of-the-art performance.", "motivation": "Existing RL methods for Text-to-SQL rely on static execution feedback, limiting real-time error correction. Multi-turn tool invocation with dynamic feedback could enhance adaptability and robustness.", "method": "Proposes MTIR-SQL framework with execution-aware multi-turn reasoning that incorporates database execution feedback at each step. Extends GRPO algorithm with trajectory filtering and removes KL loss constraints to address training instability.", "result": "Achieves 64.4% accuracy on BIRD Dev and 84.6% execution accuracy on SPIDER Dev with 4B parameters, significantly outperforming existing approaches.", "conclusion": "The MTIR-SQL framework successfully demonstrates that dynamic multi-turn reasoning with execution feedback significantly improves Text-to-SQL performance and robustness."}}
{"id": "2510.25520", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2510.25520", "abs": "https://arxiv.org/abs/2510.25520", "authors": ["Shengyao Zhang", "Yiyuan Zhang", "Chenrui Zhang", "Yiming Li", "Wenci Xin", "Yuliang Liufu", "Hong Wei Ng", "Cecilia Laschi"], "title": "Octopus-like Reaching Motion: A Perspective Inspired by Whipping", "comment": "The first two listed authors contributed equally. Yiyuan Zhang is the\n  corresponding author", "summary": "The stereotypical reaching motion of the octopus arm has drawn growing\nattention for its efficient control of a highly deformable body. Previous\nstudies suggest that its characteristic bend propagation may share underlying\nprinciples with the dynamics of a whip. This work investigates whether\nwhip-like passive dynamics in water can reproduce the kinematic features\nobserved in biological reaching and their similarities and differences.\nPlatform-based whipping tests were performed in water and air while\nsystematically varying material stiffness and driving speed. Image-based\nquantification revealed that the Ecoflex Gel 2 arm driven at 150 rpm (motor\nspeed) reproduced curvature propagation similar to that observed in octopus\nreaching. However, its bend-point velocity decreased monotonically rather than\nexhibiting the biological bell-shaped profile, confirming that the octopus\nreaching movement is not merely a passive whipping behavior. The absence of\npropagation in air further highlights the critical role of the surrounding\nmedium in forming octopus-like reaching motion. This study provides a new\nperspective for understand biological reaching movement, and offers a potential\nplatform for future hydrodynamic research.", "AI": {"tldr": "This study investigates whether passive whip-like dynamics in water can reproduce octopus arm reaching motion, finding that while some kinematic features are replicated, the octopus movement involves more than just passive whipping.", "motivation": "To understand if the characteristic bend propagation in octopus arm reaching shares principles with whip dynamics and whether passive whipping in water can reproduce biological reaching features.", "method": "Platform-based whipping tests in water and air with systematic variation of material stiffness and driving speed, using image-based quantification to analyze curvature propagation.", "result": "Ecoflex Gel 2 arm driven at 150 rpm reproduced curvature propagation similar to octopus reaching, but bend-point velocity decreased monotonically rather than showing the biological bell-shaped profile, and no propagation occurred in air.", "conclusion": "Octopus reaching movement is not merely passive whipping behavior, with the surrounding medium playing a critical role, providing new perspective for understanding biological reaching and a platform for hydrodynamic research."}}
{"id": "2510.24830", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24830", "abs": "https://arxiv.org/abs/2510.24830", "authors": ["Anne Gagneux", "S\u00e9gol\u00e8ne Martin", "R\u00e9mi Gribonval", "Mathurin Massias"], "title": "The Generation Phases of Flow Matching: a Denoising Perspective", "comment": null, "summary": "Flow matching has achieved remarkable success, yet the factors influencing\nthe quality of its generation process remain poorly understood. In this work,\nwe adopt a denoising perspective and design a framework to empirically probe\nthe generation process. Laying down the formal connections between flow\nmatching models and denoisers, we provide a common ground to compare their\nperformances on generation and denoising. This enables the design of principled\nand controlled perturbations to influence sample generation: noise and drift.\nThis leads to new insights on the distinct dynamical phases of the generative\nprocess, enabling us to precisely characterize at which stage of the generative\nprocess denoisers succeed or fail and why this matters.", "AI": {"tldr": "The paper analyzes flow matching from a denoising perspective, establishing connections between flow matching models and denoisers, and investigates how noise and drift perturbations affect the generation process.", "motivation": "Flow matching has achieved remarkable success but the factors influencing generation quality remain poorly understood. The authors aim to better understand the generation process through a denoising perspective.", "method": "The authors design a framework to empirically probe the generation process, establish formal connections between flow matching models and denoisers, and use principled perturbations (noise and drift) to influence sample generation.", "result": "The study reveals distinct dynamical phases in the generative process and enables precise characterization of when denoisers succeed or fail during generation.", "conclusion": "The denoising perspective provides new insights into flow matching generation processes, allowing better understanding of generation quality factors and denoiser performance at different stages."}}
{"id": "2510.25517", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25517", "abs": "https://arxiv.org/abs/2510.25517", "authors": ["Elisabetta Gentili", "Tony Ribeiro", "Fabrizio Riguzzi", "Katsumi Inoue"], "title": "Predicate Renaming via Large Language Models", "comment": null, "summary": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.", "AI": {"tldr": "Using LLMs to name unnamed predicates in logic rules to improve readability and interpretability.", "motivation": "Unnamed predicates in logic rules from methods like Predicate Invention hinder readability, interpretability, and reusability of logic theories.", "method": "Leverage LLMs' natural language and code processing capabilities to provide semantically meaningful names for unnamed predicates.", "result": "Evaluation on hand-crafted logic rules shows LLMs have potential for this naming task.", "conclusion": "LLMs show promise for automatically naming unnamed predicates in logic rules to enhance theory interpretability."}}
{"id": "2510.25548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25548", "abs": "https://arxiv.org/abs/2510.25548", "authors": ["Muyang Yan", "Miras Mengdibayev", "Ardon Floros", "Weihang Guo", "Lydia E. Kavraki", "Zachary Kingston"], "title": "Using VLM Reasoning to Constrain Task and Motion Planning", "comment": "8 pages, 7 figures, 1 table. Submitted to ICRA 2026", "summary": "In task and motion planning, high-level task planning is done over an\nabstraction of the world to enable efficient search in long-horizon robotics\nproblems. However, the feasibility of these task-level plans relies on the\ndownward refinability of the abstraction into continuous motion. When a\ndomain's refinability is poor, task-level plans that appear valid may\nultimately fail during motion planning, requiring replanning and resulting in\nslower overall performance. Prior works mitigate this by encoding refinement\nissues as constraints to prune infeasible task plans. However, these approaches\nonly add constraints upon refinement failure, expending significant search\neffort on infeasible branches. We propose VIZ-COAST, a method of leveraging the\ncommon-sense spatial reasoning of large pretrained Vision-Language Models to\nidentify issues with downward refinement a priori, bypassing the need to fix\nthese failures during planning. Experiments on two challenging TAMP domains\nshow that our approach is able to extract plausible constraints from images and\ndomain descriptions, drastically reducing planning times and, in some cases,\neliminating downward refinement failures altogether, generalizing to a diverse\nrange of instances from the broader domain.", "AI": {"tldr": "VIZ-COAST uses Vision-Language Models to predict downward refinement failures in task and motion planning, reducing planning times by identifying infeasible task plans before motion planning.", "motivation": "Traditional task and motion planning methods only detect refinement failures after they occur during motion planning, leading to wasted search effort and slower performance.", "method": "Leverages large pretrained Vision-Language Models for common-sense spatial reasoning to identify potential downward refinement issues from images and domain descriptions before planning.", "result": "Drastically reduces planning times and eliminates downward refinement failures in some cases, generalizing across diverse domain instances.", "conclusion": "Using Vision-Language Models for a priori refinement feasibility assessment is an effective approach to improve task and motion planning efficiency."}}
{"id": "2510.24885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24885", "abs": "https://arxiv.org/abs/2510.24885", "authors": ["Sidharth Rai", "Rahul Harsha Cheppally", "Benjamin Vail", "Keziban Yal\u00e7\u0131n Dokumac\u0131", "Ajay Sharda"], "title": "FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and Vegetables", "comment": "Sidharth Rai, Rahul Harsha Cheppally contributed equally to this work", "summary": "Maturity estimation of fruits and vegetables is a critical task for\nagricultural automation, directly impacting yield prediction and robotic\nharvesting. Current deep learning approaches predominantly treat maturity as a\ndiscrete classification problem (e.g., unripe, ripe, overripe). This rigid\nformulation, however, fundamentally conflicts with the continuous nature of the\nbiological ripening process, leading to information loss and ambiguous class\nboundaries. In this paper, we challenge this paradigm by reframing maturity\nestimation as a continuous, probabilistic learning task. We propose a novel\narchitectural modification to the state-of-the-art, real-time object detector,\nRT-DETRv2, by introducing a dedicated probabilistic head. This head enables the\nmodel to predict a continuous distribution over the maturity spectrum for each\ndetected object, simultaneously learning the mean maturity state and its\nassociated uncertainty. This uncertainty measure is crucial for downstream\ndecision-making in robotics, providing a confidence score for tasks like\nselective harvesting. Our model not only provides a far richer and more\nbiologically plausible representation of plant maturity but also maintains\nexceptional detection performance, achieving a mean Average Precision (mAP) of\n85.6\\% on a challenging, large-scale fruit dataset. We demonstrate through\nextensive experiments that our probabilistic approach offers more granular and\naccurate maturity assessments than its classification-based counterparts,\npaving the way for more intelligent, uncertainty-aware automated systems in\nmodern agriculture", "AI": {"tldr": "This paper proposes reframing fruit maturity estimation from discrete classification to continuous probabilistic learning, introducing a probabilistic head to RT-DETRv2 that predicts continuous maturity distributions with uncertainty measures.", "motivation": "Current deep learning approaches treat fruit maturity as discrete classification, which conflicts with the continuous nature of biological ripening processes, leading to information loss and ambiguous class boundaries.", "method": "Proposes architectural modification to RT-DETRv2 object detector by adding a dedicated probabilistic head that predicts continuous distribution over maturity spectrum, learning mean maturity state and associated uncertainty simultaneously.", "result": "Achieves mean Average Precision (mAP) of 85.6% on challenging fruit dataset while providing richer, more biologically plausible maturity representations with uncertainty measures for robotic decision-making.", "conclusion": "The probabilistic approach offers more granular and accurate maturity assessments than classification-based methods, enabling more intelligent, uncertainty-aware automated systems in modern agriculture."}}
{"id": "2510.25518", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25518", "abs": "https://arxiv.org/abs/2510.25518", "authors": ["Thomas Cook", "Richard Osuagwu", "Liman Tsatiashvili", "Vrynsia Vrynsia", "Koustav Ghosal", "Maraim Masoud", "Riccardo Mattivi"], "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "comment": "Keywords: RAG Agentic AI Fintech NLP KB Domain-Specific Ontology\n  Query Understanding", "summary": "Retrieval-Augmented Generation (RAG) systems often face limitations in\nspecialized domains such as fintech, where domain-specific ontologies, dense\nterminology, and acronyms complicate effective retrieval and synthesis. This\npaper introduces an agentic RAG architecture designed to address these\nchallenges through a modular pipeline of specialized agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-query\ndecomposition guided by keyphrase extraction, contextual acronym resolution,\nand cross-encoder-based context re-ranking. We evaluate our approach against a\nstandard RAG baseline using a curated dataset of 85 question--answer--reference\ntriples derived from an enterprise fintech knowledge base. Experimental results\ndemonstrate that the agentic RAG system outperforms the baseline in retrieval\nprecision and relevance, albeit with increased latency. These findings suggest\nthat structured, multi-agent methodologies offer a promising direction for\nenhancing retrieval robustness in complex, domain-specific settings.", "AI": {"tldr": "Agentic RAG system outperforms standard RAG in fintech domain through specialized agents for query reformulation, sub-query decomposition, acronym resolution, and context re-ranking.", "motivation": "Standard RAG systems struggle in specialized domains like fintech due to domain-specific ontologies, dense terminology, and acronyms that complicate effective retrieval and synthesis.", "method": "Modular pipeline of specialized agents supporting intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking.", "result": "Outperforms standard RAG baseline in retrieval precision and relevance on curated dataset of 85 question-answer-reference triples from enterprise fintech knowledge base, though with increased latency.", "conclusion": "Structured, multi-agent methodologies offer promising direction for enhancing retrieval robustness in complex, domain-specific settings."}}
{"id": "2510.25634", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25634", "abs": "https://arxiv.org/abs/2510.25634", "authors": ["Weikang Wan", "Fabio Ramos", "Xuning Yang", "Caelan Garrett"], "title": "Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills", "comment": null, "summary": "Long-horizon contact-rich bimanual manipulation presents a significant\nchallenge, requiring complex coordination involving a mixture of parallel\nexecution and sequential collaboration between arms. In this paper, we\nintroduce a hierarchical framework that frames this challenge as an integrated\nskill planning & scheduling problem, going beyond purely sequential\ndecision-making to support simultaneous skill invocation. Our approach is built\nupon a library of single-arm and bimanual primitive skills, each trained using\nReinforcement Learning (RL) in GPU-accelerated simulation. We then train a\nTransformer-based planner on a dataset of skill compositions to act as a\nhigh-level scheduler, simultaneously predicting the discrete schedule of skills\nas well as their continuous parameters. We demonstrate that our method achieves\nhigher success rates on complex, contact-rich tasks than end-to-end RL\napproaches and produces more efficient, coordinated behaviors than traditional\nsequential-only planners.", "AI": {"tldr": "A hierarchical framework for bimanual manipulation that treats the problem as skill planning & scheduling, using RL-trained primitive skills and a Transformer-based planner to coordinate parallel and sequential arm movements.", "motivation": "Long-horizon contact-rich bimanual manipulation requires complex coordination involving both parallel execution and sequential collaboration between arms, which is challenging for traditional approaches.", "method": "Hierarchical framework with RL-trained primitive skills (single-arm and bimanual) and a Transformer-based planner that acts as a high-level scheduler, predicting both discrete skill schedules and continuous parameters simultaneously.", "result": "Achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.", "conclusion": "The proposed hierarchical skill planning & scheduling framework effectively addresses the challenges of bimanual manipulation by enabling simultaneous skill invocation and coordination."}}
{"id": "2510.24887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24887", "abs": "https://arxiv.org/abs/2510.24887", "authors": ["Daniele L. V. dos Santos", "Thiago B. Pereira", "Carlos Eduardo G. R. Alves", "Richard J. M. G. Tello", "Francisco de A. Boldt", "Thiago M. Paix\u00e3o"], "title": "Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS", "comment": "Submitted to Int. Conf. on Computer Vision Theory and Applications\n  (VISAPP 2026)", "summary": "This paper investigates the feasibility of using lightweight body landmark\ndetection for the recognition of isolated signs in Brazilian Sign Language\n(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled\nsubstantial improvements in recognition performance, the use of OpenPose for\nlandmark extraction hindered time performance. In a preliminary investigation,\nwe observed that simply replacing OpenPose with the lightweight MediaPipe,\nwhile improving processing speed, significantly reduced accuracy. To overcome\nthis limitation, we explored landmark subset selection strategies aimed at\noptimizing recognition performance. Experimental results showed that a proper\nlandmark subset achieves comparable or superior performance to state-of-the-art\nmethods while reducing processing time by more than 5X compared to Alves et al.\n(2024). As an additional contribution, we demonstrated that spline-based\nimputation effectively mitigates missing landmark issues, leading to\nsubstantial accuracy gains. These findings highlight that careful landmark\nselection, combined with simple imputation techniques, enables efficient and\naccurate isolated sign recognition, paving the way for scalable Sign Language\nRecognition systems.", "AI": {"tldr": "This paper shows that careful selection of body landmarks combined with spline-based imputation enables efficient and accurate isolated sign recognition in Brazilian Sign Language (LIBRAS), achieving comparable or better performance than state-of-the-art methods while reducing processing time by over 5X.", "motivation": "Previous skeleton-based approaches using OpenPose for landmark extraction achieved good recognition performance but suffered from poor time performance. Simply replacing OpenPose with lightweight MediaPipe improved speed but significantly reduced accuracy, creating a need for optimization strategies.", "method": "The authors explored landmark subset selection strategies to optimize recognition performance, combined with spline-based imputation to handle missing landmarks. This approach aims to balance processing speed and accuracy in sign language recognition.", "result": "Experimental results demonstrated that proper landmark subset selection achieves comparable or superior performance to state-of-the-art methods while reducing processing time by more than 5X compared to previous approaches. Spline-based imputation also provided substantial accuracy gains by mitigating missing landmark issues.", "conclusion": "Careful landmark selection combined with simple imputation techniques enables efficient and accurate isolated sign recognition, paving the way for scalable Sign Language Recognition systems that balance both speed and accuracy requirements."}}
{"id": "2510.25528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25528", "abs": "https://arxiv.org/abs/2510.25528", "authors": ["Yuyuan Zeng", "Yufei Huang", "Can Xu", "Qingfeng Sun", "Jianfeng Yan", "Guanghui Xu", "Tao Yang", "Fengzong Lian"], "title": "Zero Reinforcement Learning Towards General Domains", "comment": null, "summary": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.", "AI": {"tldr": "Zero-RL enhances LLM reasoning without supervised fine-tuning, but current methods focus on verifiable domains. This paper proposes a novel zero-RL paradigm combining verifiable rewards with generative reward models for both verifiable and non-verifiable domains, using multi-task training and smooth length penalty to prevent reward hacking.", "motivation": "Current zero-RL research primarily focuses on domains with easily verifiable reward signals (mathematics, programming), leaving more diverse scenarios with non-verifiable rewards underexplored. The challenge is to elicit reasoning abilities in domains where verification is not straightforward.", "method": "Proposes a novel zero-RL paradigm combining verifiable rewards with a generative reward model, conducting multi-task zero-RL training across both verifiable and non-verifiable domains. Uses a smooth length penalty to mitigate reward hacking and encourage comprehensive thinking tokens in general domains.", "result": "Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate superior reasoning performance, achieving improvements not only on tasks requiring extensive reasoning but also on more general tasks.", "conclusion": "The proposed zero-RL approach successfully transfers reasoning capabilities between verifiable and non-verifiable domains, achieving enhanced performance across diverse reasoning scenarios through multi-task training and careful reward design."}}
{"id": "2510.25650", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.25650", "abs": "https://arxiv.org/abs/2510.25650", "authors": ["Ahmad Kokhahi", "Mary Kurz"], "title": "Collision avoidance and path finding in a robotic mobile fulfillment system using multi-objective meta-heuristics", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) has gained significant attention, with most\nresearch focusing on minimizing collisions and travel time. This paper also\nconsiders energy consumption in the path planning of automated guided vehicles\n(AGVs). It addresses two main challenges: i) resolving collisions between AGVs\nand ii) assigning tasks to AGVs. We propose a new collision avoidance strategy\nthat takes both energy use and travel time into account. For task assignment,\nwe present two multi-objective algorithms: Non-Dominated Sorting Genetic\nAlgorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative\nevaluations show that these proposed methods perform better than existing\napproaches in both collision avoidance and task assignment.", "AI": {"tldr": "This paper proposes energy-aware multi-agent path finding for AGVs with new collision avoidance and multi-objective task assignment algorithms (NSGA and ALNS) that outperform existing methods.", "motivation": "Most MAPF research focuses on minimizing collisions and travel time, but this paper also considers energy consumption in AGV path planning to address both collision resolution and task assignment challenges.", "method": "Proposed a new collision avoidance strategy considering both energy use and travel time, and two multi-objective algorithms for task assignment: Non-Dominated Sorting Genetic Algorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS).", "result": "Comparative evaluations demonstrate that the proposed methods perform better than existing approaches in both collision avoidance and task assignment.", "conclusion": "The energy-aware approach with multi-objective optimization algorithms effectively addresses MAPF challenges for AGVs, achieving superior performance in collision resolution and task allocation."}}
{"id": "2510.24902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24902", "abs": "https://arxiv.org/abs/2510.24902", "authors": ["H Mhatre", "M Vyas", "A Mittal"], "title": "Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation", "comment": null, "summary": "Traffic congestion is becoming a challenge in the rapidly growing urban\ncities, resulting in increasing delays and inefficiencies within urban\ntransportation systems. To address this issue a comprehensive methodology is\ndesigned to optimize traffic flow and minimize delays. The framework is\nstructured with three primary components: (a) vehicle detection, (b) traffic\nprediction, and (c) traffic signal optimization. This paper presents the first\ncomponent, vehicle detection. The methodology involves analyzing multiple\nsequential frames from a camera feed to compute the background, i.e. the\nunderlying roadway, by averaging pixel values over time. The computed\nbackground is then utilized to extract the foreground, where the Density-Based\nSpatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to\ndetect vehicles. With its computational efficiency and minimal infrastructure\nmodification requirements, the proposed methodology offers a practical and\nscalable solution for real-world deployment.", "AI": {"tldr": "Proposes a vehicle detection method using background subtraction and DBSCAN clustering from camera feeds to address urban traffic congestion.", "motivation": "Traffic congestion causes increasing delays and inefficiencies in rapidly growing urban cities, requiring solutions to optimize traffic flow and minimize delays.", "method": "Analyzes sequential camera frames to compute background by averaging pixel values over time, then extracts foreground and applies DBSCAN clustering algorithm for vehicle detection.", "result": "Developed a computationally efficient vehicle detection system that requires minimal infrastructure modifications.", "conclusion": "The proposed methodology offers a practical and scalable solution for real-world deployment in traffic management systems."}}
{"id": "2510.25529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25529", "abs": "https://arxiv.org/abs/2510.25529", "authors": ["Likun Wang", "Xiangteng Zhang", "Yinuo Wang", "Guojian Zhan", "Wenxuan Wang", "Haoyu Gao", "Jingliang Duan", "Shengbo Eben Li"], "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation", "comment": null, "summary": "Exploration is fundamental to reinforcement learning (RL), as it determines\nhow effectively an agent discovers and exploits the underlying structure of its\nenvironment to achieve optimal performance. Existing exploration methods\ngenerally fall into two categories: active exploration and passive exploration.\nThe former introduces stochasticity into the policy but struggles in\nhigh-dimensional environments, while the latter adaptively prioritizes\ntransitions in the replay buffer to enhance exploration, yet remains\nconstrained by limited sample diversity. To address the limitation in passive\nexploration, we propose Modelic Generative Exploration (MoGE), which augments\nexploration through the generation of under-explored critical states and\nsynthesis of dynamics-consistent experiences through transition models. MoGE is\ncomposed of two components: (1) a diffusion-based generator that synthesizes\ncritical states under the guidance of a utility function evaluating each\nstate's potential influence on policy exploration, and (2) a one-step\nimagination world model for constructing critical transitions based on the\ncritical states for agent learning. Our method adopts a modular formulation\nthat aligns with the principles of off-policy learning, allowing seamless\nintegration with existing algorithms to improve exploration without altering\ntheir core structures. Empirical results on OpenAI Gym and DeepMind Control\nSuite reveal that MoGE effectively bridges exploration and policy learning,\nleading to remarkable gains in both sample efficiency and performance across\ncomplex control tasks.", "AI": {"tldr": "MoGE is a novel exploration method that uses diffusion models to generate critical states and world models to create synthetic transitions, enhancing passive exploration in reinforcement learning without modifying core algorithms.", "motivation": "Existing exploration methods have limitations: active exploration struggles in high-dimensional environments, while passive exploration is constrained by limited sample diversity in replay buffers.", "method": "MoGE has two components: (1) a diffusion-based generator that synthesizes critical states guided by a utility function, and (2) a one-step imagination world model that constructs critical transitions from these states for agent learning.", "result": "Empirical results on OpenAI Gym and DeepMind Control Suite show MoGE effectively bridges exploration and policy learning, achieving remarkable gains in sample efficiency and performance across complex control tasks.", "conclusion": "MoGE provides a modular approach that seamlessly integrates with existing off-policy algorithms to improve exploration without altering their core structures, addressing limitations of both active and passive exploration methods."}}
{"id": "2510.25713", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25713", "abs": "https://arxiv.org/abs/2510.25713", "authors": ["Boshi An", "Chenyu Yang", "Robert Katzschmann"], "title": "Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models", "comment": null, "summary": "We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for\ndexterous human-robot collaboration with minimal language prompting. Our\napproach adds (i) FiLM conditioning to visual backbones for task-aware\nperception, (ii) an auxiliary intent head that predicts collaborator hand pose\nand target cues, and (iii) action-space post-processing that predicts compact\ndeltas (position/rotation) and PCA-reduced finger joints before mapping to full\ncommands. Using a multi-view, teleoperated Franka and Mimic-hand dataset\naugmented with MediaPipe hand poses, we demonstrate that delta actions are\nwell-behaved and that four principal components explain ~96% of hand-joint\nvariance. Ablations identify action post-processing as the primary performance\ndriver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is\ndetrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes\n\"pick-up\" and \"pass\" into a long-horizon behavior. We surface \"trainer\noverfitting\" to specific demonstrators as the key limitation.", "AI": {"tldr": "Adapted Open-VLA model for dexterous human-robot collaboration with minimal language prompting using FiLM conditioning, auxiliary intent prediction, and action-space post-processing.", "motivation": "Enable dexterous human-robot collaboration with minimal language requirements by adapting pre-trained VLA models.", "method": "Added FiLM conditioning for task-aware perception, auxiliary intent head for hand pose prediction, and action-space post-processing with delta actions and PCA-reduced finger joints.", "result": "Delta actions are well-behaved, 4 principal components explain ~96% of hand-joint variance, and real-time performance achieved with ~0.3s latency.", "conclusion": "Action post-processing is the primary performance driver, with trainer overfitting to specific demonstrators identified as the key limitation."}}
{"id": "2510.24904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24904", "abs": "https://arxiv.org/abs/2510.24904", "authors": ["Qiucheng Wu", "Handong Zhao", "Zhixin Shu", "Jing Shi", "Yang Zhang", "Shiyu Chang"], "title": "VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos", "comment": "19 pages, 9 figures", "summary": "Although recent text-to-video generative models are getting more capable of\nfollowing external camera controls, imposed by either text descriptions or\ncamera trajectories, they still struggle to generalize to unconventional camera\nmotions, which is crucial in creating truly original and artistic videos. The\nchallenge lies in the difficulty of finding sufficient training videos with the\nintended uncommon camera motions. To address this challenge, we propose\nVividCam, a training paradigm that enables diffusion models to learn complex\ncamera motions from synthetic videos, releasing the reliance on collecting\nrealistic training videos. VividCam incorporates multiple disentanglement\nstrategies that isolates camera motion learning from synthetic appearance\nartifacts, ensuring more robust motion representation and mitigating domain\nshift. We demonstrate that our design synthesizes a wide range of precisely\ncontrolled and complex camera motions using surprisingly simple synthetic data.\nNotably, this synthetic data often consists of basic geometries within a\nlow-poly 3D scene and can be efficiently rendered by engines like Unity. Our\nvideo results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .", "AI": {"tldr": "VividCam enables text-to-video models to learn complex camera motions from synthetic videos instead of relying on realistic training data, using disentanglement strategies to isolate motion learning from appearance artifacts.", "motivation": "Current text-to-video models struggle with unconventional camera motions due to lack of sufficient training data with intended uncommon camera movements, which limits artistic video creation.", "method": "Proposes a training paradigm using synthetic videos from engines like Unity, incorporating multiple disentanglement strategies to isolate camera motion learning from synthetic appearance artifacts and mitigate domain shift.", "result": "The method successfully synthesizes a wide range of precisely controlled and complex camera motions using surprisingly simple synthetic data consisting of basic geometries in low-poly 3D scenes.", "conclusion": "VividCam demonstrates that complex camera motions can be effectively learned from simple synthetic data, releasing the reliance on collecting realistic training videos for text-to-video generation."}}
{"id": "2510.25588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25588", "abs": "https://arxiv.org/abs/2510.25588", "authors": ["Eranga Bandara", "Ross Gore", "Atmaram Yarlagadda", "Anita H. Clayton", "Preston Samuel", "Christopher K. Rhea", "Sachin Shetty"], "title": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System", "comment": null, "summary": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses.", "AI": {"tldr": "A Fine-Tuned LLM Consortium integrated with OpenAI-gpt-oss reasoning LLM for clinical diagnosis of mental disorders, addressing subjective variability in psychiatric evaluations through AI standardization.", "motivation": "To address the subjective nature and variability in psychiatric diagnoses across clinicians and patients, which leads to inconsistencies and unreliable outcomes in mental health assessment.", "method": "Leverages fine-tuned LLMs trained on psychiatrist-patient conversational datasets, aggregates diagnostic predictions through consensus-based decision-making, and refines decisions using OpenAI-gpt-oss reasoning LLM with LLM agents orchestrating communication.", "result": "Experimental results demonstrate transformative potential with robust and highly accurate diagnostic system. A prototype integrating three fine-tuned LLMs with reasoning LLM was developed in collaboration with U.S. Army Medical Research Team.", "conclusion": "This represents the first application of fine-tuned LLM consortium integrated with reasoning LLM for clinical mental health diagnosis, paving the way for next-generation AI-powered eHealth systems to standardize psychiatric diagnoses."}}
{"id": "2510.25725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25725", "abs": "https://arxiv.org/abs/2510.25725", "authors": ["Eunju Kwon", "Seungwon Oh", "In-Chang Baek", "Yucheon Park", "Gyungbo Kim", "JaeYoung Moon", "Yunho Choi", "Kyung-Joong Kim"], "title": "A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation", "comment": null, "summary": "Contact-rich manipulation has become increasingly important in robot\nlearning. However, previous studies on robot learning datasets have focused on\nrigid objects and underrepresented the diversity of pressure conditions for\nreal-world manipulation. To address this gap, we present a humanoid\nvisual-tactile-action dataset designed for manipulating deformable soft\nobjects. The dataset was collected via teleoperation using a humanoid robot\nequipped with dexterous hands, capturing multi-modal interactions under varying\npressure conditions. This work also motivates future research on models with\nadvanced optimization strategies capable of effectively leveraging the\ncomplexity and diversity of tactile signals.", "AI": {"tldr": "A humanoid visual-tactile-action dataset for manipulating deformable soft objects, addressing the underrepresentation of pressure conditions in robot learning.", "motivation": "Previous robot learning datasets focused on rigid objects and underrepresented pressure condition diversity, creating a gap for real-world manipulation of deformable objects.", "method": "Dataset collected via teleoperation using a humanoid robot with dexterous hands, capturing multi-modal interactions under varying pressure conditions.", "result": "Created a comprehensive dataset for manipulating deformable soft objects with visual-tactile-action data.", "conclusion": "This work motivates future research on models with advanced optimization strategies to effectively leverage the complexity and diversity of tactile signals."}}
{"id": "2510.24907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24907", "abs": "https://arxiv.org/abs/2510.24907", "authors": ["Michal Stary", "Julien Gaubil", "Ayush Tewari", "Vincent Sitzmann"], "title": "Understanding Multi-View Transformers", "comment": "Presented at the ICCV 2025 E2E3D Workshop", "summary": "Multi-view transformers such as DUSt3R are revolutionizing 3D vision by\nsolving 3D tasks in a feed-forward manner. However, contrary to previous\noptimization-based pipelines, the inner mechanisms of multi-view transformers\nare unclear. Their black-box nature makes further improvements beyond data\nscaling challenging and complicates usage in safety- and reliability-critical\napplications. Here, we present an approach for probing and visualizing 3D\nrepresentations from the residual connections of the multi-view transformers'\nlayers. In this manner, we investigate a variant of the DUSt3R model, shedding\nlight on the development of its latent state across blocks, the role of the\nindividual layers, and suggest how it differs from methods with stronger\ninductive biases of explicit global pose. Finally, we show that the\ninvestigated variant of DUSt3R estimates correspondences that are refined with\nreconstructed geometry. The code used for the analysis is available at\nhttps://github.com/JulienGaubil/und3rstand .", "AI": {"tldr": "This paper presents an analysis method for understanding the inner workings of multi-view transformers like DUSt3R in 3D vision, focusing on probing and visualizing 3D representations from residual connections to reveal how these models develop their latent states.", "motivation": "Multi-view transformers have become black-box systems where inner mechanisms are unclear, making improvements challenging and complicating usage in safety-critical applications. Understanding how these models work is crucial for further development and reliable deployment.", "method": "The authors developed an approach for probing and visualizing 3D representations from the residual connections of multi-view transformers' layers, specifically analyzing a DUSt3R model variant to examine latent state development across blocks and individual layer roles.", "result": "The analysis revealed how DUSt3R develops its latent state across layers, the role of individual layers, and differences from methods with stronger inductive biases. It also showed that DUSt3R estimates correspondences that are refined with reconstructed geometry.", "conclusion": "The proposed probing and visualization method successfully illuminates the inner workings of multi-view transformers, providing insights into their operation that could enable further improvements and safer deployment in critical applications."}}
{"id": "2510.25612", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25612", "abs": "https://arxiv.org/abs/2510.25612", "authors": ["Amit Giloni", "Chiara Picardi", "Roy Betser", "Shamik Bose", "Aishvariya Priya Rathina Sabapathy", "Roman Vainshtein"], "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows", "comment": "Accepted to EMNLP 2025, 27 pages, 6 figures", "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.", "AI": {"tldr": "CAIR is the first method to assess agent influence in LLM-based multi-agent systems using counterfactual analysis, providing task-agnostic influence rankings that work both offline and at inference time.", "motivation": "The growing adoption of Agentic AI Workflows (AAWs) requires better understanding of agent operations, but existing methods only perform static structural analysis which is unsuitable for inference time execution.", "method": "Counterfactual-based Agent Influence Ranker (CAIR) performs counterfactual analysis to assess each agent's influence level on the AAW's final output and determine the most influential agents.", "result": "Evaluation on a custom AAWs dataset with 30 use cases and 230 functionalities showed CAIR produces consistent rankings, outperforms baseline methods, and enhances downstream task effectiveness.", "conclusion": "CAIR successfully addresses the gap in agent influence assessment for AAWs, providing a practical solution for understanding and improving multi-agent system operations."}}
{"id": "2510.25727", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25727", "abs": "https://arxiv.org/abs/2510.25727", "authors": ["Ciera McFarland", "Margaret McGuinness"], "title": "Modeling Collapse of Steered Vine Robots Under Their Own Weight", "comment": null, "summary": "Soft, vine-inspired growing robots that move by eversion are highly mobile in\nconfined environments, but, when faced with gaps in the environment, they may\ncollapse under their own weight while navigating a desired path. In this work,\nwe present a comprehensive collapse model that can predict the collapse length\nof steered robots in any shape using true shape information and tail tension.\nWe validate this model by collapsing several unsteered robots without true\nshape information. The model accurately predicts the trends of those\nexperiments. We then attempt to collapse a robot steered with a single actuator\nat different orientations. Our models accurately predict collapse when it\noccurs. Finally, we demonstrate how this could be used in the field by having a\nrobot attempt a gap-crossing task with and without inflating its actuators. The\nrobot needs its actuators inflated to cross the gap without collapsing, which\nour model supports. Our model has been specifically tested on straight and\nseries pouch motor-actuated robots made of non-stretchable material, but it\ncould be applied to other robot variations. This work enables us to model the\nrobot's collapse behavior in any open environment and understand the parameters\nit needs to succeed in 3D navigation tasks.", "AI": {"tldr": "A collapse model for soft, vine-inspired growing robots that predicts collapse length using true shape information and tail tension, validated through experiments with unsteered and steered robots.", "motivation": "Soft growing robots that move by eversion are highly mobile in confined environments but may collapse under their own weight when facing gaps, limiting their navigation capabilities.", "method": "Developed a comprehensive collapse model using true shape information and tail tension, validated through experiments with unsteered robots and steered robots with single actuators at different orientations.", "result": "The model accurately predicts collapse trends in unsteered robots and collapse occurrences in steered robots. Demonstrated practical application in gap-crossing tasks where robots need inflated actuators to avoid collapse.", "conclusion": "The collapse model enables prediction of robot collapse behavior in open environments and understanding of parameters needed for successful 3D navigation tasks, with potential application to various robot variations beyond the tested configurations."}}
{"id": "2510.24919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24919", "abs": "https://arxiv.org/abs/2510.24919", "authors": ["Hossein R. Nowdeh", "Jie Ji", "Xiaolong Ma", "Fatemeh Afghah"], "title": "Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning", "comment": null, "summary": "In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.", "AI": {"tldr": "M-SAM is a model-agnostic framework that addresses modality imbalance in multimodal learning by identifying dominant modalities using Shapley values, modulating the loss landscape to prioritize robustness for dominant modalities, and updating weights with modulated gradients.", "motivation": "To overcome the limitation where dominant modalities overshadow others in multimodal learning, which restricts generalization and balanced learning across all modalities.", "method": "Three-step iterative optimization: 1) Identify dominant modality using Shapley values based on accuracy contributions, 2) Decompose loss landscape by modulating loss to prioritize robustness for dominant modality, 3) Update weights via backpropagation of modulated gradients.", "result": "Extensive experiments on four diverse datasets show M-SAM outperforms state-of-the-art optimization and gradient manipulation methods, significantly balancing and improving multimodal learning performance.", "conclusion": "M-SAM effectively ensures robust learning for dominant modalities while enhancing contributions from others, enabling models to explore and exploit complementary features that strengthen overall multimodal performance."}}
{"id": "2510.25668", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25668", "abs": "https://arxiv.org/abs/2510.25668", "authors": ["Tianyu Yang", "Terry Ruas", "Yijun Tian", "Jan Philip Wahle", "Daniel Kurzawe", "Bela Gipp"], "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents", "comment": null, "summary": "Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.", "AI": {"tldr": "ALDEN is a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents for actively navigating long, visually rich documents, achieving state-of-the-art performance on five benchmarks.", "motivation": "Vision-language models struggle with long, complex documents that require analysis across multiple pages, and existing approaches use rigid templates that limit efficiency and generalization.", "method": "ALDEN uses multi-turn reinforcement learning with fetch actions for direct page access, cross-level rewards for supervision, and visual-semantic anchoring to stabilize training with numerous visual tokens.", "result": "ALDEN achieves state-of-the-art performance on five long-document benchmarks after training on a corpus from three open-source datasets.", "conclusion": "ALDEN represents a shift from passive document reading to autonomous agents that navigate and reason across long documents, providing a robust path for more accurate and efficient long-document understanding."}}
{"id": "2510.25754", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25754", "abs": "https://arxiv.org/abs/2510.25754", "authors": ["Bohan Wu", "Paul de La Sayette", "Li Fei-Fei", "Roberto Mart\u00edn-Mart\u00edn"], "title": "GET-USE: Learning Generalized Tool Usage for Bimanual Mobile Manipulation via Simulated Embodiment Extensions", "comment": "8 pages, 7 figures", "summary": "The ability to use random objects as tools in a generalizable manner is a\nmissing piece in robots' intelligence today to boost their versatility and\nproblem-solving capabilities. State-of-the-art robotic tool usage methods\nfocused on procedurally generating or crowd-sourcing datasets of tools for a\ntask to learn how to grasp and manipulate them for that task. However, these\nmethods assume that only one object is provided and that it is possible, with\nthe correct grasp, to perform the task; they are not capable of identifying,\ngrasping, and using the best object for a task when many are available,\nespecially when the optimal tool is absent. In this work, we propose GeT-USE, a\ntwo-step procedure that learns to perform real-robot generalized tool usage by\nlearning first to extend the robot's embodiment in simulation and then\ntransferring the learned strategies to real-robot visuomotor policies. Our key\ninsight is that by exploring a robot's embodiment extensions (i.e., building\nnew end-effectors) in simulation, the robot can identify the general tool\ngeometries most beneficial for a task. This learned geometric knowledge can\nthen be distilled to perform generalized tool usage tasks by selecting and\nusing the best available real-world object as tool. On a real robot with 22\ndegrees of freedom (DOFs), GeT-USE outperforms state-of-the-art methods by\n30-60% success rates across three vision-based bimanual mobile manipulation\ntool-usage tasks.", "AI": {"tldr": "GeT-USE enables robots to identify, grasp, and use the best available objects as tools by learning geometric knowledge through embodiment extension in simulation and transferring it to real-world visuomotor policies.", "motivation": "Current robotic tool usage methods can only handle single provided objects and assume the object can perform the task with correct grasp, lacking the ability to select the best object from multiple options when the optimal tool is absent.", "method": "Two-step procedure: first learns to extend robot's embodiment in simulation to identify general tool geometries beneficial for tasks, then transfers learned strategies to real-robot visuomotor policies.", "result": "On a real robot with 22 DOFs, GeT-USE outperforms state-of-the-art methods by 30-60% success rates across three vision-based bimanual mobile manipulation tool-usage tasks.", "conclusion": "Learning geometric knowledge through embodiment extension in simulation enables effective generalized tool usage by allowing robots to select and use the best available real-world objects as tools."}}
{"id": "2510.24936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24936", "abs": "https://arxiv.org/abs/2510.24936", "authors": ["Alison M. Fernandes", "Hermes I. Del Monego", "Bruno S. Chang", "Anelise Munaretto", "H\u00e9lder M. Fontes", "Rui L. Campos"], "title": "IBIS: A Powerful Hybrid Architecture for Human Activity Recognition", "comment": "8 pages. 8 figures. Wireless Days Conference, December 2025", "summary": "The increasing interest in Wi-Fi sensing stems from its potential to capture\nenvironmental data in a low-cost, non-intrusive way, making it ideal for\napplications like healthcare, space occupancy analysis, and gesture-based IoT\ncontrol. However, a major limitation in this field is the common problem of\noverfitting, where models perform well on training data but fail to generalize\nto new data. To overcome this, we introduce a novel hybrid architecture that\nintegrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer\nto as IBIS. Our IBIS approach is uniquely engineered to improve model\ngeneralization and create more robust classification boundaries. By applying\nthis method to Doppler-derived data, we achieve a movement recognition accuracy\nof nearly 99%. Comprehensive performance metrics and confusion matrices confirm\nthe significant effectiveness of our proposed solution.", "AI": {"tldr": "The paper introduces IBIS, a hybrid Inception-BiLSTM-SVM architecture for Wi-Fi sensing that achieves 99% movement recognition accuracy by addressing overfitting and improving generalization.", "motivation": "Wi-Fi sensing offers low-cost, non-intrusive environmental data capture for healthcare, occupancy analysis, and IoT control, but suffers from overfitting problems where models fail to generalize to new data.", "method": "Proposed IBIS - a novel hybrid architecture integrating Inception-BiLSTM with Support Vector Machine (SVM) to improve model generalization and create robust classification boundaries, applied to Doppler-derived data.", "result": "Achieved movement recognition accuracy of nearly 99% on Doppler-derived data, with comprehensive performance metrics and confusion matrices confirming significant effectiveness.", "conclusion": "The IBIS hybrid architecture successfully addresses overfitting in Wi-Fi sensing and demonstrates superior generalization capabilities for robust movement recognition applications."}}
{"id": "2510.25679", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.25679", "abs": "https://arxiv.org/abs/2510.25679", "authors": ["Federica Tonti", "Ricardo Vinuesa"], "title": "Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for\ndelivery and surveillance purposes. In this work, we develop an optimal\nnavigation strategy based on Deep Reinforcement Learning. The environment is\nrepresented by a three-dimensional high-fidelity simulation of an urban flow,\ncharacterized by turbulence and recirculation zones. The algorithm presented\nhere is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated\nTransformer eXtra Large (GTrXL) architecture, giving the agent richer\ninformation about the turbulent flow field in which it navigates. The results\nare compared with a PPO+GTrXL without the secondary prediction tasks, a PPO\ncombined with Long Short Term Memory (LSTM) cells and a traditional navigation\nalgorithm. The obtained results show a significant increase in the success rate\n(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the\nclassical Zermelo's navigation algorithm, paving the way to a completely\nreimagined UAV landscape in complex urban environments.", "AI": {"tldr": "Developed an optimal UAV navigation strategy using Deep Reinforcement Learning with flow-aware PPO and GTrXL architecture, achieving higher success rates and lower crash rates compared to baseline methods in turbulent urban environments.", "motivation": "UAVs are increasingly used in urban areas for delivery and surveillance, but face challenges navigating complex turbulent urban flows with recirculation zones.", "method": "Flow-aware Proximal Policy Optimization (PPO) combined with Gated Transformer eXtra Large (GTrXL) architecture, using a 3D high-fidelity urban flow simulation with turbulence.", "result": "Significant increase in success rate (SR) and lower crash rate (CR) compared to PPO+LSTM, PPO+GTrXL without secondary prediction tasks, and traditional Zermelo's navigation algorithm.", "conclusion": "The approach paves the way for completely reimagined UAV navigation in complex urban environments by effectively handling turbulent flow conditions."}}
{"id": "2510.25768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25768", "abs": "https://arxiv.org/abs/2510.25768", "authors": ["Kush Hari", "Ziyang Chen", "Hansoul Kim", "Ken Goldberg"], "title": "STITCH 2.0: Extending Augmented Suturing with EKF Needle Estimation and Thread Management", "comment": "Published in RA-L 2025", "summary": "Surgical suturing is a high-precision task that impacts patient healing and\nscarring. Suturing skill varies widely between surgeons, highlighting the need\nfor robot assistance. Previous robot suturing works, such as STITCH 1.0 [1],\nstruggle to fully close wounds due to inaccurate needle tracking and poor\nthread management. To address these challenges, we present STITCH 2.0, an\nelevated augmented dexterity pipeline with seven improvements including:\nimproved EKF needle pose estimation, new thread untangling methods, and an\nautomated 3D suture alignment algorithm. Experimental results over 15 trials\nfind that STITCH 2.0 on average achieves 74.4% wound closure with 4.87 sutures\nper trial, representing 66% more sutures in 38% less time compared to the\nprevious baseline. When two human interventions are allowed, STITCH 2.0\naverages six sutures with 100% wound closure rate. Project website:\nhttps://stitch-2.github.io/", "AI": {"tldr": "STITCH 2.0 improves robot-assisted surgical suturing with enhanced needle tracking, thread management, and automated suture alignment, achieving 74.4% wound closure and 66% more sutures in 38% less time compared to previous methods.", "motivation": "Surgical suturing quality varies widely between surgeons and affects patient outcomes. Previous robot suturing systems like STITCH 1.0 struggled with inaccurate needle tracking and poor thread management, limiting their ability to fully close wounds.", "method": "STITCH 2.0 introduces an augmented dexterity pipeline with seven key improvements: enhanced EKF needle pose estimation, new thread untangling methods, and an automated 3D suture alignment algorithm.", "result": "In 15 trials, STITCH 2.0 achieved 74.4% wound closure with 4.87 sutures per trial - 66% more sutures in 38% less time than baseline. With two human interventions, it averaged six sutures with 100% wound closure rate.", "conclusion": "STITCH 2.0 significantly advances robot-assisted surgical suturing by addressing key limitations in needle tracking and thread management, demonstrating substantial improvements in wound closure efficiency and effectiveness."}}
{"id": "2510.24980", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24980", "abs": "https://arxiv.org/abs/2510.24980", "authors": ["Reza Saadati Fard", "Emmanuel Agu", "Palawat Busaranuvong", "Deepak Kumar", "Shefalika Gautam", "Bengisu Tulu", "Diane Strong", "Lorraine Loretz"], "title": "FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning", "comment": null, "summary": "Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.", "AI": {"tldr": "FT-ARM is a fine-tuned multimodal large language model with agentic self-reflection that achieves 85% accuracy in pressure ulcer severity classification, outperforming prior CNN models by 4% while providing interpretable natural-language explanations.", "motivation": "Pressure ulcer severity classification is challenging due to subtle visual distinctions and subjective interpretation among clinicians, leading to variability. Prior AI approaches lacked interpretability despite promising accuracy.", "method": "FT-ARM fine-tunes LLaMA 3.2 90B with an agentic self-reflection mechanism that iteratively refines predictions by reasoning over visual features and clinical knowledge from text, mimicking clinician-style diagnostic reassessment.", "result": "On the Pressure Injury Image Dataset, FT-ARM achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based models by +4%. It also produces clinically grounded natural-language explanations and is designed for live inference.", "conclusion": "FT-ARM advances the reliability, transparency, and clinical applicability of automated wound assessment systems by integrating fine-tuning and reflective reasoning across multimodal inputs, addressing the need for consistent and explainable PU staging."}}
{"id": "2510.25724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25724", "abs": "https://arxiv.org/abs/2510.25724", "authors": ["Vanya Arikutharam", "Arkadiy Ukolov"], "title": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph", "comment": null, "summary": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.", "AI": {"tldr": "BambooKG is a knowledge graph with frequency-based weights on non-triplet edges that improves retrieval-augmented generation by reducing information loss and enhancing multi-hop reasoning.", "motivation": "Retrieval-Augmented Generation struggles with multi-hop reasoning across documents and knowledge graphs miss non-triplet information, leading to information loss.", "method": "Introduces BambooKG with frequency-based weights on non-triplet edges based on the Hebbian principle \"fire together, wire together\" to capture link strength.", "result": "BambooKG decreases information loss and outperforms existing solutions on both single- and multi-hop reasoning tasks.", "conclusion": "The frequency-weighted non-triplet edges in BambooKG effectively enhance structured reasoning capabilities in retrieval-augmented systems."}}
{"id": "2510.25032", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25032", "abs": "https://arxiv.org/abs/2510.25032", "authors": ["Zahra Ebrahimi Vargoorani", "Amir Mohammad Ghoreyshi", "Ching Yee Suen"], "title": "Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8", "comment": "6 pages, 8 figures. Presented at 2025 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,\n  Istanbul, Turkey", "summary": "Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.", "AI": {"tldr": "Proposes a deep learning-based ALPR system using YOLOv8 and semi-supervised learning with Grounding DINO for automatic annotation, achieving high recall rates on multiple datasets.", "motivation": "ALPR systems face challenges from environmental factors, vehicle speeds, camera angles, and low-quality images, while being vital for traffic control, parking, vehicle tracking, toll collection, and law enforcement.", "method": "Uses YOLOv8 for license plate detection and recognition, combined with semi-supervised learning framework that integrates manually labeled data with pseudo-labels generated by Grounding DINO for automatic annotation.", "result": "Achieved 94% recall on CENPARMI dataset and 91% on UFPR-ALPR dataset, with character error rates reported for both datasets.", "conclusion": "The semi-supervised approach with Grounding DINO enables efficient dataset scaling while maintaining label quality, significantly enhancing training process and overall model performance in license plate recognition."}}
{"id": "2510.25758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25758", "abs": "https://arxiv.org/abs/2510.25758", "authors": ["He Hu", "Yucheng Zhou", "Chiyuan Ma", "Qianning Wang", "Zheng Zhang", "Fei Ma", "Laizhong Cui", "Qi Tian"], "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling", "comment": null, "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.", "AI": {"tldr": "TheraMind introduces a dual-loop architecture for longitudinal psychological counseling that separates tactical dialogue management (Intra-Session Loop) from strategic therapeutic planning (Cross-Session Loop), enabling emotional understanding, adaptive strategies, and long-term memory across multiple sessions.", "motivation": "Existing LLM approaches in psychological counseling lack emotional understanding, adaptive strategies, and therapeutic methods across multiple sessions with long-term memory, making them inadequate for real clinical practice.", "method": "A novel dual-loop architecture with Intra-Session Loop for tactical dialogue management (perceiving emotional states, dynamic strategy selection) and Cross-Session Loop for strategic therapeutic planning (evaluating therapy efficacy, adjusting methods across sessions) with cross-session memory.", "result": "TheraMind outperforms other methods in high-fidelity simulations, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design.", "conclusion": "The dual-loop architecture effectively emulates strategic, adaptive, and longitudinal therapeutic behavior, addressing critical gaps in existing psychological counseling approaches."}}
{"id": "2510.25051", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25051", "abs": "https://arxiv.org/abs/2510.25051", "authors": ["Shunjie-Fabian Zheng", "Hyeonjun Lee", "Thijs Kooi", "Ali Diba"], "title": "Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models", "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025", "summary": "Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.", "AI": {"tldr": "A novel multi-modal framework combining 2D mammogram visual features with clinical metadata and synthesized radiological reports achieves superior breast cancer detection performance compared to unimodal approaches.", "motivation": "Address limitations of existing CAD systems in handling multi-modal data interpretation and clinical deployment feasibility, particularly the requirement for prior clinical history.", "method": "Synergistic integration of convolutional neural networks (ConvNets) with language representations using innovative tokenization modules to combine visual features from mammograms with structured textual descriptors from clinical metadata and synthesized reports.", "result": "Superior performance in cancer detection and calcification identification compared to unimodal baselines, with particular improvements across diverse populations in multi-national cohort screening mammograms.", "conclusion": "Establishes a new paradigm for clinically viable VLM-based CAD systems that effectively leverage imaging data and contextual patient information through effective fusion mechanisms."}}
{"id": "2510.25058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25058", "abs": "https://arxiv.org/abs/2510.25058", "authors": ["Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu"], "title": "Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023 Challenge", "comment": "BraTS23 winner", "summary": "In this work, we describe our solution to the BraTS 2023 cluster of\nchallenges using Auto3DSeg from MONAI. We participated in all 5 segmentation\nchallenges, and achieved the 1st place results in three of them: Brain\nMetastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place\nresults in the remaining two: Adult and Pediatic Glioma challenges.", "AI": {"tldr": "The authors achieved top results in 3 out of 5 BraTS 2023 segmentation challenges using Auto3DSeg from MONAI, placing 1st in Brain Metastasis, Brain Meningioma, and BraTS-Africa, and 2nd in Adult and Pediatric Glioma.", "motivation": "To participate in and excel across all 5 segmentation challenges in the BraTS 2023 competition, demonstrating the effectiveness of their automated segmentation approach.", "method": "Used Auto3DSeg from MONAI, an automated 3D segmentation framework, to develop solutions for brain tumor segmentation across different tumor types and patient populations.", "result": "Achieved 1st place in Brain Metastasis, Brain Meningioma, and BraTS-Africa challenges, and 2nd place in Adult and Pediatric Glioma challenges.", "conclusion": "The Auto3DSeg framework proved highly effective for brain tumor segmentation, achieving top-tier performance across multiple challenge categories in the BraTS 2023 competition."}}
{"id": "2510.25314", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.25314", "abs": "https://arxiv.org/abs/2510.25314", "authors": ["Zongxi Yu", "Xiaolong Qian", "Shaohua Gao", "Qi Jiang", "Yao Gao", "Kailun Yang", "Kaiwei Wang"], "title": "Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design", "comment": "The source code will be publicly available at\n  https://github.com/ZongxiYu-ZJU/BMI", "summary": "Achieving high-fidelity, compact RGBD imaging presents a dual challenge:\nconventional compact optics struggle with RGB sharpness across the entire\ndepth-of-field, while software-only Monocular Depth Estimation (MDE) is an\nill-posed problem reliant on unreliable semantic priors. While deep optics with\nelements like DOEs can encode depth, they introduce trade-offs in fabrication\ncomplexity and chromatic aberrations, compromising simplicity. To address this,\nwe first introduce a novel bio-inspired all-spherical monocentric lens, around\nwhich we build the Bionic Monocentric Imaging (BMI) framework, a holistic\nco-design. This optical design naturally encodes depth into its depth-varying\nPoint Spread Functions (PSFs) without requiring complex diffractive or freeform\nelements. We establish a rigorous physically-based forward model to generate a\nsynthetic dataset by precisely simulating the optical degradation process. This\nsimulation pipeline is co-designed with a dual-head, multi-scale reconstruction\nnetwork that employs a shared encoder to jointly recover a high-fidelity\nAll-in-Focus (AiF) image and a precise depth map from a single coded capture.\nExtensive experiments validate the state-of-the-art performance of the proposed\nframework. In depth estimation, the method attains an Abs Rel of 0.026 and an\nRMSE of 0.130, markedly outperforming leading software-only approaches and\nother deep optics systems. For image restoration, the system achieves an SSIM\nof 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior\nbalance between image fidelity and depth accuracy. This study illustrates that\nthe integration of bio-inspired, fully spherical optics with a joint\nreconstruction algorithm constitutes an effective strategy for addressing the\nintrinsic challenges in high-performance compact RGBD imaging. Source code will\nbe publicly available at https://github.com/ZongxiYu-ZJU/BMI.", "AI": {"tldr": "The paper presents Bionic Monocentric Imaging (BMI), a co-designed framework using bio-inspired all-spherical monocentric lenses that naturally encode depth into depth-varying PSFs, enabling joint recovery of high-fidelity All-in-Focus images and precise depth maps from single coded captures.", "motivation": "Address the dual challenge in compact RGBD imaging where conventional optics struggle with RGB sharpness across depth-of-field and software-only Monocular Depth Estimation relies on unreliable semantic priors, while avoiding the fabrication complexity and chromatic aberrations of deep optics with elements like DOEs.", "method": "Introduces a bio-inspired all-spherical monocentric lens that naturally encodes depth into depth-varying PSFs without complex elements. Establishes a physically-based forward model for synthetic dataset generation, co-designed with a dual-head, multi-scale reconstruction network using shared encoder to jointly recover AiF images and depth maps.", "result": "Achieves state-of-the-art performance: depth estimation with Abs Rel of 0.026 and RMSE of 0.130, outperforming software-only approaches and other deep optics systems; image restoration with SSIM of 0.960 and LPIPS of 0.082, confirming superior balance between image fidelity and depth accuracy.", "conclusion": "Integration of bio-inspired fully spherical optics with joint reconstruction algorithm effectively addresses intrinsic challenges in high-performance compact RGBD imaging, demonstrating that simple spherical optics can achieve superior performance without complex diffractive or freeform elements."}}
{"id": "2510.25067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25067", "abs": "https://arxiv.org/abs/2510.25067", "authors": ["Yusen Peng", "Sachin Kumar"], "title": "DRIP: Dynamic patch Reduction via Interpretable Pooling", "comment": null, "summary": "Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.", "AI": {"tldr": "DRIP is a method that dynamically reduces visual tokens in deeper layers of vision encoders to improve efficiency while maintaining performance in vision-language models.", "motivation": "Vision-language models require expensive large-scale pretraining, creating efficiency concerns that discourage researchers from pretraining from scratch.", "method": "Dynamic patch Reduction via Interpretable Pooling (DRIP) adapts to input images and dynamically merges tokens in deeper layers of visual encoders.", "result": "Significant GFLOP reduction while maintaining comparable classification/zero-shot performance on ImageNet and CLIP pretraining, with validation on biology datasets.", "conclusion": "DRIP provides an effective approach to improve efficiency in vision-language model pretraining without sacrificing performance."}}
{"id": "2510.25463", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25463", "abs": "https://arxiv.org/abs/2510.25463", "authors": ["Hongjie Zhang", "Gideon Billings", "Stefan B. Williams"], "title": "SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments", "comment": null, "summary": "Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.", "AI": {"tldr": "SPADE is a monocular depth estimation pipeline that combines pre-trained relative depth estimators with sparse depth priors to produce dense, metric scale depth maps for underwater vehicle navigation.", "motivation": "Underwater infrastructure inspection faces challenges with human divers and ROVs due to perceptual limitations in complex structures and turbid water. Enhancing spatial awareness of underwater vehicles is crucial for reducing piloting risks and enabling autonomy.", "method": "Two-stage approach: first scales relative depth map with sparse depth points, then refines final metric prediction using Cascade Conv-Deformable Transformer blocks.", "result": "Achieves improved accuracy and generalization over state-of-the-art baselines, runs efficiently at over 15 FPS on embedded hardware.", "conclusion": "SPADE promises to support practical underwater inspection and intervention by providing reliable depth estimation for autonomous underwater vehicles."}}
{"id": "2510.25070", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25070", "abs": "https://arxiv.org/abs/2510.25070", "authors": ["Manjunath Prasad Holenarasipura Rajiv", "B. M. Vidyavathi"], "title": "Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments", "comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025", "summary": "Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.", "AI": {"tldr": "A vision-language integration framework that combines CLIP/ViT visual encoders with GPT-based language models for zero-shot scene understanding, achieving significant improvements in object recognition, activity detection, and scene captioning without labeled examples.", "motivation": "To address the challenges of zero-shot scene understanding in complex real-world settings where models must recognize new objects, actions, and contexts without prior labeled examples.", "method": "Develops a unified model that embeds visual inputs and textual prompts into a shared space, followed by multimodal fusion and reasoning layers for contextual interpretation, leveraging natural language as a bridge for generalization.", "result": "Achieves up to 18% improvement in top-1 accuracy and notable gains in semantic coherence metrics on Visual Genome, COCO, ADE20K, and custom real-world datasets, outperforming state-of-the-art zero-shot models.", "conclusion": "The framework demonstrates the effectiveness of cross-modal alignment and language grounding in enhancing generalization for real-world scene understanding through vision-language integration."}}
{"id": "2510.25077", "categories": ["cs.CV", "eess.IV", "68T07", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.25077", "abs": "https://arxiv.org/abs/2510.25077", "authors": ["Fahimeh Orvati Nia", "Amirmohammad Mohammadi", "Salim Al Kharsa", "Pragati Naikare", "Zigfried Hampel-Arias", "Joshua Peeples"], "title": "Neighborhood Feature Pooling for Remote Sensing Image Classification", "comment": "9 pages, 5 figures. Accepted to WACV 2026 (Winter Conference on\n  Applications of Computer Vision)", "summary": "In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.", "AI": {"tldr": "Proposes neighborhood feature pooling (NFP) as a novel texture feature extraction method for remote sensing image classification that captures local neighborhood relationships and improves performance across datasets.", "motivation": "To develop an efficient texture feature extraction method for remote sensing image classification that can capture local similarities and relationships between neighboring inputs.", "method": "Uses neighborhood feature pooling (NFP) implemented with convolutional layers to capture relationships between neighboring inputs and aggregate local similarities across feature dimensions. Can be seamlessly integrated into any network architecture.", "result": "NFP consistently improves performance across diverse datasets and architectures compared to baseline models, while maintaining minimal parameter overhead.", "conclusion": "NFP is an effective texture feature extraction method that enhances remote sensing image classification performance with minimal computational cost and can be easily integrated into existing networks."}}
{"id": "2510.25084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25084", "abs": "https://arxiv.org/abs/2510.25084", "authors": ["Xiang liu", "Zhaoxiang Liu", "Huan Hu", "Zipeng Wang", "Ping Chen", "Zezhou Chen", "Kai Wang", "Shiguo Lian"], "title": "PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation with Controllable Face Attributes", "comment": "Accepted by Image and Vision Computing (18 pages, 8 figures)", "summary": "Recent advancements in personalized image generation have significantly\nimproved facial identity preservation, particularly in fields such as\nentertainment and social media. However, existing methods still struggle to\nachieve precise control over facial attributes in a per-subject-tuning-free\n(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by\nproviding fine-grained control over facial features, but they often require\nextensive technical expertise and additional training data, limiting their\naccessibility. In contrast, PSTF approaches simplify the process by enabling\nimage generation from a single facial input, but they lack precise control over\nfacial attributes. In this paper, we introduce a novel, PSTF method that\nenables both precise control over facial attributes and high-fidelity\npreservation of facial identity. Our approach utilizes a face recognition model\nto extract facial identity features, which are then mapped into the $W^+$\nlatent space of StyleGAN2 using the e4e encoder. We further enhance the model\nwith a Triplet-Decoupled Cross-Attention module, which integrates facial\nidentity, attribute features, and text embeddings into the UNet architecture,\nensuring clean separation of identity and attribute information. Trained on the\nFFHQ dataset, our method allows for the generation of personalized images with\nfine-grained control over facial attributes, while without requiring additional\nfine-tuning or training data for individual identities. We demonstrate that our\napproach successfully balances personalization with precise facial attribute\ncontrol, offering a more efficient and user-friendly solution for high-quality,\nadaptable facial image synthesis. The code is publicly available at\nhttps://github.com/UnicomAI/PSTF-AttControl.", "AI": {"tldr": "A novel per-subject-tuning-free (PSTF) method for personalized image generation that enables precise control over facial attributes while preserving facial identity, using face recognition features mapped to StyleGAN2's latent space and a Triplet-Decoupled Cross-Attention module.", "motivation": "Existing methods struggle with precise facial attribute control in PSTF approaches, while tuning-based methods require extensive expertise and training data. There's a need for a solution that combines precise attribute control with PSTF simplicity.", "method": "Uses face recognition model to extract identity features, maps them to StyleGAN2's W+ latent space via e4e encoder, and employs Triplet-Decoupled Cross-Attention module to integrate identity, attribute features, and text embeddings in UNet architecture.", "result": "The method successfully generates personalized images with fine-grained facial attribute control while preserving identity, without requiring additional fine-tuning or training data for individual identities.", "conclusion": "The approach balances personalization with precise facial attribute control, offering an efficient and user-friendly solution for high-quality facial image synthesis that outperforms existing PSTF methods."}}
{"id": "2510.25094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25094", "abs": "https://arxiv.org/abs/2510.25094", "authors": ["Chanhyeong Yang", "Taehoon Song", "Jihwan Park", "Hyunwoo J. Kim"], "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.", "AI": {"tldr": "VDRP is a framework for zero-shot human-object interaction detection that addresses visual complexity through diversity-aware prompt learning and region-specific concept retrieval.", "motivation": "Existing approaches fail to handle visual complexity in interactions, including intra-class diversity (same verb appearing in different poses/contexts) and inter-class entanglement (different verbs having similar visual patterns).", "method": "Uses visual diversity-aware prompt learning with group-wise variance injection and Gaussian perturbation, plus region-specific concept retrieval from human/object/union regions to create region-aware prompts.", "result": "Achieves state-of-the-art performance on HICO-DET benchmark under four zero-shot evaluation settings, effectively addressing both intra-class diversity and inter-class visual entanglement.", "conclusion": "The proposed VDRP framework successfully handles visual complexity in zero-shot HOI detection through diversity-aware and region-aware prompt learning strategies."}}
{"id": "2510.25129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25129", "abs": "https://arxiv.org/abs/2510.25129", "authors": ["Xiyu Zhang", "Chong Bao", "Yipeng Chen", "Hongjia Zhai", "Yitong Dong", "Hujun Bao", "Zhaopeng Cui", "Guofeng Zhang"], "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians", "comment": "18 pages, 11 figures. NeurIPS 2025; Project page:\n  https://zju3dv.github.io/AtlasGS/", "summary": "3D reconstruction of indoor and urban environments is a prominent research\ntopic with various downstream applications. However, existing geometric priors\nfor addressing low-texture regions in indoor and urban settings often lack\nglobal consistency. Moreover, Gaussian Splatting and implicit SDF fields often\nsuffer from discontinuities or exhibit computational inefficiencies, resulting\nin a loss of detail. To address these issues, we propose an Atlanta-world\nguided implicit-structured Gaussian Splatting that achieves smooth indoor and\nurban scene reconstruction while preserving high-frequency details and\nrendering efficiency. By leveraging the Atlanta-world model, we ensure the\naccurate surface reconstruction for low-texture regions, while the proposed\nnovel implicit-structured GS representations provide smoothness without\nsacrificing efficiency and high-frequency details. Specifically, we propose a\nsemantic GS representation to predict the probability of all semantic regions\nand deploy a structure plane regularization with learnable plane indicators for\nglobal accurate surface reconstruction. Extensive experiments demonstrate that\nour method outperforms state-of-the-art approaches in both indoor and urban\nscenes, delivering superior surface reconstruction quality.", "AI": {"tldr": "Proposes Atlanta-world guided implicit-structured Gaussian Splatting for smooth indoor/urban scene reconstruction that preserves details while maintaining efficiency.", "motivation": "Existing geometric priors lack global consistency for low-texture regions, and current methods (Gaussian Splatting, implicit SDF) suffer from discontinuities or computational inefficiencies causing detail loss.", "method": "Combines Atlanta-world model for accurate surface reconstruction in low-texture regions with novel implicit-structured GS representations. Uses semantic GS representation to predict semantic region probabilities and structure plane regularization with learnable plane indicators.", "result": "Extensive experiments show the method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.", "conclusion": "The proposed approach achieves smooth reconstruction while preserving high-frequency details and rendering efficiency, addressing key limitations of existing methods."}}
{"id": "2510.25134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25134", "abs": "https://arxiv.org/abs/2510.25134", "authors": ["Qingdong Cai", "Charith Abhayaratne"], "title": "Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks", "comment": "Preprint for journal paper", "summary": "Class Activation Mapping (CAM) methods are widely applied in weakly\nsupervised learning tasks due to their ability to highlight object regions.\nHowever, conventional CAM methods highlight only the most discriminative\nregions of the target. These highlighted regions often fail to cover the entire\nobject and are frequently misaligned with object boundaries, thereby limiting\nthe performance of downstream weakly supervised learning tasks, particularly\nWeakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise\naccurate activation maps to get the best results. To alleviate the above\nproblems, we propose a novel activation method, Region-CAM. Distinct from\nnetwork feature weighting approaches, Region-CAM generates activation maps by\nextracting semantic information maps (SIMs) and performing semantic information\npropagation (SIP) by considering both gradients and features in each of the\nstages of the baseline classification model. Our approach highlights a greater\nproportion of object regions while ensuring activation maps to have precise\nboundaries that align closely with object edges. Region-CAM achieves 60.12% and\n58.43% mean intersection over union (mIoU) using the baseline model on the\nPASCAL VOC training and validation datasets, respectively, which are\nimprovements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On\nthe MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement\nover the original CAM (20.15%). We also demonstrate the superiority of\nRegion-CAM in object localization tasks, using the ILSVRC2012 validation set.\nRegion-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with\nLayerCAM, an activation method designed for weakly supervised object\nlocalization, Region-CAM achieves 4.5% better performance in Loc1.", "AI": {"tldr": "Region-CAM is a novel activation method that improves upon conventional CAM by generating more complete object coverage and precise boundaries through semantic information maps and propagation using both gradients and features.", "motivation": "Conventional CAM methods only highlight the most discriminative regions, failing to cover entire objects and having poor boundary alignment, which limits performance in weakly supervised semantic segmentation tasks.", "method": "Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) using both gradients and features from all stages of the baseline classification model.", "result": "Achieved 60.12% mIoU on PASCAL VOC training set (13.61% improvement over CAM), 58.43% on validation set (13.13% improvement), and 36.38% on MS COCO (16.23% improvement). Also achieved 51.7% Top-1 localization accuracy on ILSVRC2012, outperforming LayerCAM by 4.5%.", "conclusion": "Region-CAM significantly outperforms conventional CAM methods by providing more complete object coverage and precise boundaries, making it highly effective for weakly supervised learning tasks."}}
{"id": "2510.25140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25140", "abs": "https://arxiv.org/abs/2510.25140", "authors": ["Malaisree P", "Youwai S", "Kitkobsin T", "Janrungautai S", "Amorndechaphon D", "Rojanavasu P"], "title": "DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications", "comment": null, "summary": "Object detection in civil engineering applications is constrained by limited\nannotated data in specialized domains. We introduce DINO-YOLO, a hybrid\narchitecture combining YOLOv12 with DINOv3 self-supervised vision transformers\nfor data-efficient detection. DINOv3 features are strategically integrated at\ntwo locations: input preprocessing (P0) and mid-backbone enhancement (P3).\nExperimental validation demonstrates substantial improvements: Tunnel Segment\nCrack detection (648 images) achieves 12.4% improvement, Construction PPE (1K\nimages) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while\nmaintaining real-time inference (30-47 FPS). Systematic ablation across five\nYOLO scales and nine DINOv3 variants reveals that Medium-scale architectures\nachieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while\nSmall-scale requires Triple Integration (53.63%). The 2-4x inference overhead\n(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on\nNVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil\nengineering datasets (<10K images) while preserving computational efficiency,\nproviding practical solutions for construction safety monitoring and\ninfrastructure inspection in data-constrained environments.", "AI": {"tldr": "DINO-YOLO combines YOLOv12 with DINOv3 self-supervised transformers for data-efficient object detection in civil engineering, achieving significant performance improvements (12.4-88.6%) across specialized datasets while maintaining real-time inference (30-47 FPS).", "motivation": "Address the challenge of limited annotated data in specialized civil engineering domains by leveraging self-supervised learning to improve object detection performance in data-constrained environments.", "method": "Hybrid architecture integrating DINOv3 self-supervised vision transformers with YOLOv12 at two strategic locations: input preprocessing (P0) and mid-backbone enhancement (P3), with systematic ablation studies across different scales.", "result": "Achieved substantial improvements: 12.4% on Tunnel Segment Crack detection (648 images), 13.7% on Construction PPE (1K images), 88.6% on KITTI (7K images), with optimal performance at Medium-scale with DualP0P3 integration (55.77% mAP@0.5).", "conclusion": "DINO-YOLO establishes state-of-the-art performance for civil engineering datasets with limited data (<10K images) while preserving computational efficiency, providing practical solutions for construction safety monitoring and infrastructure inspection."}}
{"id": "2510.25141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25141", "abs": "https://arxiv.org/abs/2510.25141", "authors": ["Wan Jiang", "Jing Yan", "Ruixuan Zhang", "Xiaojing Chen", "Changtao Miao", "Zhe Li", "Chenhao Lin", "Yunfeng Diao", "Richang Hong"], "title": "Revisiting Reconstruction-based AI-generated Image Detection: A Geometric Perspective", "comment": null, "summary": "The rise of generative Artificial Intelligence (AI) has made detecting\nAI-generated images a critical challenge for ensuring authenticity. Existing\nreconstruction-based methods lack theoretical foundations and on empirical\nheuristics, limiting interpretability and reliability. In this paper, we\nintroduce the Jacobian-Spectral Lower Bound for reconstruction error from a\ngeometric perspective, showing that real images off the reconstruction manifold\nexhibit a non-trivial error lower bound, while generated images on the manifold\nhave near-zero error. Furthermore, we reveal the limitations of existing\nmethods that rely on static reconstruction error from a single pass. These\nmethods often fail when some real images exhibit lower error than generated\nones. This counterintuitive behavior reduces detection accuracy and requires\ndata-specific threshold tuning, limiting their applicability in real-world\nscenarios. To address these challenges, we propose ReGap, a training-free\nmethod that computes dynamic reconstruction error by leveraging structured\nediting operations to introduce controlled perturbations. This enables\nmeasuring error changes before and after editing, improving detection accuracy\nby enhancing error separation. Experimental results show that our method\noutperforms existing baselines, exhibits robustness to common post-processing\noperations and generalizes effectively across diverse conditions.", "AI": {"tldr": "The paper introduces ReGap, a training-free method for detecting AI-generated images by computing dynamic reconstruction error through controlled editing perturbations, outperforming existing baselines.", "motivation": "Existing reconstruction-based methods for detecting AI-generated images lack theoretical foundations, rely on empirical heuristics, and have limitations in interpretability and reliability. They often fail when real images exhibit lower error than generated ones, requiring data-specific threshold tuning.", "method": "Proposes ReGap method that computes dynamic reconstruction error by leveraging structured editing operations to introduce controlled perturbations, measuring error changes before and after editing to enhance error separation.", "result": "Experimental results show the method outperforms existing baselines, exhibits robustness to common post-processing operations, and generalizes effectively across diverse conditions.", "conclusion": "The proposed ReGap method provides a more reliable and interpretable approach for AI-generated image detection by addressing limitations of static reconstruction error methods through dynamic error measurement."}}
{"id": "2510.25146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25146", "abs": "https://arxiv.org/abs/2510.25146", "authors": ["Xiaoyu Zhou", "Jingqi Wang", "Yuang Jia", "Yongtao Wang", "Deqing Sun", "Ming-Hsuan Yang"], "title": "EA3D: Online Open-World 3D Object Extraction from Streaming Videos", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)", "summary": "Current 3D scene understanding methods are limited by offline-collected\nmulti-view data or pre-constructed 3D geometry. In this paper, we present\nExtractAnything3D (EA3D), a unified online framework for open-world 3D object\nextraction that enables simultaneous geometric reconstruction and holistic\nscene understanding. Given a streaming video, EA3D dynamically interprets each\nframe using vision-language and 2D vision foundation encoders to extract\nobject-level knowledge. This knowledge is integrated and embedded into a\nGaussian feature map via a feed-forward online update strategy. We then\niteratively estimate visual odometry from historical frames and incrementally\nupdate online Gaussian features with new observations. A recurrent joint\noptimization module directs the model's attention to regions of interest,\nsimultaneously enhancing both geometric reconstruction and semantic\nunderstanding. Extensive experiments across diverse benchmarks and tasks,\nincluding photo-realistic rendering, semantic and instance segmentation, 3D\nbounding box and semantic occupancy estimation, and 3D mesh generation,\ndemonstrate the effectiveness of EA3D. Our method establishes a unified and\nefficient framework for joint online 3D reconstruction and holistic scene\nunderstanding, enabling a broad range of downstream tasks.", "AI": {"tldr": "EA3D is a unified online framework for open-world 3D object extraction that enables simultaneous geometric reconstruction and holistic scene understanding from streaming video, using vision-language models and Gaussian feature maps.", "motivation": "Current 3D scene understanding methods are limited by offline-collected multi-view data or pre-constructed 3D geometry, lacking real-time online capabilities for dynamic scene interpretation.", "method": "Uses vision-language and 2D vision foundation encoders to extract object-level knowledge from streaming video, integrates knowledge into Gaussian feature maps via online update strategy, estimates visual odometry from historical frames, and employs recurrent joint optimization for attention to regions of interest.", "result": "Demonstrates effectiveness across diverse benchmarks and tasks including photo-realistic rendering, semantic/instance segmentation, 3D bounding box/semantic occupancy estimation, and 3D mesh generation.", "conclusion": "Establishes a unified and efficient framework for joint online 3D reconstruction and holistic scene understanding, enabling a broad range of downstream tasks."}}
{"id": "2510.25157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25157", "abs": "https://arxiv.org/abs/2510.25157", "authors": ["Gautam A. Viruthagiri", "Arnuv Tandon", "Gerald G. Fuller", "Vinny Chandran Suja"], "title": "Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from Interference Patterns Using Vision Transformers", "comment": "6 pages, 2 figures, will be updated", "summary": "Thin film interferometry is a powerful technique for non-invasively measuring\nliquid film thickness with applications in ophthalmology, but its clinical\ntranslation is hindered by the challenges in reconstructing thickness profiles\nfrom interference patterns - an ill-posed inverse problem complicated by phase\nperiodicity, imaging noise and ambient artifacts. Traditional reconstruction\nmethods are either computationally intensive, sensitive to noise, or require\nmanual expert analysis, which is impractical for real-time diagnostics. To\naddress this challenge, here we present a vision transformer-based approach for\nreal-time inference of thin liquid film thickness profiles directly from\nisolated interferograms. Trained on a hybrid dataset combining\nphysiologically-relevant synthetic and experimental tear film data, our model\nleverages long-range spatial correlations to resolve phase ambiguities and\nreconstruct temporally coherent thickness profiles in a single forward pass\nfrom dynamic interferograms acquired in vivo and ex vivo. The network\ndemonstrates state-of-the-art performance on noisy, rapidly-evolving films with\nmotion artifacts, overcoming limitations of conventional phase-unwrapping and\niterative fitting methods. Our data-driven approach enables automated,\nconsistent thickness reconstruction at real-time speeds on consumer hardware,\nopening new possibilities for continuous monitoring of pre-lens ocular tear\nfilms and non-invasive diagnosis of conditions such as the dry eye disease.", "AI": {"tldr": "Vision transformer-based approach for real-time inference of thin liquid film thickness profiles from interferograms, enabling automated thickness reconstruction for clinical applications like dry eye diagnosis.", "motivation": "Clinical translation of thin film interferometry is hindered by challenges in reconstructing thickness profiles from interference patterns - an ill-posed inverse problem complicated by phase periodicity, imaging noise, and ambient artifacts. Traditional methods are computationally intensive, noise-sensitive, or require manual expert analysis.", "method": "Vision transformer-based approach trained on hybrid dataset combining physiologically-relevant synthetic and experimental tear film data. Leverages long-range spatial correlations to resolve phase ambiguities and reconstruct temporally coherent thickness profiles in a single forward pass from dynamic interferograms.", "result": "State-of-the-art performance on noisy, rapidly-evolving films with motion artifacts, overcoming limitations of conventional phase-unwrapping and iterative fitting methods. Enables automated, consistent thickness reconstruction at real-time speeds on consumer hardware.", "conclusion": "The data-driven approach opens new possibilities for continuous monitoring of pre-lens ocular tear films and non-invasive diagnosis of conditions such as dry eye disease, making thin film interferometry clinically practical for real-time diagnostics."}}
{"id": "2510.25163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25163", "abs": "https://arxiv.org/abs/2510.25163", "authors": ["Wenhao Zheng", "Chenwei Sun", "Wenbo Zhang", "Jiancheng Lv", "Xianggen Liu"], "title": "Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation", "comment": null, "summary": "Deep generative models, such as diffusion models, have shown promising\nprogress in image generation and audio generation via simplified continuity\nassumptions. However, the development of generative modeling techniques for\ngenerating multi-modal data, such as parametric CAD sequences, still lags\nbehind due to the challenges in addressing long-range constraints and parameter\nsensitivity. In this work, we propose a novel framework for quantitatively\nconstrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).\nFor the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,\ndiscrete commands and continuous parameters) in a unified continuous and\ndifferentiable parameter space rather than in the discrete data space. In\naddition, TGBFN penetrates the parameter update kernel and introduces a guided\nBayesian flow to control the CAD properties. To evaluate TGBFN, we construct a\nnew dataset for quantitatively constrained CAD generation. Extensive\ncomparisons across single-condition and multi-condition constrained generation\ntasks demonstrate that TGBFN achieves state-of-the-art performance in\ngenerating high-fidelity, condition-aware CAD sequences. The code is available\nat https://github.com/scu-zwh/TGBFN.", "AI": {"tldr": "TGBFN is a novel framework for quantitatively constrained CAD generation that handles multi-modal CAD sequences in a unified continuous parameter space using guided Bayesian flow.", "motivation": "Current generative models lag in multi-modal data generation like parametric CAD sequences due to challenges with long-range constraints and parameter sensitivity.", "method": "TGBFN handles discrete commands and continuous parameters in a unified continuous differentiable space, introduces guided Bayesian flow to control CAD properties, and constructs a new dataset for evaluation.", "result": "Extensive comparisons show TGBFN achieves state-of-the-art performance in generating high-fidelity, condition-aware CAD sequences across single-condition and multi-condition tasks.", "conclusion": "TGBFN successfully addresses the challenges of multi-modal CAD generation and demonstrates superior performance in constrained generation tasks."}}
{"id": "2510.25166", "categories": ["cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.25166", "abs": "https://arxiv.org/abs/2510.25166", "authors": ["Zhuojin Li", "Marco Paolieri", "Leana Golubchik"], "title": "A Study on Inference Latency for Vision Transformers on Mobile Devices", "comment": "To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS\n  2024", "summary": "Given the significant advances in machine learning techniques on mobile\ndevices, particularly in the domain of computer vision, in this work we\nquantitatively study the performance characteristics of 190 real-world vision\ntransformers (ViTs) on mobile devices. Through a comparison with 102 real-world\nconvolutional neural networks (CNNs), we provide insights into the factors that\ninfluence the latency of ViT architectures on mobile devices. Based on these\ninsights, we develop a dataset including measured latencies of 1000 synthetic\nViTs with representative building blocks and state-of-the-art architectures\nfrom two machine learning frameworks and six mobile platforms. Using this\ndataset, we show that inference latency of new ViTs can be predicted with\nsufficient accuracy for real-world applications.", "AI": {"tldr": "This paper quantitatively studies the performance of 190 real-world vision transformers (ViTs) on mobile devices, comparing them with 102 CNNs, and develops a dataset of 1000 synthetic ViTs to predict inference latency.", "motivation": "Given the significant advances in machine learning techniques on mobile devices, particularly in computer vision, there is a need to understand the performance characteristics of vision transformers on mobile platforms and compare them with traditional CNNs.", "method": "The study quantitatively analyzes 190 real-world ViTs and 102 CNNs on mobile devices, develops a dataset of 1000 synthetic ViTs with representative building blocks and state-of-the-art architectures from two ML frameworks and six mobile platforms, and uses this dataset to predict inference latency.", "result": "The research provides insights into factors influencing ViT latency on mobile devices and shows that inference latency of new ViTs can be predicted with sufficient accuracy for real-world applications using the developed dataset.", "conclusion": "The study successfully quantifies ViT performance on mobile devices, provides comparative analysis with CNNs, and demonstrates the feasibility of accurately predicting ViT inference latency for practical mobile applications."}}
{"id": "2510.25173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25173", "abs": "https://arxiv.org/abs/2510.25173", "authors": ["Kejing Xia", "Jidong Jia", "Ke Jin", "Yucai Bai", "Li Sun", "Dacheng Tao", "Youjian Zhang"], "title": "$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction", "comment": null, "summary": "Recently, Gaussian Splatting (GS) has shown great potential for urban scene\nreconstruction in the field of autonomous driving. However, current urban scene\nreconstruction methods often depend on multimodal sensors as inputs,\n\\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR\npoint clouds can largely mitigate ill-posedness in reconstruction, acquiring\nsuch accurate LiDAR data is still challenging in practice: i) precise\nspatiotemporal calibration between LiDAR and other sensors is required, as they\nmay not capture data simultaneously; ii) reprojection errors arise from spatial\nmisalignment when LiDAR and cameras are mounted at different locations. To\navoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a\nLiDAR-free urban scene reconstruction framework. In this work, we obtain\ngeometry priors that are as effective as LiDAR while being denser and more\naccurate. $\\textbf{First}$, we initialize a dense point cloud by\nback-projecting multi-view metric depth predictions. This point cloud is then\noptimized by a Progressive Pruning strategy to improve the global consistency.\n$\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense\nmetric depth via a Depth Enhancer. Specifically, we leverage diffusion priors\nfrom a depth foundation model to enhance the depth maps rendered by Gaussians.\nIn turn, the enhanced depths provide stronger geometric constraints during\nGaussian training. $\\textbf{Finally}$, we improve the accuracy of ground\ngeometry by constraining the shape and normal attributes of Gaussians within\nroad regions. Extensive experiments on the Waymo dataset demonstrate that our\nmethod consistently outperforms state-of-the-art methods, producing more\naccurate geometry even when compared with those using ground-truth LiDAR data.", "AI": {"tldr": "D\u00b2GS is a LiDAR-free urban scene reconstruction framework that uses multi-view depth predictions and Gaussian Splatting to achieve geometry quality comparable to LiDAR-based methods.", "motivation": "Current urban scene reconstruction methods rely on multimodal sensors (LiDAR + images), which face challenges in acquiring accurate LiDAR data due to spatiotemporal calibration requirements and reprojection errors from sensor misalignment.", "method": "1) Initialize dense point cloud from multi-view metric depth predictions with Progressive Pruning optimization; 2) Jointly refine Gaussian geometry and depth via Depth Enhancer using diffusion priors; 3) Improve ground geometry by constraining Gaussian shape/normal attributes in road regions.", "result": "Extensive experiments on Waymo dataset show the method consistently outperforms state-of-the-art methods, producing more accurate geometry even compared to methods using ground-truth LiDAR data.", "conclusion": "D\u00b2GS successfully achieves LiDAR-free urban scene reconstruction with geometry quality comparable to LiDAR-based approaches, overcoming the practical challenges of acquiring accurate LiDAR data."}}
{"id": "2510.25174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25174", "abs": "https://arxiv.org/abs/2510.25174", "authors": ["Huadong Tang", "Youpeng Zhao", "Min Xu", "Jun Wang", "Qiang Wu"], "title": "Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation", "comment": "Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)", "summary": "Prevalent semantic segmentation methods generally adopt a vanilla classifier\nto categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data,\nthis information is represented by a set of fixed parameters (weights and\nbiases).\n  However, each image has a different class distribution, which prevents the\nclassifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being\nbiased towards majority classes, limiting the model's effectiveness in\nidentifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that\ndynamically adjusts the classifier using global (dataset-level) and local\n(image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual\ninformation of each class, incorporating the class-specific contextual\ninformation from the current image to improve the classifier for precise pixel\nlabeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain\nexpert (teacher network) dynamically adjusts contextual information with ground\ntruth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve\nstate-of-the-art performance across several datasets, including ADE20K,\nCOCO-Stuff10K, and Pascal-Context.", "AI": {"tldr": "Proposes an Extended Context-Aware Classifier (ECAC) that dynamically adjusts semantic segmentation classifiers using both dataset-level and image-level contextual information to address class imbalance and improve pixel-level labeling accuracy.", "motivation": "Standard semantic segmentation classifiers use fixed parameters that don't adapt to individual image characteristics, and class imbalance causes bias toward majority classes, limiting effectiveness for minority class segmentation.", "method": "Uses memory bank to learn dataset-level contextual information, incorporates image-specific contextual information, and employs teacher-student network paradigm where teacher dynamically adjusts contextual information with ground truth and transfers knowledge to student.", "result": "Achieves state-of-the-art performance on ADE20K, COCO-Stuff10K, and Pascal-Context datasets.", "conclusion": "The proposed ECAC effectively addresses class imbalance and improves semantic segmentation by dynamically adapting classifiers to both global and local contextual information."}}
{"id": "2510.25175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25175", "abs": "https://arxiv.org/abs/2510.25175", "authors": ["Yingjie Gao", "Yanan Zhang", "Zhi Cai", "Di Huang"], "title": "Test-Time Adaptive Object Detection with Foundation Model", "comment": "Accepted by NeurIPS 2025", "summary": "In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.", "AI": {"tldr": "A foundation model-powered test-time adaptive object detection method that eliminates source data dependency and closed-set limitations, using multi-modal prompt tuning and instance dynamic memory for domain adaptation.", "motivation": "Existing test-time adaptive object detection methods rely on source data and assume identical category spaces between source and target domains, which limits real-world applicability.", "method": "Multi-modal Prompt-based Mean-Teacher framework with text and visual prompt tuning, Test-time Warm-start strategy for visual prompts, and Instance Dynamic Memory module with Memory Enhancement and Memory Hallucination strategies.", "result": "Extensive experiments show consistent outperformance over state-of-the-art methods on cross-corruption and cross-dataset benchmarks, adapting to arbitrary cross-domain and cross-category target data.", "conclusion": "The proposed method successfully eliminates source data dependency, overcomes closed-set limitations, and achieves superior performance in test-time adaptive object detection across various domain shifts."}}
{"id": "2510.25184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25184", "abs": "https://arxiv.org/abs/2510.25184", "authors": ["Zhifeng Wang", "Minghui Wang", "Chunyan Zeng", "Jialong Yao", "Yang Yang", "Hongmin Xu"], "title": "Mask-Robust Face Verification for Online Learning via YOLOv5 and Residual Networks", "comment": "9 pages, 10 figures", "summary": "In the contemporary landscape, the fusion of information technology and the\nrapid advancement of artificial intelligence have ushered school education into\na transformative phase characterized by digitization and heightened\nintelligence. Concurrently, the global paradigm shift caused by the Covid-19\npandemic has catalyzed the evolution of e-learning, accentuating its\nsignificance. Amidst these developments, one pivotal facet of the online\neducation paradigm that warrants attention is the authentication of identities\nwithin the digital learning sphere. Within this context, our study delves into\na solution for online learning authentication, utilizing an enhanced\nconvolutional neural network architecture, specifically the residual network\nmodel. By harnessing the power of deep learning, this technological approach\naims to galvanize the ongoing progress of online education, while concurrently\nbolstering its security and stability. Such fortification is imperative in\nenabling online education to seamlessly align with the swift evolution of the\neducational landscape. This paper's focal proposition involves the deployment\nof the YOLOv5 network, meticulously trained on our proprietary dataset. This\nnetwork is tasked with identifying individuals' faces culled from images\ncaptured by students' open online cameras. The resultant facial information is\nthen channeled into the residual network to extract intricate features at a\ndeeper level. Subsequently, a comparative analysis of Euclidean distances\nagainst students' face databases is performed, effectively ascertaining the\nidentity of each student.", "AI": {"tldr": "This paper proposes an online learning authentication system using YOLOv5 for face detection and a residual network for feature extraction, with identity verification through Euclidean distance comparison against student face databases.", "motivation": "The rapid digitization of education and increased importance of e-learning during COVID-19 highlighted the need for secure identity authentication in online learning environments to ensure security and stability.", "method": "Uses YOLOv5 network trained on proprietary dataset for face detection from students' webcams, followed by residual network for deep feature extraction, and Euclidean distance comparison against student face databases for identity verification.", "result": "The proposed system successfully identifies individuals' faces from online camera images and performs accurate identity authentication through deep feature extraction and database comparison.", "conclusion": "The deep learning-based authentication approach enhances online education security and stability, enabling it to keep pace with the rapidly evolving educational landscape."}}
{"id": "2510.25234", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.25234", "abs": "https://arxiv.org/abs/2510.25234", "authors": ["Yuxiang Mao", "Zhijie Zhang", "Zhiheng Zhang", "Jiawei Liu", "Chen Zeng", "Shihong Xia"], "title": "Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation", "comment": "18 pages, 6 figures, accepted to ICXR 2025 conference", "summary": "Expressions are fundamental to conveying human emotions. With the rapid\nadvancement of AI-generated content (AIGC), realistic and expressive 3D facial\nanimation has become increasingly crucial. Despite recent progress in\nspeech-driven lip-sync for talking-face animation, generating emotionally\nexpressive talking faces remains underexplored. A major obstacle is the\nscarcity of real emotional 3D talking-face datasets due to the high cost of\ndata capture. To address this, we model facial animation driven by both speech\nand emotion as a linear additive problem. Leveraging a 3D talking-face dataset\nwith neutral expressions (VOCAset) and a dataset of 3D expression sequences\n(Florence4D), we jointly learn a set of blendshapes driven by speech and\nemotion. We introduce a sparsity constraint loss to encourage disentanglement\nbetween the two types of blendshapes while allowing the model to capture\ninherent secondary cross-domain deformations present in the training data. The\nlearned blendshapes can be further mapped to the expression and jaw pose\nparameters of the FLAME model, enabling the animation of 3D Gaussian avatars.\nQualitative and quantitative experiments demonstrate that our method naturally\ngenerates talking faces with specified expressions while maintaining accurate\nlip synchronization. Perceptual studies further show that our approach achieves\nsuperior emotional expressivity compared to existing methods, without\ncompromising lip-sync quality.", "AI": {"tldr": "The paper proposes a method for generating emotionally expressive 3D talking faces by modeling facial animation as a linear additive problem that combines speech-driven and emotion-driven blendshapes with sparsity constraints for disentanglement.", "motivation": "Current 3D facial animation methods focus mainly on speech-driven lip-sync but lack emotional expressivity. The scarcity of real emotional 3D talking-face datasets due to high capture costs motivates the need for a data-efficient approach.", "method": "Model facial animation as linear additive problem using speech and emotion blendshapes. Use VOCAset (neutral expressions) and Florence4D (expression sequences) datasets. Apply sparsity constraint loss for disentanglement while capturing cross-domain deformations. Map learned blendshapes to FLAME model parameters for 3D Gaussian avatar animation.", "result": "Qualitative and quantitative experiments show natural generation of talking faces with specified expressions while maintaining accurate lip synchronization. Perceptual studies demonstrate superior emotional expressivity compared to existing methods without compromising lip-sync quality.", "conclusion": "The proposed method successfully generates emotionally expressive 3D talking faces by combining speech and emotion blendshapes with sparsity constraints, achieving both accurate lip-sync and enhanced emotional expressivity using limited training data."}}
{"id": "2510.25199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25199", "abs": "https://arxiv.org/abs/2510.25199", "authors": ["Manisha More", "Kavya Bhand", "Kaustubh Mukdam", "Kavya Sharma", "Manas Kawtikwar", "Hridayansh Kaware", "Prajwal Kavhar"], "title": "AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis", "comment": null, "summary": "Early diagnosis of critical diseases can significantly improve patient\nsurvival and reduce treatment costs. However, existing diagnostic techniques\nare often costly, invasive, and inaccessible in low-resource regions. This\npaper presents a multimodal artificial intelligence (AI) diagnostic framework\nintegrating image analysis, thermal imaging, and audio signal processing for\nearly detection of three major health conditions: skin cancer, vascular blood\nclots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2\nconvolutional neural network was trained on the ISIC 2019 dataset for skin\nlesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%\nspecificity. A support vector machine (SVM) with handcrafted features was\nemployed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on\nsynthetic and clinical data. For cardiopulmonary analysis, lung and heart sound\ndatasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral\nCoefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy\nand 85.7% sensitivity. Comparative evaluation against state-of-the-art models\ndemonstrates that the proposed system achieves competitive results while\nremaining lightweight and deployable on low-cost devices. The framework\nprovides a promising step toward scalable, real-time, and accessible AI-based\npre-diagnostic healthcare solutions.", "AI": {"tldr": "Multimodal AI diagnostic framework combining image analysis, thermal imaging, and audio processing for early detection of skin cancer, vascular clots, and cardiopulmonary abnormalities, achieving competitive accuracy while being lightweight for low-cost deployment.", "motivation": "Early disease diagnosis improves survival and reduces costs, but existing methods are costly, invasive, and inaccessible in low-resource regions.", "method": "Fine-tuned MobileNetV2 for skin lesion classification on ISIC 2019, SVM with handcrafted features for thermal clot detection, and Random Forest with MFCC features for cardiopulmonary sound analysis.", "result": "89.3% accuracy for skin cancer, 86.4% accuracy for clot detection (AUC=0.89), and 87.2% accuracy for cardiopulmonary analysis, with competitive performance against state-of-the-art models.", "conclusion": "The framework provides a promising step toward scalable, real-time, and accessible AI-based pre-diagnostic healthcare solutions deployable on low-cost devices."}}
{"id": "2510.25210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25210", "abs": "https://arxiv.org/abs/2510.25210", "authors": ["Junsheng Zhou", "Xingyu Shi", "Haichuan Song", "Yi Fang", "Yu-Shen Liu", "Zhizhong Han"], "title": "U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching", "comment": "Accepted by NeurIPS 2025. Project page:\n  https://gloriasze.github.io/U-CAN/", "summary": "Point clouds captured by scanning sensors are often perturbed by noise, which\nhave a highly negative impact on downstream tasks (e.g. surface reconstruction\nand shape understanding). Previous works mostly focus on training neural\nnetworks with noisy-clean point cloud pairs for learning denoising priors,\nwhich requires extensively manual efforts. In this work, we introduce U-CAN, an\nUnsupervised framework for point cloud denoising with Consistency-Aware\nNoise2Noise matching. Specifically, we leverage a neural network to infer a\nmulti-step denoising path for each point of a shape or scene with a noise to\nnoise matching scheme. We achieve this by a novel loss which enables\nstatistical reasoning on multiple noisy point cloud observations. We further\nintroduce a novel constraint on the denoised geometry consistency for learning\nconsistency-aware denoising patterns. We justify that the proposed constraint\nis a general term which is not limited to 3D domain and can also contribute to\nthe area of 2D image denoising. Our evaluations under the widely used\nbenchmarks in point cloud denoising, upsampling and image denoising show\nsignificant improvement over the state-of-the-art unsupervised methods, where\nU-CAN also produces comparable results with the supervised methods.", "AI": {"tldr": "U-CAN is an unsupervised framework for point cloud denoising using Consistency-Aware Noise2Noise matching, achieving results comparable to supervised methods without requiring clean data.", "motivation": "Point clouds from scanning sensors often contain noise that negatively impacts downstream tasks. Previous supervised methods require extensive manual effort to create noisy-clean pairs.", "method": "U-CAN uses a neural network to infer multi-step denoising paths with Noise2Noise matching, employing statistical reasoning on multiple noisy observations and enforcing denoised geometry consistency constraints.", "result": "Significant improvement over state-of-the-art unsupervised methods in point cloud denoising, upsampling, and image denoising benchmarks, producing comparable results to supervised methods.", "conclusion": "U-CAN provides an effective unsupervised alternative to supervised denoising methods, with the consistency constraint being generalizable beyond 3D to 2D image denoising."}}
{"id": "2510.25221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25221", "abs": "https://arxiv.org/abs/2510.25221", "authors": ["Shiyu Qin", "Zhihao Cai", "Kaixuan Wang", "Lin Qi", "Junyu Dong"], "title": "MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo", "comment": null, "summary": "Photometric stereo is a technique aimed at determining surface normals\nthrough the utilization of shading cues derived from images taken under\ndifferent lighting conditions. However, existing learning-based approaches\noften fail to accurately capture features at multiple stages and do not\nadequately promote interaction between these features. Consequently, these\nmodels tend to extract redundant features, especially in areas with intricate\ndetails such as wrinkles and edges. To tackle these issues, we propose MSF-Net,\na novel framework for extracting information at multiple stages, paired with\nselective update strategy, aiming to extract high-quality feature information,\nwhich is critical for accurate normal construction. Additionally, we have\ndeveloped a feature fusion module to improve the interplay among different\nfeatures. Experimental results on the DiLiGenT benchmark show that our proposed\nMSF-Net significantly surpasses previous state-of-the-art methods in the\naccuracy of surface normal estimation.", "AI": {"tldr": "MSF-Net is a novel photometric stereo framework that extracts multi-stage features with selective update strategy and feature fusion to improve surface normal estimation accuracy, outperforming state-of-the-art methods on the DiLiGenT benchmark.", "motivation": "Existing learning-based photometric stereo approaches fail to accurately capture features at multiple stages and don't adequately promote interaction between features, leading to redundant feature extraction especially in intricate areas like wrinkles and edges.", "method": "Proposed MSF-Net with multi-stage feature extraction, selective update strategy to extract high-quality features, and a feature fusion module to improve interplay among different features for accurate normal construction.", "result": "Experimental results on the DiLiGenT benchmark show that MSF-Net significantly surpasses previous state-of-the-art methods in surface normal estimation accuracy.", "conclusion": "The proposed MSF-Net framework effectively addresses limitations of existing photometric stereo methods by enabling better multi-stage feature extraction and interaction, leading to superior performance in surface normal estimation."}}
{"id": "2510.25327", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25327", "abs": "https://arxiv.org/abs/2510.25327", "authors": ["Runxi Huang", "Mingxuan Yu", "Mingyu Tsoi", "Xiaomin Ouyang"], "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding", "comment": "Accepted by SenSys 2026", "summary": "Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.", "AI": {"tldr": "MMEdge is a real-time multimodal inference framework for edge devices that uses pipelined sensing and encoding to reduce latency while maintaining accuracy through temporal aggregation and adaptive optimization.", "motivation": "Real-time multimodal inference on resource-constrained edge devices is essential for applications like autonomous driving and human-computer interaction, but prior work overlooks the coupling between sensing dynamics and model execution, as well as inter-modality dependencies.", "method": "MMEdge decomposes inference into fine-grained sensing/encoding units for incremental computation, uses temporal aggregation to capture dynamics, incorporates adaptive configuration optimization for resource constraints, and employs cross-modal speculative skipping to bypass slower modalities when early predictions are confident.", "result": "Evaluation on multimodal datasets and UAV testbed shows MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.", "conclusion": "MMEdge provides an effective solution for real-time multimodal inference on edge devices by addressing the tight coupling between sensing and execution through pipelined design and adaptive optimization techniques."}}
{"id": "2510.25227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25227", "abs": "https://arxiv.org/abs/2510.25227", "authors": ["Quang-Khai Bui-Tran", "Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Nguyen Lan Vi Vu", "Phat K. Huynh", "Ulas Bagci", "Min Xu"], "title": "Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation", "comment": "5 pages, 3 figures", "summary": "Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for\nmedical image segmentation under privacy constraints, yet current approaches\noften ignore sample difficulty and struggle with noisy supervision under domain\nshift. We present a new SFDA framework that leverages Hard Sample Selection and\nDenoised Patch Mixing to progressively align target distributions. First,\nunlabeled images are partitioned into reliable and unreliable subsets through\nentropy-similarity analysis, allowing adaptation to start from easy samples and\ngradually incorporate harder ones. Next, pseudo-labels are refined via Monte\nCarlo-based denoising masks, which suppress unreliable pixels and stabilize\ntraining. Finally, intra- and inter-domain objectives mix patches between\nsubsets, transferring reliable semantics while mitigating noise. Experiments on\nbenchmark datasets show consistent gains over prior SFDA and UDA methods,\ndelivering more accurate boundary delineation and achieving state-of-the-art\nDice and ASSD scores. Our study highlights the importance of progressive\nadaptation and denoised supervision for robust segmentation under domain shift.", "AI": {"tldr": "A new SFDA framework for medical image segmentation that uses hard sample selection and denoised patch mixing to progressively adapt to target domains while handling noisy supervision.", "motivation": "Current SFDA approaches ignore sample difficulty and struggle with noisy supervision under domain shift, especially in medical imaging with privacy constraints.", "method": "Partitions unlabeled images into reliable/unreliable subsets via entropy-similarity analysis, refines pseudo-labels with Monte Carlo-based denoising masks, and uses intra-/inter-domain patch mixing objectives.", "result": "Achieves state-of-the-art Dice and ASSD scores on benchmark datasets, showing consistent gains over prior SFDA and UDA methods with better boundary delineation.", "conclusion": "Progressive adaptation and denoised supervision are crucial for robust medical image segmentation under domain shift in privacy-constrained settings."}}
{"id": "2510.25229", "categories": ["cs.CV", "68T07, 68T45, 65C20", "I.2.10; I.4.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.25229", "abs": "https://arxiv.org/abs/2510.25229", "authors": ["Kim Shin Seong", "Mingi Kwon", "Jaeseok Jeong", "Youngjung Uh"], "title": "Balanced conic rectified flow", "comment": "Main paper: 10 pages (total 40 pages including appendix), 5 figures.\n  Accepted at NeurIPS 2025 (Poster). Acknowledgment: Supported by the NRF of\n  Korea (RS-2023-00223062) and IITP grants (RS-2020-II201361, RS-2024-00439762)\n  funded by the Korean government (MSIT)", "summary": "Rectified flow is a generative model that learns smooth transport mappings\nbetween two distributions through an ordinary differential equation (ODE).\nUnlike diffusion-based generative models, which require costly numerical\nintegration of a generative ODE to sample images with state-of-the-art quality,\nrectified flow uses an iterative process called reflow to learn smooth and\nstraight ODE paths. This allows for relatively simple and efficient generation\nof high-quality images. However, rectified flow still faces several challenges.\n1) The reflow process requires a large number of generative pairs to preserve\nthe target distribution, leading to significant computational costs. 2) Since\nthe model is typically trained using only generated image pairs, its\nperformance heavily depends on the 1-rectified flow model, causing it to become\nbiased towards the generated data.\n  In this work, we experimentally expose the limitations of the original\nrectified flow and propose a novel approach that incorporates real images into\nthe training process. By preserving the ODE paths for real images, our method\neffectively reduces reliance on large amounts of generated data. Instead, we\ndemonstrate that the reflow process can be conducted efficiently using a much\nsmaller set of generated and real images. In CIFAR-10, we achieved\nsignificantly better FID scores, not only in one-step generation but also in\nfull-step simulations, while using only of the generative pairs compared to the\noriginal method. Furthermore, our approach induces straighter paths and avoids\nsaturation on generated images during reflow, leading to more robust ODE\nlearning while preserving the distribution of real images.", "AI": {"tldr": "The paper proposes a novel approach to improve rectified flow by incorporating real images into training, reducing reliance on large generated datasets while achieving better FID scores and straighter ODE paths.", "motivation": "Rectified flow faces challenges including high computational costs from requiring many generative pairs and bias towards generated data. The authors aim to address these limitations by preserving ODE paths for real images.", "method": "The approach incorporates real images into the training process, preserving ODE paths for real images and using a smaller set of generated and real images for efficient reflow.", "result": "In CIFAR-10, achieved significantly better FID scores in both one-step and full-step simulations using only a fraction of generative pairs compared to original method. Also induced straighter paths and avoided saturation on generated images.", "conclusion": "The proposed method enables more robust ODE learning while preserving real image distribution, demonstrating that reflow can be conducted efficiently with smaller datasets of generated and real images."}}
{"id": "2510.25522", "categories": ["cs.CV", "cs.AI", "I.4.6"], "pdf": "https://arxiv.org/pdf/2510.25522", "abs": "https://arxiv.org/abs/2510.25522", "authors": ["Doan-Van-Anh Ly", "Thi-Thu-Hien Pham", "Thanh-Hai Le"], "title": "Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography", "comment": "27 pages, 8 figures", "summary": "Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.", "AI": {"tldr": "ResNet-based UNet3+ with CBAM attention module outperforms Transformer and Mamba backbones for liver tumor segmentation in CECT images, achieving best Dice score (0.755), IoU (0.662), and boundary precision.", "motivation": "Liver structure segmentation in multi-phase CECT is crucial for computer-aided diagnosis and treatment planning of liver diseases including tumor detection.", "method": "Evaluated UNet-based architectures with various backbones (ResNet, Transformer, Mamba) pretrained weights, then integrated attention mechanisms including CBAM to improve segmentation quality.", "result": "ResNetUNet3+ with CBAM achieved best performance: Dice 0.755, IoU 0.662, HD95 77.911, accuracy 0.925, specificity 0.926. Transformer and Mamba backbones underperformed compared to ResNet.", "conclusion": "Classical ResNet architecture combined with modern attention modules remains highly competitive for medical image segmentation, offering promising direction for liver tumor detection in clinical practice."}}
{"id": "2510.25590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25590", "abs": "https://arxiv.org/abs/2510.25590", "authors": ["Pengtao Chen", "Xianfang Zeng", "Maosen Zhao", "Mingzhu Shen", "Peng Ye", "Bangyin Xiang", "Zhibo Wang", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing", "comment": "26 pages, 10 figures, 18 tables", "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.", "AI": {"tldr": "RegionE is a region-aware generation framework that accelerates instruction-based image editing by distinguishing between edited and unedited regions, applying different denoising strategies to each region for faster processing while maintaining quality.", "motivation": "Existing instruction-based image editing models apply uniform generation processes across entire images, failing to account for the significant differences in generation difficulty and computational redundancy between edited and unedited regions.", "method": "RegionE framework includes: 1) Adaptive region partition based on trajectory analysis, 2) Region-aware generation with one-step prediction for unedited areas and local iterative denoising for edited regions using Region-Instruction KV Cache, 3) Adaptive velocity decay cache to accelerate local denoising by leveraging velocity similarity between adjacent timesteps.", "result": "Applied to state-of-the-art IIE models (Step1X-Edit, FLUX.1 Kontext, Qwen-Image-Edit), RegionE achieved acceleration factors of 2.57, 2.41, and 2.06 respectively, while GPT-4o evaluations confirmed preserved semantic and perceptual fidelity.", "conclusion": "RegionE provides an effective training-free acceleration framework for instruction-based image editing by exploiting regional differences in generation requirements, achieving significant speed improvements without compromising quality."}}
{"id": "2510.25237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25237", "abs": "https://arxiv.org/abs/2510.25237", "authors": ["Yinqi Cai", "Jichang Li", "Zhaolun Li", "Weikai Chen", "Rushi Lan", "Xi Xie", "Xiaonan Luo", "Guanbin Li"], "title": "DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis", "comment": "ICCV 2025", "summary": "Recent advances in deep generative models have made it easier to manipulate\nface videos, raising significant concerns about their potential misuse for\nfraud and misinformation. Existing detectors often perform well in in-domain\nscenarios but fail to generalize across diverse manipulation techniques due to\ntheir reliance on forgery-specific artifacts. In this work, we introduce\nDeepShield, a novel deepfake detection framework that balances local\nsensitivity and global generalization to improve robustness across unseen\nforgeries. DeepShield enhances the CLIP-ViT encoder through two key components:\nLocal Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG\napplies spatiotemporal artifact modeling and patch-wise supervision to capture\nfine-grained inconsistencies often overlooked by global models. GFD introduces\ndomain feature augmentation, leveraging domain-bridging and boundary-expanding\nfeature generation to synthesize diverse forgeries, mitigating overfitting and\nenhancing cross-domain adaptability. Through the integration of novel local and\nglobal analysis for deepfake detection, DeepShield outperforms state-of-the-art\nmethods in cross-dataset and cross-manipulation evaluations, achieving superior\nrobustness against unseen deepfake attacks.", "AI": {"tldr": "DeepShield is a novel deepfake detection framework that combines local patch guidance and global forgery diversification to improve robustness across unseen manipulation techniques.", "motivation": "Existing deepfake detectors perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to reliance on forgery-specific artifacts, raising concerns about fraud and misinformation.", "method": "DeepShield enhances CLIP-ViT encoder with two components: Local Patch Guidance (LPG) for spatiotemporal artifact modeling and patch-wise supervision, and Global Forgery Diversification (GFD) for domain feature augmentation through domain-bridging and boundary-expanding feature generation.", "result": "DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks.", "conclusion": "The integration of novel local and global analysis in DeepShield provides effective deepfake detection with enhanced cross-domain adaptability and robustness."}}
{"id": "2510.25238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25238", "abs": "https://arxiv.org/abs/2510.25238", "authors": ["Qianqian Qiao", "DanDan Zheng", "Yihang Bo", "Bao Peng", "Heng Huang", "Longteng Jiang", "Huaye Wang", "Jingdong Chen", "Jun Zhou", "Xin Jin"], "title": "VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations", "comment": null, "summary": "Video aesthetic assessment, a vital area in multimedia computing, integrates\ncomputer vision with human cognition. Its progress is limited by the lack of\nstandardized datasets and robust models, as the temporal dynamics of video and\nmultimodal fusion challenges hinder direct application of image-based methods.\nThis study introduces VADB, the largest video aesthetic database with 10,490\ndiverse videos annotated by 37 professionals across multiple aesthetic\ndimensions, including overall and attribute-specific aesthetic scores, rich\nlanguage comments and objective tags. We propose VADB-Net, a dual-modal\npre-training framework with a two-stage training strategy, which outperforms\nexisting video quality assessment models in scoring tasks and supports\ndownstream video aesthetic assessment tasks. The dataset and source code are\navailable at https://github.com/BestiVictory/VADB.", "AI": {"tldr": "This paper introduces VADB, the largest video aesthetic database with 10,490 videos professionally annotated, and proposes VADB-Net, a dual-modal pre-training framework that outperforms existing video quality assessment models.", "motivation": "Video aesthetic assessment progress is limited by lack of standardized datasets and robust models, as temporal dynamics and multimodal fusion challenges prevent direct application of image-based methods.", "method": "Created VADB database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, and proposed VADB-Net - a dual-modal pre-training framework with two-stage training strategy.", "result": "VADB-Net outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks.", "conclusion": "The study provides both a comprehensive dataset (VADB) and effective model (VADB-Net) to advance video aesthetic assessment, with dataset and source code publicly available."}}
{"id": "2510.25239", "categories": ["cs.CV", "I.4.6"], "pdf": "https://arxiv.org/pdf/2510.25239", "abs": "https://arxiv.org/abs/2510.25239", "authors": ["Moritz Lucas", "Hamid Ebrahimy", "Viacheslav Barkov", "Ralf Pecenka", "Kai-Uwe K\u00fchnberger", "Bj\u00f6rn Waske"], "title": "Mapping and Classification of Trees Outside Forests using Deep Learning", "comment": null, "summary": "Trees Outside Forests (TOF) play an important role in agricultural landscapes\nby supporting biodiversity, sequestering carbon, and regulating microclimates.\nYet, most studies have treated TOF as a single class or relied on rigid\nrule-based thresholds, limiting ecological interpretation and adaptability\nacross regions. To address this, we evaluate deep learning for TOF\nclassification using a newly generated dataset and high-resolution aerial\nimagery from four agricultural landscapes in Germany. Specifically, we compare\nconvolutional neural networks (CNNs), vision transformers, and hybrid\nCNN-transformer models across six semantic segmentation architectures (ABCNet,\nLSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of\nwoody vegetation: Forest, Patch, Linear, and Tree, derived from previous\nstudies and governmental products. Overall, the models achieved good\nclassification accuracy across the four landscapes, with the FT-UNetFormer\nperforming best (mean Intersection-over-Union 0.74; mean F1 score 0.84),\nunderscoring the importance of spatial context understanding in TOF mapping and\nclassification. Our results show good results for Forest and Linear class and\nreveal challenges particularly in classifying complex structures with high edge\ndensity, notably the Patch and Tree class. Our generalization experiments\nhighlight the need for regionally diverse training data to ensure reliable\nlarge-scale mapping. The dataset and code are openly available at\nhttps://github.com/Moerizzy/TOFMapper", "AI": {"tldr": "This paper evaluates deep learning models for classifying Trees Outside Forests (TOF) using high-resolution aerial imagery, comparing six semantic segmentation architectures to map four woody vegetation categories.", "motivation": "TOF play crucial ecological roles but are often treated as a single class or use rigid thresholds, limiting ecological interpretation and regional adaptability.", "method": "Used deep learning with CNNs, vision transformers, and hybrid models across six architectures (ABCNet, LSKNet, FT-UNetFormer, DC-Swin, BANet, U-Net) on high-resolution aerial imagery from four German agricultural landscapes to classify Forest, Patch, Linear, and Tree categories.", "result": "Models achieved good accuracy overall, with FT-UNetFormer performing best (mean IoU 0.74; mean F1 score 0.84). Good results for Forest and Linear classes, but challenges with complex Patch and Tree structures with high edge density.", "conclusion": "Spatial context understanding is crucial for TOF mapping. Generalization experiments show need for regionally diverse training data for reliable large-scale mapping. Dataset and code are publicly available."}}
{"id": "2510.25257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25257", "abs": "https://arxiv.org/abs/2510.25257", "authors": ["Zijun Liao", "Yian Zhao", "Xin Shan", "Yu Yan", "Chang Liu", "Lei Lu", "Xiangyang Ji", "Jie Chen"], "title": "RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models", "comment": null, "summary": "Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.", "AI": {"tldr": "A distillation framework using Vision Foundation Models (VFMs) to enhance lightweight object detectors without increasing deployment overhead, achieving state-of-the-art performance on COCO dataset.", "motivation": "Lightweight network designs for high-speed inference often degrade feature representation, limiting performance improvements and practical on-device deployment. The paper aims to leverage VFMs' capabilities to enhance lightweight detectors.", "method": "Proposes a distillation framework with Deep Semantic Injector (DSI) module for integrating VFM representations into detector layers, and Gradient-guided Adaptive Modulation (GAM) strategy for dynamic semantic transfer adjustment based on gradient norm ratios.", "result": "RT-DETRv4 achieves state-of-the-art results on COCO with AP scores of 49.7/53.5/55.4/57.0 at speeds of 273/169/124/78 FPS respectively, delivering consistent performance gains across diverse DETR-based models.", "conclusion": "The proposed framework provides a cost-effective and adaptable solution for enhancing lightweight object detectors using VFMs, enabling practical real-time detection without increasing deployment and inference overhead."}}
{"id": "2510.25263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25263", "abs": "https://arxiv.org/abs/2510.25263", "authors": ["Yang Miao", "Jan-Nico Zaech", "Xi Wang", "Fabien Despinoy", "Danda Pani Paudel", "Luc Van Gool"], "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation", "comment": "10 pages, 5 figures, 14 tables, Neurips 2025", "summary": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.", "AI": {"tldr": "LangHOPS is the first MLLM-based framework for open-vocabulary object-part instance segmentation that grounds object-part hierarchies in language space rather than using visual grouping methods.", "motivation": "To address the limitations of prior approaches that rely on heuristic or learnable visual grouping for object-part segmentation, and to leverage the rich knowledge and reasoning capabilities of multimodal large language models.", "method": "Integrates MLLM into object-part parsing pipeline to ground object-part hierarchies in language space and link multi-granularity concepts within hierarchies, using language-grounded hierarchy and MLLM-driven part query refinement.", "result": "Achieves state-of-the-art results: 5.5% AP improvement (in-domain) and 4.8% AP (cross-dataset) on PartImageNet, and 2.5% mIOU improvement on unseen object parts in ADE20K (zero-shot).", "conclusion": "LangHOPS demonstrates the effectiveness of language-grounded hierarchy and MLLM-driven part query refinement for open-vocabulary object-part instance segmentation across multiple challenging scenarios."}}
{"id": "2510.25279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25279", "abs": "https://arxiv.org/abs/2510.25279", "authors": ["Yuyang Huang", "Yabo Chen", "Junyu Zhou", "Wenrui Dai", "Xiaopeng Zhang", "Junni Zou", "Hongkai Xiong", "Qi Tian"], "title": "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation", "comment": "Accepted by NeurIPS 2025", "summary": "Source-free domain adaptation (SFDA) is a challenging task that tackles\ndomain shifts using only a pre-trained source model and unlabeled target data.\nExisting SFDA methods are restricted by the fundamental limitation of\nsource-target domain discrepancy. Non-generation SFDA methods suffer from\nunreliable pseudo-labels in challenging scenarios with large domain\ndiscrepancies, while generation-based SFDA methods are evidently degraded due\nto enlarged domain discrepancies in creating pseudo-source data. To address\nthis limitation, we propose a novel generation-based framework named\nDiffusion-Driven Progressive Target Manipulation (DPTM) that leverages\nunlabeled target data as references to reliably generate and progressively\nrefine a pseudo-target domain for SFDA. Specifically, we divide the target\nsamples into a trust set and a non-trust set based on the reliability of\npseudo-labels to sufficiently and reliably exploit their information. For\nsamples from the non-trust set, we develop a manipulation strategy to\nsemantically transform them into the newly assigned categories, while\nsimultaneously maintaining them in the target distribution via a latent\ndiffusion model. Furthermore, we design a progressive refinement mechanism that\nprogressively reduces the domain discrepancy between the pseudo-target domain\nand the real target domain via iterative refinement. Experimental results\ndemonstrate that DPTM outperforms existing methods by a large margin and\nachieves state-of-the-art performance on four prevailing SFDA benchmark\ndatasets with different scales. Remarkably, DPTM can significantly enhance the\nperformance by up to 18.6% in scenarios with large source-target gaps.", "AI": {"tldr": "DPTM is a novel generation-based SFDA framework that uses target data as references to generate and progressively refine a pseudo-target domain, addressing domain discrepancy limitations in source-free domain adaptation.", "motivation": "Existing SFDA methods are limited by source-target domain discrepancy - non-generation methods suffer from unreliable pseudo-labels in large domain gaps, while generation-based methods degrade due to enlarged discrepancies when creating pseudo-source data.", "method": "Divides target samples into trust/non-trust sets based on pseudo-label reliability, uses latent diffusion model to semantically transform non-trust samples while maintaining target distribution, and employs progressive refinement mechanism to iteratively reduce domain discrepancy.", "result": "Outperforms existing methods by large margin, achieves SOTA on four SFDA benchmark datasets, and enhances performance by up to 18.6% in scenarios with large source-target gaps.", "conclusion": "DPTM effectively addresses domain discrepancy limitations in SFDA through target-driven generation and progressive refinement, demonstrating superior performance especially in challenging scenarios with large domain gaps."}}
{"id": "2510.25301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25301", "abs": "https://arxiv.org/abs/2510.25301", "authors": ["Yang Jin", "Guangyu Guo", "Binglu Wang"], "title": "GaTector+: A Unified Head-free Framework for Gaze Object and Gaze Following Prediction", "comment": null, "summary": "Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks.", "AI": {"tldr": "GaTector+ is a unified framework for gaze object detection and gaze following that eliminates head-related priors dependency during inference, using shared backbone with task-specific blocks, head detection branch, attention mechanisms, and a new evaluation metric mSoC.", "motivation": "Previous methods solve gaze object detection and gaze following separately and depend on head-related prior knowledge, requiring auxiliary networks for head location extraction, which prevents joint optimization and limits practical applicability.", "method": "Uses expanded specific-general-specific feature extractor with shared backbone, embeds head detection branch, proposes head-based attention mechanism to fuse features, implements attention supervision for gaze heatmap learning, and introduces mSoC evaluation metric.", "result": "Experimental results on multiple benchmark datasets demonstrate effectiveness in both gaze object detection and gaze following tasks.", "conclusion": "GaTector+ provides a unified solution that eliminates head-related priors dependency while achieving strong performance across both gaze-related tasks through its novel architecture and attention mechanisms."}}
{"id": "2510.25318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25318", "abs": "https://arxiv.org/abs/2510.25318", "authors": ["Yushen Huang", "Zhiming Wang"], "title": "Prototype-Driven Adaptation for Few-Shot Object Detection", "comment": "7 pages,1 figure,2 tables,Preprint", "summary": "Few-shot object detection (FSOD) often suffers from base-class bias and\nunstable calibration when only a few novel samples are available. We propose\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\nthat provides a prototype-based \"second opinion\" complementary to the linear\nclassifier. PDA maintains support-only prototypes in a learnable\nidentity-initialized projection space and optionally applies\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\nupdates on labeled foreground RoIs-without introducing class-specific\nparameters-and are frozen at inference to ensure strict protocol compliance.\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\nand temperature-scaled fusion to combine metric similarities with detector\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\nimproves novel-class performance with minimal impact on base classes and\nnegligible computational overhead.", "AI": {"tldr": "PDA is a lightweight plug-in metric head for DeFRCN that uses prototype-based matching to improve few-shot object detection by reducing base-class bias and improving calibration.", "motivation": "Few-shot object detection suffers from base-class bias and unstable calibration when only limited novel samples are available, requiring methods that can work effectively with scarce data.", "method": "PDA maintains support-only prototypes in a learnable projection space, uses prototype-conditioned RoI alignment, adapts prototypes via EMA updates during fine-tuning, and employs best-of-K matching with temperature-scaled fusion to combine metric similarities with detector logits.", "result": "Experiments on VOC FSOD and GFSOD benchmarks show PDA consistently improves novel-class performance with minimal impact on base classes and negligible computational overhead.", "conclusion": "PDA provides an effective prototype-driven approach that enhances few-shot object detection by offering complementary metric-based classification while maintaining protocol compliance and computational efficiency."}}
{"id": "2510.25332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25332", "abs": "https://arxiv.org/abs/2510.25332", "authors": ["Yuhang Hu", "Zhenyu Yang", "Shihan Wang", "Shengsheng Qian", "Bin Wen", "Fan Yang", "Tingting Gao", "Changsheng Xu"], "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA", "comment": null, "summary": "The rapid growth of streaming video applications demands multimodal models\nwith enhanced capabilities for temporal dynamics understanding and complex\nreasoning. However, current Video Question Answering (VideoQA) datasets suffer\nfrom two critical limitations: 1) Static annotation mechanisms fail to capture\nthe evolving nature of answers in temporal video streams, and 2) The absence of\nexplicit reasoning process annotations restricts model interpretability and\nlogical deduction capabilities. To address these challenges, We introduce\nStreamingCoT, the first dataset explicitly designed for temporally evolving\nreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our\nframework first establishes a dynamic hierarchical annotation architecture that\ngenerates per-second dense descriptions and constructs temporally-dependent\nsemantic segments through similarity fusion, paired with question-answer sets\nconstrained by temporal evolution patterns. We further propose an explicit\nreasoning chain generation paradigm that extracts spatiotemporal objects via\nkeyframe semantic alignment, derives object state transition-based reasoning\npaths using large language models, and ensures logical coherence through\nhuman-verified validation. This dataset establishes a foundation for advancing\nresearch in streaming video understanding, complex temporal reasoning, and\nmultimodal inference. Our StreamingCoT and its construction toolkit can be\naccessed at https://github.com/Fleeting-hyh/StreamingCoT.", "AI": {"tldr": "StreamingCoT is a new dataset for streaming VideoQA that addresses limitations in temporal dynamics and reasoning process annotations through dynamic hierarchical annotation and explicit reasoning chain generation.", "motivation": "Current VideoQA datasets fail to capture evolving answers in temporal video streams and lack explicit reasoning process annotations, limiting model interpretability and logical deduction capabilities.", "method": "Dynamic hierarchical annotation architecture with per-second dense descriptions and temporally-dependent semantic segments, plus explicit reasoning chain generation using spatiotemporal object extraction, state transition-based reasoning paths, and human-verified validation.", "result": "StreamingCoT establishes the first dataset for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought tasks, providing a foundation for advancing streaming video understanding.", "conclusion": "The dataset addresses critical limitations in current VideoQA approaches and enables research in complex temporal reasoning and multimodal inference for streaming video applications."}}
{"id": "2510.25345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25345", "abs": "https://arxiv.org/abs/2510.25345", "authors": ["Zhigang Tu", "Zhengbo Zhang", "Jia Gong", "Junsong Yuan", "Bo Du"], "title": "Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples", "comment": "Accepted by IEEE Transactions on Image Processing (TIP), 2025", "summary": "Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.", "AI": {"tldr": "The paper proposes a novel approach to semi-supervised 3D action recognition using active learning, reformulating it as a Markov Decision Process (MDP) to intelligently select informative skeleton sequences for annotation, with enhancements including hyperbolic space projection and meta tuning.", "motivation": "To address the limitation that representative skeleton sequences may not be the most informative for action recognition, as the model may already have similar knowledge from previously seen samples. The goal is to reduce reliance on costly annotations while maintaining competitive accuracy.", "method": "Reformulates semi-supervised 3D action recognition via active learning as a Markov Decision Process (MDP), trains an informative sample selection model, projects state-action pairs from Euclidean to hyperbolic space for enhanced representation, and introduces meta tuning for faster real-world deployment.", "result": "Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of the proposed method.", "conclusion": "The MDP-based approach with hyperbolic space projection and meta tuning provides an effective solution for semi-supervised 3D action recognition, enabling intelligent selection of informative samples for annotation while reducing annotation costs."}}
{"id": "2510.25347", "categories": ["cs.CV", "cs.LG", "68U10", "I.2.1"], "pdf": "https://arxiv.org/pdf/2510.25347", "abs": "https://arxiv.org/abs/2510.25347", "authors": ["Ayman Abaid", "Gianpiero Guidone", "Sara Alsubai", "Foziyah Alquahtani", "Talha Iqbal", "Ruth Sharif", "Hesham Elzomor", "Emiliano Bianchini", "Naeif Almagal", "Michael G. Madden", "Faisal Sharif", "Ihsan Ullah"], "title": "3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework", "comment": "11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in\n  Volume 16206 of the Lecture Notes in Computer Science series", "summary": "Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.", "AI": {"tldr": "Proposed radiomics-based pipeline using pseudo-labeling for CAC scoring on non-contrast CCTA scans, outperforming foundation model features with 84% accuracy without expert annotations.", "motivation": "Address limited annotated data for coronary artery calcium scoring in non-contrast CCTA scans and eliminate need for expert-defined segmentations.", "method": "Radiomics-based pipeline with pseudo-labeling for training labels, comparison of radiomics features vs pretrained foundation models (CT-FM, RadImageNet) with traditional classifiers, evaluated on 182-patient CCTA dataset.", "result": "Radiomics-based models significantly outperformed CNN-derived embeddings from foundation models (84% accuracy, p<0.05) despite no expert annotations.", "conclusion": "Radiomics with pseudo-labeling is effective for CAC scoring without expert annotations, superior to foundation model features for this task."}}
{"id": "2510.25372", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25372", "abs": "https://arxiv.org/abs/2510.25372", "authors": ["M Yashwanth", "Sharannya Ghosh", "Aditay Tripathi", "Anirban Chakraborty"], "title": "Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers", "comment": null, "summary": "Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.", "AI": {"tldr": "PEP-FedPT is a federated prompt tuning framework for Vision Transformers that achieves both generalization and personalization through Class-Contextualized Mixed Prompts, combining global and class-specific prompts adaptively per sample.", "motivation": "Visual Prompt Tuning is effective for parameter-efficient fine-tuning but struggles in federated learning - global prompts don't generalize well across heterogeneous clients, while personalized prompts overfit to local data.", "method": "Proposes Class-Contextualized Mixed Prompt (CCMP) that maintains class-specific prompts alongside global prompts, adaptively combining them using weights from global class prototypes and client class priors for per-sample personalization without client-dependent parameters.", "result": "Comprehensive evaluations on CIFAR-100, TinyImageNet, DomainNet, and iNaturalist show PEP-FedPT consistently surpasses state-of-the-art baselines under diverse data heterogeneity scenarios.", "conclusion": "PEP-FedPT establishes a strong foundation for efficient and generalizable federated prompt tuning of Vision Transformers, achieving both generalization and personalization effectively."}}
{"id": "2510.25387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25387", "abs": "https://arxiv.org/abs/2510.25387", "authors": ["Bill Psomas", "George Retsinas", "Nikos Efthymiadis", "Panagiotis Filntisis", "Yannis Avrithis", "Petros Maragos", "Ondrej Chum", "Giorgos Tolias"], "title": "Instance-Level Composed Image Retrieval", "comment": "NeurIPS 2025", "summary": "The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.", "AI": {"tldr": "The paper introduces i-CIR, a new instance-level composed image retrieval dataset, and BASIC, a training-free method that uses pre-trained VLMs to achieve state-of-the-art performance on CIR tasks.", "motivation": "Progress in composed image retrieval (CIR) is hindered by the lack of high-quality training and evaluation data, particularly for instance-level retrieval where the goal is to find the same specific object under various modifications.", "method": "Proposes BASIC - a training-free approach that leverages pre-trained vision-and-language models to separately estimate query-image-to-image and query-text-to-image similarities, then performs late fusion to upweight images satisfying both queries while downweighting those with high similarity to only one.", "result": "BASIC achieves state-of-the-art performance on both the new i-CIR dataset and existing CIR datasets with semantic-level class definitions, demonstrating effectiveness across different retrieval paradigms.", "conclusion": "The combination of the i-CIR dataset and BASIC method addresses key challenges in CIR research, providing both better evaluation benchmarks and a simple yet effective training-free approach that outperforms existing methods."}}
{"id": "2510.25440", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25440", "abs": "https://arxiv.org/abs/2510.25440", "authors": ["Eshika Khandelwal", "Junyu Xie", "Tengda Han", "Max Bain", "Arsha Nagrani", "Andrew Zisserman", "G\u00fcl Varol", "Makarand Tapaswi"], "title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions", "comment": null, "summary": "Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.", "AI": {"tldr": "CoherentAD is a training-free method that generates coherent Audio Descriptions by selecting from multiple candidates across sequences, outperforming independent generation approaches.", "motivation": "Current automatic AD generation methods produce descriptions independently, leading to repetitive and incoherent sequences that fail to help visually impaired audiences visualize unfolding scenes.", "method": "Generate multiple candidate descriptions for each AD time interval, then perform auto-regressive selection across the sequence to form coherent narratives.", "result": "Produces coherent AD sequences with enhanced narrative understanding, outperforming prior independent generation approaches.", "conclusion": "The proposed CoherentAD method effectively addresses the coherence problem in AD generation through sequence-level candidate selection, validated by new evaluation metrics."}}
{"id": "2510.25739", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25739", "abs": "https://arxiv.org/abs/2510.25739", "authors": ["Zhi-Kai Chen", "Jun-Peng Jiang", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation", "comment": null, "summary": "Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.", "AI": {"tldr": "Hawk introduces speculative decoding for autoregressive image generation, achieving 1.71x speedup while maintaining image quality by leveraging spatial structure to improve draft model accuracy.", "motivation": "Autoregressive image generation models produce high-quality images but suffer from slow inference due to sequential token-by-token decoding. While speculative decoding has accelerated text generation, it hasn't been effectively applied to images due to larger sampling spaces and poor utilization of spatial structure.", "method": "Hawk uses speculative decoding with a lightweight draft model that leverages the spatial structure of images to guide predictions. It models local dependencies more effectively by utilizing the two-dimensional nature of images.", "result": "Experimental results on multiple text-to-image benchmarks show a 1.71x speedup over standard autoregressive models while preserving both image fidelity and diversity.", "conclusion": "Hawk successfully adapts speculative decoding to image generation by exploiting spatial structure, achieving significant speed improvements without compromising image quality."}}
{"id": "2510.25760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25760", "abs": "https://arxiv.org/abs/2510.25760", "authors": ["Xu Zheng", "Zihao Dongfang", "Lutao Jiang", "Boyuan Zheng", "Yulong Guo", "Zhenquan Zhang", "Giuliano Albanese", "Runyi Yang", "Mengjiao Ma", "Zixin Zhang", "Chenfei Liao", "Dingcheng Zhen", "Yuanhuiyi Lyu", "Yuqian Fu", "Bin Ren", "Linfeng Zhang", "Danda Pani Paudel", "Nicu Sebe", "Luc Van Gool", "Xuming Hu"], "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks", "comment": null, "summary": "Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.", "AI": {"tldr": "This survey provides a comprehensive review of multimodal spatial reasoning with large models, covering MLLMs, open benchmarks, and various spatial tasks from 2D to 3D, including embodied AI and emerging modalities like audio and egocentric video.", "motivation": "Humans have strong spatial reasoning abilities through multimodal observations, but systematic reviews and public benchmarks for large multimodal reasoning models remain limited, creating a need for comprehensive evaluation frameworks.", "method": "The survey categorizes recent progress in multimodal large language models (MLLMs), outlines general spatial reasoning techniques, and examines various spatial tasks including 2D/3D understanding, visual question answering, embodied AI, and emerging modalities.", "result": "The survey establishes a foundation for multimodal spatial reasoning research and provides open benchmarks for evaluation, with codes and implementations available on GitHub.", "conclusion": "This survey offers comprehensive insights into the growing field of multimodal spatial reasoning and serves as a solid foundation for future research in this area."}}
{"id": "2510.25765", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.25765", "abs": "https://arxiv.org/abs/2510.25765", "authors": ["Chuhao Chen", "Isabella Liu", "Xinyue Wei", "Hao Su", "Minghua Liu"], "title": "FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion", "comment": null, "summary": "Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.", "AI": {"tldr": "FreeArt3D is a training-free framework that generates articulated 3D objects by repurposing pre-trained static 3D diffusion models and extending Score Distillation Sampling to handle articulation as an additional dimension.", "motivation": "Current methods for articulated 3D object modeling either require dense-view supervision or produce coarse approximations without textures. While static 3D generation has advanced significantly, extending native 3D diffusion models to articulated objects is challenging due to limited training data.", "method": "FreeArt3D uses a pre-trained static 3D diffusion model as a shape prior and extends Score Distillation Sampling (SDS) to the 3D-to-4D domain, treating articulation as an additional generative dimension. It jointly optimizes geometry, texture, and articulation parameters from a few images in different articulation states.", "result": "The method generates high-fidelity geometry and textures, accurately predicts kinematic structures, and generalizes well across diverse object categories. It completes in minutes and significantly outperforms prior state-of-the-art approaches in quality and versatility.", "conclusion": "FreeArt3D demonstrates that articulated 3D generation can be achieved without task-specific training or large-scale articulated datasets by leveraging pre-trained static 3D models and extending existing optimization techniques to handle articulation."}}
{"id": "2510.25772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25772", "abs": "https://arxiv.org/abs/2510.25772", "authors": ["Baolu Li", "Yiming Zhang", "Qinghe Wang", "Liqian Ma", "Xiaoyu Shi", "Xintao Wang", "Pengfei Wan", "Zhenfei Yin", "Yunzhi Zhuge", "Huchuan Lu", "Xu Jia"], "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning", "comment": "Project Page URL:https://libaolu312.github.io/VFXMaster/", "summary": "Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.", "AI": {"tldr": "VFXMaster is a unified reference-based framework for VFX video generation that treats effect generation as an in-context learning task, enabling reproduction of diverse dynamic effects from reference videos to target content with strong generalization to unseen effects.", "motivation": "Current VFX generation methods rely on one-LoRA-per-effect paradigm which is resource-intensive and cannot generalize to unseen effects, limiting scalability and creative potential.", "method": "Uses in-context conditioning strategy with reference examples, in-context attention masks to decouple and inject effect attributes, and one-shot effect adaptation mechanism for rapid generalization to tough unseen effects.", "result": "Effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects, demonstrating capability to reproduce diverse dynamic effects from reference videos.", "conclusion": "VFXMaster provides a scalable and unified approach to VFX generation that overcomes limitations of previous methods, with plans to release code, models, and dataset to foster future research."}}
