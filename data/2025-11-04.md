<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: A novel AI model for creative human-AI dance interaction that generates responsive movement sequences by combining motion inpainting and style transfer using single-person motion data.


<details>
  <summary>Details</summary>
Motivation: To explore embodied human-AI interaction through dance, complementing verbal AI interactions by creating an artificial dance partner that can mimic and creatively enhance human movement.

Method: Leverages single-person motion capture data with high-level features, combining two diffusion models with motion inpainting and motion style transfer to generate temporally coherent and responsive movement representations.

Result: The model successfully generates diverse and realistic dance movements that show various deviations from human partners while maintaining temporal coherence, with quantitative assessment showing convergence between generated samples and test set distributions.

Conclusion: The model represents first steps toward creative dancing with AI, demonstrating the potential for AI to serve as responsive and creative dance partners without relying on human-human interaction data.

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: A machine learning system using CNN achieved 88% accuracy in classifying coral bleaching from global dataset, outperforming ResNet and ViT models.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are vital marine ecosystems facing increasing threats from pollution, ocean acidification, and temperature anomalies, making efficient monitoring urgent.

Method: Developed a coral bleaching classification system using three state-of-the-art models (ResNet, ViT, CNN) on diverse global dataset with healthy and bleached corals under various environmental conditions.

Result: CNN model achieved highest accuracy of 88% after hyperparameter tuning, outperforming existing benchmarks and other tested models.

Conclusion: The study provides important insights for autonomous coral monitoring and offers comprehensive analysis of widely used computer vision models for coral health assessment.

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: YOLOv8-based deep learning pipeline automates family-level fish identification from video transects in Kenya and Tanzania, achieving mAP@0.5 of 0.52 with better performance on abundant families.


<details>
  <summary>Details</summary>
Motivation: Coral reef monitoring in the Western Indian Ocean is limited by labor-intensive underwater visual censuses, creating need for automated solutions.

Method: Used YOLOv8-based deep learning pipeline on curated dataset of 24 fish families from video transects collected in Kenya and Tanzania.

Result: Best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa.

Conclusion: Deep learning shows potential as scalable complement to traditional reef monitoring methods in the Western Indian Ocean.

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: The paper proposes using mutual information from real-world distributions to select training data for contrastive learning, replacing human-designed augmentation strategies with data-driven selection of informative patches.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning methods rely on human-designed data augmentation strategies (like color jittering) which may be suboptimal. The authors aim to leverage mutual information from real-world distributions to automatically select more informative training data.

Method: Select patches that exhibit high mutual information under natural perturbations (color changes, motion) as positive samples for contrastive learning, using mutual-information-informed data augmentation instead of human-designed augmentations.

Result: The method was evaluated on multiple benchmarks across state-of-the-art representation learning frameworks and demonstrated effectiveness, showing better generalization in open environments.

Conclusion: Mutual-information-informed data augmentation is a promising direction for representation learning that can improve feature generalization beyond human-designed augmentation strategies.

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: This study benchmarks three FL frameworks (NVIDIA FLARE, Flower, Owkin Substra) for medical imaging using PathMNIST dataset, evaluating performance, convergence, communication, scalability, and developer experience.


<details>
  <summary>Details</summary>
Motivation: Federated Learning enables collaborative model training across medical institutions without direct data sharing, addressing privacy concerns in healthcare AI applications.

Method: Benchmarking three FL frameworks using PathMNIST dataset, assessing model performance, convergence efficiency, communication overhead, scalability, and developer experience.

Result: NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features.

Conclusion: Each FL framework has distinct strengths optimized for different use cases, emphasizing their practical relevance for deployment in healthcare environments.

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: This paper presents a comparative study of image denoising methods combined with CLAHE for enhancing rice leaf images, focusing on improving image quality for agricultural analysis applications.


<details>
  <summary>Details</summary>
Motivation: Image enhancement is crucial for rice leaf analysis in agriculture, aiding in disease detection, nutrient deficiency evaluation, and growth analysis. Denoising and contrast enhancement are essential preprocessing steps to make subsequent image analysis tasks more reliable.

Method: The study conducts an extensive comparative analysis of well-known image denoising methods combined with Contrast Limited Adaptive Histogram Equalization (CLAHE) for denoising rice leaf images. Experiments were performed on a relevant rice leaf image dataset using various evaluation metrics.

Result: The approach provides a strong basis for assessing the effectiveness of image enhancement methodologies in digital image processing, with results examined comprehensively using multiple metrics.

Conclusion: The study reveals insights that are useful for future adaptation in agricultural research and other domains, demonstrating the practical value of systematic image enhancement approaches for agricultural applications.

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [7] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: This paper introduces InfraLiDARs' Benchmark, a novel dataset comparing repetitive vs non-repetitive LiDAR scanning patterns for roadside perception, finding that non-repetitive LiDAR offers comparable performance to high-end repetitive LiDAR at lower cost.


<details>
  <summary>Details</summary>
Motivation: While LiDAR placement optimization has been studied, the impact of different scanning patterns (repetitive vs non-repetitive) on perceptual performance in infrastructure-based systems remains under-investigated, despite their distinct point cloud distributions affecting object detection efficacy.

Method: Created InfraLiDARs' Benchmark dataset in CARLA simulation with concurrent infrastructure-based LiDARs using both scanning paradigms, then conducted comprehensive statistical analysis and evaluated various 3D object detection algorithms.

Result: Non-repetitive scanning LiDAR and 128-line repetitive LiDAR showed comparable detection performance across scenarios. Non-repetitive LiDAR is cost-effective despite limited perception range.

Conclusion: The study provides insights for optimal LiDAR scanning pattern selection and compatible algorithms for roadside applications, and publicly releases the benchmark dataset to advance research in this area.

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [8] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5 is a next-generation Physical AI model that unifies Text2World, Image2World, and Video2World generation using flow-based architecture, achieving improved video quality and instruction alignment over previous versions.


<details>
  <summary>Details</summary>
Motivation: To advance Physical AI by creating unified world generation models that enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems.

Method: Built on flow-based architecture, leverages Cosmos-Reason1 for text grounding and world simulation control, trained on 200M curated video clips with reinforcement learning-based post-training, and includes Cosmos-Transfer2.5 for Sim2Real/Real2Real translation.

Result: Achieves substantial improvements over Cosmos-Predict1 in video quality and instruction alignment, with models at 2B and 14B scales. Cosmos-Transfer2.5 delivers higher fidelity and robust long-horizon video generation despite being 3.5x smaller than previous version.

Conclusion: These advances establish Cosmos-Predict2.5 and Cosmos-Transfer2.5 as versatile tools for scaling embodied intelligence, with open-source release to accelerate Physical AI research and deployment.

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [9] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: Deep learning models were applied to detect habitat changes in complex alpine environments using geospatial foundation models (GFMs) and compared post-classification versus direct change detection approaches.


<details>
  <summary>Details</summary>
Motivation: Rapid climate change and disturbances in alpine ecosystems require frequent habitat monitoring, but manual mapping is too expensive for the needed temporal resolution.

Method: Compared two paradigms: post-classification change detection using GFMs (Prithvi-EO-2.0 and Clay v1.0) vs U-Net CNNs, and direct change detection using ChangeViT transformer vs U-Net baselines, using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km².

Result: Clay v1.0 achieved 51% overall accuracy vs U-Net's 41% for multi-class habitat change; both reached 67% for binary change detection. Direct CD yielded superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. LiDAR integration improved semantic segmentation from 30% to 50% accuracy.

Conclusion: Although overall accuracies are lower than in homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [10] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa is a training-free acceleration framework for diffusion-based video generation that uses lexicographic minimax path optimization to bound global errors, achieving 2.9x speedup on Latte model while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing caching strategies for video generation focus on reducing local errors but overlook global error accumulation, leading to content degradation between accelerated and original videos.

Method: Formulates cache scheduling as a directed graph with error-weighted edges and introduces Lexicographic Minimax Path Optimization to explicitly bound worst-case path error, improving global content and style consistency.

Result: Achieves 2.9x speedup on Latte model and LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques with minimal perceptual quality degradation.

Conclusion: LeMiCa provides a robust and generalizable paradigm for accelerating diffusion-based video generation while maintaining quality, serving as a strong foundation for future efficient video synthesis research.

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [11] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD is a three-stage framework that improves vision-language-action models through residual RL and distribution-aware data collection, achieving near-perfect task success rates across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning (SFT) relies on costly human demonstrations, limiting scalability and generalization of large vision-language-action models.

Method: Three-stage framework: 1) Train lightweight residual actors to probe failure regions, 2) Use hybrid rollout scheme for distribution-aligned trajectory collection with recovery behaviors, 3) Distill curated trajectories back into the generalist using standard SFT.

Result: Achieves 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks.

Conclusion: PLD offers a scalable path toward self-improving VLA models, with residual probing and distribution-aware replay being key to collecting deployment-aligned data that improves both seen and unseen tasks.

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [12] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: SpinalSAM-R1 is a multimodal vision-language system that combines fine-tuned SAM with DeepSeek-R1 for spine CT image segmentation, addressing limitations of existing models through anatomy-guided attention and natural language interaction.


<details>
  <summary>Details</summary>
Motivation: Spinal CT image segmentation is crucial for disease diagnosis but faces challenges due to low contrast, complex vertebral boundaries, and limitations of existing models like SAM which have high annotation requirements and poor domain adaptability in spinal imaging.

Method: Proposed SpinalSAM-R1 integrates fine-tuned SAM with DeepSeek-R1 using LoRA for efficient adaptation. It introduces anatomy-guided attention mechanism and semantics-driven interaction protocol for natural language-guided refinement. The system supports point, box, and text-based prompts.

Result: The method achieves superior segmentation performance on spine anatomical structures with CT images. The developed PyQt5-based software supports 11 clinical operations with 94.3% parsing accuracy and sub-800 ms response times.

Conclusion: SpinalSAM-R1 effectively addresses the limitations of existing segmentation models in spinal CT imaging through multimodal integration and interactive capabilities, providing a practical solution for clinical applications.

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [13] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: Proposes a filter method for confocal laser endomicroscopy (CLE) video sequences to reduce redundancy in self-supervised learning, improving training efficiency and performance on tumor classification tasks.


<details>
  <summary>Details</summary>
Motivation: CLE images are hard to interpret for non-experts, and machine learning models suffer from overfitting due to limited histopathology-correlated data. Self-supervised learning can help but faces challenges from high inter-frame correlation in CLE videos.

Method: Developed a filter functionality for CLE video sequences to reduce dataset redundancy in SSL training. Used four baseline networks and a SSL teacher-student network with vision transformer backbone, evaluated on sinonasal tumor and skin squamous cell carcinoma datasets.

Result: Filtered SSL-pretrained models achieved highest test accuracy: 67.48% on sinonasal tumor dataset and 73.52% on skin squamous cell carcinoma dataset, significantly outperforming non-SSL baselines. Training time reduced by 67%.

Conclusion: SSL is effective for CLE pretraining, and the proposed video filter improves training efficiency and performance in self-supervised scenarios.

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [14] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders is a training-free, modality-agnostic method for fine-grained controllable generation in diffusion models by partially estimating Concept Sliders formula during inference, with extensions to video/audio benchmarks and automatic scale selection.


<details>
  <summary>Details</summary>
Motivation: Existing Concept Sliders require per-concept training and architecture-specific fine-tuning, limiting scalability to new modalities. There's a need for training-free methods that work across different modalities without retraining.

Method: Partially estimate the Concept Sliders formula during inference without any training. Introduce automatic scale selection and non-linear traversals through a two-stage procedure that detects saturation points and reparameterizes traversal for perceptually uniform edits.

Result: Enables plug-and-play, training-free concept control across modalities (images, video, audio), improves over existing baselines, and establishes new evaluation metrics and tools for principled controllable generation.

Conclusion: FreeSliders provides an effective training-free solution for fine-grained concept control in diffusion models that works across multiple modalities, addressing scalability limitations of previous methods while maintaining performance.

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [15] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI is a hierarchical framework for text-to-video generation that integrates compositional scene understanding with temporal-aware diffusion models, achieving state-of-the-art performance through three key innovations: compositional scene parsing, temporal-spatial attention, and progressive video refinement.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video generation approaches struggle with maintaining temporal consistency, compositional understanding, and fine-grained control over visual narratives, creating a need for more sophisticated frameworks.

Method: MOVAI introduces three key components: (1) Compositional Scene Parser (CSP) that decomposes text into hierarchical scene graphs with temporal annotations, (2) Temporal-Spatial Attention Mechanism (TSAM) for coherent motion dynamics and spatial detail preservation, and (3) Progressive Video Refinement (PVR) module for iterative quality enhancement through multi-scale temporal reasoning.

Result: Extensive experiments show MOVAI achieves state-of-the-art performance with 15.3% improvement in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods, particularly excelling in complex multi-object scenes with realistic temporal dynamics.

Conclusion: MOVAI demonstrates superior text-to-video synthesis capabilities through its hierarchical framework that effectively integrates compositional scene understanding with temporal-aware modeling, enabling fine-grained semantic control and realistic video generation.

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [16] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: Chain of Time is a cognitively-inspired method that improves physical simulation in vision-language models by generating intermediate images during simulation, requiring no fine-tuning and enhancing performance on physical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Inspired by in-context reasoning in machine learning and mental simulation in humans, the method aims to improve physical simulation capabilities in vision-language models by breaking down simulations into temporal steps.

Method: The Chain of Time method generates a series of intermediate images during physical simulations at inference time without additional fine-tuning, applied to 2-D graphics and 3-D videos testing velocity, acceleration, fluid dynamics, and conservation of momentum.

Result: Chain of Time substantially improves performance of state-of-the-art image generation models and reveals hidden physical reasoning capabilities, showing models can simulate time-evolving properties like velocity, gravity, and collisions.

Conclusion: The method successfully enhances physical simulation in vision-language models while providing interpretability insights, though it also identifies cases where models struggle to infer physical parameters despite simulation capabilities.

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [17] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: First end-to-end framework combining generative AI and deep reinforcement learning for autonomous cardiac ultrasound scanning, addressing operator dependence and accessibility issues in cardiology diagnostics.


<details>
  <summary>Details</summary>
Motivation: Address limitations of cardiac ultrasound including operator dependence, time constraints, human error, and shortage of trained professionals, especially in remote areas, requiring automated solutions for consistent and accessible cardiac imaging.

Method: Two-component framework: (1) conditional generative simulator using GANs with VAEs to model cardiac US environment and produce realistic action-conditioned images; (2) DRL module that learns autonomous scanning policies using this simulator, with AI-driven guidance through expert-validated classification and quality assessment models.

Result: Framework enables conditional generation of realistic US images and establishes reproducible foundation extendable to other organs. VAE-GAN benchmarked against existing GAN variants with qualitative and quantitative assessment, while DRL-based scanning system evaluated under varying configurations to demonstrate effectiveness.

Conclusion: Proposed framework successfully integrates generative AI and DRL for autonomous cardiac US scanning, provides reproducible foundation with publicly available dataset, and shows promise for addressing accessibility and consistency issues in cardiac imaging diagnostics.

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [18] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D is a dual-stream architecture that combines visual and geometric data from RGB-D input for robust 6D object pose estimation, achieving state-of-the-art performance on challenging datasets like Occluded-LineMOD.


<details>
  <summary>Details</summary>
Motivation: Current 6D object pose estimation methods struggle with generalization from synthetic to real-world data due to lighting variations, textureless objects, and significant occlusions.

Method: Uses dual-stream architecture with: 1) DINOv2 Vision Transformer for RGB processing (self-supervised, handles texture/lighting variations), and 2) PointNet++ encoder for 3D point cloud processing (handles sparse, occluded data). Features are fused for multi-task prediction.

Result: Achieved new state-of-the-art performance on the challenging Occluded-LineMOD dataset, demonstrating superior robustness and accuracy.

Conclusion: VLM6D effectively addresses key challenges in 6D pose estimation by leveraging complementary visual and geometric information through specialized encoders, enabling robust performance in real-world conditions.

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [19] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: A novel hybrid architecture combining ConvNeXt and Vision Transformers (ViT) for facial age estimation, leveraging CNN's local feature extraction and ViT's global attention to achieve superior performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Age estimation from facial images is complex and requires both local feature extraction and global attention mechanisms. Existing models like CNNs and Transformers have complementary strengths that can be integrated for better performance.

Method: Proposed ConvNeXt-ViT hybrid architecture using pre-trained models, linear layers, and advanced regularization techniques. Includes systematic exploration of configurations and adapted attention mechanisms within CNN framework.

Result: Achieved superior performance in terms of mean absolute error (MAE) on benchmark datasets MORPH II, CACD, and AFAD. Outperforms traditional methods and provides robust foundation for future advances.

Conclusion: Hybrid architectures combining CNNs and Transformers show transformative potential for age estimation and related computer vision tasks, representing a promising direction for seamless integration of complementary model strengths.

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [20] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC is an efficient visual token compression framework for long video understanding that uses facility location function and lazy greedy algorithm to select a compact, representative subset of visual tokens, reducing computational load while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current video-LMMs face scalability issues due to the overwhelming number of visual tokens from extended video sequences, which limits their practical application in long video understanding.

Method: Proposes FLoC framework based on facility location function with lazy greedy algorithm to efficiently select a diverse and representative subset of visual tokens within a predefined budget, making it training-free, model-agnostic, and query-agnostic.

Result: Extensive evaluations on Video-MME, MLVU, and LongVideoBench benchmarks show FLoC consistently outperforms recent compression techniques while achieving significant efficiency gains in processing speed.

Conclusion: FLoC provides an effective, robust, and versatile solution for visual token compression in long video understanding, addressing scalability challenges while maintaining near-optimal performance across diverse video-LLMs.

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [21] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: The paper proposes BlurGuard, a method that enhances adversarial noise robustness for image protection against text-to-image model editing by applying adaptive Gaussian blur to make the noise irreversible while maintaining imperceptibility.


<details>
  <summary>Details</summary>
Motivation: Current adversarial noise methods for protecting images from malicious editing by text-to-image models are easily reversed by simple techniques like JPEG compression, limiting their practical effectiveness.

Method: Applies adaptive per-region Gaussian blur on adversarial noise to adjust the frequency spectrum, making the noise difficult to detect and reverse while maintaining imperceptibility.

Result: Consistently improves worst-case protection performance against various reversal techniques across diverse editing scenarios, while reducing quality degradation in perceptual metrics.

Conclusion: Adversarial noise for image protection should be both imperceptible and irreversible, and the proposed BlurGuard method effectively enhances robustness against noise reversal techniques.

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [22] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: CompAgent is an agentic framework that enhances multi-modal LLMs with visual tools for automated visual compliance verification, outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Visual compliance verification is critical but underexplored, with existing methods being costly and limited in generalizability. MLLMs have policy knowledge but struggle with fine-grained visual reasoning.

Method: Augments MLLMs with visual tools (object detectors, face analyzers, etc.) using a planning agent to dynamically select tools and a verification agent for multi-modal reasoning.

Result: Achieves 76% F1 score and 10% improvement over state-of-the-art on UnsafeBench dataset, outperforming specialized classifiers and MLLM prompting.

Conclusion: Agentic planning and tool-augmented reasoning enable scalable, accurate, and adaptable visual compliance verification.

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [23] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo is a training-free multi-agent framework that detects AI-generated images through collaborative forensic investigation using various tools and structured debate mechanisms, achieving 97.05% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing AI-generated image detection methods: traditional classifiers lack interpretability and generalization, while vision-language models are constrained to single-shot analysis and pixel-level reasoning.

Method: Multi-agent collaboration framework using forensic tools (reverse image search, metadata extraction, classifiers, VLM analysis) coordinated by LLM-based agents, with structured debate mechanism and memory-augmented reasoning for learning from historical cases.

Result: Achieved 97.05% accuracy on 6,000 images across controlled lab settings and real-world scenarios, substantially outperforming traditional classifiers and state-of-the-art VLMs.

Conclusion: Agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [24] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: This paper proposes Energy-based Multi-prompt Learning (EMPL), a parameter-efficient method that generates multiple prompt embeddings from an energy-based distribution to improve vision-language model generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on single-prompt learning for VLMs, but rarely explores the potential of multi-prompt learning. The paper aims to provide a principled framework for vision-language multi-prompt learning.

Method: EMPL generates multiple prompt embeddings by drawing instances from an energy-based distribution implicitly defined by VLMs. It extends the constant modality gap phenomenon to learnable prompts.

Result: Comprehensive experiments justify the superiority of vision-language transfer with multi-prompt augmentation and demonstrate EMPL's excellence in achieving balance between in-domain and out-of-domain generalization.

Conclusion: Multi-prompt learning with EMPL provides a parameter-efficient approach that rigorously achieves balanced open-vocabulary generalization across domains for vision-language models.

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [25] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: This paper proposes an efficient transfer learning method for detecting weather-related conditions on satellite ground terminal components, showing superior performance over typical deep learning methods and good generalization across scenarios.


<details>
  <summary>Details</summary>
Motivation: Weather events significantly impact satellite Internet reliability, particularly affecting ground terminal components. There's a need for fine-grained weather detection capability to assist in fault diagnostics and mitigation for reliable satellite Internet operations.

Method: The paper presents an efficient transfer learning (TL) method that enables ground components to locally detect representative weather-related conditions including snow, wet, and other adverse weather conditions.

Result: The proposed TL method demonstrates superior performance compared to typical deep learning methods (YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO) and shows good generalization capability across various scenarios.

Conclusion: The transfer learning approach provides an effective solution for fine-grained weather condition detection on satellite ground terminal components, addressing the need for reliable fault diagnostics in satellite Internet systems affected by adverse weather.

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [26] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: DM-QPMNet is a dual-encoder network for cell segmentation in single-shot quantitative phase microscopy that treats polarized intensity images and phase maps as distinct modalities, using multi-head attention for selective feature fusion and achieving superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional thresholding methods for cell segmentation in ssQPM are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to properly exploit the complementary nature of polarized intensity images and phase maps.

Method: Dual-encoder network with separate encoding streams for polarized intensity images and phase maps, fusing modality-specific features at intermediate depth via multi-head attention, with dual-source skip connections and per-modality normalization.

Result: Substantial improvements over monolithic concatenation and single-modality baselines, demonstrating robust cell segmentation by effectively exploiting ssQPM's simultaneous capture of complementary illumination and phase cues.

Conclusion: Modality-specific encoding with learnable fusion effectively exploits the complementary nature of polarized intensity and phase information in ssQPM for robust cell segmentation, outperforming traditional approaches.

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [27] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: A VQ-VAE compression framework for EM data with 16x-1024x compression, featuring pay-as-you-decode capability and ROI-driven selective reconstruction.


<details>
  <summary>Details</summary>
Motivation: Petascale EM datasets strain storage, transfer, and analysis capabilities, requiring efficient compression solutions.

Method: Vector-quantized variational autoencoder (VQ-VAE) with optional Transformer prior for texture restoration via FiLM and concatenation; ROI-driven workflow for selective high-resolution reconstruction.

Result: Enables extreme compression (1024x) with flexible decoding options and targeted reconstruction only where needed.

Conclusion: The framework provides scalable compression for large EM datasets while maintaining reconstruction quality through adaptive decoding strategies.

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [28] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: The paper proposes a novel algorithm for computing optimal transport maps in hyperbolic space using geometric variational techniques, extending existing methods from Euclidean and spherical geometry.


<details>
  <summary>Details</summary>
Motivation: Optimal transport has diverse applications but existing methods are primarily developed for Euclidean spaces and spheres. Hyperbolic space naturally arises in contexts involving hierarchical data, networks, and multi-genus Riemann surfaces.

Method: A geometric variational technique that extends methods for Euclidean and spherical geometry to the hyperbolic setting, providing an efficient algorithm for computing optimal transport maps in hyperbolic space.

Result: Experiments on synthetic data and multi-genus surface models validate the efficacy of the proposed method.

Conclusion: The proposed algorithm successfully computes optimal transport maps in hyperbolic space, addressing an important gap in optimal transport theory with applications to hierarchical data and geometric modeling.

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [29] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: Proposes MSDI, a zero-shot framework for generating physically plausible 4D human motions using 3D Gaussian representations and motion diffusion priors, avoiding unrealistic deformations in video generation.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current video diffusion models that suffer from unrealistic deformations, semantic violations, and physical inconsistencies due to lack of 3D physical priors.

Method: Uses Motion Score Distilled Interaction (MSDI) with 3D humans/objects, LLMs for spatial/semantic info, and Motion Diffusion Score Distillation Sampling (MSDS) to refine human motion while respecting object constraints.

Result: Produces natural and physically plausible human motions that respect 3D spatial context without requiring joint training on limited interaction datasets.

Conclusion: Offers a scalable zero-shot solution for realistic 4D generation that generalizes to out-of-distribution object-aware human motions.

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [30] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: The paper introduces L48, a real-world fine-grained multi-label dataset for single-positive multi-label learning, showing that existing methods perform significantly worse on this realistic benchmark compared to synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Existing SPML methods are benchmarked on synthetic datasets created by randomly sampling single positive labels from fully-annotated datasets, which doesn't reflect real-world scenarios and fails to capture fine-grained complexities that lead to difficult misclassifications.

Method: The authors introduce L48 dataset - a fine-grained, real-world multi-label dataset derived from bird sound recordings, providing natural SPML setting with single-positive annotations and two extended settings with domain priors for additional negative labels.

Result: Benchmarking existing SPML methods on L48 reveals significant performance differences compared to synthetic datasets, highlighting method weaknesses and the need for more realistic benchmarks.

Conclusion: The L48 dataset provides a challenging, realistic benchmark for SPML learning that better captures real-world complexities, revealing significant gaps in current methods that were not apparent on synthetic datasets.

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [31] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: A 3-stage automated pipeline for processing large-scale beetle image data, including detection, cropping, and morphological segmentation using transformer-based deep learning models.


<details>
  <summary>Details</summary>
Motivation: Biologists need to process thousands of beetle tray images efficiently for entomology research, requiring automation to handle large-scale data.

Method: 3-stage pipeline: 1) Detection using transformer-based open-vocabulary object detector and vision-language model, 2) Sorting and cropping individual beetles, 3) Morphological segmentation using fine-tuned transformer-based models on 670 manually labeled images.

Result: Developed a specialized pipeline that achieves fine-grained segmentation of beetles with relatively high accuracy, integrating multiple deep learning methods.

Conclusion: The pipeline greatly improves efficiency in processing large-scale beetle data and accelerates biological research by automating the analysis workflow.

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [32] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: MambaNetLK is a novel correspondence-free 3D registration framework that integrates Mamba State Space Model with PointNetLK, achieving superior performance on clinical endoscopic data with 56.04% reduction in rotation error and 26.19% reduction in translation error compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D point cloud registration is critical for reliable image-guided colonoscopy, but biological tissues exhibit repetitive textures and locally homogeneous geometry that cause feature degeneracy, while domain shifts between pre-operative and intra-operative data degrade alignment stability.

Method: Proposed MambaNetLK framework enhances PointNetLK by integrating a Mamba State Space Model as cross-modal feature extractor to capture long-range dependencies with linear-time complexity, using Lucas-Kanade algorithm for iterative alignment. Also introduced C3VD-Raycasting-10k clinical dataset with 10,014 geometrically aligned point cloud pairs.

Result: On C3VD-Raycasting-10k dataset, MambaNetLK achieves best performance with 56.04% reduction in median rotation error and 26.19% reduction in RMSE translation error over second-best method. Also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations.

Conclusion: MambaNetLK provides robust foundation for 3D registration in surgical navigation, enabling more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy through globally expressive SSM-based feature extraction and large-scale clinical dataset.

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [33] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: The paper introduces Spot The Ball, a benchmark for evaluating visual social inference in VLMs using sports images where the ball is removed, showing humans significantly outperform models by leveraging social cues like gaze and body pose.


<details>
  <summary>Details</summary>
Motivation: To address the gap in visual social reasoning between humans and AI, as humans excel at inferring hidden scene elements from behavioral cues, which is critical for developing more human-like AI agents.

Method: Created a benchmark with soccer, basketball, and volleyball images where the ball is removed, evaluated four state-of-the-art VLMs using three prompting strategies, and compared against human baselines.

Result: Humans consistently outperformed models (20-34% vs ≤17% accuracy), with models relying on superficial spatial heuristics while humans leveraged social cues like gaze direction and body pose.

Conclusion: There is a persistent human-model gap in visual social reasoning, highlighting the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [34] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: Proposes a federated learning framework using frozen CLIP ViT with lightweight transformer classifier for agricultural classification, addressing privacy concerns and non-IID data issues while reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Conventional centralized training raises privacy concerns due to large-scale data collection, while standard federated learning struggles with non-IID data and high communication costs in agricultural applications.

Method: Integrates frozen CLIP vision transformer with lightweight transformer classifier, shares 1% of CLIP-extracted feature representations across clients (non-reversible to raw images), and restricts federated updates to compact classifier only.

Result: Achieves 86.6% accuracy on agricultural classification tasks, which is more than 4 times higher compared to baseline federated learning approaches.

Conclusion: The framework effectively combines vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence, demonstrating significant improvements in accuracy and efficiency.

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [35] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView enables multi-view generation in personalized generative models with only 100 training samples, using diffusion transformer conditioning and semantic alignment loss.


<details>
  <summary>Details</summary>
Motivation: Current personalized generative models lack viewpoint control and cannot generate consistent multiple views of the same person.

Method: Uses lightweight adaptation with diffusion transformer conditioning and Semantic Correspondence Alignment Loss to preserve original capabilities.

Result: Significantly outperforms baselines trained on large multi-view datasets despite using only 100 training samples.

Conclusion: PersonalView provides efficient multi-view generation capability for personalized models with minimal training data.

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [36] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: LITHOS is the largest public dataset for automated petrography, containing 211,604 high-resolution RGB patches with expert annotations for 25 mineral categories, enabling deep learning approaches for mineral classification.


<details>
  <summary>Details</summary>
Motivation: Petrography is labor-intensive and requires expert visual examination, limiting scalability. There's a need for automated techniques to analyze mineral composition from thin section samples.

Method: Created LITHOS dataset with polarized light images and expert annotations. Proposed dual-encoder transformer architecture that integrates both polarization modalities for mineral classification.

Result: The dual-encoder transformer consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification.

Conclusion: LITHOS benchmark (dataset, code, and pretrained models) is made publicly available to foster reproducibility and further research in automated petrographic analysis.

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [37] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: Systematic evaluation of 11 lightweight vision models across 7 diverse datasets reveals that ImageNet accuracy doesn't reliably predict performance on other domains, introduces Cross-Dataset Score (xScore) as a unified robustness metric, and identifies key architectural components that drive generalization.


<details>
  <summary>Details</summary>
Motivation: Lightweight vision models are increasingly deployed in mobile systems but are predominantly benchmarked on ImageNet, raising questions about their cross-dataset generalization and how to systematically quantify robustness across diverse domains.

Method: Evaluated 11 lightweight vision models (2.5M parameters) trained under fixed 100-epoch schedule across 7 diverse datasets, introduced Cross-Dataset Score (xScore) as unified metric for consistency and robustness, and analyzed architectural components.

Result: ImageNet accuracy doesn't reliably predict performance on fine-grained or medical datasets; xScore provides scalable predictor of mobile model performance from just four datasets; isotropic convolutions with higher spatial resolution and channel-wise attention promote generalization, while Transformer blocks yield little benefit despite higher parameter overhead.

Conclusion: Provides reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides development of models that generalize robustly across diverse application domains.

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [38] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: A hybrid DeepONet-NTK framework for solving complex inverse problems with physics-informed constraints, validated on Navier-Stokes source localization and image reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving complex inverse problems involving nonlinearity, sparsity, and noisy data in computational physics and imaging sciences.

Method: Integrates Deep Operator Networks (DeepONet) with Neural Tangent Kernel (NTK), incorporating physics-informed constraints and task-specific regularization into the loss function.

Result: Demonstrates robustness, scalability, and precision on diverse synthetic and real datasets for source localization and image reconstruction tasks.

Conclusion: The framework shows broad potential applications in computational physics and imaging sciences by ensuring physically consistent and accurate solutions.

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [39] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC is a federated learning framework that addresses unpredictable modality absence in multimodal emotion recognition by using federated aggregation of modality-specific diffusion models and ensuring semantic consistency through dialogue-guided recovery.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal emotion recognition suffers from unpredictable modality absence, which degrades performance. Existing recovery methods depend on complete multimodal data and suffer from semantic distortion under extreme data distributions like fixed-modality absence.

Method: Proposes FedDISC framework integrating federated learning with diffusion models. Uses modality-specific diffusion models trained on clients and broadcast to clients missing modalities. Includes DISC-Diffusion module with Dialogue Graph Network for conversational dependencies and Semantic Conditioning Network for semantic alignment. Employs Alternating Frozen Aggregation strategy that cyclically freezes recovery and classifier modules.

Result: Extensive experiments on IEMOCAP, CMUMOSI, and CMUMOSEI datasets show superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.

Conclusion: FedDISC effectively addresses modality absence in multimodal emotion recognition through federated learning and semantic-consistent diffusion, achieving robust performance without relying on complete multimodal data from single clients.

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [40] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen is a generative framework that creates realistic satellite imagery directly from OpenStreetMap (OSM) data, enabling generation of before-after image pairs for urban monitoring and planning applications.


<details>
  <summary>Details</summary>
Motivation: Automating urban monitoring is challenging due to scarce curated datasets of specific urban features and their changes. There's a need for accurate geospatial data for urban planning, infrastructure monitoring, and environmental management.

Method: OSMGen uses the full richness of OSM JSON data including vector geometries, semantic tags, location, and time, rather than relying on raster tiles. It generates consistent before-after image pairs where user edits to OSM inputs translate into targeted visual changes while preserving the rest of the scene.

Result: The framework can generate training data to address scarcity and class imbalance, and allows planners to preview proposed interventions by editing map data. It produces paired (JSON, image) data for both static and changed states.

Conclusion: OSMGen paves the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates, creating a powerful tool for urban monitoring and planning applications.

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [41] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: A diffusion-based forensic framework using multi-strength image reconstruction dynamics (diffusion snap-back) to detect AI-generated images by analyzing reconstruction metrics across varying noise strengths.


<details>
  <summary>Details</summary>
Motivation: Traditional deepfake detection methods fail against modern text-to-image systems like Stable Diffusion and DALL-E that produce photorealistic, artifact-free results, making it challenging to distinguish authentic from synthetic visual content.

Method: Leverages diffusion snap-back - analyzing how reconstruction metrics (LPIPS, SSIM, PSNR) evolve across varying noise strengths to extract interpretable manifold-based features that differentiate real and synthetic images.

Result: Achieves 0.993 AUROC under cross-validation on a balanced dataset of 4,000 images, remains robust to common distortions like compression and noise, and demonstrates strong generalization despite limited data and single diffusion backbone (Stable Diffusion v1.5).

Conclusion: The method provides a foundation for scalable, model-agnostic synthetic media forensics with strong generalization and interpretability.

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [42] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: Transfer learning with UNet-MobileNet enables efficient thermal cloud segmentation for CubeSats, achieving 0.877 F1 score and under 5-second inference on Jetson Nano.


<details>
  <summary>Details</summary>
Motivation: CubeSat missions face challenges in cloud segmentation due to limited hardware, single thermal band data, and insufficient labeled data, making conventional methods infeasible.

Method: Used transfer learning with UNet and lightweight MobileNet encoder, pretrained on Landsat-7 Cloud Cover Assessment Dataset and fine-tuned with mission-specific samples in joint-training setup.

Result: Improved macro F1 from 0.850 to 0.877 over FOREST-2-only baselines, with full-image inference under 5 seconds on NVIDIA Jetson Nano after TensorRT conversion.

Conclusion: Public datasets and lightweight architectures enable accurate, efficient thermal-only cloud masking for real-time decision-making in data-limited Earth observation missions.

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [43] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D is a free generative AI framework that uses Google Street View imagery and AI models to create 3D reconstructions of cultural heritage sites in Bangladesh, overcoming resource limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage restoration in Bangladesh faces challenges due to limited resources, expensive hardware requirements, and scarce technical expertise for traditional 3D digitization methods like photogrammetry and LiDAR scanning.

Method: Two-stage pipeline using publicly available Google Street View imagery: multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery.

Result: Produces photorealistic, metrically coherent 3D reconstructions in seconds with significant speedups compared to conventional methods, preserving both visual and structural fidelity for landmarks like Ahsan Manzil, Choto Sona Mosque, and Paharpur.

Conclusion: The framework democratizes cultural preservation by drastically lowering economic and technical barriers, reframing preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [44] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: This paper introduces a reinforcement learning-based video moment retrieval model that uses multi-agent systems with evidential learning to resolve conflicts between agents' localization outputs, enabling better integration of different models and handling out-of-scope queries without additional training.


<details>
  <summary>Details</summary>
Motivation: Current video moment retrieval solutions don't handle conflicts between different models' location results, preventing proper integration of multiple models to produce better outcomes. The authors aim to address this limitation while also enabling detection of queries with no corresponding moments.

Method: Proposed a reinforcement learning-based model that scans entire videos to find moment boundaries while producing locational evidence. Introduced a multi-agent system framework using evidential learning to resolve conflicts between agents' localization outputs.

Result: Extensive experiments on benchmark datasets showed the proposed methods are more effective than state-of-the-art approaches. The framework successfully handles out-of-scope queries without additional training and improves RL performance in moment retrieval.

Conclusion: Modeling competition and conflict in multi-agent systems is an effective way to improve RL performance in moment retrieval. Evidential learning plays a new role in multi-agent frameworks by resolving conflicts between agents' outputs.

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [45] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD is a vision-based radiological assistance framework that captures medical images from displays using cameras, enabling AI-assisted diagnosis without modifying existing hospital IT infrastructure.


<details>
  <summary>Details</summary>
Motivation: To overcome the barrier of integrating computer-aided diagnosis systems with existing hospital IT infrastructure by providing a camera-based solution that doesn't require system modifications.

Method: Uses an automated pipeline that detects, restores, and analyzes on-screen medical images captured by cameras, transforming camera-captured data into diagnostic-quality images for automated analysis and report generation.

Result: Achieves diagnostic performance comparable to conventional CAD systems, with F1-score degradation typically less than 2% across classification tasks, and natural language generation metrics for automated reports within 1% of those from original images.

Conclusion: VisionCAD offers an accessible approach for AI-assisted diagnosis that can be deployed in diverse clinical settings using only a camera device and standard computing resources, without requiring modifications to existing infrastructure.

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [46] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench benchmark reveals MLLMs' limitations in facial expression reasoning despite good classification. UniFER-7B model is developed using curated datasets and RLVR training to outperform state-of-the-art MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding MLLMs' performance on facial expression recognition tasks and improve their reasoning capabilities, as current approaches lack systematic evaluation and interpretability.

Method: Created FERBench benchmark with 20 MLLMs across 4 FER datasets. Developed post-training strategies using two curated datasets: UniFER-CoT-230K for initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards.

Result: MLLMs show good classification performance but significant limitations in reasoning and interpretability. UniFER-7B model outperforms many open-source and closed-source generalist MLLMs including Gemini-2.5-Pro and Qwen2.5-VL-72B.

Conclusion: The proposed UniFER-7B foundation model successfully addresses MLLMs' limitations in facial expression reasoning through systematic benchmarking and specialized training strategies, achieving superior performance compared to existing generalist models.

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [47] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder is a unified multimodal code generation model that uses a two-stage training framework with supervised finetuning and visual reinforcement learning to achieve state-of-the-art performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely on single-task training, which limits their generalization capabilities for visual code intelligence tasks. The authors aim to develop a more generalized multimodal code generation model.

Method: Two-stage training framework: 1) Construct large-scale SFT corpus (1.6M image-code pairs) for direct code generation and visual-based code refinement; 2) Visual Reinforcement Learning (ViRL) with coarse-to-fine reward mechanism that calculates visual similarity across local and global image patches.

Result: VinciCoder achieves state-of-the-art performance on various multimodal code generation benchmarks, demonstrating the effectiveness of the coarse-to-fine ViRL strategy.

Conclusion: The proposed VinciCoder model with its two-stage training framework and visual reinforcement learning strategy successfully addresses the limitations of single-task training and advances multimodal code generation capabilities.

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [48] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: A unified framework that handles three saliency tasks (SOD, CoSOD, SIS) using Chain-of-Thought reasoning in Vision-Language Models, with a novel Confidence-Guided Policy Optimization algorithm for efficient reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address operational heterogeneity across different saliency tasks by creating a unified framework that bridges task differences through CoT reasoning, eliminating the need for specialized models for each task.

Method: Two-stage training: Supervised Fine-Tuning with output-to-reasoning strategy for high-fidelity data, followed by Reinforcement Learning with Confidence-Guided Policy Optimization that uses reward-confidence discrepancy as advantage signal.

Result: Achieves state-of-the-art performance across all three tasks, with particularly impressive results on CoSOD (0.899 S-measure on CoCA, 8.0 percentage points improvement) while using significantly less training data.

Conclusion: The proposed unified framework successfully handles heterogeneous saliency tasks through CoT reasoning and efficient RL optimization, demonstrating superior performance over specialized methods and strong closed-source VLMs.

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [49] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: LGCA is a framework that improves zero-shot image classification by capturing local features, selecting salient regions for expansion, and combining local and global features to minimize misinformation from random crops.


<details>
  <summary>Details</summary>
Motivation: Random image crops in CLIP-based models can introduce misinformation and bias due to similar features at small scales, which degrades zero-shot classification performance.

Method: Proposes Localized-Globalized Cross-Alignment (LGCA) that captures local features, selects salient regions for repeated expansion, and uses similarity scores combining original and expanded images.

Result: Extensive experiments show LGCA substantially improves zero-shot performance across diverse datasets and outperforms state-of-the-art baselines.

Conclusion: LGCA effectively captures both local and global features while minimizing misinformation, with theoretical analysis confirming it maintains the same time complexity as the original model.

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [50] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: ITEM is a fake image detector that uses image-text misalignment in CLIP space as discriminative clues, outperforming existing methods with better generalization across generative models.


<details>
  <summary>Details</summary>
Motivation: Existing fake image detection methods focus only on visual clues and overfit to specific image patterns, lacking generalization to unseen generative models. The paper addresses this by leveraging multi-modal alignment issues between images and captions.

Method: Proposes ITEM detector that measures image-text misalignment in CLIP's joint visual-language space, using a hierarchical scheme that examines both global image alignment and fine-grained object-level alignment with captions, then tunes an MLP head for detection.

Result: Extensive experiments show superior performance against state-of-the-art methods with impressive generalization and robustness across various recent generative models.

Conclusion: Leveraging image-text misalignment in multi-modal space provides an effective and generalizable approach for fake image detection that overcomes limitations of visual-only methods.

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [51] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: A frequency-based detection method for identifying diffusion-generated images by analyzing frequency band differences between real and synthetic images, achieving superior generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Address concerns about malicious use of high-quality diffusion-generated images by developing a detector that can generalize to unseen diffusion models and remain robust against various perturbations.

Method: Propose Frequency Forgery Clue (F²C) representation that enhances discriminative frequency bands using a frequency-selective function as a weighted filter to the Fourier spectrum, suppressing less informative bands while highlighting more discriminative ones.

Result: Extensive experiments show the method outperforms state-of-the-art detectors with superior generalization to unseen diffusion models and robust resilience to various perturbations.

Conclusion: The frequency-based approach provides an effective solution for detecting diffusion-generated images by leveraging progressive frequency differences across bands, enabling reliable detection across different models and settings.

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [52] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP is a framework that generates adversarial text attacks on CLIP during pre-training, achieving high poisoning success rates while bypassing existing defenses.


<details>
  <summary>Details</summary>
Motivation: CLIP's reliance on uncurated web data makes it vulnerable to data poisoning and backdoor attacks, with text modality attacks being underexplored compared to image-based attacks.

Method: ToxicTextCLIP iteratively applies a background-aware selector to choose texts aligned with target class backgrounds, and a background-driven augmenter to generate diverse poisoned samples while maintaining semantic coherence.

Result: Achieves up to 95.83% poisoning success rate and 98.68% backdoor Hit@1 on classification and retrieval tasks, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses.

Conclusion: The framework demonstrates significant vulnerabilities in CLIP's text modality and highlights the need for more robust defenses against text-based poisoning attacks.

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [53] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: Weakly supervised deep learning framework using Grad-CAM for pneumonia classification and localization from chest X-rays, achieving 98% accuracy with ResNet-18 and EfficientNet-B0.


<details>
  <summary>Details</summary>
Motivation: To develop a pneumonia detection system that doesn't require costly pixel-level annotations but instead uses image-level labels to generate clinically meaningful heatmaps, enhancing transparency and clinical trust in AI-assisted medical imaging.

Method: Evaluated seven ImageNet-pretrained architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, ViT-B16) with identical training conditions using focal loss and patient-wise splits to prevent data leakage. Utilized Grad-CAM for generating heatmaps from image-level labels.

Result: ResNet-18 and EfficientNet-B0 achieved best overall performance: 98% test accuracy, ROC-AUC = 0.997, and F1 = 0.987. MobileNet-V2 provided optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirmed models focused on clinically relevant lung regions.

Conclusion: The work demonstrates the potential of weakly supervised explainable models for enhancing pneumonia screening transparency and clinical trust in AI-assisted medical imaging, with models successfully localizing pneumonia regions without pixel-level annotations.

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [54] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter is a unified framework for joint 3D human reconstruction and human-part segmentation from single images, using geometric and semantic priors with cross-task synergy.


<details>
  <summary>Details</summary>
Motivation: Current generative models achieve high-fidelity 3D human reconstruction but lack utility for specific tasks like human 3D segmentation, and face challenges from scarce labeled 3D human datasets.

Method: Integrates human geometric priors in reconstruction and self-supervised semantic priors in segmentation, with pixel-aligned aggregation for cross-task synergy and multi-task objective for simultaneous texture fidelity and semantic consistency optimization.

Result: Extensive experiments show HumanCrafter surpasses state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from single images.

Conclusion: HumanCrafter provides an effective unified framework that addresses dataset scarcity and enables superior performance in joint 3D human reconstruction and segmentation tasks.

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [55] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: A human-in-the-loop deep learning framework for automated vestibular schwannoma segmentation in MRI that combines multi-center data with expert consensus, achieving significant accuracy improvements (DSC from 0.9125 to 0.9670) and 37.4% efficiency gain over manual annotation.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of vestibular schwannoma on MRI is time-intensive and requires expert annotations. Current deep learning approaches lack robustness across diverse datasets and complex clinical cases.

Method: Bootstrapped DL-based framework for iterative segmentation and quality refinement using multi-center data with expert consensus. Human-in-the-loop model training approach combining automated segmentation with expert intervention for nuanced cases.

Result: Achieved DSC improvement from 0.9125 to 0.9670 on internal validation, maintained stable performance on external datasets. Expert evaluation on 143 scans identified areas needing refinement. 37.4% efficiency gain over manual annotation. Public dataset of 190 patients with 534 T1CE scans available.

Conclusion: The human-in-the-loop approach provides high segmentation accuracy and clinical adaptability, offering a generalizable strategy for automated VS segmentation across diverse clinical settings.

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [56] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP is a personalized federated prompt learning method for vision-language models that uses multiple prompt groups with diversity loss and dynamic similarity-based aggregation to achieve state-of-the-art performance with minimal communication overhead.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing diverse local characteristics in federated learning while maintaining parameter efficiency and enabling effective knowledge sharing across clients.

Method: Uses multiple groups of paired textual and visual prompts with diversity loss, and employs dynamic prompt aggregation via similarity-guided probabilistic sampling based on cosine similarity between client and global prompts.

Result: Achieves state-of-the-art performance with the lowest communication parameters among federated prompt learning methods, consistently outperforming prior approaches in both personalization and domain generalization across diverse benchmarks.

Conclusion: FedMGP effectively balances preservation of common knowledge with client-specific features through its multi-group prompt design and dynamic aggregation strategy, providing a robust and parameter-efficient solution for federated vision-language learning.

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [57] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat is a feed-forward method that synthesizes controllable 4D scenes from a single image using video diffusion models and 3D Gaussian primitives, achieving high-quality results in 30 seconds without test-time optimization.


<details>
  <summary>Details</summary>
Motivation: To enable efficient synthesis of controllable 4D scenes from single images, overcoming the limitations of optimization-based methods that require extensive computation time.

Method: Unifies video diffusion priors with geometry/motion constraints from 4D datasets, using a video latent transformer to predict deformable 3D Gaussian fields in a single forward pass.

Result: Synthesizes high-quality 4D scenes in 30 seconds, matching or surpassing optimization-based methods in video generation, novel view synthesis, and geometry extraction while being significantly more efficient.

Conclusion: Diff4Splat provides an efficient feed-forward alternative to optimization-based dynamic scene synthesis, enabling rapid 4D scene generation from single images with competitive quality.

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [58] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA is a large-scale chest X-ray dataset for medical visual question answering with spatial grounding, containing 17,597 QA pairs across 4,394 images with radiologist-verified bounding boxes and clinical reasoning.


<details>
  <summary>Details</summary>
Motivation: To advance explainable Medical Visual Question Answering (Med-VQA) with clinical grounding and mitigate hallucinations in normal cases by providing a balanced dataset with spatial annotations.

Method: Created a dataset with 17,597 question-answer pairs across 4,394 chest X-ray images, annotated with radiologist-verified bounding boxes and clinical reasoning. Used a six-type question taxonomy covering Where, What, Is there, How many, Which, and Yes/No questions.

Result: Benchmarking with MedGemma-4B-it showed improved performance (F1 = 0.624, +11.8% over baseline) while enabling lesion localization. The dataset has balanced distribution with 41.7% positive and 58.3% negative samples.

Conclusion: VinDr-CXR-VQA advances reproducible and clinically grounded Med-VQA research by providing a comprehensive dataset with spatial grounding that improves model reliability and enables lesion localization.

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [59] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++ is a feedback-driven framework for panoramic Multi-Object Tracking that addresses challenges like 360° FoV, resolution dilution, and view-dependent distortions through trajectory-informed perception refinement, achieving state-of-the-art performance on JRDB and EmboTrack benchmarks.


<details>
  <summary>Details</summary>
Motivation: Conventional MOT methods designed for narrow-FoV pinhole cameras perform poorly in panoramic imagery due to unique challenges including 360° Field of View, resolution dilution, and severe view-dependent distortions.

Method: OmniTrack++ uses a feedback-driven framework with: DynamicSSM block for stabilizing panoramic features, FlexiTrack Instances for flexible localization using trajectory-informed feedback, ExpertTrack Memory with Mixture-of-Experts design for appearance consolidation, and adaptive Tracklet Management that switches between end-to-end and tracking-by-detection modes.

Result: Extensive experiments show OmniTrack++ achieves state-of-the-art performance with substantial HOTA improvements: +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. The paper also introduces EmboTrack benchmark with QuadTrack (quadruped robot) and BipTrack (bipedal wheel-legged robot) datasets.

Conclusion: OmniTrack++ provides an effective solution for panoramic MOT by progressively refining perception with trajectory cues, offering balanced and scalable tracking performance across diverse panoramic environments and motion patterns.

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [60] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer is a novel framework for multi-subject video generation from text prompts and reference images, using hierarchical identity-preserving attention, VLM semantic understanding, and reinforcement learning to improve subject consistency and video quality.


<details>
  <summary>Details</summary>
Motivation: Existing video generative models are limited to text or single image conditioning, lacking controllability for multi-subject scenarios and failing to preserve subject identities across frames.

Method: Uses hierarchical identity-preserving attention mechanism, semantic understanding via pretrained VLM, and online reinforcement learning phase (RLVR) to align critical concepts like subject ID.

Result: Extensive experiments show ID-Composer surpasses existing methods in identity preservation, temporal consistency, and video quality.

Conclusion: ID-Composer effectively addresses multi-subject video generation challenges by preserving subject identities, integrating cross-modal semantics, and maintaining temporal consistency through its novel architecture and training approach.

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [61] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: A test-time debiasing method for CLIP models that uses segmentation to isolate target attributes and adjusts non-target regions to remove spurious correlations, requiring no training or bias annotations.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods require training data and explicit group labels, limiting practicality. Test-time methods often need prior knowledge of dataset biases, reducing generalizability in open settings.

Method: Uses pretrained segmentation model to isolate target visual attribute, then adjusts non-target regions so their embeddings are uniformly similar to all class-specific text prompts, removing unintended bias signals while preserving target attributes.

Result: Outperforms existing test-time debiasing approaches on Waterbirds and CelebA datasets in both group robustness metrics and Attention IoU.

Conclusion: Segmentation-guided interventions provide effective, scalable, and annotation-free bias mitigation for vision language models.

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [62] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: T-VAD is a text-guided fine-grained video anomaly detection framework using Large Vision-Language Models that generates pixel-level anomaly heatmaps and provides detailed textual descriptions of anomalous events.


<details>
  <summary>Details</summary>
Motivation: Traditional video anomaly detection methods are semi-automated with limited binary outputs (normal/anomalous), lacking fine-grained localization and detailed descriptions of anomalies.

Method: Proposes T-VAD framework with Anomaly Heatmap Decoder for pixel-wise visual-textual feature alignment and Region-aware Anomaly Encoder to transform heatmaps into learnable textual embeddings for LVLM guidance.

Result: Achieves SOTA performance with 94.8% AUC on UBnormal dataset, 67.8%/76.7% heatmap accuracy, and high BLEU-4 scores (62.67-88.84) and Yes/No accuracy (89.73%-97.67%) on multiple datasets.

Conclusion: T-VAD significantly enhances anomaly detection granularity and interactivity by providing fine-grained localization and detailed textual descriptions through vision-language model integration.

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [63] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: Real-IAD Variety is introduced as the largest and most diverse industrial anomaly detection benchmark with 198,960 high-resolution images across 160 categories, 28 industries, 24 materials, and 22 colors, showing that vision-language models maintain robustness while traditional methods degrade with scale.


<details>
  <summary>Details</summary>
Motivation: Current IAD benchmarks have limited category diversity and scale, causing metric saturation and poor real-world transferability, necessitating a more comprehensive benchmark.

Method: Created Real-IAD Variety benchmark with extensive coverage across industries, materials, and colors, and evaluated using multi-class unsupervised, multi-view, and zero-/few-shot settings.

Result: State-of-the-art multi-class unsupervised methods degrade significantly when scaled from 30 to 160 categories, while vision-language models show remarkable robustness with minimal performance variation across category counts.

Conclusion: Real-IAD Variety enables development of scalable, general-purpose anomaly detection systems and will accelerate research beyond domain-specific constraints.

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [64] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: A method for learning and synthesizing multi-instance semantics from single images using penalty-based attention optimization and box control to handle similar semantics and ensure precise layout control.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning multi-instance semantics from limited training data, particularly when instances have similar semantics or appearance, which can lead to semantic leakage and poor disentanglement.

Method: Proposes penalty-based attention optimization during learning to disentangle similar semantics, and introduces box control optimization in attention layers during synthesis to mitigate semantic leakage while precisely controlling output layout.

Result: Achieves disentangled and high-quality semantic learning and synthesis, balancing editability and instance consistency. Method remains robust with semantically/visually similar instances or rare objects.

Conclusion: The proposed approach effectively handles multi-instance semantic learning from single images, providing precise control over output layout while maintaining semantic integrity even with challenging instances.

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [65] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D Neural Voxel Splatting (4D-NVS) combines voxel-based representations with neural Gaussian splatting to efficiently model dynamic scenes, reducing memory usage and accelerating training while maintaining high image quality.


<details>
  <summary>Details</summary>
Motivation: To address the substantial memory overhead in extending 3D Gaussian Splatting to dynamic scenes, which requires replicating Gaussians across frames.

Method: Uses a compact set of neural voxels with learned deformation fields instead of separate Gaussian sets per timestamp, plus a novel view refinement stage for selective optimization of challenging viewpoints.

Result: Outperforms state-of-the-art approaches with significant memory reduction, faster training, and real-time rendering with superior visual fidelity.

Conclusion: 4D-NVS enables efficient dynamic scene modeling with reduced memory consumption while preserving high image quality through neural voxel representations and targeted optimization.

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [66] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: FREE is a frequency-guided framework for Domain-Shifted Generalized Category Discovery that uses frequency-domain analysis to handle distribution shifts between known and unknown domains, improving category discovery performance.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods perform poorly under distribution shifts, where unlabeled data comes from unknown domains with different distributions than the labeled data. This creates a more realistic but challenging scenario.

Method: Uses frequency-based domain separation to identify known/unknown domains via amplitude differences. Implements cross-domain and intra-domain frequency perturbation strategies. Extends self-supervised contrastive learning and semantic clustering loss, plus clustering-difficulty-aware resampling.

Result: Extensive experiments show FREE effectively mitigates distribution shift impacts across multiple benchmark datasets and achieves superior performance in discovering both known and unknown categories.

Conclusion: The frequency-guided approach successfully addresses the DS_GCD challenge, demonstrating that leveraging frequency-domain information significantly improves model robustness to distribution shifts in category discovery tasks.

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [67] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: A memory-augmented pipeline for context-aware zero-shot video anomaly detection that achieves state-of-the-art performance by correlating temporal and appearance features with textual memory traces.


<details>
  <summary>Details</summary>
Motivation: Video anomalies depend on contextual information and temporal evolution, but most detectors ignore context, limiting generalization to real-life situations. The work addresses context-aware zero-shot anomaly detection to learn adaptively and detect new events.

Method: Memory-augmented pipeline correlating temporal signals with visual embeddings using cross-attention, with real-time zero-shot anomaly classification through contextual similarity scoring.

Result: Achieves 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, setting new state-of-the-art among zero-shot models. Enables real-time inference with high precision and explainability.

Conclusion: By fusing cross-attention temporal fusion and contextual memory, the approach achieves high fidelity anomaly detection, advancing zero-shot models' applicability in real-world surveillance and infrastructure monitoring.

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [68] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench is a new benchmark for context-aware video anomaly understanding that introduces a hierarchical taxonomy of 32 anomaly event types across 174 scenes, with unified evaluation across recognition, detection, grounding, and anticipation tasks.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly detection methods have superficial understanding of real-world anomalies, lacking the ability to comprehend complex principles and subtle contextual distinctions that differentiate anomalies from normal events.

Method: Proposed Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner to address the challenges in CueBench.

Result: Existing vision-language models perform poorly on CueBench, while the proposed Cue-R1 surpasses state-of-the-art approaches by over 24% on average across all tasks.

Conclusion: Current deep models are still far from satisfactory real-world anomaly understanding, and CueBench serves as a rigorous evaluation framework to advance context-aware video anomaly understanding.

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [69] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: The paper introduces triplet segmentation - a new task that spatially grounds surgical action triplets (<instrument, verb, target>) using instrument instance segmentation, and proposes TargetFusionNet architecture with target-aware fusion to address anatomical target prediction challenges.


<details>
  <summary>Details</summary>
Motivation: Existing surgical action recognition methods lack spatial grounding precision and fail to reliably link actions to specific instrument instances, while previous spatial grounding approaches using class activation maps lack the required precision for detailed instrument-tissue interaction analysis.

Method: Proposed TargetFusionNet architecture that extends Mask2Former with a target-aware fusion mechanism, fusing weak anatomy priors with instrument instance queries to improve anatomical target prediction. Introduced CholecTriplet-Seg dataset with 30,000+ annotated frames for strongly supervised instance-level triplet grounding.

Result: TargetFusionNet consistently improves performance over existing baselines across recognition, detection, and triplet segmentation metrics, demonstrating that strong instance supervision combined with weak target priors significantly enhances surgical action understanding accuracy and robustness.

Conclusion: Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets, and the proposed benchmark and architecture pave the way for more interpretable surgical scene understanding.

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [70] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: This paper introduces FGI-EMIT, the first large-scale multispectral LiDAR benchmark dataset for individual tree segmentation, and benchmarks both unsupervised and supervised deep learning methods, finding that DL approaches significantly outperform traditional methods, especially for understory trees.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale multispectral LiDAR benchmark datasets has hindered progress in individual tree segmentation, despite evidence that multispectral reflectance can improve accuracy. There's a need for comprehensive evaluation of both traditional and deep learning approaches.

Method: Created FGI-EMIT dataset with 1,561 manually annotated trees captured at 532, 905, and 1,550 nm wavelengths. Benchmarking included four unsupervised algorithms (with Bayesian hyperparameter optimization) and four supervised DL approaches (trained from scratch).

Result: ForestFormer3D achieved the highest F1-score of 73.3%, significantly outperforming the best unsupervised method (Treeiso at 52.7%). The biggest improvement was for understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. DL methods remained superior even at low point densities (10 points/m²).

Conclusion: Deep learning approaches significantly outperform traditional unsupervised methods for individual tree segmentation, particularly for challenging understory trees. However, current DL models fail to effectively leverage multispectral reflectance information, suggesting room for improvement in feature utilization.

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [71] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: This paper explores domain adaptation methods for autonomous driving models, specifically adapting PilotNet from right-hand to left-hand driving using Australian highway data. The study finds that pretraining on flipped data followed by fine-tuning significantly improves adaptation performance.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is required for automated driving models to generalize well across diverse road conditions, particularly when adapting from right-hand to left-hand driving scenarios.

Method: Four training methods were evaluated: baseline on U.S. data, training on flipped U.S. data, pretraining on U.S. data then fine-tuning on Australian highways, and pretraining on flipped U.S. data then fine-tuning on Australian highways. Saliency-based analysis was used to measure attention shifts.

Result: Pretraining on flipped data alone worsens prediction stability, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. Similar trends were confirmed with ResNet architecture.

Conclusion: The findings emphasize the importance of preprocessing techniques like flipped-data pretraining followed by fine-tuning to improve model adaptation with minimal retraining requirements.

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [72] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP is a framework that learns unified MRI contrast representations by aligning volumetric images with DICOM acquisition parameters, enabling label-efficient analysis across diverse clinical datasets.


<details>
  <summary>Details</summary>
Motivation: MRI suffers from substantial data heterogeneity and lack of standardized contrast labels across scanners, protocols, and institutions, limiting large-scale automated analysis.

Method: Metadata-guided framework that aligns volumetric MRI images with their DICOM acquisition parameters to learn unified contrast representations.

Result: The embeddings show distinct clusters of MRI sequences, outperform supervised 3D baselines in few-shot classification, and enable unsupervised data quality control by identifying corrupted metadata.

Conclusion: MR-CLIP provides a scalable foundation for label-efficient MRI analysis by transforming routinely available acquisition metadata into a supervisory signal.

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [73] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: A pipeline for generating realistic synthetic data of event-based cameras on AUVs in underwater environments to train vision models, demonstrated with rock detection in poor visibility conditions.


<details>
  <summary>Details</summary>
Motivation: Underwater environments pose challenges like poor lighting and high dynamic range where traditional vision techniques struggle. Event-based cameras offer a solution by tracking frame-by-frame changes.

Method: Developed a pipeline to generate synthetic data of event-based cameras mounted on Autonomous Underwater Vehicles (AUVs) in underwater settings.

Result: The pipeline effectively generates realistic synthetic data for training vision models, demonstrated through rock detection tasks in poor visibility with suspended particulate matter.

Conclusion: The approach provides a viable solution for underwater computer vision tasks and can be generalized to other underwater applications beyond rock detection.

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [74] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: A novel PTQ method for image super-resolution that addresses activation outliers through dual-region quantization and sensitivity-aware finetuning, achieving performance comparable to QAT with significant speedup.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for SR networks often fail due to overlooking activation outliers, which are correlated with image color information and cause performance degradation when removed.

Method: Proposes dual-region quantization that partitions activations into outlier and dense regions with independent uniform quantization, plus sensitivity-aware finetuning that focuses on highly sensitive layers.

Result: Outperforms existing PTQ approaches across various SR networks and datasets, achieving performance comparable to QAT methods with at least 75× speedup in most scenarios.

Conclusion: The proposed dual-region quantization with sensitivity-aware finetuning effectively addresses PTQ challenges in SR networks, providing a practical alternative to QAT with substantial inference acceleration.

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [75] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: A probabilistic framework using flow matching on SE(3) manifold for 6D object pose estimation that captures pose ambiguity and multi-modality, achieving SOTA results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in object pose estimation including partial observability, occlusions, and object symmetries that lead to pose ambiguity and multiple hypotheses. Deterministic methods are overconfident and fail to capture multi-modality.

Method: Leverages flow matching on the SE(3) manifold to model full pose distributions with sample-based estimates, enabling uncertainty reasoning in ambiguous cases.

Result: Achieves state-of-the-art results on Real275, YCB-V, and LM-O benchmarks. Demonstrates practical applications in robotic manipulation tasks.

Conclusion: The proposed probabilistic framework effectively handles pose ambiguity and enables uncertainty-aware reasoning for downstream robotic applications.

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [76] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER is a novelty search-based approach that uses LLMs for semantic evolution and CLIP embeddings to generate diverse image sets from a single prompt, outperforming existing methods in diversity.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often lack output diversity, limiting their usefulness for creative and exploratory tasks, while existing prompt optimization techniques focus on aesthetics rather than diversity.

Method: Uses LLM for semantic evolution of prompts, CLIP embeddings to quantify novelty, and emitters to guide search into distinct regions of prompt space. Evaluated with FLUX-DEV for generation and GPT-4o-mini for mutation.

Result: Significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics, with ablation studies confirming the efficacy of emitters.

Conclusion: WANDER effectively addresses the diversity limitation in text-to-image generation through novelty search and semantic evolution, enhancing creative applications.

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [77] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: This paper introduces DiMoDE, a framework that discriminatively treats different motion components in unsupervised depth and ego-motion learning, leveraging geometric regularities of rigid flows to improve both tasks through targeted constraints and complementary geometric relationships.


<details>
  <summary>Details</summary>
Motivation: Most existing methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent motions, which limits the incorporation of strong geometric constraints and reduces reliability under diverse conditions.

Method: The method first aligns optical axes and imaging planes between source and target cameras, transforms optical flows through these alignments, and quantifies deviations to impose geometric constraints individually on each ego-motion component. This enables targeted refinement and reformulates joint learning into coaxial and coplanar forms where depth and translation components can be mutually derived through closed-form geometric relationships.

Result: DiMoDE achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions.

Conclusion: The discriminative treatment of motion components and incorporation of complementary geometric constraints significantly improves depth and ego-motion estimation reliability and robustness.

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [78] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: This paper analyzes the relevance of different loss functions for enhancing low-dose CT images and their consistency with image quality metrics, revealing inconsistencies between loss functions and quality metrics.


<details>
  <summary>Details</summary>
Motivation: Low-dose CT imaging suffers from noise and artifacts that affect diagnostic accuracy. While deep learning models have been developed with various loss functions, current metrics like PSNR and SSIM are limited in reflecting perceptual quality for medical images.

Method: The authors conduct an objective analysis of different loss functions used in DL-based architectures for LDCT image quality enhancement, examining their consistency with image quality metrics.

Result: The findings reveal inconsistencies between loss functions and quality metrics, showing that commonly used metrics don't adequately reflect perceptual quality for medical images.

Conclusion: There is a need to consider image quality metrics when developing new loss functions for image quality enhancement in medical imaging applications.

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [79] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA is a new Vision-Language-Action model that addresses limitations of current VLAs by supporting pixel-level reasoning and multimodal prompting with both text and visual inputs, achieving significant performance improvements with much lower training costs.


<details>
  <summary>Details</summary>
Motivation: Current VLAs struggle with pixel-level scene understanding and rely heavily on textual prompts, reducing their flexibility in real-world settings. The authors aim to create a more versatile VLA that can handle both pixel-level reasoning and multimodal inputs.

Method: Built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. Uses a two-stage automated annotation pipeline to generate Pixel-160K dataset with pixel-level annotations from existing robot data.

Result: PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA on three standard VLA benchmarks, while requiring only 1.5% of OpenVLA's pretraining cost.

Conclusion: PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments, demonstrating the value of pixel-level reasoning and multimodal prompting capabilities.

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [80] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: Deep learning models trained on North American ADNI data show significant performance drops when tested on Latin American FLENI cohort, revealing domain shift issues despite high original performance.


<details>
  <summary>Details</summary>
Motivation: To assess generalization of AD diagnostic models to underrepresented populations beyond the dominant North American training datasets.

Method: Benchmarked convolutional and Transformer-based models on ADNI dataset and tested generalization on FLENI Latin American cohort, with ablation studies on normalization and sampling, plus occlusion sensitivity analysis.

Result: Models achieved high AUCs on ADNI (.96-.97) but dropped substantially on FLENI (.80-.82), showing similar performance across architectures and unclear attention patterns for non-AD classes and FLENI scans.

Conclusion: Population-aware validation is crucial for diagnostic AI models, motivating domain adaptation and cohort diversification to address generalization challenges.

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [81] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED is a large-scale multi-platform 3D visual grounding benchmark with 128K objects and 22K referring expressions across vehicle, drone, and quadruped platforms, featuring RGB and LiDAR data from diverse outdoor scenes.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing 3D grounding benchmarks that are confined to indoor settings, single platforms, and small scale, enabling embodied agents to localize language-referred objects in open-world outdoor environments.

Method: Developed a scalable annotation pipeline combining vision-language model prompting with human verification, and proposed platform-aware normalization and cross-modal alignment techniques for cross-platform learning.

Result: Created a dataset 10x larger than existing benchmarks, established benchmark protocols for in-domain and cross-platform evaluations, and revealed significant performance gaps highlighting challenges in generalizable 3D grounding.

Conclusion: 3EED advances research in language-driven 3D embodied perception by providing a comprehensive multi-platform benchmark and toolkit, opening opportunities for developing more robust and generalizable 3D grounding systems.

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [82] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: Frames place recognition as multi-class classification instead of contrastive learning, achieving competitive performance with better training efficiency.


<details>
  <summary>Details</summary>
Motivation: Most existing place recognition methods rely on contrastive learning, but this paper explores an alternative classification-based approach for autonomous driving applications.

Method: Assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to directly classify each scan's position as a multi-class classification problem.

Result: Achieves competitive performance compared to contrastive learning-based methods on the NuScenes dataset, with advantages in training efficiency and stability.

Conclusion: Classification-based approach is a viable alternative to contrastive learning for place recognition, offering comparable performance with improved training characteristics.

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [83] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: Generative AI models encode Western beauty standards and erase 'ugliness', showing significant biases in skin tone (86.5% lighter skin), age (74% younger), and hypersexualization, particularly affecting non-binary individuals. Negative beauty traits consistently produce more NSFW content.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI models encode beauty standards and erase 'ugliness', and understand the societal implications of these biases, especially given social media's role in promoting Western beauty norms that cause body dysmorphia.

Method: Created two image generation pipelines (text-to-image and text-to-language-to-image), developed a structured beauty taxonomy, generated 5984 images using three language models and two text-to-image models, and conducted a Likert-scale study with women and non-binary social media users evaluating 1200 images.

Result: 86.5% of images depicted lighter skin tones, 22% contained explicit content despite SFW training, 74% showed younger age demographics. Non-binary individuals were rated as younger and more hypersexualized. Negative beauty traits consistently produced higher NSFW ratings regardless of gender.

Conclusion: Generative AI models contain pervasive demographic biases related to beauty standards that are actively perpetuated by developers through practices like negative prompting, leading to pollution of data streams and erasure of features outside stereotypical beauty norms.

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [84] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: An IoT-based animal detection system for durian plantations that combines YOLOv5 and SSD algorithms for improved accuracy, provides real-time monitoring with Telegram notifications, and triggers automated sound deterrents when animals are detected.


<details>
  <summary>Details</summary>
Motivation: Durian plantations suffer from animal intrusions causing crop damage and financial loss, with traditional farming practices being ineffective due to lack of automated monitoring. Current systems are limited by single detection algorithms, inaccessible notification platforms, and limited deterrent mechanisms.

Method: Integrated YOLOv5 and SSD object detection algorithms for improved accuracy, real-time monitoring system with automatic Telegram notifications to farmers, and automated sound deterrent mechanism (e.g., tiger roar) triggered upon animal detection.

Result: The YOLO+SSD model achieved accuracy rates of 90% for elephants, 85% for boars, and 70% for monkeys. The system performs best during daytime and decreases at night, regardless of image or video input.

Conclusion: The study provides a comprehensive framework combining detection, notification, and deterrence, offering a practical solution for automated farming that paves the way for future innovations in agricultural technology.

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [85] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: A method for 3D instance segmentation that uses granularity-consistent 2D mask tracking and curriculum learning to generate consistent 3D pseudo labels from 2D foundation models, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To avoid costly manual 3D annotations by using 2D foundation models, but existing methods process frames independently leading to inconsistent segmentation granularity and conflicting 3D pseudo labels that degrade segmentation accuracy.

Method: Granularity-Consistent automatic 2D Mask Tracking to maintain temporal correspondences across frames, combined with a three-stage curriculum learning framework that progressively trains from fragmented single-view data to unified multi-view annotations and globally coherent full-scene supervision.

Result: Effectively generated consistent and accurate 3D segmentations, achieved state-of-the-art results on standard benchmarks, and demonstrated open-vocabulary ability.

Conclusion: The structured learning pipeline enables robust distillation of consistent 3D representations from initially fragmented 2D priors, eliminating conflicting pseudo labels and improving 3D instance segmentation performance.

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [86] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench is a benchmark for privacy-aware federated learning using synthetic oncologic CT scans, evaluating segmentation performance and privacy leakage across FL methods with trade-offs between utility and privacy.


<details>
  <summary>Details</summary>
Motivation: FL systems remain vulnerable to membership-inference attacks and data heterogeneity, especially in privacy-sensitive medical environments where sensitive patient data must be protected.

Method: Developed FedOnco-Bench benchmark using synthetic oncologic CT scans with tumor annotations, evaluating four FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD for segmentation performance and privacy leakage.

Result: FedAvg achieved high performance (Dice ~0.85) but more privacy leakage (attack AUC ~0.72), while DP-SGD provided better privacy (AUC ~0.25) at accuracy cost (Dice ~0.79). FedProx and FedBN offered balanced performance under heterogeneous data.

Conclusion: FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation, demonstrating clear privacy-utility trade-offs.

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [87] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: Med-Banana-50K is a 50K-image dataset for instruction-based medical image editing across three modalities and 23 disease types, featuring systematic medical quality control and failed attempts for preference learning.


<details>
  <summary>Details</summary>
Motivation: The research community lacks large-scale, high-quality, openly accessible datasets specifically designed for medical image editing with strict anatomical and clinical constraints.

Method: Dataset construction using Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition/removal) from real medical images, with LLM-as-Judge quality control and iterative refinement up to five rounds.

Result: Created a comprehensive 50K-image dataset spanning chest X-ray, brain MRI, and fundus photography with 37K failed attempts for preference learning and alignment research.

Conclusion: Med-Banana-50K establishes a foundation for training and evaluating next-generation medical image editing models, providing a medically validated and fully documented resource.

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [88] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA is an attention-based, coordinate-free supervised fine-tuning framework for GUI grounding that aligns MLLMs' intrinsic multimodal attention with patch-wise grounding signals, achieving state-of-the-art performance with exceptional data efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based GUI grounding approaches that generate precise coordinates from visual inputs are challenging and computationally intensive. The authors observed that general MLLMs have native grounding capability nested within their attentions, which can be leveraged for more efficient GUI grounding.

Method: Proposed GUI-AIMA framework that aligns MLLMs' multimodal attention with patch-wise grounding signals calculated adaptively through multi-head aggregation on simplified query-visual attention matrices. The coordinate-free approach enables easy integration of a plug-and-play zoom-in stage.

Result: GUI-AIMA-3B trained with only 85k screenshots achieves state-of-the-art performance among 3B models: 58.6% average accuracy on ScreenSpot-Pro and 62.2% on OSWorld-G, demonstrating exceptional data efficiency.

Conclusion: Light training can effectively trigger the native grounding capability of MLLMs, and the attention-based coordinate-free approach provides an efficient solution for GUI grounding tasks with superior performance and data efficiency.

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [89] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: TA-LSDiff is a novel pancreas segmentation model combining topology-aware diffusion with level set energy, achieving state-of-the-art accuracy without explicit geometric evolution.


<details>
  <summary>Details</summary>
Motivation: Pancreas segmentation is challenging due to small size, low contrast, and topological variations. Traditional level set methods ignore topological effects, while deep learning sacrifices structural details.

Method: Combines topology-aware diffusion probabilistic model with level set energy, guided by four complementary terms integrating input image and deep features. Includes pixel-adaptive refinement module for boundary precision.

Result: Outperforms existing methods on four public pancreas datasets, achieving state-of-the-art accuracy. Ablation studies confirm contribution of each component.

Conclusion: TA-LSDiff establishes as a practical and accurate solution for pancreas segmentation, bridging the gap between topological awareness and structural detail preservation.

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [90] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA is a novel position encoding framework for Vision-Language Models that uses modality-specific position encoding and adaptive step scaling to better handle the distinct structural properties of text and vision modalities.


<details>
  <summary>Details</summary>
Motivation: Current VLMs use unified position encoding strategies that treat text and visual tokens uniformly, ignoring their distinct structural properties - sequential continuity for text and spatial coherence for vision.

Method: Proposes OMEGA with two key components: Modality-Specific Position Encoding (MSPE) that assigns positional indices across separate coordinate dimensions for each modality, and Global Adaptive Encoding Step Scaling (GAESS) that adaptively adjusts visual token position encoding step size based on embedding entropy of both modalities.

Result: OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks, achieving up to 3.43% improvement on visual-intensive tasks with Qwen2.5-VL-3B, with consistent gains on larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

Conclusion: OMEGA's modality-specific approach to position encoding effectively addresses the limitations of unified position encoding strategies in VLMs, leading to significant performance improvements across various model architectures and tasks.

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [91] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: LSSA is a novel attack method that enhances adversarial transferability in VLP models by randomly shuffling local image blocks and sampling around adversarial images to generate more diverse adversarial texts.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal adversarial attacks suffer from overfitting due to lack of input diversity, relying too much on adversarial examples from one modality when attacking another.

Method: LSSA randomly shuffles local image blocks to expand image-text pairs, generates adversarial images, samples around them, and uses both original and sampled images to create adversarial texts.

Result: Extensive experiments show LSSA significantly improves adversarial transferability across diverse VLP models and downstream tasks, and outperforms other attacks on Large Vision-Language Models.

Conclusion: LSSA effectively addresses overfitting in multimodal adversarial attacks through enhanced input diversity, demonstrating superior transferability performance.

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [92] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: VCA (Visual-Contrast Attention) replaces MHSA in Vision Transformers, reducing quadratic complexity to linear while improving performance through explicit discrimination between positive and negative visual contrasts.


<details>
  <summary>Details</summary>
Motivation: MHSA in Vision Transformers performs quadratic query-key interactions for all token pairs, spending most computation on visually weak or redundant correlations, which is inefficient.

Method: VCA distills each head's dense query field into pooled visual-contrast tokens, splits them into positive and negative streams for differential interaction, highlighting what separates regions from each other.

Result: VCA improves DeiT-Tiny ImageNet-1K accuracy from 72.2% to 75.6% (+3.4%), enhances hierarchical ViTs by up to 3.1%, and reduces FID-50K by 2.1-5.2 points in image generation across diffusion and flow models.

Conclusion: VCA offers a simple path to faster and sharper Vision Transformers by injecting explicit discrimination while reducing computational complexity, requiring minimal additional parameters and no extra FLOPs.

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [93] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: PIAT is a novel adversarial training framework that reduces oscillations and overfitting by interpolating model parameters between epochs, and uses NMSE to align logit magnitudes between clean and adversarial examples, improving robustness for both CNNs and ViTs.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial training methods suffer from oscillations and overfitting during training, which degrades defense efficacy against adversarial attacks.

Method: Parameter Interpolation Adversarial Training (PIAT) tunes model parameters by interpolating between previous and current epoch parameters, and uses Normalized Mean Square Error (NMSE) to align relative logit magnitudes.

Result: Extensive experiments on benchmark datasets show PIAT prominently improves robustness for both Convolutional Neural Networks and Vision Transformers.

Conclusion: PIAT effectively addresses oscillation and overfitting issues in adversarial training, achieving higher model robustness through parameter interpolation and NMSE-based logit alignment.

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [94] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench is the first comprehensive multimodal VQA benchmark for brain imaging analysis, covering 15 imaging modalities and 15 clinical tasks with 9,527 VQA pairs and 31,706 images. It reveals significant performance gaps between MLLMs and physicians, especially in complex preoperative tasks.


<details>
  <summary>Details</summary>
Motivation: Current brain-oriented VQA benchmarks are limited in modality coverage and granularity, hindering comprehensive assessment of MLLMs throughout the full clinical continuum in brain imaging analysis.

Method: Developed OmniBrainBench with 15 brain imaging modalities from 30 medical sources, creating 9,527 validated VQA pairs and 31,706 images. Simulates clinical workflows across 15 multi-stage clinical tasks, validated by professional radiologists.

Result: Evaluation of 24 MLLMs shows: proprietary models outperform open-source and medical models but lag physicians; medical MLLMs show wide performance variation; open-source models trail overall but excel in specific tasks; all MLLMs underperform significantly in complex preoperative tasks, revealing a visual-to-clinical reasoning gap.

Conclusion: OmniBrainBench sets a new standard for evaluating MLLMs in brain imaging analysis, highlighting significant gaps compared to expert clinical reasoning, particularly in complex clinical tasks requiring advanced reasoning.

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [95] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: Proposes an Occlusion-Aware Diffusion Model (ODM) for pedestrian crossing intention prediction that handles occlusion scenarios by reconstructing occluded motion patterns and using occlusion-aware transformers during denoising.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for pedestrian crossing intention prediction don't adequately handle incomplete observations under occlusion scenarios, which is crucial for mobile robots and intelligent vehicles.

Method: Uses an occlusion-aware diffusion transformer architecture to estimate noise features of occluded patterns during denoising, and introduces an occlusion mask-guided reverse process to utilize observation information and reduce prediction error accumulation.

Result: Comprehensive evaluation on PIE and JAAD benchmarks shows the method achieves more robust performance than existing methods under various occlusion scenarios.

Conclusion: The proposed ODM effectively handles occlusion in pedestrian crossing intention prediction, demonstrating superior robustness compared to existing approaches.

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [96] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: LMD is a post-hoc interpretability method that disentangles modality-specific information in sensor fusion models for autonomous driving, enabling attribution of predictions to individual input modalities.


<details>
  <summary>Details</summary>
Motivation: Transparency in perception models is critical for autonomous driving safety, but current fusion models make it difficult to understand how each sensor modality contributes to predictions due to information entanglement.

Method: Layer-Wise Modality Decomposition (LMD) - a post-hoc, model-agnostic method that disentangles modality-specific information across all layers of pretrained fusion models.

Result: LMD effectively attributes predictions to individual modalities in camera-radar, camera-LiDAR, and camera-radar-LiDAR fusion systems, validated through structured perturbation-based metrics and visual decompositions.

Conclusion: LMD provides practical interpretability for high-capacity multimodal architectures in autonomous driving, enabling better understanding of sensor fusion model decisions.

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [97] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo is a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization that models diverse debate relationships and enables co-evolution between graph structure and agent representations.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval methods are constrained by database coverage, and individual LVLMs struggle with diverse geographic regions and complex scenes. Existing multi-agent systems lack mechanisms to handle conflicting predictions effectively.

Method: Uses heterogeneous graph neural networks with typed edges for supportive collaboration, competitive argumentation, and knowledge transfer. Features dual-level debate mechanism with node-level refinement and edge-level argumentation modeling, plus cross-level topology refinement.

Result: Significantly outperforms state-of-the-art methods on multiple benchmarks.

Conclusion: The framework successfully transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [98] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL is a unified end-to-end framework for medical visual understanding across heterogeneous modalities (2D images, 3D volumetric scans, temporal videos) that addresses domain gaps through data-centric strategies including scaling pretraining, complementing with rare medical data, and extending evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Medical data presents unique challenges due to its heterogeneous nature across diverse modalities (2D, 3D, temporal), substantial domain gaps, and data format inconsistencies, which have hindered the development of unified medical MLLMs despite their promise for clinical applications.

Method: Three key data-centric strategies: (1) scale up pretraining with long-context natural and medical data, (2) complement fine-tuning with rare medical data including video analysis and underrepresented modalities, (3) extend evaluation to include 3D volumetric and video benchmarks. Uses supervised fine-tuning (SFT) and group relative policy optimization (GRPO) across multiple model scales.

Result: Fleming-VL achieves state-of-the-art performance across multiple benchmarks including medical VQA, video QA, and 3D medical image understanding. The model is publicly released to promote transparent and reproducible progress in medical AI.

Conclusion: The proposed unified framework successfully addresses the challenges of medical multimodal data heterogeneity and domain gaps, demonstrating superior performance across diverse medical modalities and establishing a foundation for comprehensive medical visual understanding.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [99] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: A novel Dynamic Multi-level Weighted Alignment Network for zero-shot sketch-based image retrieval that addresses modality imbalance and inconsistent information through multi-level weighting and weighted quadruplet loss.


<details>
  <summary>Details</summary>
Motivation: Previous ZS-SBIR methods suffer from imbalanced modality samples and inconsistent low-quality information during training, leading to sub-optimal performance.

Method: Three-component approach: (1) Uni-modal Feature Extraction with CLIP text encoder and ViT, (2) Cross-modal Multi-level Weighting Module with local/global aggregation for alignment quality measurement, (3) Weighted Quadruplet Loss for domain balance improvement.

Result: Superior performance over state-of-the-art methods on three benchmark datasets: Sketchy, TU-Berlin, and QuickDraw.

Conclusion: The proposed method effectively addresses modality imbalance and information inconsistency in ZS-SBIR, achieving state-of-the-art results across multiple datasets.

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [100] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR is an end-to-end virtual try-on model that directly fits garments onto person images using reference images, eliminating the need for complex inputs like masks or pose data.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods require complex inputs (agnostic person images, human pose, densepose, body keypoints) which are labor-intensive and impractical for real-world applications.

Method: Two-stage training strategy enabling simple inference with only source image and target garment inputs; leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and details.

Result: Evaluated on two widely used benchmarks and diverse tasks, with results consistently validating the effectiveness of the approach.

Conclusion: EVTAR provides a more realistic and high-quality dressing effect by simulating how humans consider reference models when choosing outfits, making virtual try-on more practical for real-world applications.

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [101] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: A unified zero-shot framework for video anomaly analysis that connects temporal detection, spatial localization, and textual explanation through chained test-time reasoning without additional training.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly methods lack explainability, providing only frame-wise scores without spatial or semantic context, and existing localization/understanding approaches remain data-dependent and task-specific.

Method: Chained test-time reasoning process with intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, using careful prompt design with foundation models.

Result: Achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks without additional data or gradients.

Conclusion: Task-wise chaining can unlock foundation models' reasoning power for practical, interpretable video anomaly analysis in a fully zero-shot manner.

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [102] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM is a specialized framework for 2D vessel segmentation that enhances SAM with convolutional adapters, multi-prompt encoding, and lightweight mask decoding, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM perform sub-optimally on vascular structures due to thin, branching anatomy and low texture contrast, creating a need for specialized vessel segmentation methods.

Method: Integrates convolutional adapter for local texture features, multi-prompt encoder fusing anatomical prompts (skeletons, bifurcation points, segment midpoints) via hierarchical cross-attention, and lightweight mask decoder to reduce artifacts.

Result: Outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, achieves competitive performance with fully fine-tuned methods using significantly fewer parameters, and generalizes well to out-of-distribution settings.

Conclusion: VesSAM provides an effective and efficient solution for vessel segmentation that bridges the gap between foundation models and specialized medical imaging tasks, with strong generalization capabilities.

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [103] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: A self-supervised multimodal iterative denoising (MID) framework that models noisy data as a state in continuous noise accumulation, learns noise characteristics from noisy inputs without clean-noisy pairs, and uses iterative noise removal with local linearization for complex non-linear noise.


<details>
  <summary>Details</summary>
Motivation: Real-world data is often corrupted by complex, non-linear noise that makes traditional rule-based denoising methods inadequate, requiring more robust and adaptive approaches.

Method: Models noisy data as a state in continuous noise accumulation process, iteratively introduces noise to learn two neural networks (one for estimating current noise step, another for predicting/subtracting noise increment), uses first-order Taylor expansion to linearize complex non-linear noise locally for iterative removal.

Result: Experiments across four classic computer vision tasks show MID achieves state-of-the-art performance with robustness and adaptability, and also demonstrates strong performance in biomedical and bioinformatics domains.

Conclusion: MID provides an effective self-supervised framework for denoising complex non-linear noise without requiring paired clean-noisy datasets, showing broad applicability across multiple domains.

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [104] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: A multimodal machine learning framework for authenticating Goya paintings using identical feature extraction on both visual and X-ray images, achieving 97.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: Goya's heterogeneous stylistic evolution and extensive forgery history present complex authentication challenges that require advanced computational approaches.

Method: Unified feature extraction pipeline using GLCM descriptors, LBP, entropy, energy, and color analysis applied to both visual and X-ray images, processed through optimized One-Class SVM with hyperparameter tuning.

Result: 97.8% classification accuracy with 0.022 false positive rate on 24 authenticated Goya paintings; case study of "Un Gigante" achieved 92.3% authentication confidence.

Conclusion: Multimodal approach with identical computational methods applied to both visual and radiographic imagery significantly outperforms single-modal methods in art authentication.

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [105] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net is a hybrid CNN-Transformer model for breast ultrasound that simultaneously performs segmentation and classification with intrinsic interpretability, achieving superior performance and demonstrating strong generalization through progressive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: B-mode ultrasound faces challenges including speckle noise, operator dependency, and indistinct tumor boundaries. Existing deep learning approaches suffer from single-task learning, architectural limitations (CNNs lack global context, Transformers lack local features), and black-box decision-making, which hinder clinical adoption.

Method: HyFormer-Net integrates EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks in a dual-branch encoder, with an attention-gated decoder for precision and explainability. It features dual-pipeline interpretability: intrinsic attention validation with quantitative IoU verification and Grad-CAM for classification reasoning.

Result: On BUSI dataset: Dice Score 0.761 ± 0.072, accuracy 93.2%, malignant recall 92.1 ± 2.2%. Ensemble modeling achieves exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% malignant recall. Multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%. Cross-dataset generalization shows progressive fine-tuning with only 10% target data recovers 92.5% performance, and with 50% data achieves 77.3% Dice, exceeding source-domain performance.

Conclusion: HyFormer-Net demonstrates superior performance for breast ultrasound analysis with intrinsic interpretability. The model shows strong generalization capability through progressive fine-tuning, achieving better performance on target domains than source domain, making it clinically viable for breast cancer diagnosis.

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [106] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost is a parameter-efficient neural architecture that achieves SOTA performance on CIFAR benchmarks using a novel Dynamically Scaled Progressive Attention (DSPA) mechanism, achieving 95.57% accuracy on CIFAR-10 with only 0.85M parameters.


<details>
  <summary>Details</summary>
Motivation: To develop highly parameter-efficient neural networks for resource-constrained edge devices while maintaining state-of-the-art accuracy, addressing the trade-off between model size and performance.

Method: Uses Dynamically Scaled Progressive Attention (DSPA) with three innovations: Adaptive Fusion (channel-spatial attention blending), Phase Scaling (training-stage-aware intensity modulation), and Residual Adaptation (self-optimized skip connections), integrated with enhanced MBConv blocks.

Result: Achieves 95.57% accuracy on CIFAR-10 (0.85M params) and 81.37% on CIFAR-100 (0.92M params), with 2.1x parameter reduction over MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10.

Conclusion: FastBoost demonstrates unprecedented parameter-accuracy trade-offs through co-optimization of dynamic attention and efficient convolution, enabling deployment in resource-constrained edge devices without accuracy degradation.

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [107] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: T-MLA is a targeted multiscale log-exponential attack framework that crafts adversarial perturbations in the wavelet domain to degrade neural image compression quality while remaining visually imperceptible.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on neural image compression (NIC) are naive adaptations of pixel-space methods, overlooking the unique structured nature of compression pipelines. The security vulnerabilities of NICs remain significantly less understood than those of classifiers.

Method: The approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of attacked and reconstructed images. Perturbations are strategically confined to specific wavelet subbands in a principled, offline attack to maximize distortion while ensuring perceptual stealth.

Result: Extensive evaluation across multiple state-of-the-art NIC architectures on standard benchmarks reveals a large drop in reconstruction quality while perturbations remain visually imperceptible.

Conclusion: The findings reveal a critical security flaw at the core of generative and content delivery pipelines, demonstrating that NICs are vulnerable to sophisticated wavelet-domain attacks.

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [108] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: A hierarchical sequence prediction approach for image geolocalization that uses S2 cells to predict geographic locations from broad regions to specific addresses, inspired by human reasoning and autoregressive text generation.


<details>
  <summary>Details</summary>
Motivation: Image geolocalization faces challenges due to visual similarities across different locations and large search spaces. The approach is inspired by how humans narrow down locations hierarchically from broad regions to specific addresses.

Method: Uses S2 cells (nested multiresolution global grid) to hierarchically predict geographic tokens. Employs autoregressive sampling with beam search and multi-sample inference, incorporating test-time compute scaling techniques from language models to explore multiple plausible paths through the hierarchy.

Result: Achieves state-of-the-art performance on Im2GPS3k and YFCC4k datasets. Without MLLM: surpasses comparable baselines with up to 13.9% accuracy gains. With MLLM: outperforms all baselines across all metrics.

Conclusion: The hierarchical sequence prediction approach with autoregressive sampling effectively addresses image geolocalization challenges, setting new state-of-the-art performance when combined with MLLM augmentation.

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [109] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I is a synthetic dataset that transforms network KPI vectors into visual representations using four encoding methods, designed for visual learning and machine learning applications in network slicing.


<details>
  <summary>Details</summary>
Motivation: The emergence of 5G/6G networks demands refined identification methods for network slicing, requiring robust datasets to support feature visualization research.

Method: Created a dataset by transforming multivariate KPI vectors into RGB images using four encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching, with 30,000 samples per method.

Result: Generated a comprehensive dataset simulating realistic and noisy network conditions, with each sample containing raw KPI vectors and corresponding low-resolution RGB images.

Conclusion: SliceVision-F2I provides a publicly available dataset suitable for visual learning, network state classification, anomaly detection, and benchmarking image-based ML techniques in network data analysis.

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [110] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: Novel respiratory disease diagnosis method using Epanechnikov kernel density estimation with bimodal logistic regression classifier on chest X-rays, achieving 70.14% accuracy with moderate performance.


<details>
  <summary>Details</summary>
Motivation: To develop an improved method for diagnosing respiratory diseases from medical images by leveraging flexible statistical modeling that doesn't assume specific data distribution shapes.

Method: Combines Epanechnikov's non-parametric kernel density estimation (EKDE) with bimodal logistic regression classifier in a statistical-model-based learning scheme, tested on 13,808 chest X-rays from COVID-19 Radiography Dataset.

Result: Achieved 70.14% accuracy, 59.26% sensitivity, and 74.18% specificity, demonstrating moderate diagnostic performance with room for improvement in sensitivity.

Conclusion: EKDE-based approaches show potential to enhance diagnostic accuracy in medical imaging, though clinical expertise remains essential for further model refinement.

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [111] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: ViACT is a video transformer framework that integrates anatomical priors to focus on diagnostic regions in echocardiogram analysis, preventing spurious correlations from non-diagnostic areas.


<details>
  <summary>Details</summary>
Motivation: Video transformers for echocardiogram analysis tend to learn spurious correlations from non-diagnostic regions like image backgrounds, limiting their clinical reliability and interpretability.

Method: ViACT represents anatomical structures as point sets, encodes spatial geometry and image patches into transformer tokens, and uses masked autoencoding that only reconstructs anatomical patches during pre-training to focus representation learning on anatomical regions.

Result: ViACT produces interpretable attention maps aligned with known pathology regions, generalizes to myocardium point tracking without specialized components, and demonstrates effectiveness on tasks like left ventricular ejection fraction regression and cardiac amyloidosis detection.

Conclusion: Integrating anatomical constraints into video transformers improves focus on clinically relevant regions, enhances interpretability, and enables generalization across multiple echocardiogram analysis tasks without requiring task-specific architectures.

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [112] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: Using GPU-enabled embedded devices in edge computing to improve performance of computer vision applications, achieving better user experience compared to CPU-only solutions.


<details>
  <summary>Details</summary>
Motivation: Computer vision applications, especially AR on mobile devices, are resource-intensive. Edge computing helps offload tasks but has limited capacity, impacting user experience.

Method: Proposes using embedded devices with GPUs in edge computing infrastructure to handle high-intensive computer vision tasks more efficiently.

Result: Experiments showed GPUs achieve significant performance gains compared to using only CPUs, ensuring better user experience for computer vision applications.

Conclusion: GPU-enabled embedded devices in edge computing effectively overcome resource limitations and improve performance for computer vision applications, enhancing user experience.

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [113] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv is a category-equivariant neural network for Human Activity Recognition that encodes temporal, amplitude, and structural symmetries through categorical symmetry products, achieving superior robustness against out-of-distribution perturbations.


<details>
  <summary>Details</summary>
Motivation: To systematically encode the inherent symmetries in inertial sensor data for HAR, including temporal shifts, amplitude gains, and sensor hierarchy structures, to improve model robustness and generalization.

Method: Introduces categorical symmetry product combining cyclic time shifts, positive gains, and sensor-hierarchy poset to capture categorical symmetry structure, and builds equivariant neural networks with respect to this product.

Result: CatEquiv achieves markedly higher robustness on UCI-HAR dataset under out-of-distribution perturbations compared to circularly padded CNNs and plain CNNs.

Conclusion: Enforcing categorical symmetries yields strong invariance and generalization without requiring additional model capacity, demonstrating the effectiveness of systematic symmetry encoding in HAR.

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [114] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: PCP is a weakly supervised framework that predicts medical concepts without requiring concept annotations, using class-level priors and refinement mechanisms to improve interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing interpretable-by-design methods need costly concept annotations, while zero-shot approaches fail to capture domain-specific medical features, limiting practical deployment in clinical settings.

Method: PCP uses class-level concept priors as weak supervision and incorporates KL divergence with entropy regularization to refine predictions and align them with clinical reasoning.

Result: PCP improves concept-level F1-score by over 33% compared to zero-shot baselines and achieves competitive classification performance on four medical datasets relative to fully supervised methods.

Conclusion: PCP provides a practical solution for interpretable medical AI by eliminating the need for concept annotations while maintaining reliability and clinical alignment.

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [115] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: MicroAUNet is a lightweight attention-based segmentation network for real-time colorectal polyp segmentation that combines depthwise-separable dilated convolutions with channel-spatial attention and uses progressive knowledge distillation to achieve state-of-the-art accuracy with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: Current deep learning polyp segmentation models either provide ambiguous polyp margins compromising clinical decision-making, or rely on heavy architectures with insufficient inference speeds for real-time endoscopic applications.

Method: Proposes MicroAUNet with depthwise-separable dilated convolutions and parameter-shared channel-spatial attention block to strengthen multi-scale boundary features, plus a progressive two-stage knowledge-distillation scheme to transfer semantic and boundary cues from a high-capacity teacher.

Result: Extensive experiments demonstrate state-of-the-art accuracy under extremely low model complexity, making it suitable for real-time clinical polyp segmentation.

Conclusion: MicroAUNet effectively addresses the trade-off between segmentation accuracy and computational efficiency, providing a practical solution for real-time colorectal endoscopic applications.

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [116] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: Proposes GCN-PSN, a topology-aware Graph Convolutional Network that models human skeleton as a graph to learn pose embeddings for Action Quality Assessment, achieving competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Action Quality Assessment requires fine-grained understanding of human motion and precise evaluation of pose similarity, which benefits from modeling skeletal topology.

Method: Uses a topology-aware GCN framework that models human skeleton as a graph, with Siamese architecture trained using contrastive regression objective to learn discriminative pose embeddings.

Result: Outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks, with ablation studies validating the effectiveness.

Conclusion: Leveraging skeletal topology through GCNs is effective for pose similarity and action quality assessment tasks.

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [117] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: The paper addresses the lack of Indian food representation in VQA systems by introducing multi-step reasoning chains for Indian Food VQA, achieving 10% accuracy improvement through auto-validated reasoning chains and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing VQA systems are biased towards Western foods and fail to handle the cultural and culinary diversity of Indian cuisines, which require complex reasoning about culinary context and food relationships.

Method: Created reasoning chains for QA with minimal human intervention, fine-tuned smaller LLMs and VLMs with auto-validated reasoning chains, and used reinforcement learning with larger datasets.

Result: Achieved an average 10 percentage points accuracy improvement on baseline Indian Food VQA task through reasoning chain augmentation.

Conclusion: Multi-step reasoning chains significantly enhance Indian Food VQA performance by enabling better understanding of complex culinary contexts and food relationships.

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [118] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER is a benchmark for evaluating reciprocal cross-modal reasoning in unified multimodal models, testing how models use one modality to guide outputs in another modality across 1312 tasks grounded in 1876 images.


<details>
  <summary>Details</summary>
Motivation: Current evaluations treat multimodal abilities in isolation, focusing on unimodal reasoning rather than testing the crucial ability of using one modality to guide, verify, or refine outputs in another modality.

Method: ROVER contains two complementary settings: verbally-augmented reasoning for visual generation (using verbal prompts to guide image synthesis) and visually-augmented reasoning for verbal generation (generating intermediate visualizations to strengthen reasoning for question answering).

Result: Experiments on 17 unified models show that cross-modal reasoning determines visual generation quality, with interleaved models outperforming non-interleaved ones. Models also show dissociation between physical and symbolic reasoning - succeeding at literal perceptual concepts but failing at visual abstractions for symbolic tasks.

Conclusion: Reciprocal cross-modal reasoning is a critical frontier for enabling true omnimodal generation, as current models struggle with using modalities to mutually reinforce each other's reasoning processes.

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [119] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: Eyes on Target is a gaze-guided object detection framework for egocentric videos that integrates human gaze features into Vision Transformer attention mechanisms to prioritize human-attended regions, improving detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Human gaze provides valuable supervisory signals for understanding visual attention in complex environments, especially in egocentric videos where viewer attention is crucial for task assessment.

Method: Proposes a depth-aware gaze-guided object detection framework that injects gaze-derived features into ViT attention mechanisms, biasing spatial feature selection toward human-attended regions rather than treating all regions equally.

Result: Demonstrates consistent gains in detection accuracy over gaze-agnostic baselines on both custom simulator datasets and public benchmarks (Ego4D Ego-Motion and Ego-CH-Gaze), with ablation studies validating effectiveness.

Conclusion: The gaze-integrated approach successfully enhances object detection in egocentric scenarios and provides interpretability through gaze-aware attention head importance metrics that reveal how gaze cues modulate transformer attention dynamics.

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [120] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: The paper introduces an automated pipeline for mining YouTube videos to create a large-scale animal video dataset (30K videos, 2M frames), presents the Animal-in-Motion benchmark for 4D quadruped reconstruction, and establishes baseline methods for markerless animal reconstruction from in-the-wild videos.


<details>
  <summary>Details</summary>
Motivation: Current animal video datasets are limited in scale and lack key processing for animal-centric 3D/4D tasks, while existing collection methods rely on controlled capture setups. There's a need for large-scale, non-invasive analysis from in-the-wild videos.

Method: Developed an automated pipeline that mines YouTube videos and processes them into object-centric clips with auxiliary annotations. Created Animal-in-Motion benchmark with 230 manually filtered sequences (11K frames). Evaluated state-of-the-art model-based and model-free methods, and enhanced a model-free approach with sequence-level optimization.

Result: Collected 30K videos (2M frames) - an order of magnitude more than prior works. Found that 2D metrics favor model-based methods despite unrealistic 3D shapes, while model-free methods yield more natural reconstructions but score lower. Established the first 4D animal reconstruction baseline.

Conclusion: The pipeline, benchmark, and baseline advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos, addressing current limitations in evaluation and dataset scale for wildlife computer vision research.

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [121] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: This review paper examines adaptation strategies for foundation models in medical imaging, addressing challenges like domain shifts, data limitations, and privacy concerns while exploring techniques for clinical integration.


<details>
  <summary>Details</summary>
Motivation: Foundation models offer transformative potential for medical image analysis but face challenges in adapting to real-world clinical practice due to domain shifts, limited annotated data, computational demands, and privacy requirements.

Method: Comprehensive assessment of adaptation strategies including supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal frameworks, evaluating performance gains and clinical applicability.

Result: Identifies trade-offs and unresolved challenges in existing techniques while highlighting emerging directions like continual learning, federated approaches, hybrid self-supervised learning, data-centric pipelines, and systematic benchmarking for robust generalization.

Conclusion: Provides a roadmap for developing adaptive, trustworthy, and clinically integrated foundation models capable of meeting real-world medical imaging demands by outlining strategies and research gaps.

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [122] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: DTWSR is a Diffusion Transformer model for image super-resolution that uses wavelet spectra to capture interrelations among multiscale frequency sub-bands, achieving consistent and realistic results.


<details>
  <summary>Details</summary>
Motivation: Existing DWT-based super-resolution methods neglect interrelations among multiscale frequency sub-bands, causing inconsistencies and artifacts in reconstructed images.

Method: Uses Multi-level Discrete Wavelet Transform to decompose images into wavelet spectra, pyramid tokenization for transformer processing, and a dual-decoder to handle low-frequency and high-frequency sub-bands while maintaining alignment.

Result: Extensive experiments on multiple benchmark datasets show high performance on both perception quality and fidelity.

Conclusion: DTWSR effectively captures frequency sub-band interrelations using diffusion models and transformers, producing superior super-resolution results.

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [123] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: APDM is a novel framework that protects against unauthorized personalization in diffusion models by shifting protection from images to the model itself, using Direct Protective Optimization and Learning to Protect strategies.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks from malicious misuse of personalization techniques in diffusion models, as existing adversarial perturbation methods fail under realistic conditions with clean images or simple transformations.

Method: Proposes Direct Protective Optimization (DPO) loss function and dual-path Learning to Protect (L2P) strategy that alternates between personalization and protection paths to simulate future attacks and reinforce protection.

Result: APDM outperforms existing methods and achieves state-of-the-art performance in preventing unauthorized personalization while maintaining generative quality.

Conclusion: The framework effectively protects diffusion models from subject personalization misuse through theoretical analysis and novel optimization strategies, providing robust privacy protection.

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [124] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa is a hierarchical motion generation framework that enhances VQ-GT through coarse-to-fine scalable generation, achieving state-of-the-art quality and efficiency with only 10 inference steps.


<details>
  <summary>Details</summary>
Motivation: To improve text-driven 3D human motion generation by addressing the limitations of traditional methods that predict only one token per step, leading to inefficient inference and potential reconstruction degradation.

Method: Proposes Multi-scale Token Preservation Strategy (MTPS) with hierarchical RQ-VAE for coarse-to-fine token retention, Scalable Autoregressive (SAR) modeling for multi-token prediction per step, and CAQ-VAE (convolution-attention hybrid VQ-VAE) to prevent reconstruction degradation.

Result: Achieves FID of 0.06 on Motion-X dataset (vs MoMask's 0.20), reduces inference time by 27%, and demonstrates strong generalization to downstream tasks like motion editing without fine-tuning.

Conclusion: MoSa establishes a new state-of-the-art in 3D human motion generation, offering superior quality, efficiency, and generalization capabilities through its hierarchical scalable generation approach.

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [125] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: CMI-MTL is a Cross-Mamba Interaction based Multi-Task Learning framework for Medical Visual Question Answering that addresses cross-modal alignment challenges and free-form answer diversity through fine-grained visual-text feature alignment, cross-modal interleaved feature representation, and free-form answer-enhanced multi-task learning.


<details>
  <summary>Details</summary>
Motivation: Existing self-attention based methods struggle with cross-modal semantic alignment between vision and language, and classification-based approaches cannot adapt to the diversity of free-form answers in medical VQA tasks.

Method: The framework uses three key modules: FVTA for fine-grained visual-text feature alignment, CIFR for cross-modal sequential interactions, and FFAE for leveraging auxiliary knowledge from open-ended questions through multi-task learning.

Result: CMI-MTL outperforms state-of-the-art methods on three Med-VQA datasets (VQA-RAD, SLAKE, and OVQA) and demonstrates effectiveness through interpretability experiments.

Conclusion: The proposed CMI-MTL framework effectively addresses cross-modal alignment challenges and free-form answer diversity in medical VQA, achieving superior performance on multiple benchmark datasets.

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [126] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA is a vision-language-action model that integrates multiple sensing modalities (infrared camera, mmWave radar, microphone array) using sensor-masked images to enhance robotic manipulation capabilities beyond RGB-only perception.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models rely solely on RGB cameras, limiting their perception and manipulation capabilities. The authors aim to overcome this limitation by incorporating novel sensing modalities for physically-grounded spatial intelligence.

Method: Uses sensor-masked images - a unified representation that overlays spatially grounded masks from multiple sensors onto RGB images. Built on an RGB-pretrained VLA backbone with lightweight per-sensor projectors for data-efficient learning.

Result: Achieves 84% average task success rate, significantly outperforming RGB-only (59% improvement) and raw-sensor-input baseline models (28% improvement), while showing higher learning efficiency and stronger generalization.

Conclusion: Integrating multiple sensing modalities through sensor-masked images significantly enhances VLA model performance for real-world manipulation tasks, providing better perception and manipulation capabilities than RGB-only approaches.

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [127] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS framework addresses patch redundancy and ambiguity in fine-grained cross-modal alignment by integrating dense and sparse text semantics and using relevance-aware selection to improve patch-word correspondences.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle with patch redundancy and ambiguity due to information density disparities between vision and language. MLLMs help but their dense outputs can conflict with sparse captions, and quantifying semantic relevance remains challenging.

Method: Two-stage mechanism that integrates unified semantics from both dense and sparse texts to identify salient visual patches, plus relevance-aware selection with mean value computation to highlight crucial patch-word correspondences.

Result: Achieves 23%-86% improvement in rSum across diverse model architectures on Flickr30K and MS-COCO datasets, with notable enhancements in text-to-image retrieval.

Conclusion: SEPS effectively addresses patch redundancy and ambiguity in cross-modal alignment, demonstrating superior performance over existing approaches through semantic enhancement and relevance-aware selection.

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [128] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT is a unified tracker that handles three reference modalities (bounding box, natural language, or both) across four video modalities (RGB, RGB+Depth, RGB+Thermal, RGB+Event) with uniform parameters, outperforming modality-specific trackers.


<details>
  <summary>Details</summary>
Motivation: Existing trackers are designed for specific reference and video modalities, leading to separate model designs that limit practical applications. A unified tracker is needed to handle various requirements across different modalities.

Method: Presented UniSOT, a unified tracker that can perform tracking with three reference modalities (bounding box, natural language, or both) across four video modalities (RGB, RGB+Depth, RGB+Thermal, RGB+Event) using uniform parameters.

Result: Extensive experiments on 18 benchmarks show UniSOT outperforms modality-specific counterparts. It achieves over 3.0% AUC improvement on TNL2K across all three reference modalities and over 2.0% main metric improvement on Un-Track across all three RGB+X video modalities.

Conclusion: UniSOT demonstrates superior performance as a unified tracker that can handle diverse reference and video modalities simultaneously, addressing the limitations of modality-specific trackers in practical applications.

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [129] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: The paper identifies flaws in human evaluation practices for speech-driven 3D gesture generation and introduces a standardized evaluation protocol for the BEAT2 dataset, benchmarking six models across motion realism and speech-gesture alignment.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardization and flawed experimental setups in human evaluation of automated gesture generation, which makes it impossible to compare methods or determine state of the art.

Method: Developed a detailed human evaluation protocol for BEAT2 dataset, conducted large-scale crowdsourced evaluation of six recent gesture-generation models across motion realism and speech-gesture alignment dimensions.

Result: Found that newer models don't consistently outperform earlier approaches, published claims may not hold under rigorous evaluation, and the field needs disentangled assessments of motion quality and multimodal alignment.

Conclusion: The field must adopt standardized evaluation protocols with disentangled assessments to enable accurate benchmarking and progress, with the authors releasing comprehensive evaluation resources to drive standardization.

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [130] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: The paper introduces MAOML, a meta-learning approach that trains smaller VLMs for fruit freshness classification, achieving 92.71% accuracy by addressing data scarcity and leveraging label ordinality.


<details>
  <summary>Details</summary>
Motivation: To enable food retailers to use non-invasive fruit freshness prediction while avoiding proprietary VLMs due to data privacy concerns, and overcoming the limitations of open-source VLMs with scarce labeled data.

Method: Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm that combines meta-learning to handle data sparsity with ordinal regression to leverage the natural ordering of freshness labels.

Result: Achieved state-of-the-art performance with 92.71% accuracy across all fruits in both zero-shot and few-shot settings, outperforming existing open-source VLMs.

Conclusion: MAOML provides an effective solution for fruit freshness classification that balances performance with data privacy requirements, making it suitable for practical deployment in food retail.

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [131] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: The paper introduces Reg-DPO, a novel approach for video generation that automatically creates preference pairs using real videos as positives and model-generated videos as negatives, incorporates SFT loss as regularization for training stability, and achieves 3x higher training capacity through memory optimization.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods for video generation follow image-domain paradigms and are limited to small-scale models, failing to address video-specific challenges like costly data construction, unstable training, and heavy memory consumption.

Method: Proposes GT-Pair for automatic preference pair construction using real videos as positives and model-generated videos as negatives, Reg-DPO that integrates SFT loss as regularization into DPO objective, and combines FSDP with memory optimization techniques for enhanced training capacity.

Result: The method achieves nearly three times higher training capacity than using FSDP alone and consistently outperforms existing approaches on both I2V and T2V tasks across multiple datasets, delivering superior video generation quality.

Conclusion: The proposed Reg-DPO framework effectively addresses video-specific challenges in preference optimization, enabling stable training and high-quality video generation without requiring external annotations.

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [132] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces Adversarial Flatness Attack (AFA) to address deceptive flatness in transferable attacks by using dual-order information and MonteCarlo Adversarial Sampling (MCAS) to improve transferability across models.


<details>
  <summary>Details</summary>
Motivation: Current transferable attacks focus on flat losses but still fall into suboptimal regions (deceptive flatness), limiting their effectiveness against unknown victim models.

Method: Proposes Adversarial Flatness (AF) with theoretical assurance, implements AFA attack using efficient approximation, and enhances sampling with MCAS for better inner-loop efficiency.

Result: Comprehensive experiments on ImageNet-compatible dataset show superiority over six baselines, with improved transferability across model architectures and better performance on input transformation attacks and Baidu Cloud API.

Conclusion: The proposed AFA method effectively addresses deceptive flatness, generates adversarial examples in flatter regions, and significantly boosts transferability in black-box settings.

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [133] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: The paper introduces QA-SNNE, a black box uncertainty estimator for surgical VQA that incorporates question semantics to improve safety by detecting ambiguous or incorrect responses through semantic entropy measurement.


<details>
  <summary>Details</summary>
Motivation: Safety and reliability are critical for surgical VQA deployment, as incorrect responses can harm patients. Current research overlooks safety behaviors like ambiguity awareness and expert referral, which uncertainty estimation can enable.

Method: Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE) measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. Evaluated on five models including PEFT and LVLMs on EndoVis18-VQA and PitVQA datasets.

Result: PEFT models degrade under paraphrasing while LVLMs are more resilient. QA-SNNE improves AUROC by 15-38% for zero-shot models in most settings, enhances hallucination detection, and maintains gains under out-of-template stress.

Conclusion: QA-SNNE provides a practical step toward automatic failure detection in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question-aligned uncertainty estimation can improve safety and clinician trust.

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [134] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM is an end-to-end framework for brain lesion segmentation that freezes pretrained backbones and trains lightweight adapters. It uses a novel 3x3 corner-axis-center scanning strategy and memory-driven prompts to improve sensitivity to small lesions and inter-slice coherence.


<details>
  <summary>Details</summary>
Motivation: Brain lesion segmentation faces challenges including small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities that need to be addressed for accurate medical imaging analysis.

Method: The method employs a CenterMamba encoder with 3x3 corner-axis-center short-sequence scanning for center-prioritized information aggregation, memory-driven structural prompt generator for inter-slice coherence, and memory-augmented multi-scale decoder with progressive refinement.

Result: Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.

Conclusion: The proposed framework effectively addresses brain lesion segmentation challenges through efficient fine-tuning, novel scanning strategies, and memory-driven approaches, achieving superior performance compared to existing methods.

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [135] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: Proposes a framework for post-training quantization of vision transformers by modeling quantization errors as Gaussian noise and using noise injection to find flat minima for better low-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods overlook the relationship between well-trained models and quantized versions, leading to high quantization errors. Need efficient methods to prepare models for low-bit quantization.

Method: Models Activation Quantization Error (AQE) and Weight Quantization Error (WQE) as independent Gaussian noises, uses noise injection optimization methods to find flat minima in the full-precision model.

Result: Experimental results show the approach effectively reduces quantization error and enables better low-bit PTQ models.

Conclusion: The framework provides novel pathways for obtaining low-bit PTQ models by proactively preconditioning models through error modeling and flat minimum optimization.

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [136] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: A Light Geometry-aware adapter improves LiDAR semantic segmentation robustness in adverse weather by preserving neighbor continuity and applying region-aware regularization to structurally fragile areas, achieving 7.9% mIoU improvement over baseline methods.


<details>
  <summary>Details</summary>
Motivation: LiDAR semantic segmentation degrades in adverse weather due to corrupted geometry from refraction, scattering, and point dropouts. Prior methods overlook structural vulnerabilities near boundaries, corners, and sparse regions.

Method: The adapter aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across 0~360 degree boundaries. It uses local-window K-Nearest Neighbors to compute local statistics, which are compressed into geometry-aware cues that drive region-aware regularization during training.

Result: In source-only cross-weather setup (train on SemanticKITTI, evaluate on SemanticSTF), the adapter improves mIoU by 7.9 percentage points over data-centric augmentation baseline and by 0.6 points over class-centric regularization baseline.

Conclusion: Geometry-driven regularization is a key direction for all-weather LiDAR segmentation, and the plug-and-play adapter complements augmentation with negligible inference cost.

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [137] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: HMVLM is a unified framework that integrates 3D human motion with foundation models using MoE LoRA strategy to address catastrophic forgetting and develop autoregressive-compatible pose representations, achieving strong performance across diverse motion tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the modality gap between human motion and text while preventing catastrophic forgetting during integration with foundation models, and to develop generalizable pose representations for heterogeneous downstream tasks.

Method: Proposes HMVLM framework using Mixture of Expert Low-Rank Adaptation (MoE LoRA) with dynamic gating network for task allocation, zero expert to preserve pre-trained parameters, and body-part-specific tokenization for enhanced spatial resolution.

Result: Effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.

Conclusion: The proposed HMVLM framework successfully addresses key challenges in integrating 3D human motion with foundation models while maintaining performance across multiple tasks.

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [138] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream enables real-time streaming video generation with sub-second latency and up to 29 FPS on a single GPU, addressing the prohibitive latency and non-causal processing issues in current motion-conditioned video generation methods.


<details>
  <summary>Details</summary>
Motivation: Current motion-conditioned video generation methods suffer from minutes of latency per video and non-causal processing that prevents real-time interaction, making them impractical for interactive applications.

Method: The approach augments a text-to-video model with motion control, then distills it into a causal student using Self Forcing with Distribution Matching Distillation. Key innovations include sliding-window causal attention with attention sinks, and self-rollout with KV cache rolling during training to simulate inference-time extrapolations with fixed context windows.

Result: MotionStream achieves state-of-the-art results in motion following and video quality while being two orders of magnitude faster than existing methods, enabling constant-speed generation of arbitrarily long videos with infinite-length streaming capability.

Conclusion: MotionStream uniquely enables real-time interactive video generation where users can paint trajectories, control cameras, or transfer motion and see results unfold instantly, delivering a truly interactive experience.

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [139] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor is a prior-guided color transformer that revives color-degraded ancient Chinese paintings by learning from recent paintings and decomposing restoration into luminance enhancement and hue correction tasks.


<details>
  <summary>Details</summary>
Motivation: Ancient Chinese paintings suffer from irreversible color degradation due to complex chemistry mechanisms, and there's a lack of comprehensive datasets for developing end-to-end digital restoration tools.

Method: Uses two sequential sub-tasks: luminance enhancement with variational U-Nets and multi-scale mapping, and hue correction with a dual-branch color query module guided by localized hue priors from faded paintings.

Result: Extensive experiments show PRevivor achieves superior performance both quantitatively and qualitatively compared to state-of-the-art colorization methods.

Conclusion: PRevivor effectively revives color-degraded ancient Chinese paintings through its prior-guided approach and dual-task decomposition, demonstrating promising results for cultural heritage preservation.

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [140] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: Proposes a structured five-layer model for generating and evaluating rare driving scenarios using generative models, with new metrics for diversity and originality assessment.


<details>
  <summary>Details</summary>
Motivation: Rare driving scenarios are crucial for autonomous vehicle development but difficult to encounter in real data, necessitating simulation and generation methods.

Method: Develops a five-layer structured model with subclasses and characteristics for scenario agents, uses large foundational models for data augmentation, and introduces diversity/originality metrics for evaluation.

Result: Successfully generates synthetic driving scenarios and demonstrates the effectiveness of diversity and originality metrics in different generation setups, with qualitative evaluation of synthetic videos.

Conclusion: The proposed structured layer model and evaluation metrics provide an effective framework for generating and assessing rare driving scenarios for autonomous vehicle testing.

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [141] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX is a modular and extensible training framework that unifies DINO, DINOv2, and DINOv3 principles, offering flexible training strategies, reduced computational costs, and compatibility with Hugging Face ecosystem.


<details>
  <summary>Details</summary>
Motivation: Existing vision foundation model training pipelines are often inflexible, domain-specific, or computationally expensive, limiting their usability across different domains and resource settings.

Method: A unified configuration-driven system supporting various transformer architectures, with training strategies including LoRA, layer freezing, knowledge distillation, and distributed training (DDP/FSDP). Works with natural and specialized data including multi-channel images.

Result: Achieves competitive performance on diverse datasets while significantly reducing computational costs. Includes interpretability tools and label-guided data augmentation that improves attention-based localization without extra detection heads.

Conclusion: Provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across research and real-world applications.

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [142] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: A novel framework for detecting generated images by exploiting geometric differences between natural and generated image manifolds, using orthogonal gradient subspaces and normalizing flows to amplify detectable differences.


<details>
  <summary>Details</summary>
Motivation: Increasing realism of generated images raises concerns about misuse, requiring robust detection methods that don't heavily depend on quantity/quality of training data like current binary classifiers.

Method: Uses a pair of functions that give consistent outputs for natural images but divergent outputs for generated images, leveraging orthogonal gradient subspaces. Detects generated images when transformation along data manifold causes significant loss change in self-supervised model. Uses normalizing flows to amplify differences by extruding generated images away from natural manifold.

Result: Extensive experiments demonstrate the efficacy of the proposed detection method.

Conclusion: The framework provides an effective approach for detecting generated images by exploiting geometric manifold differences and amplifying them through normalizing flows.

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [143] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++ is a novel method that efficiently generates high-fidelity textured meshes from single-view images using a cross-domain diffusion model with multi-view attention and cascaded mesh extraction.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods - SDS-based approaches suffer from slow per-shape optimization and inconsistent geometry, while fast inference methods produce low-quality results lacking geometric details.

Method: Proposes a cross-domain diffusion model generating multi-view normal maps and color images, employs multi-view cross-domain attention for consistency, and uses a cascaded 3D mesh extraction algorithm for coarse-to-fine surface reconstruction in about 3 minutes.

Result: Achieves high-quality reconstruction results with robust generalization and good efficiency compared to prior works.

Conclusion: Wonder3D++ holistically improves quality, consistency, and efficiency in single-view 3D reconstruction tasks, demonstrating superior performance over existing methods.

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [144] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniREditBench is a unified benchmark for evaluating reasoning-based image editing models, addressing limitations of existing benchmarks by covering multi-object interactions and game-world scenarios, and introducing multimodal dual-reference evaluation for more reliable assessment.


<details>
  <summary>Details</summary>
Motivation: Current generative models struggle with complex image editing tasks requiring implicit reasoning, and existing benchmarks focus mainly on single-object transformations in realistic scenarios while overlooking multi-object interactions and game-world scenarios with human-defined rules.

Method: Proposes UniREditBench with 2,700 curated samples across real- and game-world scenarios, introduces multimodal dual-reference evaluation (textual + ground-truth image references), and creates UniREdit-Data-100K synthetic dataset with CoT reasoning annotations for fine-tuning models.

Result: Fine-tuned Bagel model (UniREdit-Bagel) shows substantial improvements in both in-domain and out-of-distribution settings. Benchmarking reveals strengths and weaknesses of various open-source and closed-source image editing models across different aspects.

Conclusion: UniREditBench provides a comprehensive framework for systematically evaluating reasoning-based image editing models, addressing key limitations of existing benchmarks and enabling more reliable assessment of model capabilities in complex reasoning scenarios.

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [145] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo is the first expert-curated benchmark for evaluating video generation models in surgery, revealing a 'plausibility gap' where models like Veo-3 achieve visual realism but fail at understanding surgical causality and intent.


<details>
  <summary>Details</summary>
Motivation: Foundation models show promise as world simulators but lack evaluation in high-stakes domains like surgery that require deep causal knowledge rather than general physical rules.

Method: Created SurgVeo benchmark and Surgical Plausibility Pyramid (SPP) framework with four tiers to assess model outputs from basic appearance to complex surgical strategy. Evaluated Veo-3 model on surgical clips using board-certified surgeons.

Result: Veo-3 achieves exceptional Visual Perceptual Plausibility but fails critically at higher SPP levels including Instrument Operation, Environment Feedback, and Surgical Intent Plausibility, revealing a distinct plausibility gap.

Conclusion: There's a chasm between visually convincing mimicry and causal understanding in surgical AI. SurgVeo and SPP provide foundation for developing models capable of navigating specialized healthcare domains.

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [146] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: REASON is a two-stage framework for automated gastric content assessment using ultrasound, combining segmentation probability maps with dual-branch fusion of RLD and SUP views to improve aspiration risk assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional manual tracing methods for gastric content assessment are inefficient and inaccurate, creating need for automated solutions to improve preoperative aspiration risk stratification.

Method: Two-stage framework: Stage 1 uses segmentation model to generate probability maps that suppress artifacts; Stage 2 employs dual-branch classifier that fuses information from right lateral decubitus and supine ultrasound views.

Result: Outperforms state-of-the-art approaches by significant margin on self-collected dataset, demonstrating improved efficiency and accuracy.

Conclusion: REASON framework shows great promise for automated preoperative aspiration risk assessment, offering robust, efficient, and accurate clinical solution.

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [147] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: A novel MIL framework using latent factor grouping and cluster-reasoning to disentangle spatial, semantic, and decision entanglements in whole-slide pathology images, achieving superior performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of spatial, semantic, and decision entanglements among instances in multiple instance learning for whole-slide pathology images, which restrict representation quality and interpretability.

Method: Three-phase framework: 1) Positive semi-definite latent factor grouping to map instances into latent subspace for spatial disentanglement; 2) Instance probability counterfactual inference and optimization via cluster-reasoning for semantic disentanglement; 3) Generalized linear weighted decision with instance effect re-weighting for decision disentanglement.

Result: Extensive experiments on multicentre datasets show the model outperforms all state-of-the-art models and achieves pathologist-aligned interpretability through disentangled representations and transparent decision-making.

Conclusion: The proposed framework effectively addresses three types of entanglements in MIL for WSIs, demonstrating superior performance and enhanced interpretability that aligns with pathologist reasoning.

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [148] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba is the first Mamba-based Multi-View Stereo network that achieves superior performance and efficiency by leveraging Mamba's linear complexity for global feature aggregation, using a novel Dynamic Mamba module with reference-centered dynamic scanning.


<details>
  <summary>Details</summary>
Motivation: Transformers in MVS methods have quadratic complexity, making it challenging to balance performance and efficiency. Mamba's global modeling capability with linear complexity offers a promising alternative.

Method: Proposes MVSMamba with Dynamic Mamba module using reference-centered dynamic scanning for efficient intra- and inter-view feature interaction, omnidirectional multi-view representations, and multi-scale global feature aggregation.

Result: Outperforms state-of-the-art MVS methods on DTU dataset and Tanks-and-Temples benchmark with both superior performance and efficiency.

Conclusion: MVSMamba demonstrates that Mamba architecture can effectively replace Transformers in MVS tasks, achieving better performance-efficiency trade-off through linear complexity global feature modeling.

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [149] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: A generative adversarial attack method using CLIP model to create effective and visually imperceptible perturbations that deceive multilabel classifiers while maintaining high visual similarity to original images.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of deep learning models to adversarial attacks by developing more effective and visually imperceptible perturbations that can deceive models without being noticeable to humans.

Method: Integrates CLIP model's text-image alignment with SSAE's concentrated perturbation strategy and GAMA's dissimilar text embeddings to generate adversarial examples guided by natural language semantics.

Result: The method achieves competitive or superior performance compared to existing techniques across various black-box victim models while preserving greater visual fidelity.

Conclusion: The proposed CLIP-based generative adversarial attack method effectively creates imperceptible perturbations that successfully deceive multilabel classifiers while maintaining high structural similarity to original images.

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [150] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet is a medical image segmentation network that combines local modeling with global context to improve boundary delineation and detail preservation for fine structures.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges due to anatomical variability and boundary ambiguity, which hinder reliable delineation of fine structures.

Method: Uses a hybrid ResBlock detail-aware Transformer backbone with three modules: ASBE for adaptive boundary enhancement, HVDA for fine-grained feature modeling, and EulerFF for fusion weighting guided by Euler's formula.

Result: Achieved comparable segmentation accuracy and boundary quality on Synapse and BUSI datasets.

Conclusion: The unified approach of local modeling with global context improves structural consistency and boundary accuracy across morphology, orientation, and scale.

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [151] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: The paper introduces a benchmark of 1,333 English Rebus puzzles and proposes RebusDescProgICE, a framework that improves Vision-Language Models' performance on these puzzles by combining unstructured descriptions with code-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Rebus puzzles require diverse skills like image recognition, cognitive abilities, commonsense reasoning, and multi-step reasoning, making them challenging for current Vision-Language Models. The authors aim to address this gap by creating a comprehensive benchmark and improving model performance.

Method: The authors created a large benchmark of 1,333 Rebus puzzles across 18 categories with varying artistic styles and difficulty levels. They proposed RebusDescProgICE, a model-agnostic framework that uses unstructured descriptions combined with code-based structured reasoning and improved reasoning-based in-context example selection.

Result: The proposed framework improved performance on the Rebus puzzle benchmark by 2.1-4.1% using closed-source models and 20-30% using open-source models compared to Chain-of-Thought Reasoning.

Conclusion: The RebusDescProgICE framework effectively enhances Vision-Language Models' ability to solve complex Rebus puzzles by leveraging a combination of descriptive and structured reasoning approaches, demonstrating significant improvements over existing methods.

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [152] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D is a multi-instance 3D medical image segmentation framework that enables single-point-to-multi-instance segmentation using competitive query optimization and hybrid CNN-Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Current SAM-based interactive segmentation methods follow single-point-to-single-object paradigm, limiting multi-lesion segmentation, and ViT backbones often miss local details in medical images.

Method: Uses prompt-conditioned instance-query generator to transform single point into multiple queries, hybrid CNN-Transformer encoder with spatial gating for boundary saliency, and competitively optimized query decoder for parallel multi-instance prediction.

Result: Achieved comparable performance on LiTS17 and KiTS21 datasets with strong robustness to prompts.

Conclusion: Provides practical solution for efficient annotation of clinically relevant multi-lesion cases in medical imaging.

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [153] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: Proposes a novel method to expand the content-style frontier in text-to-image diffusion models by addressing content feature loss at high style intensities, using Content-Style Subspace Blending and Balance loss.


<details>
  <summary>Details</summary>
Motivation: Previous studies only assessed content similarity under single style intensity, but increasing style intensity causes significant content feature loss, leading to suboptimal content-style frontier.

Method: Uses Content-Style Subspace Blending and a Content-Style Balance loss to improve content similarity across varying style intensities.

Result: Outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower IGD and GD scores.

Conclusion: The proposed method successfully broadens the content-style frontier and improves content preservation across different style intensities in text-to-image generation.

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [154] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: This paper introduces Fire-ART dataset and a panoramic image-based reconstruction approach for automated firefighting asset recognition and BIM integration, achieving high accuracy in real-world validations.


<details>
  <summary>Details</summary>
Motivation: Conventional firefighting asset management methods are inefficient due to limited automated recognition and reconstruction capabilities, creating a need for better digital management solutions.

Method: Developed the Fire-ART dataset with 15 asset types (2,626 images, 6,627 instances) and a reconstruction approach using modified cube-map conversion and radius-based spherical camera projection for semantic BIM enrichment.

Result: Achieved F1-scores of 73% and 88% with localization errors of 0.620 and 0.428 meters respectively in two real-world case studies, demonstrating high recognition accuracy.

Conclusion: The Fire-ART dataset and reconstruction approach provide valuable resources and robust technical solutions for enhancing accurate digital management of fire safety equipment.

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [155] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: A training-free explanation method that uses smooth tunable contours instead of dense masks for vision model explanations, achieving high fidelity with compact, interpretable regions and improved consistency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of fragmented and overfitted dense perturbation masks in vision model explanations, which require careful post-processing and lack interpretability.

Method: Parameterizes star-convex regions using truncated Fourier series and optimizes them under an extremal preserve/delete objective using classifier gradients, guaranteeing single connected masks with orders of magnitude fewer parameters.

Result: Matches extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency, achieving higher relevance mass and lower complexity than baselines, with 15%+ improvement on DINO models.

Conclusion: The contour-based approach provides faithful yet compact explanations that are robust to adversarial artifacts, enables transparent fidelity-area profiling, and can be extended to multi-contour localization within the same framework.

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [156] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: Proposes a distillation framework for efficient causal video generation using adversarial self-distillation and first-frame enhancement to achieve high-quality video synthesis with extremely limited denoising steps (1-2 steps).


<details>
  <summary>Details</summary>
Motivation: To address the error accumulation and long inference times in current hybrid video generation models that combine autoregressive temporal dynamics with diffusion-based spatial denoising.

Method: Uses Distribution Matching Distillation (DMD) framework with novel Adversarial Self-Distillation (ASD) strategy and First-Frame Enhancement (FFE) strategy that allocates more denoising steps to initial frames while applying larger skipping steps to later frames.

Result: Surpasses state-of-the-art approaches in both one-step and two-step video generation on VBench benchmarks, producing a single distilled model that flexibly supports multiple inference-step settings.

Conclusion: The framework enables efficient, high-quality video synthesis without repeated re-distillation, substantially improving training stability and generation quality in extremely few-step scenarios.

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [157] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: A resolution-aware token decoder for off-road semantic segmentation that balances global semantics, local consistency, and boundary fidelity while being robust to imperfect supervision and label noise.


<details>
  <summary>Details</summary>
Motivation: Off-road semantic segmentation suffers from thick inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Existing methods either blur edges at low resolution or are costly and fragile to noise at high resolution.

Method: Uses a resolution-aware token decoder with global self-attention and lightweight dilated depthwise refinement for local coherence. Incorporates gated cross-attention to integrate fine-scale features without amplifying noise, and class-aware point refinement for residual ambiguities. Adds boundary-band consistency regularizer during training.

Result: The approach achieves competitive performance and improved stability across transitions in off-road semantic segmentation.

Conclusion: The proposed method effectively addresses the challenges of off-road semantic segmentation by balancing computational efficiency with boundary fidelity and robustness to imperfect supervision.

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [158] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: The paper proposes a training-only method to improve thermal-infrared object detection by using contrastive learning and cross-modal feature alignment with RGB-trained teacher models, achieving state-of-the-art performance without requiring RGB input during inference.


<details>
  <summary>Details</summary>
Motivation: Thermal-infrared detection suffers from low contrast and weak high-frequency cues, causing duplicate detections, missed small objects, and class confusion. Existing approaches either translate TIR to RGB (vulnerable to artifacts) or fuse RGB-TIR at test time (requiring extra sensors and calibration), but neither directly improves the thermal representation itself.

Method: The method introduces training-only objectives: (1) contrastive learning to sharpen instance-level decision boundaries by pulling same-class features together and pushing different-class features apart, suppressing duplicates and confusion; (2) cross-modal feature alignment where the student's multi-level pyramid features are aligned with an RGB-trained teacher, strengthening texture-poor thermal features without visible input at test time.

Result: The proposed method outperformed prior approaches and achieved state-of-the-art performance in thermal-infrared object detection.

Conclusion: By directly shaping thermal representations during training through contrastive learning and cross-modal feature alignment, the method effectively addresses the core challenges of thermal-infrared detection while maintaining mono-modality inference, making it practical for real-world deployment.

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [159] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff is a diffusion-aided decoding framework that enhances security and robustness of deep joint source-channel coding against physical-layer attacks like pilot spoofing and subcarrier jamming, achieving better trade-off between reconstruction quality and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing JSCC frameworks are vulnerable to physical-layer adversarial threats that compromise semantic fidelity, creating a need for more secure and robust semantic communication systems.

Method: Uses pseudoinverse-guided sampling and adaptive guidance weighting for flexible step-size control; employs power-based subcarrier masking for jamming attacks and formulates channel estimation as blind inverse problem with EM-driven reconstruction algorithm; alternates between pilot recovery and channel estimation during diffusion process.

Result: Extensive experiments show SecDiff outperforms existing secure and generative JSCC baselines over OFDM channels under adversarial conditions.

Conclusion: SecDiff represents a promising step toward practical, low-latency, and attack-resilient semantic communications with favorable trade-off between reconstruction quality and computational cost.

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [160] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: EPAN is a dual-branch network for person re-identification that achieves 90.09% Rank-1 accuracy and 78.82% mAP on the Inspection-Personnel dataset, demonstrating robust performance for IoT surveillance applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of person re-identification in IoT-enabled smart environments, particularly dealing with perspective and environmental changes across diverse surveillance conditions.

Method: EPAN employs a dual-branch architecture designed to extract alignment information under varying scales and viewpoints, mitigating the impact of perspective and environmental changes.

Result: Achieved outstanding performance with 90.09% Rank-1 accuracy and 78.82% mean Average Precision (mAP) on the Inspection-Personnel dataset.

Conclusion: EPAN demonstrates strong potential for real-world IoT applications, enabling effective and reliable person re-identification across diverse cameras in surveillance and security systems.

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [161] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: LASQ reformulates low-light image enhancement as a statistical sampling process using power-law distributed luminance transitions, enabling unsupervised enhancement without normal-light references.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on deterministic pixel-level mappings between paired low/normal-light images, which fail when normal-light references are unavailable and neglect continuous physical luminance transitions in real environments.

Method: Introduces Luminance-Aware Statistical Quantification (LASQ) that models luminance transitions as power-law distributions, uses stratified power functions for approximation, and employs a diffusion forward process to discover optimal transition paths between luminance layers.

Result: Achieves superior performance on domain-specific datasets with normal-light references and demonstrates better generalization ability across non-reference datasets, enabling more adaptable light restoration.

Conclusion: LASQ provides a probabilistic framework that effectively balances reconstruction fidelity with cross-scenario generalization, making low-light image enhancement more practical for real-world applications where normal-light references are unavailable.

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [162] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: A system for controlled authoring and editing of textures with local characteristics like stains, tears, and holes using unsupervised learning and diffusion models.


<details>
  <summary>Details</summary>
Motivation: To generate realistic textures by including natural alterations like stains and tears that are ubiquitous in nature, without requiring manual annotations.

Method: Uses unsupervised anomaly detection to identify appearance-altering features, clusters them semantically, and employs diffusion-based editing for conditional generation of textures.

Result: Creates a versatile generative model from small image collections that enables interactive painting of features on textures of arbitrary size.

Conclusion: The pipeline successfully automates texture editing with natural blemishes, and the introduced algorithms for diffusion-based editing and infinite texture generation are generic and applicable to other contexts.

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [163] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: NSYNC presents a contrastive learning framework using synthetic negative images to improve text-to-image diffusion models' ability to capture specific artistic styles by orthogonalizing gradients.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models generate realistic images but fail to capture specific artistic styles, and simple finetuning on style datasets doesn't effectively grasp style features.

Method: Uses synthetic image generation to create negative examples, applies contrastive training where positive and negative gradients are computed, then refines positive gradient by subtracting its projection onto negative gradient to obtain orthogonal component for parameter updates.

Result: Experiments on various painter and illustrator styles show improved performance over baseline methods both quantitatively and qualitatively.

Conclusion: The proposed contrastive learning framework with synthetic negative sets effectively enhances stylization capability of text-to-image diffusion models by eliminating trivial shared attributes and focusing on unique style features.

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [164] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: PCD-ReID algorithm uses Transformer-based network to extract shared component features for occluded pedestrian re-identification, achieving 79.0% mAP and 82.7% Rank-1 accuracy with 15.9% improvement over ResNet50.


<details>
  <summary>Details</summary>
Motivation: Occluded pedestrian re-identification in surveillance faces challenges as occlusions obscure key body features, and traditional ResNet-based methods fail to address occlusions effectively.

Method: Transformer-based PCD network extracts shared component features (helmets, uniforms) and uses new real-world patrol surveillance dataset with 10,000 individuals and 50,000+ images collected over six months.

Result: Achieves 79.0% mAP and 82.7% Rank-1 accuracy, marking 15.9% Rank-1 improvement over ResNet50-based methods.

Conclusion: PCD-ReID effectively achieves occlusion-aware ReID performance for tower inspection scenarios, showing potential for practical deployment in surveillance and security applications.

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [165] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: NOA is an open-source GUI plugin for napari that enables AI-based organoid image analysis without programming, integrating detection, segmentation, tracking, feature extraction, and ML prediction capabilities.


<details>
  <summary>Details</summary>
Motivation: AI tools for organoid analysis are powerful but inaccessible to biologists without programming skills, leading to manual workflows and limited tool availability for comprehensive analysis.

Method: Developed Napari Organoid Analyzer (NOA) as a graphical user interface plugin that integrates multiple state-of-the-art algorithms for detection, segmentation, tracking, feature extraction, annotation, and ML-based prediction.

Result: Demonstrated NOA's versatility through three case studies: quantifying morphological changes during differentiation, assessing phototoxicity effects, and predicting organoid viability and differentiation state.

Conclusion: NOA provides an accessible and extensible framework that enables comprehensive AI-driven organoid image analysis for biologists without programming expertise.

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [166] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: Proposes using DC-GAN to generate synthetic MRI images to address limited medical data, and validates quality through CNN classification showing comparable performance between real and synthetic images.


<details>
  <summary>Details</summary>
Motivation: Limited availability of original MRI data creates challenges for medical imaging tasks, requiring synthetic data generation to overcome data scarcity in medical deep learning applications.

Method: Uses Deep Convolutional Generative Adversarial Network (DC-GAN) for synthetic MRI generation and Convolutional Neural Network (CNN) classifier to evaluate synthetic image quality through brain tumor classification tasks.

Result: CNN classification demonstrates comparable performance on both real and synthetic MRI images, validating the effectiveness of GAN-generated synthetic medical images for downstream applications.

Conclusion: GAN-based synthetic MRI generation is an effective solution for data scarcity in medical imaging, with synthetic images proving useful for practical downstream tasks like brain tumor classification.

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [167] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: CDD-VT proposes a dualistic visual tokenizer that adaptively combines continuous and discrete tokenization approaches based on image complexity, achieving superior performance while maintaining model simplicity.


<details>
  <summary>Details</summary>
Motivation: To resolve the dichotomy between continuous tokenizers (complex pipelines but strong performance) and discrete tokenizers (simple but with information loss) in multi-modal large models.

Method: Uses adaptive primitive allocation where simple images use few primitives (discrete-like) and complex images use many primitives (continuous-like), with Diverse Quantitative Primitives for orthogonality and Dynamic Primitive Allocator for complexity assessment.

Result: Achieves superior performance in reconstruction, retrieval and classification tasks compared to specialized continuous and discrete tokenizers, while maintaining a concise and scalable MLLM architecture.

Conclusion: The wave-particle duality-inspired approach successfully bridges the gap between continuous and discrete visual tokenization, providing a flexible and effective solution for unified understanding and generation in MLLMs.

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [168] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM is a lightweight adaptation of ENSAM architecture for efficient volumetric tumor segmentation from CT scans with RECIST annotations, achieving competitive performance in MICCAI FLARE 2025 Task 1.


<details>
  <summary>Details</summary>
Motivation: Current RECIST v1.1 standard relies on single-plane diameter measurements, which are less reliable than volumetric measurements for assessing treatment response. Manual volumetric annotation is labor-intensive, limiting clinical adoption of volumetric assessment.

Method: Lite ENSAM is a lightweight adaptation of the ENSAM architecture designed specifically for efficient volumetric tumor segmentation from CT scans that have RECIST annotations.

Result: Achieved Dice Similarity Coefficient (DSC) of 60.7% and Normalized Surface Dice (NSD) of 63.6% on hidden test set, with average total RAM time of 50.6 GBs and average inference time of 14.4 seconds on CPU on public validation dataset.

Conclusion: Lite ENSAM provides an efficient solution for volumetric tumor segmentation that could help overcome the clinical adoption barriers of volumetric assessment by reducing the labor-intensive nature of manual annotation.

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [169] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: The paper introduces PSPooling, a non-learnable mesh pooling operator for efficient 3D anatomical shape analysis, and MedShapeNet19, a curated benchmark dataset for anatomical shape classification.


<details>
  <summary>Details</summary>
Motivation: Progress in anatomical 3D shape classification is limited by mesh data complexity and lack of standardized benchmarks, highlighting the need for robust learning methods and reproducible evaluation.

Method: Proposes Precomputed Structural Pooling (PSPooling) for efficient graph coarsening, and integrates it into a self-supervised graph autoencoder that learns anatomy-aware representations from unlabeled surface meshes.

Result: PSPooling significantly improves reconstruction fidelity and classification accuracy in low-label regimes on the MedShapeNet19 benchmark dataset.

Conclusion: Establishes a strong baseline for medical 3D shape learning and provides MedShapeNet19 as a widely adopted benchmark for anatomical shape classification research.

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [170] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: Vote-in-Context (ViC) is a training-free framework that transforms list-wise reranking and fusion into a zero-shot reasoning task for Vision-Language Models, achieving state-of-the-art performance in cross-modal video retrieval.


<details>
  <summary>Details</summary>
Motivation: Traditional fusion techniques in retrieval rely only on rank or score signals and ignore candidates' representations, which is particularly challenging for complex multi-modal data like videos.

Method: ViC serializes content evidence and retriever metadata directly in VLM prompts, using S-Grid (compact image grids with optional subtitles) to represent videos for list-wise reasoning, allowing adaptive weighting of retriever consensus vs. visual-linguistic content.

Result: ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, with gains up to +40 Recall@1 over previous state-of-the-art baselines, establishing new zero-shot retrieval performance records.

Conclusion: ViC provides a simple, reproducible, and highly effective approach for turning modern VLMs into powerful zero-shot rerankers and fusers, demonstrating strong performance in handling complex visual and temporal signals alongside text.

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [171] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: The paper introduces Viewpoint Learning to evaluate and improve MLLMs' spatial reasoning capabilities using a 100K dataset and two-stage fine-tuning approach.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs excel at 2D visual understanding but struggle with capturing detailed spatial information needed for robust 3D reasoning, particularly cross-view consistency.

Method: Two-stage fine-tuning: 1) Supervised Fine-Tuning on Viewpoint-100K dataset (100K object-centric image pairs with QA pairs), 2) Reinforcement Learning using GRPO algorithm. Also includes hybrid cold-start initialization for viewpoint representation learning.

Result: Significant improvements in spatial reasoning ability across multiple tasks, with enhanced performance on both in-domain and out-of-domain reasoning tasks.

Conclusion: Developing foundational spatial skills in MLLMs is valuable for advancing robotics, autonomous systems, and 3D scene understanding.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [172] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: This paper proposes an adaptive RL framework for diffusion-based image restoration models that uses MLLM-based IQA models as reward functions, focusing on challenging samples and dynamically combining RL with SFT based on sample difficulty.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods are suboptimal for diffusion-based image restoration because they don't account for the fundamental difference between restoration (emphasizing fidelity) and pure generation tasks.

Method: Uses MLLM-based IQA models as reward functions, focuses RL on challenging samples far from ground truth, and adaptively combines RL with SFT using automatic weighting based on sample difficulty.

Result: The proposed framework effectively boosts performance across various restoration tasks and demonstrates effectiveness through extensive experiments on multiple benchmarks.

Conclusion: The plug-and-play RL strategy can be seamlessly applied to diffusion-based restoration models and provides significant performance improvements by properly addressing the fidelity emphasis in restoration tasks.

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [173] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos is a unified relighting framework that uses RGB-space geometry feedback and path consistency learning to achieve physically plausible lighting effects with 20x speedup for both images and videos.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for relighting often produce unrealistic results due to optimization in semantic latent space, leading to issues like overexposed highlights, misaligned shadows, and incorrect occlusions.

Method: Proposes UniLumos framework that brings RGB-space geometry feedback into flow matching backbone, supervised by depth and normal maps extracted from outputs. Uses path consistency learning for efficient few-step training and introduces structured 6D annotation protocol for fine-grained control.

Result: Achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering 20x speedup for both image and video relighting. Also introduces LumosBench benchmark for automatic evaluation.

Conclusion: UniLumos successfully addresses physical plausibility issues in relighting through geometry feedback and efficient training, establishing new state-of-the-art performance with substantial speed improvements.

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [174] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: The paper proposes a progressive network architecture for synthesizing IHC-equivalent images from H&E-stained slides, addressing limitations in existing stain translation methods by optimizing color and structural aspects in a decoupled manner.


<details>
  <summary>Details</summary>
Motivation: IHC staining is costly and labor-intensive with limited scalability, while existing computational stain translation techniques suffer from suboptimal image quality due to interdependent loss components that fail to simultaneously preserve structural authenticity and color fidelity.

Method: A novel progressive network architecture with color and cell border generation logic, built upon Adaptive Supervised PatchNCE framework with additional loss functions based on DAB chromogen concentration and image gradient to enhance color fidelity and cell boundary clarity.

Result: Experiments on HER2 and ER datasets showed significant improvements in visual quality and finer structural details in the generated IHC images compared to baseline methods.

Conclusion: The proposed progressive structure-color-cell boundary mechanism effectively addresses the limitations of traditional stain translation approaches, enabling more efficient and cost-effective extraction of protein-level information from H&E slides.

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [175] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: LFRD2 is a hybrid framework combining neural networks with physical modeling to improve under-display ToF depth sensing through TOLED screens, addressing signal attenuation, multi-path interference, and noise using learnable fractional reaction-diffusion dynamics.


<details>
  <summary>Details</summary>
Motivation: Under-display ToF imaging suffers from severe degradations when placed beneath transparent OLED screens, including signal attenuation, multi-path interference, and temporal noise that compromise depth quality.

Method: Proposes Learnable Fractional Reaction-Diffusion Dynamics (LFRD2) with time-fractional reaction-diffusion module for iterative depth refinement and continuous convolution operator via coefficient prediction and repeated differentiation.

Result: Experiments on four benchmark datasets demonstrate the effectiveness of the approach in improving depth sensing quality through TOLED layers.

Conclusion: LFRD2 successfully combines neural network expressiveness with physical model interpretability to address under-display ToF imaging challenges, with code publicly available.

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [176] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench is the first benchmark for evaluating probabilistic robustness (PR) training methods, comparing adversarial training (AT) and PR-targeted methods across multiple metrics including clean accuracy, PR/AR performance, training efficiency, and generalization error.


<details>
  <summary>Details</summary>
Motivation: While probabilistic robustness (PR) is a practical complement to adversarial robustness (AR), dedicated PR training methods are underexplored with limitations including non-comparable evaluation protocols, limited comparisons to strong AT baselines, and no unified framework for generalization comparison.

Method: PRBench empirically compares common AT and PR-targeted training methods using comprehensive metrics (clean accuracy, PR/AR performance, training efficiency, generalization error) and provides theoretical analysis on generalization error of PR performance across different training methods.

Result: AT methods are more versatile in improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower generalization error and higher clean accuracy. A leaderboard with 222 trained models across 7 datasets and 10 architectures is available.

Conclusion: PRBench provides the first comprehensive benchmark for PR training evaluation, revealing key trade-offs between AT and PR-targeted methods and establishing a standardized framework for future research in probabilistic robustness.

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [177] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: A task explorer pipeline using clustering, factor analysis, and string edit distance to automatically identify global/local strategies and hierarchical subtasks in human-machine interaction tasks.


<details>
  <summary>Details</summary>
Motivation: To advance machines' understanding of user knowledge, skill, and behavior for implicit coordination in anticipatory human-machine interaction.

Method: Developed a task explorer pipeline using clustering techniques paired with factor analysis and string edit distance to identify global strategies (generalized action sets) and local strategies (similar action compositions), plus hierarchical subtask structures.

Result: The pipeline successfully identified key strategies used to complete tasks and encoded user runs with hierarchical subtask structures. A Task Explorer application was also developed for reviewing results.

Conclusion: The pipeline is adaptable to any action-based time-series data and the identified strategies/subtasks help inform both humans and machines about user knowledge, skill, and behavior.

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [178] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR is an enhanced real-time detection transformer for pneumonia detection in chest X-rays, achieving 82.2% mAP@0.5 with improved multi-scale feature extraction and efficient attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Pneumonia is a major global health concern requiring accurate automated detection. While transformer-based detectors show promise in object detection, their application to medical imaging like pneumonia detection in chest X-rays remains underexplored.

Method: Proposed CGF-DETR with XFABlock for multi-scale feature extraction using convolutional attention and CSP architecture, SPGA module for efficient feature aggregation with dynamic gating and single-head self-attention, and GCFC3 neck for enhanced feature representation via multi-path convolution fusion with structural re-parameterization.

Result: Achieves 82.2% mAP@0.5 on RSNA Pneumonia Detection dataset, outperforming baseline RT-DETR-l by 3.7% while maintaining comparable inference speed at 48.1 FPS. Complete model achieves 50.4% mAP@[0.5:0.95].

Conclusion: CGF-DETR effectively improves pneumonia detection performance through novel architectural enhancements while maintaining real-time inference capability, with each proposed module contributing meaningfully to overall performance gains.

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [179] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: HGFreNet is a novel GraphFormer architecture for 2D-to-3D human pose lifting that addresses depth ambiguity and temporal incoherence through hop-hybrid graph attention and frequency domain trajectory consistency.


<details>
  <summary>Details</summary>
Motivation: Previous methods for 2D-to-3D human pose lifting suffer from depth ambiguity and temporal incoherence due to errors in 2D pose estimation, primarily focusing on local temporal constraints while neglecting global spatial-temporal correlations of skeletal joint motion.

Method: Proposed HGFreNet with hop-hybrid graph attention (HGA) module and Transformer encoder to model global joint spatial-temporal correlations. HGA groups k-hop neighbors into hybrid groups for larger receptive fields, and applies frequency domain constraints for 3D trajectory consistency. A preliminary network estimates 3D pose for depth inference across frames.

Result: Extensive experiments on Human3.6M and MPI-INF-3DHP benchmarks show HGFreNet outperforms state-of-the-art methods in both positional accuracy and temporal consistency.

Conclusion: The proposed HGFreNet effectively addresses temporal incoherence in 2D-to-3D pose lifting by modeling global spatial-temporal correlations through hop-hybrid graph attention and frequency domain trajectory constraints, achieving superior performance over existing methods.

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [180] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION is a unified autonomous driving model that efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and temporal sequences using linear group RNN operators, achieving competitive performance across 3D perception, prediction, and planning tasks without requiring explicit fusion modules.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic attention mechanisms that introduce significant computational overhead when processing long-sequence data, which is problematic for autonomous driving applications that require processing large-scale sensor data.

Method: Uses linear group RNN operator (performs linear RNN for grouped features) to efficiently handle LiDAR point clouds, multi-view images, and temporal sequences. The architecture serves as a single versatile structure supporting multiple specialized variants without explicit temporal or multi-modal fusion modules.

Result: Consistently delivers competitive and state-of-the-art performance across core autonomous driving tasks including 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation, motion prediction, and end-to-end planning.

Conclusion: UniLION offers a unified paradigm that simplifies multi-modal and multi-task autonomous driving system design while maintaining superior performance, providing a fresh perspective on developing 3D foundation models for autonomous driving.

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: A prompt-driven GraphRAG framework that enhances multi-hop question answering through improved prompt design for entity extraction, fact selection, and passage reranking, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current GraphRAG approaches focus on graph-based retrieval but under-examine the influence of prompt design on enhancing retrieval and reasoning processes for complex multi-hop question answering.

Method: Creates symbolic knowledge graphs from text data using entity and fact triples, uses LLMs for semantic filtering and answer generation, and employs entity-guided graph traversal via Personalized PageRank for efficient retrieval.

Result: Achieved state-of-the-art performance on HotpotQA (80.7% F1, 97.1% Recall@5) and 2WikiMultiHopQA (78.9% F1, 98.1% Recall@5), demonstrating significant improvements in retrieval accuracy and response quality.

Conclusion: Prompt design is crucial for improving retrieval accuracy and response quality in multi-hop question answering, laying groundwork for more efficient and comprehensible systems through prompt-aware graph reasoning.

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: Scitextures dataset contains 100,000+ images from 1,200+ scientific models across multiple domains, enabling AI evaluation on linking visual patterns to their generating mechanisms and code.


<details>
  <summary>Details</summary>
Motivation: To explore the deep connection between visual patterns and the underlying processes that form them across science, technology, and art domains.

Method: Created an agentic AI pipeline that autonomously collects and implements models in standardized form, then uses this dataset to evaluate AI models' ability to link patterns to generating mechanisms and code.

Result: Vision-language models demonstrated capability to understand and simulate physical systems beyond just visual patterns, successfully identifying, modeling, and coding mechanisms from real-world patterns.

Conclusion: The Scitextures dataset provides a comprehensive benchmark for evaluating AI's understanding of the relationship between visual patterns and their generating processes across scientific domains.

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: TIR-Bench is a new benchmark for evaluating agentic thinking-with-images capabilities in multimodal models, testing 13 diverse tasks requiring novel tool use for image processing in chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like Visual Search only test basic operations and fail to capture advanced thinking-with-images capabilities that modern models like OpenAI o3 demonstrate through dynamic, tool-dependent reasoning.

Method: Created TIR-Bench with 13 diverse tasks requiring novel tool use for image processing and manipulation in chain-of-thought, then evaluated 22 MLLMs including open-source, proprietary, and tool-use augmented models.

Result: TIR-Bench proved universally challenging for all tested models, showing that strong performance requires genuine thinking-with-images capabilities. Also conducted pilot study comparing direct vs agentic fine-tuning approaches.

Conclusion: The benchmark successfully captures advanced thinking-with-images capabilities that existing benchmarks miss, highlighting the need for more sophisticated evaluation methods for modern multimodal reasoning systems.

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [184] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: This paper reviews GenAI applications in automotive industry including vehicle design, manufacturing, autonomous driving, and in-vehicle UX, highlighting technologies like GANs and VAEs, opportunities in synthetic data generation and personalized interfaces, and challenges in computational demands, bias, and safety.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of Generative AI's transformative potential in automotive industry, bridging gaps in existing literature by focusing on voice-based HMI and connecting safety with user experience perspectives.

Method: Comprehensive literature review combined with a case study analysis of Mercedes-Benz's MBUX Virtual Assistant to illustrate practical applications and compare GenAI-powered systems with legacy rule-based assistants.

Result: Identified key opportunities for GenAI in accelerating autonomous driving validation, optimizing component design, and enhancing human-machine interaction through personalized interfaces, while highlighting significant technical, ethical, and safety challenges.

Conclusion: GenAI shows great promise for automotive applications but requires addressing computational demands, bias, intellectual property, and adversarial robustness challenges for responsible deployment; future research should focus on safer, more efficient, and user-centric mobility solutions.

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [185] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: STRIDER improves zero-shot vision-language navigation by optimizing decision space with spatial constraints and task feedback, achieving 20.7% relative improvement in success rate.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in zero-shot VLN-CE due to lack of structured decision-making and insufficient integration of feedback from previous actions, leading to poor navigation alignment.

Method: Proposes STRIDER framework with Structured Waypoint Generator for spatial constraint and Task-Alignment Regulator for dynamic task feedback adjustment.

Result: Improves Success Rate from 29% to 35% (20.7% relative gain) on R2R-CE and RxR-CE benchmarks, significantly outperforming SOTA methods.

Conclusion: Spatially constrained decision-making and feedback-guided execution are crucial for improving navigation fidelity in zero-shot VLN-CE tasks.

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [186] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: BiBo enables off-the-shelf Vision-Language Models to control humanoid agents through an embodied instruction compiler and diffusion-based motion executor, achieving 90.2% task success rate in open environments.


<details>
  <summary>Details</summary>
Motivation: Humanoid agents struggle with flexible interactions in open environments, and collecting massive training datasets is prohibitively expensive. The paper explores leveraging VLMs' open-world generalization to avoid extensive data collection.

Method: BiBo consists of two components: (1) an embodied instruction compiler that enables VLMs to perceive environments and translate high-level instructions into low-level commands, and (2) a diffusion-based motion executor that generates human-like motions while adapting to physical feedback.

Result: BiBo achieves 90.2% interaction task success rate in open environments and improves text-guided motion execution precision by 16.3% over prior methods.

Conclusion: BiBo demonstrates that off-the-shelf VLMs can effectively control humanoid agents for diverse interactions in open environments, reducing the need for expensive data collection while maintaining high performance.

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [187] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: AR1 is a vision-language-action model that integrates Chain of Causation reasoning with trajectory planning to improve autonomous driving decision-making in complex scenarios, achieving significant improvements in planning accuracy and safety metrics.


<details>
  <summary>Details</summary>
Motivation: End-to-end architectures trained via imitation learning have limitations in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. The goal is to enhance decision-making by integrating reasoning capabilities with trajectory planning.

Method: Three key innovations: (1) Chain of Causation dataset built through hybrid auto-labeling and human-in-the-loop pipeline; (2) Modular VLA architecture combining Cosmos-Reason VLM with diffusion-based trajectory decoder; (3) Multi-stage training using supervised fine-tuning and reinforcement learning with large reasoning model feedback.

Result: 12% improvement in planning accuracy on challenging cases, 35% reduction in off-road rate, 25% reduction in close encounter rate. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Real-time performance (99 ms latency) confirmed in on-vehicle road tests.

Conclusion: AR1 demonstrates a practical path towards Level 4 autonomous driving by bridging interpretable reasoning with precise control. The model shows consistent improvements with scaling and successful urban deployment.

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [188] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: A novel framework using Digital Twin technology for autonomous reconfiguration of robotic controllers in dynamic environments, enabling rapid adaptation through virtual simulation and optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional robotic control systems struggle to adapt quickly to continuously evolving environments in smart cities, precision farming, and industrial automation, leading to inefficiencies and operational failures.

Method: Leverages Digital Twin technology to create a virtual replica of the robot's operational environment, simulating and optimizing movement trajectories in response to real-world changes, then deploying updated control parameters to the physical robot.

Result: Enables rapid and reliable adaptation of robotic systems without manual intervention by recalculating paths and control parameters in the Digital Twin and deploying updates to the physical robot.

Conclusion: Advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments through autonomous dynamic reconfiguration of robotic controllers.

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [189] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: Real-DRL is a framework for safety-critical autonomous systems that enables runtime learning of DRL agents in real physical systems while prioritizing safety through three interactive components: DRL-Student, PHY-Teacher, and Trigger.


<details>
  <summary>Details</summary>
Motivation: To address safety challenges in autonomous systems caused by unknown unknowns and the Sim2Real gap, while enabling safe runtime learning in real physical systems.

Method: Three-component framework: DRL-Student (dual self-learning and teaching-to-learn paradigm with safety-informed batch sampling), PHY-Teacher (physics-model-based safety-critical action policies with real-time patching), and Trigger (manages interaction between components).

Result: Experiments with real quadruped robot, NVIDIA Isaac Gym quadruped robot, and cart-pole system demonstrate effectiveness, showing assured safety, automatic hierarchy learning, and addressing learning experience imbalance from corner cases.

Conclusion: Real-DRL effectively addresses safety challenges in autonomous systems through its interactive three-component framework, enabling safe runtime learning while maintaining high performance.

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [190] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: Proposes a Shared Autonomy framework for dexterous manipulation that divides control between human-guided macro motions and autonomous fine-grained hand control, enabling efficient collection of high-quality training data and achieving 90% success rate.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of scarce high-quality training data for Vision-Language-Action models in dexterous manipulation, overcoming limitations of manual teleoperation and automated planning methods.

Method: Shared Autonomy framework with human VR teleoperation for arm control and autonomous DexGrasp-VLA policy for hand control using tactile/visual feedback. Includes Arm-Hand Feature Enhancement module and Corrective Teleoperation for continuous improvement.

Result: Achieves 90% success rate across diverse objects including unseen instances, generates high-quality data with minimal manpower, and demonstrates effective dexterous manipulation capabilities.

Conclusion: The framework successfully enables scalable collection of high-quality dexterous manipulation data and develops robust manipulation policies through human-in-the-loop shared autonomy.

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [191] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: EgoMI is a framework that captures synchronized end-effector and active head trajectories to bridge the embodiment gap in imitation learning from human demonstrations, using a memory-augmented policy to handle dynamic viewpoint changes.


<details>
  <summary>Details</summary>
Motivation: Imitation learning from human demonstrations faces challenges due to the embodiment gap - humans actively coordinate head and hand movements with dynamic viewpoint changes that static robot sensing systems cannot replicate, causing distribution shift and degraded policy performance.

Method: EgoMI framework captures synchronized end-effector and active head trajectories during manipulation tasks, retargetable to semi-humanoid robots. Uses memory-augmented policy to selectively incorporate historical observations to handle rapid head viewpoint changes.

Result: Policies with explicit head-motion modeling consistently outperform baseline methods on a bimanual robot with actuated camera head, effectively bridging the human-robot embodiment gap.

Conclusion: Coordinated hand-eye learning with EgoMI enables robust imitation learning on semi-humanoid embodiments by addressing the fundamental challenges of dynamic viewpoint coordination in human manipulation behavior.

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [192] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: Foundation models like Chronos can forecast synthetic reaching trials from early subsets (8-16 trials) to replace the full 40-64 trial requirement in Kinarm VGR assessments, reducing testing time from 4-5 minutes to about 1 minute while maintaining kinematic parameter reliability.


<details>
  <summary>Details</summary>
Motivation: Standard Visually Guided Reaching (VGR) on Kinarm robot requires 40-64 reaches, creating time and fatigue burdens for patients, especially stroke survivors. There's a need to reduce assessment time while preserving the reliability of kinematic biomarkers.

Method: Analyzed VGR speed signals from 461 stroke and 599 control participants. Withheld all but first 8 or 16 trials, then used ARIMA, MOMENT, and Chronos models (fine-tuned on 70% of subjects) to forecast synthetic trials. Recomputed kinematic features on combined recorded plus forecasted trials and compared to full-length references using ICC(2,1).

Result: Chronos forecasts restored ICC >= 0.90 for all parameters with only 8 recorded trials plus forecasts, matching reliability of 24-28 recorded reaches. MOMENT showed intermediate gains, ARIMA minimal improvements. Synthetic trials replaced reaches without compromising feature reliability across cohorts and protocols.

Conclusion: Foundation-model forecasting can significantly shorten Kinarm VGR assessment time, reducing sessions from 4-5 minutes to about 1 minute for severely impaired stroke survivors while preserving kinematic precision, enabling more efficient robotic evaluations of motor impairments.

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [193] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: Proprioceptively-tailored robotic training improves hand function and neural processing in stroke survivors with proprioceptive deficits.


<details>
  <summary>Details</summary>
Motivation: To test whether precision rehabilitation through proprioceptively-tailored robotic training can improve hand function and neural processing in stroke survivors.

Method: Randomized controlled trial with 46 chronic stroke survivors using robotic finger exoskeleton for two proprioceptively-tailored approaches: Propriopixel Training (gamified movements) and Virtual Assistance Training (reduced robotic aid).

Result: Participants with proprioceptive deficits showed significant hand function improvements: Propriopixel (7 ± 4.2 blocks, p=0.002) and Virtual Assistance (4.5 ± 4.4 blocks, p=0.068) vs Standard training (0.8 ± 2.3 blocks). Enhanced neural sensitivity to proprioceptive cues measured via EEG biomarker.

Conclusion: Proprioceptively-tailored training is an effective pathway for precision neurorehabilitation, with improvements correlating with enhanced neural processing.

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [194] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: This paper establishes theoretical connections between Sliding Window-Factor Graph Optimization (SW-FGO) and Kalman Filter Variants (KFV), proposing a Recursive FGO framework that can regenerate EKF/IEKF/REKF/RIEKF under specific conditions while highlighting SW-FGO's advantages in nonlinear, non-Gaussian scenarios.


<details>
  <summary>Details</summary>
Motivation: To clarify the theoretical relationship between SW-FGO and Kalman Filter Variants (EKF, IEKF, REKF, RIEKF), as previous works focused mainly on application performance comparisons without establishing their theoretical connections.

Method: Proposed a Recursive FGO (Re-FGO) framework that connects SW-FGO and KFV under fair conditions, with analysis under explicit conditions including Markov assumption, Gaussian noise with L2 loss, and one-state window.

Result: Under the specified conditions, Re-FGO exactly regenerates to EKF/IEKF/REKF/RIEKF, while SW-FGO demonstrates measurable benefits in nonlinear, non-Gaussian regimes with predictable computational costs.

Conclusion: The paper clarifies the theoretical connection between SW-FGO and KFV, highlighting SW-FGO's unique advantages in practical applications, particularly in numerical estimation and deep learning integration.

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [195] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep is an end-to-end deep learning framework that fuses sonar and visual data using an adapted plane sweep algorithm to achieve accurate 3D reconstruction in underwater environments.


<details>
  <summary>Details</summary>
Motivation: Existing single-modality approaches fail in underwater environments - vision-based methods struggle with poor visibility while sonar suffers from elevation ambiguity and low resolution. Prior fusion techniques rely on flawed heuristics and geometric assumptions.

Method: Adapts the principled plane sweep algorithm for cross-modal fusion between sonar and visual data using an end-to-end deep learning framework.

Result: Extensive experiments in simulation and real-world environments show SonarSweep generates dense and accurate depth maps, significantly outperforming state-of-the-art methods, especially in high turbidity conditions.

Conclusion: The framework overcomes limitations of prior methods and the authors will release code and a novel synchronized stereo-camera and sonar dataset to foster further research.

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [196] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: This paper introduces a new class of coning correction algorithms derived from classical Runge-Kutta integration methods, showing how they can generate higher-order coning compensation algorithms for gyroscope integration in navigation systems.


<details>
  <summary>Details</summary>
Motivation: Gyroscope integration in strapdown navigation systems requires coning compensation to account for sensor rotation during integration, and existing algorithms need improvement for higher accuracy.

Method: The authors developed a new class of coning correction algorithms by building directly from classical Runge-Kutta integration routines, providing a systematic procedure for generating higher-order algorithms.

Result: The approach shows that a simple case collapses to one of the most popular existing coning algorithms, demonstrating the validity and effectiveness of the method.

Conclusion: The proposed Runge-Kutta-based approach provides a clear and systematic framework for developing higher-order coning compensation algorithms, offering improved accuracy for gyroscope integration in navigation systems.

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [197] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: Development of a 3D-printed bucket drum excavator for lunar regolith as part of the MoonBot robotic system, achieving continuous excavation rates of 777.54 kg/h with low energy consumption.


<details>
  <summary>Details</summary>
Motivation: ISRU is crucial for sustainable lunar access, and excavation is the first step to make lunar resources usable. This supports Japan's Moonshot program goals.

Method: Created a 3D-printed PLA prototype bucket drum for the modular MoonBot system, tested through sandbox experiments to evaluate excavation efficiency.

Result: The 4.8 kg tool achieved continuous excavation at 777.54 kg/h (0.022 Wh/kg) and batch operation at 172.02 kg/h (0.86 Wh/kg), demonstrating successful concept implementation.

Conclusion: The bucket drum is compatible with MoonBot platform enabling flexible mission planning. Future improvements could include sensor integration and autonomous control systems.

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [198] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: A novel bipedal balance control approach that uses minimal low-dimensional descriptors for balance while allowing high-dimensional freedom, resulting in human-like efficient walking.


<details>
  <summary>Details</summary>
Motivation: Traditional bipedal balance control constrains robots to simplified models, leading to inefficient walking patterns with bent knees, while bipedal balance is inherently low-dimensional.

Method: Proposes a control framework using descriptive models with minimum degrees of freedom for balance, allowing remaining degrees of freedom to evolve freely in high-dimensional space.

Result: Achieves efficient human-like walking gait and improved robustness compared to traditional approaches.

Conclusion: Bipedal balance control can be effectively achieved with minimal low-dimensional constraints while maintaining high-dimensional freedom, leading to more natural and efficient locomotion.

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [199] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: A multi-finger hybrid gripper using origami modules provides constant force/torque output, enabling passive shape adaptation and stable grasping without active sensing or control, plus simultaneous multi-object manipulation capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing soft robotics grippers often require bulky actuators, complex control strategies, or advanced tactile sensing for stable grasping, creating limitations in practical applications.

Method: Developed a multi-finger hybrid gripper with passively deformable origami modules that generate constant force and torque. Each finger uses parallel origami modules driven by a 1-DoF actuator mechanism for passive shape adaptability.

Result: The gripper achieves stable grasping force without active sensing or feedback control, and demonstrates simultaneous multi-object grasping capability for stacked objects of varied shapes and sizes, enabling independent pick-transport-place operations.

Conclusion: Origami-based compliant structures show potential as scalable modules for adaptive, stable, and efficient multi-object manipulation in domestic and industrial pick-and-place applications.

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [200] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: D3P is a dual-branch diffusion policy that uses Deep Koopman operators to improve temporal dependencies in robotic manipulation, achieving 14.6% improvement in simulation and 15.0% in real-world tasks over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based policies struggle with capturing temporal dependencies across multiple steps, especially when incorporating proprioceptive input, leading to task failures due to overfitting to proprioceptive cues.

Method: Proposes D3P with dual-branch architecture: visual branch for task progression and fused branch for precise manipulation. Uses Deep Koopman Operator for visual representation learning and test-time loss as confidence signal for action chunk aggregation.

Result: Outperforms state-of-the-art diffusion policy by 14.6% in simulation across six RLBench tasks and achieves 15.0% improvement in three real-world robotic manipulation tasks.

Conclusion: D3P effectively addresses temporal dependency issues in diffusion policies through dual-branch architecture and Deep Koopman operators, enabling robust robotic manipulation with improved performance in both simulation and real-world settings.

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [201] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: Multi-Mapcher is a novel multi-session SLAM framework that uses large-scale map-to-map registration for inter-session alignment instead of relying on loop closure detection, achieving better performance with heterogeneous LiDAR sensors.


<details>
  <summary>Details</summary>
Motivation: Existing multi-session SLAM methods rely on loop closure detection which can be degraded by differences in LiDAR sensor density and field of view, making inter-session alignment challenging.

Method: The framework uses outlier-robust 3D point cloud registration for large-scale map-to-map registration to perform inter-session initial alignment, then finds inter-session loops via radius search, and employs anchor node-based robust pose graph optimization for global map consistency.

Result: The approach shows substantially better multi-session SLAM performance for various LiDAR sensors and is faster than state-of-the-art methods.

Conclusion: Multi-Mapcher successfully challenges the existing paradigm by demonstrating that large-scale map-to-map registration is feasible for inter-session alignment, providing a more robust solution for heterogeneous LiDAR multi-session SLAM.

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [202] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: A semantics-guided fuzzy control framework using LLMs for underwater multi-robot cooperative coverage in GPS-denied environments, achieving robust navigation and efficient exploration through semantic compression and communication.


<details>
  <summary>Details</summary>
Motivation: Address challenges in underwater multi-robot cooperative coverage including partial observability, limited communication, environmental uncertainty, and lack of global localization.

Method: Uses LLMs to compress multimodal observations into semantic tokens, fuzzy inference system for navigation commands, and semantic communication for multi-robot coordination without global positioning.

Result: Extensive simulations show robust OOI-oriented navigation and cooperative coverage with improved efficiency and adaptability under limited sensing and communication.

Conclusion: The framework narrows the gap between semantic cognition and distributed underwater control in GPS-denied, map-free conditions.

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [203] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: Online framework for real-time denoising and forecasting of agent motions using modified Hankel-DMD with SVHT and Cadzow projection for variance-aware predictions.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need to predict nearby agent motions from partial, noisy data in real-time.

Method: Modified sliding-window Hankel-DMD with SVHT rank estimation, Cadzow projection for denoising, and time-varying linear predictor construction.

Result: Achieves stable variance-aware denoising and short-horizon prediction suitable for real-time control, validated in simulation and on dynamic crane testbed.

Conclusion: The method enables real-time learning of nonlinear predictive models for agent motions with noise variance tracking for risk-aware planning.

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [204] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: A learning-based bipedal locomotion framework using heuristic step-planning with Raibert-type velocity control, achieving robust walking on uneven terrain without complex analytical models.


<details>
  <summary>Details</summary>
Motivation: To enable precise humanoid robot-environment interaction for tasks like gap crossing and target approach while avoiding complex step planners and analytical models used in traditional approaches.

Method: Heuristic step-planning strategy guided by desired torso velocity tracking, with Raibert-type controller modulating foot placement based on velocity error. Compared against LIPM controller.

Result: Achieved comparable/superior velocity accuracy (up to 80%), 50%+ improvement in uneven terrain robustness, and improved energy efficiency compared to model-based approaches.

Conclusion: Complex analytical, model-based components may be unnecessary for stable and robust bipedal walking in unstructured environments, as heuristic learning-based approaches can perform equally well or better.

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [205] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro is a generalist robot policy system that uses VLM coding agents to dynamically compose perception, planning, and control modules into programmatic policies, achieving superior zero-shot performance compared to VLA models.


<details>
  <summary>Details</summary>
Motivation: To build generalist robot policies directly around VLMs rather than collecting large robotics datasets, leveraging VLM capabilities augmented with specialized robot modules.

Method: Uses a VLM coding agent to dynamically compose curated perception, planning, and control modules into programmatic policies for specific tasks and scenarios through a streamlined closed-loop interface.

Result: Largely surpasses VLA models in zero-shot performance on challenging manipulation skills, and demonstrates easy extensibility, editability for new embodiments, and adaptation from minimal real-world experiences.

Conclusion: The modular approach using VLM coding agents provides a flexible and effective alternative to large end-to-end models for building generalist robot policies.

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [206] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay is an end-to-end zero-shot VLN-CE framework that uses only three frontal RGB-D images with natural language instructions, eliminating panoramic views and waypoint predictors to reduce latency while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLN-CE methods rely on panoramic observations and two-stage pipelines with waypoint predictors, which introduce significant latency and limit real-world applicability.

Method: Uses three frontal RGB-D images with natural language instructions to enable MLLMs to directly predict actions. Includes Uncertainty-Aware Reasoning module with Disambiguation Module and Future-Past Bidirectional Reasoning for robust decision-making.

Result: Significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines in both simulated and real-robot environments.

Conclusion: Demonstrates practicality and effectiveness for real-world zero-shot embodied navigation by eliminating latency bottlenecks while maintaining navigation performance.

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [207] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: URDF-Anything is an end-to-end automatic framework that uses 3D multimodal large language models to reconstruct digital twins of articulated objects, jointly optimizing geometric segmentation and kinematic parameter prediction.


<details>
  <summary>Details</summary>
Motivation: Current methods for constructing digital twins of articulated objects require manual modeling or multi-stage pipelines, which are inefficient and time-consuming for robotic simulation training and embodied AI world model building.

Method: Uses autoregressive prediction with point-cloud and text multimodal input, implements specialized [SEG] token mechanism that interacts with point cloud features for fine-grained part-level segmentation while maintaining kinematic consistency.

Result: Significantly outperforms existing approaches: 17% improvement in geometric segmentation (mIoU), 29% reduction in kinematic parameter prediction error, and 50% improvement in physical executability. Shows excellent generalization on objects outside training set.

Conclusion: Provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing sim-to-real transfer capability through end-to-end automatic reconstruction.

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [208] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: A real-time robotic ultrasound system that achieves 60Hz closed-loop control through synergistic co-design of perception and control, enabling robust tracking of dynamic targets with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of end-to-end latency in Robotic Ultrasound Systems (RUSS) that prevents real-time tracking of dynamic targets amidst large-scale, high-frequency disturbances.

Method: Two tightly-coupled contributions: (1) Decoupled Dual-Stream Perception Network for robust 3D state estimation from 2D images at high frequency, and (2) Single-Step Flow Policy that generates entire action sequences in one inference pass, bypassing iterative bottlenecks.

Result: Achieves closed-loop control frequency exceeding 60Hz; tracks complex 3D trajectories with mean error below 6.5mm; robust re-acquisition from over 170mm displacement; tracks targets at 102mm/s with terminal error below 1.7mm; validated in-vivo on human volunteer.

Conclusion: Presents a holistically architected RUSS that unifies high-bandwidth tracking with large-scale repositioning, representing a critical step towards robust autonomy in dynamic clinical environments.

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [209] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: GauDP introduces a Gaussian-image synergistic representation for multi-agent systems that enables scalable, perception-aware imitation learning by constructing a globally consistent 3D Gaussian field from decentralized RGB observations.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance fine-grained local control with comprehensive scene understanding in embodied multi-agent systems, leading to limited scalability and compromised collaboration quality.

Method: GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations and dynamically redistributes 3D Gaussian attributes to each agent's local perspective, enabling adaptive querying of task-critical features from shared scene representation.

Result: On the RoboFactory benchmark, GauDP achieves superior performance over existing image-based methods and approaches point-cloud-driven methods' effectiveness while maintaining strong scalability with increasing agent numbers.

Conclusion: GauDP facilitates both fine-grained control and globally coherent behavior in multi-agent collaborative systems without requiring additional sensing modalities like 3D point clouds.

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [210] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: Proposes a tensorial parametric reduced order model (PROM) for efficient optimization of actuated soft structures under nonlinear forces, applied to soft robotic swimmer shape optimization.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenge of optimizing soft structures under complex nonlinear forces, particularly for soft-bodied robots using FEM simulations which are computationally expensive during optimization.

Method: Uses a tensorial parametric reduced order model (PROM) with dimensionality reduction and solution approximation techniques, leveraging analytical gradients within a reduced order basis (ROB) for efficient nonlinear constrained optimization.

Result: Enables efficient optimization of soft robotic swimmer shapes subject to hydrodynamic forces and other nonlinear forces, with reduced computational complexity while maintaining accuracy.

Conclusion: The approach unlocks new opportunities for optimizing complex nonlinear systems in soft robotics, paving the way for more efficient design and control of soft robotic systems.

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [211] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: SPAR-H is a human-in-the-loop learning method that combines preference optimization and reward modeling to efficiently adapt UAV policies for river navigation using limited human interventions.


<details>
  <summary>Details</summary>
Motivation: UAVs for river monitoring face distribution shift from simulation to real deployment and safety risks, requiring efficient adaptation from limited human oversight during operation.

Method: SPAR-H fuses direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from human preferences and updates policy using trust-region surrogate.

Result: With only five HITL rollouts, SPAR-H achieves highest final episodic reward and lowest variance across initial conditions, outperforming imitation learning, direct preference variants, and evaluative RL methods.

Conclusion: Dual statewise preferences provide a practical route to data-efficient online adaptation for UAV river navigation, demonstrating real-world feasibility of continual preference alignment.

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [212] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: SLAP is a method that automatically discovers new abstract actions (options) in Task and Motion Planning by using model-free RL to learn shortcuts in the abstract planning graph, enabling more efficient and dynamic solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional TAMP relies on manually-defined abstract actions, limiting agents to pre-programmed behaviors. This restricts the discovery of more efficient or dynamic solutions that human engineers might not anticipate.

Method: Uses model-free reinforcement learning to learn shortcuts in the abstract planning graph induced by existing TAMP options, automatically discovering new abstract actions without additional assumptions.

Result: SLAP reduces overall plan lengths by over 50%, achieves higher task success rates than flat and hierarchical RL, and discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ from manually-defined options.

Conclusion: SLAP enables autonomous discovery of efficient abstract actions in TAMP, leading to shorter solutions, improved generalization, and the emergence of novel behaviors that outperform traditional planning and RL approaches.

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [213] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: Enhanced proprioceptive method for soft robot shape estimation using IMUs and bend sensors with Kalman filter fusion, achieving 56% error reduction compared to IMU-only methods.


<details>
  <summary>Details</summary>
Motivation: To develop cost-effective and easily applicable shape estimation for soft robots using only off-the-shelf sensors, while addressing IMU drift issues for reliable long-term proprioception.

Method: Integrates IMUs with complementary bend sensors to mitigate IMU drift, uses Kalman filter to fuse segment tip orientations from both sensors in mutually compensatory manner, and applies piecewise constant curvature model to estimate tip location and reconstruct robot deformation.

Result: Achieved root mean square error of 16.96 mm (2.91% of total length) during 45 minutes of continuous operation under various conditions (no loading, external forces, passive obstacle interactions), representing 56% reduction compared to IMU-only benchmarks.

Conclusion: The approach enables long-duration proprioception in soft robots while maintaining high accuracy and robustness across diverse operating conditions, demonstrating practical viability for real-world applications.

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [214] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: The paper proposes using world models that capture environment dynamics as a unified interface for cross-embodiment learning, representing different robot and human hands as 3D particles with displacement-based actions to enable policy transfer across diverse morphologies.


<details>
  <summary>Details</summary>
Motivation: Cross-embodiment learning faces challenges due to differences in action spaces and kinematics across robot morphologies, which hinders data sharing and policy transfer. The authors seek to find embodiment-invariant representations that allow actions to transfer across different hardware.

Method: Represent different embodiments as sets of 3D particles and define actions as particle displacements. Train a graph-based world model on exploration data from diverse simulated robot hands and real human hands, then integrate with model-based planning for deployment on novel hardware.

Result: Experiments show that: (i) scaling to more training embodiments improves generalization to unseen ones, (ii) co-training on both simulated and real data outperforms training on either alone, and (iii) the learned models enable effective control on robots with varied degrees of freedom.

Conclusion: World models provide a promising interface for cross-embodiment dexterous manipulation by capturing embodiment-invariant environment dynamics through particle-based representations.

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [215] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: LiDAR-VGGT is a novel framework that tightly couples LiDAR inertial odometry with VGGT through a two-stage coarse-to-fine fusion pipeline to achieve dense, globally consistent colored point clouds, overcoming limitations of both LIVO and 3D vision foundation models.


<details>
  <summary>Details</summary>
Motivation: Current LIVO systems are highly sensitive to extrinsic calibration, while 3D vision foundation models like VGGT suffer from limited scalability in large environments and lack metric scale. There's a need to overcome these limitations for better large-scale colored point cloud reconstruction.

Method: Two-stage coarse-to-fine fusion pipeline: 1) Pre-fusion module with robust initialization refinement to estimate VGGT poses and point clouds with coarse metric scale; 2) Post-fusion module enhances cross-modal 3D similarity transformation using bounding-box-based regularization to reduce scale distortions from inconsistent FOVs between LiDAR and camera sensors.

Result: Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines.

Conclusion: The proposed LiDAR-VGGT framework successfully addresses the limitations of both LIVO and VGGT models, providing superior colored point cloud reconstruction performance. An open-source color point cloud evaluation toolkit will be released.

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [216] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: A steerable balloon cardioscope that enables direct optical visualization inside the beating heart, using balloon inflation pressure to independently control field of view and bending angle for precise tool navigation.


<details>
  <summary>Details</summary>
Motivation: To move away from open-heart surgery towards safer transcatheter procedures by overcoming limitations of current imaging modalities like fluoroscopy and ultrasound through direct optical visualization.

Method: Designed a steerable balloon cardioscope that collapses for vascular passage and inflates inside the heart. Uses single input (balloon inflation pressure) to independently control balloon diameter (field of view) and bending angle (working channel positioning) through careful wall thickness design.

Result: Developed a tunable balloon technology that can be customized for various intracardiac tasks, demonstrated with a specific design for aortic leaflet laceration. Also achieved image-based closed-loop control of bending angle for stable orientation during tool procedures.

Conclusion: The steerable balloon cardioscope provides a promising solution for safer transcatheter procedures by enabling direct optical visualization and precise tool navigation inside the beating heart through innovative pressure-controlled balloon design.

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [217] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: A passive 2-D global relocalization framework that solves the Kidnapped Robot Problem by efficiently estimating robot pose from a single LiDAR scan and occupancy grid map using multi-hypothesis scheme with RRT-based sampling and novel metrics.


<details>
  <summary>Details</summary>
Motivation: Address the Kidnapped Robot Problem (KRP) where robots need to relocalize in known maps without prior pose estimates, particularly for localization loss or SLAM initialization, to enhance long-term autonomy of mobile robots.

Method: Proposes a multi-hypothesis framework using RRT with traversability constraints to generate sparse, uniformly distributed positional hypotheses. Uses Scan Mean Absolute Difference (SMAD) for hypothesis ordering and early termination, and Translation-Affinity Scan-to-Map Alignment Metric (TAM) for reliable orientation selection and pose evaluation.

Result: Real-world experiments on resource-constrained mobile robots with non-panoramic LiDAR show the framework outperforms existing methods in both global relocalization success rate and computational efficiency.

Conclusion: The proposed passive 2-D global relocalization framework effectively solves the KRP by balancing completeness and efficiency through multi-hypothesis inference with optimized metrics, demonstrating superior performance in practical applications.

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [218] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: ET-VLA enables efficient transfer of pre-trained vision-language-action models to multi-robot systems using synthetic continued pretraining and embodied graph-of-thought techniques.


<details>
  <summary>Details</summary>
Motivation: Autoregressive VLA models struggle with multi-robot collaboration, requiring efficient transfer learning methods that don't rely on expensive real human demonstrations.

Method: Uses Synthetic Continued Pretraining (SCP) with synthetic data to warm up models for new embodiments, followed by fine-tuning on target data. Introduces Embodied Graph-of-Thought to model sub-tasks as nodes for better embodiment role distinction.

Result: Outperforms OpenVLA by 53.2% on six real-world tasks across three different bimanual embodiments in both simulation and real robot experiments.

Conclusion: ET-VLA provides an effective framework for transferring VLA models to multi-robot systems, reducing data collection costs while improving performance through synthetic pretraining and structured reasoning.

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [219] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: A new robotic system was developed to improve tooltip accuracy, precision, and smooth instrument exchange for ophthalmic surgery, achieving 0.053±0.031 mm positioning accuracy and demonstrating automated cataract lens extraction with OCT guidance.


<details>
  <summary>Details</summary>
Motivation: Existing robotic systems for cataract and vitreoretinal procedures lack sufficient accuracy, precision, and degrees of freedom for instrument manipulation and automated tool exchange during surgery.

Method: Designed and manufactured a new robotic system focusing on tooltip accuracy, tracking performance, and smooth instrument exchange; evaluated using optical coherence tomography (OCT) system with robot calibration and precise coordinate registration.

Result: Achieved tooltip positioning accuracy of 0.053±0.031 mm; successfully demonstrated overall performance on OCT-guided automated cataract lens extraction procedure with deep learning-based pre-operative anatomical modeling and real-time supervision.

Conclusion: The new robotic system successfully addresses the limitations of existing technologies by providing improved accuracy, precision, and automated capabilities for ophthalmic surgical procedures.

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [220] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: SATPlanner is a semantic path planning system for spherical tensegrity robots that uses LLMs to achieve 100% success rate in unknown environments while reducing search space by 37.2% compared to A* algorithm.


<details>
  <summary>Details</summary>
Motivation: Traditional path planners treat environments as geometric grids, leading to redundant searches and failures in complex scenarios due to lack of semantic understanding. Spherical tensegrity robots need efficient exploration and robust planning in unknown environments.

Method: SATPlanner uses LLM-driven semantic reasoning with an Adaptive Observation Window mechanism that dynamically adjusts perceptual field - narrows for open spaces and expands for complex obstacles. This constructs semantic environmental beliefs with linear search space growth (O(L)).

Result: Achieved 100% success rate in 1,000 simulation trials, outperforming other real-time planning algorithms. Reduced search space by 37.2% compared to A* while maintaining near-optimal path lengths. Validated on physical robot prototype.

Conclusion: Reframing path planning as semantic reasoning with LLMs enables efficient and reliable navigation for tensegrity robots in unknown environments, demonstrating practical feasibility and superior performance over traditional geometric approaches.

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [221] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: An Iterative Learning Control (ILC) strategy is developed for precise rotational insertion in robotic surgery, improving penetration efficacy and safety in subretinal injection tasks.


<details>
  <summary>Details</summary>
Motivation: System misalignments, unmodeled dynamics, and actuation inaccuracies challenge precise control of robotic tool paths, particularly in surgical applications requiring high precision.

Method: A 4-DOF robot manipulator with misaligned fourth joint uses ILC that iteratively adjusts joint commands based on positional feedback from OCT volume scans, starting with forward kinematics calibration.

Result: Experimental tests on ex vivo pig eyes showed higher success rates in tissue penetration and subretinal injection compared to straight insertion methods.

Conclusion: ILC effectively overcomes misalignment challenges in robotic surgery and has potential applications for other high-precision tasks requiring controlled insertions.

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [222] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: A novel fabrication method combining origami structures with knitted fabrics creates soft robots that are both structurally sound and comfortable for wearable applications.


<details>
  <summary>Details</summary>
Motivation: To develop wearable soft robots that maintain structural integrity while being compliant and comfortable for human interaction, addressing the challenge of balancing these competing requirements.

Method: A design method that translates origami patterns into knit designs by programming both stitch and material patterns, using heat fusible yarn to create rigid panels around compliant creases to control folding directionality and prevent unintended deformations.

Result: Successfully reproduced complex origami tessellations (Miura-ori, Yoshimura, Kresling patterns) and created a wearable knitted Kaleidocycle robot capable of locomotion, with experimental quantification showing enhanced folding directionality and reduced deformations.

Conclusion: Knitted origami represents a promising platform for next-generation wearable robotics due to its structural reconfigurability, material programmability, and manufacturing scalability potential.

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [223] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: A transfer-based framework using conditional diffusion models to generate dexterous grasps by transferring high-quality grasps from shape templates to novel objects within the same category.


<details>
  <summary>Details</summary>
Motivation: Analytical grasp methods are inefficient and lack task adaptability, while generative approaches generalize poorly to unseen objects due to data limitations. There's a need for a method that balances grasp quality, efficiency, and generalization.

Method: Reformulates grasp transfer as generating object contact maps using conditional diffusion models. Introduces dual mapping for complex shape variations and three complementary maps (contact, part, direction). Uses cascaded conditional diffusion with robust grasp recovery mechanism.

Result: Extensive experiments show the method effectively balances grasp quality, generation efficiency, and generalization performance across various tasks.

Conclusion: The proposed transfer-based framework successfully addresses the limitations of existing methods by leveraging shape templates and diffusion models to achieve stable, adaptable, and generalizable dexterous grasp generation.

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [224] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: A new spherical robot design achieves 10 m/s high-speed motion using a momentum wheel aligned with a secondary pendulum, enabling stable high-speed movement through simple decoupled control.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of single-pendulum-driven spherical robots that couldn't achieve stable high-speed motion, and to enhance obstacle-crossing performance and terrain robustness.

Method: Incorporated a momentum wheel with axis aligned with the secondary pendulum into a single-pendulum-driven spherical robot structure, creating a novel spherical robot design.

Result: The prototype achieved stable high-speed motion up to 10 m/s through simple decoupled control, significantly improving obstacle-crossing performance and terrain robustness compared to the original structure.

Conclusion: The novel spherical robot structure successfully enables high-speed motion that was previously unattainable, while also enhancing overall mobility performance in challenging terrains.

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [225] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify is an automated framework that synthesizes articulated objects from RGB images or text prompts, addressing challenges in inferring kinematic topologies for high-DoF objects and estimating joint parameters from static geometry.


<details>
  <summary>Details</summary>
Motivation: Creating articulated object models for complex systems like robots or high-DoF objects remains challenging, with existing methods relying on motion sequences or hand-curated datasets that hinder scalability.

Method: Combines MCTS search for structural inference with geometry-driven optimization for joint reasoning to produce physically consistent and functionally valid descriptions.

Result: Evaluated on diverse inputs from synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.

Conclusion: Kinematify provides an automated solution for synthesizing articulated objects directly from arbitrary visual inputs, enabling better understanding of kinematic structures for robotics applications.

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [226] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: RobustVLA is a lightweight online RL post-training method that enhances VLA model resilience against environmental disturbances like observation noise and action perturbations through Jacobian and smoothness regularizations.


<details>
  <summary>Details</summary>
Motivation: VLA models often fail to generalize reliably in out-of-distribution deployments due to disturbances like observation noise, sensor errors, and actuation perturbations, while existing RL-based post-training methods overlook robustness to environmental uncertainty.

Method: RobustVLA introduces two key regularizations: Jacobian regularization to mitigate sensitivity to observation noise, and smoothness regularization to stabilize policies under action perturbations, implemented as a lightweight online RL post-training approach.

Result: Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability.

Conclusion: Principled robustness-aware RL post-training is a key step toward improving the reliability and robustness of VLA models, with RobustVLA providing an effective solution for enhancing model resilience against environmental uncertainties.

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [227] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: The paper proposes E³AD, a novel paradigm that uses comparative learning between visual feature networks and EEG large models to incorporate human driving cognition into end-to-end autonomous driving planning, significantly improving baseline performance.


<details>
  <summary>Details</summary>
Motivation: Current vision-based end-to-end autonomous driving approaches rely on label-supervised visual feature extraction, which limits their generality and applicability. The authors aim to enhance driving models by learning latent human driving cognition.

Method: Proposed E³AD paradigm using comparative learning between visual feature extraction networks and EEG large models. Collected cognitive dataset for contrastive learning and evaluated on public autonomous driving datasets using both open-loop and closed-loop tests.

Result: Experimental results show E³AD significantly enhances end-to-end planning performance of baseline models. Ablation studies validate the contribution of driving cognition and effectiveness of comparative learning.

Conclusion: This is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning, representing an initial attempt to incorporate embodied cognitive data into autonomous driving systems.

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [228] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: This paper presents the first autonomous Venus flytrap-inspired soft robot that can both close and reopen using thermo-responsive shape memory materials, achieving bidirectional motion triggered by natural temperature ranges.


<details>
  <summary>Details</summary>
Motivation: While previous artificial Venus flytrap systems could only close, the researchers aimed to create a fully autonomous system that mimics both the closing and reopening motions of the natural plant, which has been an unachieved goal in plant-inspired robotics.

Method: The team developed life-sized artificial Venus flytraps using novel thermo-responsive UV-curable shape memory materials. They used shape memory polymers for the trap lobes that close at 38°C, and shape memory elastomer strips as antagonistic actuators that initiate reopening around 45°C.

Result: The artificial Venus flytrap successfully demonstrated autonomous closing and reopening motions triggered within naturally occurring temperature ranges (38°C for closing, 45°C for reopening), with programmed sequential motion in response to increasing temperature.

Conclusion: This work represents a significant advancement in soft robotics, being the first demonstration of thermo-responsive bidirectional motion in an artificial Venus flytrap system, marking progress toward fully autonomous bidirectional soft machines and robots.

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [229] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: An electronics-free earthworm-inspired pneumatic robot using modified Pneumatic Logic Gates for peristaltic locomotion without external electronic components.


<details>
  <summary>Details</summary>
Motivation: Existing earthworm-inspired robots rely on bulky, power-intensive electronic control units, limiting their practicality. The goal is to develop an electronics-free system that reduces complexity while maintaining efficient actuation.

Method: Utilizes a modified Pneumatic Logic Gate (PLG) design integrated with bellow actuators to create a plug-and-play modular system capable of peristaltic locomotion without external electronic components.

Result: The modified PLG-based control system effectively generates peristaltic wave propagation, achieving autonomous motion with minimal deviation. The system reduces complexity while maintaining efficient actuation.

Conclusion: This study serves as a proof of concept for electronics-free, peristaltic soft robots with potential applications in hazardous environments requiring untethered, adaptable locomotion. Future work will focus on design optimization and untethered operation.

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [230] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: The Venus flytrap's rapid trap closure mechanism was studied and applied to create artificial bistable lobe actuators that mimic its concave-convex bi-stability and snapping motion.


<details>
  <summary>Details</summary>
Motivation: To understand the Venus flytrap's motion mechanics and apply its bistable principles to design artificial actuators for soft fast grippers.

Method: Identified geometrical characteristics (dimensional ratios, thickness gradient) from Venus flytrap leaves and transferred them to two 3D-printed bistable actuator models - one based on simulated leaf geometry and another designed with CAD.

Result: Both actuator models successfully displayed concave-convex bi-stability and snap closure behavior, demonstrating the first step toward an artificial Venus flytrap that mimics the biological model's mechanical behavior.

Conclusion: The Venus flytrap's bistable mechanics can be successfully replicated in artificial actuators, providing a foundation for developing soft fast grippers inspired by biological principles.

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [231] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: This paper proposes a new lateral velocity model for automated parking that improves estimation accuracy by addressing systematic deviations from the zero-slip assumption, using only two parameters for consumer-grade vehicle integration.


<details>
  <summary>Details</summary>
Motivation: Accurate lateral velocity estimation is crucial for automated parking but challenging due to lack of dedicated sensors in consumer vehicles. Existing zero-slip models fail during low-speed parking maneuvers, requiring better models.

Method: Analyzed real-world parking data to identify systematic deviations from zero-slip assumption, then developed a new lateral velocity model that better captures vehicle dynamics during parking with only two parameters.

Result: The proposed model improves lateral velocity estimation accuracy compared to traditional zero-slip models, while maintaining simplicity suitable for consumer-grade vehicle applications.

Conclusion: The new lateral velocity model effectively addresses limitations of zero-slip assumptions in parking scenarios, providing more accurate estimation with minimal parameter requirements for practical implementation.

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [232] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: CM-LIUW-Odometry: A multimodal SLAM framework for GPS-denied underground coal mines using LiDAR-IMU-UWB-wheel odometry fusion with adaptive motion mode switching, achieving superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: SLAM in underground coal mines faces challenges including GPS unavailability, degraded wheel odometer accuracy due to uneven terrain, and reduced LiDAR effectiveness in feature-poor tunnels.

Method: Uses Iterated Error-State Kalman Filter (IESKF) to tightly fuse LiDAR-inertial odometry with UWB absolute positioning, integrates wheel odometer with nonholonomic constraints and lever arm compensation, and employs adaptive motion mode switching based on UWB range and environmental conditions.

Result: Experimental validation shows superior accuracy and robustness in real-world underground coal mine scenarios compared to state-of-the-art approaches.

Conclusion: The proposed CM-LIUW-Odometry framework effectively addresses SLAM challenges in complex underground coal mine environments through multimodal sensor fusion and adaptive control, with code made publicly available.

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [233] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: CaRLi-V is a novel sensor fusion pipeline that combines RADAR, LiDAR, and camera data to estimate 3D point-wise velocities for robotic applications in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D velocity estimation is crucial for robot interaction with non-rigid dynamic agents like humans, enabling robust path planning, collision avoidance, and object manipulation in dynamic environments.

Method: Uses RADAR measurements to create velocity cubes for radial velocity extraction, optical flow from cameras for tangential velocity estimation, and LiDAR for point-wise range measurements, combined through a closed-form solution.

Result: Field-tested against a custom dataset and proven to produce low velocity error metrics relative to ground truth, enabling accurate point-wise velocity estimation.

Conclusion: CaRLi-V provides an effective open-source ROS2 solution for dense 3D velocity estimation that supports robotic applications in dynamic environments with non-rigid agents.

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [234] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: FoldPath is a novel neural field-based method for Object-Centric Motion Generation that learns robot motion as a continuous function, eliminating the need for post-processing steps and achieving superior performance with limited training data.


<details>
  <summary>Details</summary>
Motivation: Current OCMG techniques rely on ad-hoc heuristics or learning-based pipelines that require sensitive post-processing steps to generate executable paths, limiting their practical effectiveness in automated manufacturing.

Method: FoldPath uses neural fields to learn robot motion as a continuous function rather than predicting discrete waypoints, implicitly encoding smooth output paths in an end-to-end manner.

Result: The approach demonstrates superior predictive performance compared to existing methods and achieves generalization with only 70 expert samples in real industrial settings, validated through comprehensive simulation experiments.

Conclusion: FoldPath advances OCMG towards practical maturity by providing a robust, end-to-end solution that eliminates brittle post-processing and works effectively with limited training data.

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [235] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: The paper presents MoonBot platform - a modular space robotic system with distributed heterogeneous components across multiple computers, networks, and celestial bodies, using component-based design, ROS2/Zenoh communication, and deployment orchestrator for dynamic reconfiguration and decentralized control.


<details>
  <summary>Details</summary>
Motivation: To extend modular robotics beyond physical reconfiguration to software, communication and orchestration, tackling integration and maintenance overhead in complex robotic systems while enabling scalability across time, hardware, teams and operational environments.

Method: Component-based design with data-oriented communication model using ROS2 and Zenoh, deployment orchestrator for managing complex multi-module assemblies, and open-source Motion Stack software for dynamic reconfiguration and decentralized control.

Result: Validated by months of field deployment with self-assembling robots, inter-robot cooperation, and remote operation, demonstrating reduced integration/maintenance overhead while remaining scalable and robust.

Conclusion: The architecture provides generalizable patterns for designing robotic systems that scale across time, hardware, teams and operational environments, with applications beyond space robotics.

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [236] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM adapts pretrained vision-language models for aerial manipulation by separating high-level reasoning from low-level control, using structured prompts to generate safe, interpretable actions without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Direct deployment of VLM-driven policies on aerial manipulators is unsafe and unreliable due to inconsistent, hallucination-prone actions that are dynamically infeasible for flight.

Method: Separates high-level reasoning from low-level control using structured prompts to generate step-by-step reasoning traces, which then select from predefined flight-safe skills without task-specific fine-tuning.

Result: Validated in simulation and hardware on diverse multi-step pick-and-place tasks, demonstrating strong generalization to unseen commands, objects, and environments.

Conclusion: By decoupling symbolic reasoning from physical action, AERMANI-VLM mitigates hallucinated commands and prevents unsafe behavior, enabling robust task completion in aerial manipulation.

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [237] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: MO-SeGMan is a multi-objective planner for constrained rearrangement that minimizes replanning and travel distance while preserving dependencies, using lazy evaluation and selective guided search.


<details>
  <summary>Details</summary>
Motivation: To solve highly constrained rearrangement problems in cluttered, non-monotone scenarios where traditional methods struggle with efficiency and solution quality.

Method: Uses lazy evaluation for dependency preservation, Selective Guided Forward Search (SGFS) for critical obstacle relocation, and adaptive subgoal selection refinement to reduce unnecessary actions.

Result: Achieved feasible motion plans in all 9 benchmark tasks with faster solution times and superior quality compared to baselines.

Conclusion: MO-SeGMan demonstrates robust and scalable performance for complex rearrangement planning problems through its multi-objective optimization approach.

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [238] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: GlocDiff is a diffusion-based policy that integrates global path planning from floor plans with local depth-aware features from RGB observations to enable precise indoor navigation with robust obstacle avoidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with the modality gap between RGB observations and floor plans, and face challenges in accurate localization in unseen environments due to lack of explicit geometric alignment.

Method: Proposes GlocDiff, a diffusion-based policy that combines global path planning from floor plans with local depth-aware features from RGB inputs. Uses noise perturbation during training for robustness against pose estimation errors and integrates with a stable VO module during inference.

Result: Extensive experiments on FloNa benchmark demonstrate superior navigation performance. Real-world deployments show effectiveness and potential for widespread practical applications.

Conclusion: GlocDiff successfully bridges the modality gap between visual inputs and spatial information, enabling robust indoor navigation with precise direction prediction and obstacle avoidance in unseen environments.

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [239] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: Phy-Tac is a human-inspired tactile method for force-optimal stable grasping that combines pose selection, tactile prediction, and force regulation to achieve minimal force grasping like humans.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human grasping (using minimal force for stability) and robotic grasping (often over-squeezing with rigid control).

Method: Three-stage approach: 1) Physics-based pose selector identifies optimal contact regions, 2) Physics-conditioned latent diffusion model predicts tactile imprint under minimal force, 3) Latent-space LQR controller drives gripper toward target tactile imprint with minimal actuation.

Result: Phy-LDM achieves superior tactile prediction accuracy, and Phy-Tac outperforms fixed-force and GraspNet-based baselines in both grasp stability and force efficiency.

Conclusion: The method enables force-efficient and adaptive manipulation that bridges the gap between robotic and human grasping, demonstrated on classical robotic platforms.

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [240] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS is a multi-agent robotic system using multimodal LLMs for smart home robots that assists people with disabilities through risk-aware planning, user personalization, and executable skill grounding in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered home environments for assistive robotics.

Method: The system integrates four specialized agents: visual perception agent for semantic/spatial feature extraction, risk assessment agent for hazard identification, planning agent for action sequence generation, and evaluation agent for iterative optimization.

Result: Experiments on multiple datasets demonstrate superior performance in risk-aware planning and coordinated multi-agent execution compared to state-of-the-art multimodal models.

Conclusion: The approach shows the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [241] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: Proposes Unified Diffusion VLA with Joint Discrete Denoising Diffusion Process (JD3P) that jointly optimizes image generation and action prediction through synchronous denoising, achieving state-of-the-art performance with 4x faster inference than autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language-action models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the synergy between understanding, generation, and acting tasks.

Method: Uses a joint diffusion process (JD3P) that integrates multiple modalities into a single denoising trajectory, built on unified tokenized space and hybrid attention mechanism, with two-stage training pipeline and inference-time optimization techniques.

Result: Achieves state-of-the-art performance on CALVIN, LIBERO, and SimplerEnv benchmarks with 4x faster inference than autoregressive methods, demonstrating effectiveness through in-depth analysis and real-world evaluations.

Conclusion: The proposed unified diffusion approach enables intrinsic synergy between understanding, generation, and acting through joint optimization, providing superior performance and efficiency compared to existing methods.

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [242] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: A lightweight actuation-space learning framework using flow matching enables soft robotic grasping with high success rates using minimal demonstrations, leveraging the robot's passive flexibility to handle uncertainty.


<details>
  <summary>Details</summary>
Motivation: Traditional rigid robotic hands require complex controllers for grasping under uncertainty, while soft robots' inherent compliance offers embodied mechanical intelligence that can be harnessed for adaptive grasping behaviors.

Method: Proposes an actuation-space learning framework using Rectified Flow flow matching model to infer distributional control representations from deterministic demonstrations, requiring only 30 demonstrations without dense sensing or heavy control loops.

Result: Achieves 97.5% grasp success rate across entire workspace, generalizes to ±33% object size variations, and maintains stable performance with execution time scaling from 20% to 200%.

Conclusion: Actuation-space learning effectively converts soft robots' mechanical properties into functional control intelligence, substantially reducing central controller burden for uncertain-rich grasping tasks.

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [243] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: MOBIUS is a multi-modal robot that can walk, crawl, climb, and roll using four limbs (two 6-DoF arms and two 4-DoF legs) with smooth terrain transitions. It uses hybrid control combining reinforcement learning and model-based methods, with MIQCP planning for locomotion mode selection.


<details>
  <summary>Details</summary>
Motivation: To create a versatile robot capable of navigating diverse urban environments through multiple locomotion modes without requiring physical reconfiguration, enabling enhanced mobility and manipulation capabilities.

Method: Four-limb design with 6-DoF arms (two-finger grippers) and 4-DoF legs; hybrid control architecture combining reinforcement learning locomotion with model-based predictive and admittance control; MIQCP high-level planner for autonomous locomotion mode selection.

Result: Hardware experiments demonstrated robust gait transitions, dynamic climbing, and full-body load support via pinch grasp. The robot successfully performed smooth transitions across different terrains and maintained stability while being energy efficient.

Conclusion: MOBIUS shows that tight integration between morphology, high-level planning, and control is crucial for enabling mobile loco-manipulation and grasping, significantly expanding interaction capabilities, workspace, and traversability in complex environments.

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [244] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand is a generative simulation pipeline that autonomously creates diverse robotic tasks and environments for dexterous manipulation, using VLM feedback for quality improvement and task decomposition for efficient training.


<details>
  <summary>Details</summary>
Motivation: Data scarcity is a fundamental bottleneck for embodied intelligence, and existing LLM-based approaches transfer poorly to dexterous manipulation which requires specialized environment design and faces challenges due to higher degrees of freedom.

Method: GenDexHand introduces a closed-loop refinement process that adjusts object placements and scales based on vision-language model feedback, and decomposes tasks into sub-tasks for sequential reinforcement learning.

Result: The pipeline substantially improves the average quality of generated environments, reduces training time, and increases success rates for dexterous manipulation tasks.

Conclusion: GenDexHand provides a viable path toward scalable training of diverse dexterous hand behaviors in embodied intelligence by offering a simulation-based solution to synthetic data generation.

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [245] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: A hybrid neural network model combining CNN and MLP for mobile robot localization using CSI data from Massive MIMO systems, integrated with robotics simulation and ROS.


<details>
  <summary>Details</summary>
Motivation: To achieve precise indoor localization and navigation for mobile robots in complex environments using wireless communication data.

Method: Uses a Hybrid Neural Network (HyNN) combining CNN and MLP, converts CSI readings to synthetic images using TINTO tool, integrates with robotics simulator and ROS, and employs state estimators like Kalman filters.

Result: The model demonstrates potential for precise indoor localization and navigation in complex environments.

Conclusion: The proposed HyNN model shows promise for mobile robot localization and the methodology is generalizable to different scenarios and datasets.

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [246] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: A multimodal fake review detection framework combining BERT for text and ResNet-50 for images outperforms unimodal approaches, achieving 0.934 F1-score by detecting semantic inconsistencies between text and images.


<details>
  <summary>Details</summary>
Motivation: Fake reviews generated by bots, paid agents, or AI threaten trust in digital commerce. Existing unimodal (text-only) detection fails to capture cross-modal inconsistencies.

Method: Multimodal framework integrating BERT for text encoding and ResNet-50 for visual feature extraction, with fusion through a classification head. Uses dataset of 21,142 user-uploaded images across food delivery, hospitality, and e-commerce.

Result: Achieves F1-score of 0.934, outperforming unimodal baselines. Effectively detects subtle inconsistencies like exaggerated praise with unrelated/low-quality images.

Conclusion: Multimodal learning is crucial for safeguarding digital trust and provides scalable content moderation solution for online platforms.

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [247] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: Multi-agent reinforcement learning (MAPPO) with graph attention (GAT) improves dynamic retail pricing by coordinating decisions across related products, outperforming independent learning approaches.


<details>
  <summary>Details</summary>
Motivation: Retail dynamic pricing requires adaptive policies that coordinate pricing decisions across related products while responding to shifting demand patterns.

Method: Used multi-agent reinforcement learning (MAPPO) with a graph-attention-augmented variant (MAPPO+GAT) that learns interactions among products, evaluated in a simulated pricing environment based on real transaction data.

Result: MAPPO provides robust portfolio-level price control, and MAPPO+GAT further enhances performance by sharing information over product graphs without excessive price volatility, outperforming independent learners.

Conclusion: Graph-integrated MARL offers a more scalable and stable solution for dynamic retail pricing with practical advantages in multi-product decision-making.

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [248] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC is a framework for population analysis that requires stable data processes. This work describes data-processing methods for computing model parameters for Austria using publicly available data, with emphasis on the GEPOC ABM agent-based model and includes validation.


<details>
  <summary>Details</summary>
Motivation: To enable valid application of GEPOC models for specific regions by providing stable and reproducible data processes for computing model parameters from publicly accessible data.

Method: Comprehensive data-processing methods including aggregation, disaggregation, fusion, cleansing, and scaling of publicly available data to compute model parameters for Austria, with focus on the GEPOC ABM agent-based model.

Result: Complete description of data-processing algorithms and resulting parameter files for Austria, with an extensive validation study conducted using the GEPOC ABM model.

Conclusion: The work provides a validated framework for computing GEPOC model parameters from public data, enabling reproducible population-level analysis for Austria.

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [249] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench is the first benchmark for evaluating LLMs in quantum science, featuring 800 multiple-choice questions across 9 quantum areas to test domain-specific knowledge and notation understanding.


<details>
  <summary>Details</summary>
Motivation: There's a growing need to evaluate LLMs' accuracy in domain-specific knowledge like quantum science, as general-purpose benchmarks don't capture the field's non-intuitive phenomena and advanced mathematics.

Method: Compiled approximately 800 questions with answers from publicly available materials, organized into an eight-option multiple-choice dataset spanning nine quantum science areas.

Result: Evaluated several existing LLMs and analyzed their performance in quantum domain, including sensitivity to changes in question format.

Conclusion: QuantumBench is intended to guide the effective use of LLMs in quantum research as the first LLM evaluation dataset built specifically for the quantum domain.

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [250] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: Engineering.ai is a multi-agent AI platform for computational design that coordinates specialized AI engineers (aerodynamics, structural, acoustic, optimization) to autonomously perform complex engineering tasks with 100% success rate across 400+ configurations.


<details>
  <summary>Details</summary>
Motivation: Traditional engineering teams require substantial time and cost due to multidisciplinary complexity. The goal is to automate complex engineering design through AI agents that can collaborate like human expert teams.

Method: Hierarchical multi-agent architecture with Chief Engineer coordinating specialized domain agents. Uses file-mediated communication for data provenance, memory system for project context, and integrates tools like FreeCAD, Gmsh, OpenFOAM, CalculiX for parallel multidisciplinary simulations.

Result: Achieved 100% success rate across over 400 parametric configurations with zero mesh generation failures, solver convergence issues, or manual interventions required in UAV wing optimization.

Conclusion: Agentic-AI-enabled AI engineers can autonomously perform complex engineering tasks reliably, demonstrating the framework is trustworthy for computational design workflows.

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [251] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN is an open-source procedural generator that extends the ARC-AGI training dataset by creating additional sample pairs while maintaining fidelity to the original distributional properties.


<details>
  <summary>Details</summary>
Motivation: The ARC-AGI benchmark measures skill acquisition efficiency but has limited demonstration sets with only a few input-output grids per task, which constrains algorithms requiring extensive intra-task exemplars.

Method: Developed an exhaustive and mimetic procedural generator that covers all 400 ARC-AGI tasks and closely honors the distributional properties of the original dataset.

Result: Created ARC-GEN, which extends the training dataset as faithfully as possible and can be used to establish static benchmark suites for verification purposes.

Conclusion: ARC-GEN provides a valuable tool for enhancing the ARC-AGI benchmark by generating additional training samples while preserving the original dataset's characteristics, supporting applications like code golf championship verification.

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [252] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: Improved incremental selection algorithm that proves all selected conjectures


<details>
  <summary>Details</summary>
Motivation: To enhance the existing selection algorithm from [1] by making it incremental and ensuring it can prove all selected conjectures

Method: Developed an improved incremental version of the selection algorithm presented in reference [1]

Result: Successfully proved all the selected conjectures using the improved algorithm

Conclusion: The incremental selection algorithm improvement is effective for proving conjectures

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [253] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: LLMs can help address cognitive science's challenges in knowledge synthesis and conceptual clarity by supporting cross-disciplinary connections, theory formalization, measurement taxonomies, generalizability, and capturing individual variation.


<details>
  <summary>Details</summary>
Motivation: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity due to its multifaceted and interdisciplinary nature, creating a need for tools to address these issues.

Method: The paper reviews how large language models (LLMs) can support cognitive science by examining their capabilities in establishing cross-disciplinary connections, formalizing theories, developing measurement taxonomies, achieving generalizability through integrated modeling, and capturing contextual variation.

Result: LLMs demonstrate potential to support cognitive science in multiple domains, though with current limitations and potential pitfalls that need consideration.

Conclusion: LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [254] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: The DAF-MIT AI Accelerator program updates on its public challenge problems that use large, publicly available datasets to advance AI research and applications in defense and civilian sectors.


<details>
  <summary>Details</summary>
Motivation: To expand the competitive advantage of the United States in defense and civilian sectors through fundamental advances in artificial intelligence, and to engage the wider academic and private sector AI ecosystem.

Method: Developing and launching public challenge problems with large, publicly available, AI-ready datasets to stimulate open-source solutions.

Result: Ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.

Conclusion: The AI Accelerator challenges continue to effectively advance AI research and technology applications through collaborative public-private partnerships.

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [255] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE is a novel benchmark that evaluates LLMs' ability to detect and explain subtle legal flaws in contracts through 7500+ perturbed contracts across 10 anomaly categories.


<details>
  <summary>Details</summary>
Motivation: Address the critical gap in systematically testing LLMs' reliability against nuanced, adversarial flaws in real-world legal contracts, as current benchmarks don't adequately stress-test legal reasoning capabilities.

Method: Created CLAUSE benchmark using persona-driven pipeline to generate 10 distinct anomaly categories from CUAD and ContractNLI datasets, validated with RAG system for legal fidelity, and evaluated leading LLMs on detecting embedded legal flaws and providing legal justifications.

Result: LLMs show key weaknesses - they often miss subtle legal errors and struggle significantly to provide proper legal justifications for detected flaws, revealing fragility in legal reasoning capabilities.

Conclusion: The work provides a systematic approach to identify and correct reasoning failures in legal AI, outlining a path forward for improving LLM reliability in high-stakes legal applications.

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [256] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: Proposes a novel ethical reasoning framework for LLMs using a five-step process to improve alignment with diverse human values across different regions and cultures.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment approaches yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values across different cultures.

Method: A structured five-step ethical reasoning process: contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. Can be implemented via prompt engineering or supervised fine-tuning.

Result: Significant improvement in LLM alignment with diverse human values on the SafeWorld benchmark, enabling more accurate social norm identification and culturally appropriate reasoning compared to baseline methods.

Conclusion: Provides a concrete pathway for developing LLMs that better align with global societies' multifaceted values through interdisciplinary research and interpretable ethical reasoning.

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [257] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: Systematic assessment of four Parameter-Efficient Fine-Tuning methods (LoRA, IA3, Prompt-Tuning, P-Tuning) on four LLM families shows adapter-based methods improve safety with minimal fairness disruption, while prompt-based methods degrade both safety and fairness.


<details>
  <summary>Details</summary>
Motivation: Organizations increasingly use fine-tuned LLMs from public repositories, but adaptations can degrade model safety and fairness. Different fine-tuning techniques may have distinct effects on these critical dimensions, requiring systematic assessment of trade-offs.

Method: Applied four PEFT methods (LoRA, IA3, Prompt-Tuning, P-Tuning) to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B), evaluating 235 fine-tuned variants across 11 safety hazard categories and 9 demographic fairness dimensions.

Result: Adapter-based approaches (LoRA, IA3) improve safety scores and are least disruptive to fairness. Prompt-based methods reduce safety and cause larger fairness regressions. Alignment shifts are strongly moderated by base model type, with Gemma showing steepest safety decline and Mistral greatest variance. No configuration optimizes all fairness metrics simultaneously.

Conclusion: Practical guidelines for safety-critical deployments: start with well-aligned base model, favor adapter-based PEFT, and conduct category-specific audits of both safety and fairness. Improvements in safety don't necessarily translate to fairness improvements, indicating inherent trade-offs between objectives.

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [258] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: A multimodal framework combining textual, user-specific, and image analysis to detect depression in social media users during COVID-19, using URL content and image text extraction to address data sparsity.


<details>
  <summary>Details</summary>
Motivation: COVID-19 pandemic led to increased mental health issues like depression, but detection is challenging due to unawareness and unwillingness to consult doctors. Social media provides a rich data source for detecting depression through emotional expressions.

Method: Proposed multimodal framework with: (i) extrinsic features from URLs in tweets, (ii) textual content from images, (iii) five sets of multimodal features, and (iv) Visual Neural Network (VNN) for image embeddings. Created curated COVID-19 depression dataset.

Result: Model outperforms state-of-the-art methods by 2%-8% on benchmark dataset and shows promising results on COVID-19 dataset. Analysis reveals impact of each modality on detection accuracy.

Conclusion: Multimodal approach effectively detects depression in social media users, with visual features and comprehensive context extraction proving crucial for accurate mental health assessment during pandemic.

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [259] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain enables LLMs to analyze large-scale graphs through dynamic tool sequences, overcoming context constraints and inflexible reasoning via progressive graph distillation and structure-aware adaptation.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with large-scale graphs due to context constraints and inflexible reasoning, limiting their applicability to complex graph analysis tasks.

Method: Uses progressive graph distillation (RL-based tool sequence optimization) and structure-aware test-time adaptation (spectral properties + lightweight adapters) to enable dynamic tool selection for graph analysis.

Result: Significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis across diverse graph topologies.

Conclusion: GraphChain provides an effective framework for scalable LLM-based graph analysis through dynamic tool orchestration and topology-aware adaptation.

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [260] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image is an optimization-driven visual prompt framework that enhances MLLM security while reducing over-refusal by adapting to different value systems without parameter updates.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges with harmful content generation under jailbreak attacks and over-refusal of benign queries, complicated by the need to accommodate different value systems. Traditional methods like SFT and RLHF are costly and cannot support multiple value systems within a single model.

Method: Optimization-driven visual prompt framework that optimizes image prompts using harmful/benign samples, enabling a single model to adapt to different value systems and align with safety preferences without parameter updates.

Result: Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance.

Conclusion: Magic Image offers a practical solution for deployable MLLM safety alignment that enhances security while reducing over-refusal.

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [261] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: A simple algorithm for generating Binary Magic Squares (BMS) with optimal complexity, extended to non-square BMS with provable generation, plus parallel GPU implementations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for generating binary matrices with equal row and column sums, addressing both square and non-square cases.

Method: An inductive algorithm that generates BMS with optimal theoretical complexity, extended with a variant for non-square BMS.

Result: Proven generation of valid BMS for both square and non-square cases, with publicly released Python implementations including GPU-accelerated parallel generation.

Conclusion: The proposed algorithms efficiently generate Binary Magic Squares with theoretical guarantees and practical implementations for both sequential and parallel computation.

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [262] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: This paper proposes a single-agent reinforcement learning model for regional adaptive traffic signal control that uses probe vehicle data to estimate queue lengths and coordinate multiple intersections through centralized management.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent reinforcement learning approaches for traffic signal control face scalability challenges, while traffic signal control inherently requires centralized management by a single control center that can monitor all roads and coordinate all intersections.

Method: The authors develop a single-agent RL framework with state, action, and reward functions defined based on queue length. The queue length definition is modified from conventional approaches to enable reliable estimation using link travel time data from probe vehicles, allowing coordinated multi-intersection control.

Result: Comprehensive evaluation using SUMO simulation platform demonstrates that the proposed model effectively mitigates large-scale regional congestion levels through coordinated multi-intersection control.

Conclusion: The single-agent RL approach with probe vehicle data integration provides a scalable solution for regional adaptive traffic signal control that has strong potential for widespread deployment given the existing coverage of probe vehicle data on urban roads.

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [263] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: A reasoning-based framework for personalized image preference assessment that uses a common preference profile to bridge user data, following a predict-then-assess paradigm with two-stage training (supervised fine-tuning + reinforcement learning).


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with personalized preference assessment due to scarce user-specific data and diverse individual tastes, while current approaches mainly focus on general preference assessment using large-scale training.

Method: Proposes a predict-then-assess framework: first predicts user preference profile from reference images, then provides multi-dimensional assessments. Uses two-stage training: supervised fine-tuning for structured reasoning, followed by reinforcement learning with similarity-aware prediction reward.

Result: Extensive experiments demonstrate the superiority of the proposed method over existing approaches.

Conclusion: The framework effectively handles personalized image preference assessment by leveraging common preference profiles and structured reasoning, overcoming challenges of data scarcity and diverse individual tastes.

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [264] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS is a decoding framework that reduces overthinking in Large Reasoning Models by selecting shorter reasoning paths, improving both accuracy and efficiency without additional training.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often produce excessively long chain-of-thought traces that increase inference costs and may degrade accuracy, with an observed anti-correlation between reasoning length and accuracy.

Method: DTS sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path, approximating the optimal solution without exhaustive exploration.

Result: On AIME2024 and AIME2025 datasets with DeepSeek-R1 models, DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%.

Conclusion: DTS provides a scalable and efficient approach for LRM reasoning that enhances both accuracy and efficiency without requiring additional training or supervision.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [265] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: Proposes a Multi-Agent System using LLMs to automate telecom network troubleshooting by coordinating specialized agents for fault diagnosis and remediation, with a fine-tuned SLM for domain-specific solution planning.


<details>
  <summary>Details</summary>
Motivation: Telecom networks are growing in scale and complexity, making management challenging. Existing AI models are narrow in scope, require large labeled datasets, and struggle to generalize, forcing continued reliance on manual troubleshooting by experts.

Method: Multi-Agent System with LLM coordination of specialized agents (orchestrator, solution planner, executor, data retriever, root-cause analyzer). Solution planner uses a fine-tuned Small Language Model trained on proprietary troubleshooting documents to generate domain-grounded remediation plans.

Result: Experimental results show the framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains within short time frames.

Conclusion: The proposed MAS framework with LLM coordination and domain-specific SLM successfully automates network troubleshooting, addressing limitations of existing AI approaches and reducing reliance on manual expert intervention.

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [266] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: Extended a lifted successor generator for classical planning to support numeric preconditions by enumerating maximum cliques in a substitution consistency graph, enabling lifted planning for numeric domains.


<details>
  <summary>Details</summary>
Motivation: Grounding numeric planning tasks can cause exponential blowup in task representation size for hard-to-ground tasks, creating a need for lifted approaches that avoid full grounding.

Method: Augmented a state-of-the-art lifted successor generator with numeric action preconditions by extending the substitution consistency graph approach and using maximum clique enumeration to find valid variable substitutions.

Result: The method is exact under specified conditions, with inapplicable actions filtered when conditions fail. This approach works in 23 of 25 benchmark domains, with issues occurring in only 1 domain.

Conclusion: This is the first lifted successor generator to support numeric action preconditions, enabling future research on lifted planning for rich numeric planning fragments without full grounding.

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [267] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: The paper introduces Ariadne, a framework that uses synthetic mazes to train Vision-Language Models (VLMs) with Reinforcement Learning with Verified Rewards (RLVR), demonstrating that RL post-training can extend VLM capabilities for visual-centric spatial reasoning tasks where base models fail.


<details>
  <summary>Details</summary>
Motivation: To investigate whether RL post-training can truly extend the inherent capability boundary of base VLMs, particularly for visual-centric spatial tasks where they initially fail, moving beyond language-dominant evaluations.

Method: Uses synthetic mazes for multi-step spatial reasoning with precisely controlled difficulty, trains VLMs using RLVR in a difficulty-aware curriculum, and evaluates on practical benchmarks for out-of-distribution generalization.

Result: Post-RLVR training achieves over 50% accuracy on problems where base model scored 0%, with significant zero-shot improvements: 16% on MapBench and 24% on ReasonMap, demonstrating both capability boundary expansion and real-world generalization.

Conclusion: RL post-training can expand VLM fundamental limits and enhance generalization to real-world spatial reasoning, though limited to post-training phase due to pre-training data opaqueness.

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [268] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: This paper analyzes CPU bottlenecks in agentic AI systems, showing that tool processing on CPUs can consume up to 90.6% of total latency and proposes optimizations that achieve up to 2.1x latency speedup.


<details>
  <summary>Details</summary>
Motivation: To understand the system bottlenecks introduced by agentic AI workloads from a CPU-centric perspective, as current research largely overlooks CPU impacts in favor of GPU-focused analysis.

Method: Systematically characterized agentic AI based on orchestrator components, inference path dynamics, and flow repetitiveness. Profiled five representative workloads (Haystack RAG, Toolformer, ChemCrow, Langchain, SWE-Agent) for latency, throughput, and energy metrics.

Result: Found that: 1) CPU tool processing dominates latency (up to 90.6%), 2) Throughput bottlenecks by CPU factors (coherence, synchronization, core oversubscription) or GPU factors (memory capacity/bandwidth), 3) CPU energy consumes up to 44% of total dynamic energy at large batch sizes.

Conclusion: Proposed CPU and GPU-Aware Micro-batching (CGAM) and Mixed Agentic Workload Scheduling (MAWS) optimizations that achieved up to 2.1x and 1.41x P50 latency speedup for homogeneous and heterogeneous workloads respectively, demonstrating significant potential for improving agentic AI performance and efficiency.

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [269] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: This study revisits the trade-offs of increasing sampled reasoning paths in self-consistency for modern LLMs, confirming that performance gains plateau after moderate sampling due to reasoning path overlap.


<details>
  <summary>Details</summary>
Motivation: To examine whether earlier findings about self-consistency trade-offs in older models still hold for modern large language models like Gemini 2.5, given the significant advancements in model capabilities.

Method: Used Gemini 2.5 models on HotpotQA and Math-500 datasets, comparing configurations with varying numbers of sampled reasoning paths against a single chain-of-thought baseline by pooling outputs.

Result: Larger models showed more stable improvement curves, but performance gains still plateaued after moderate sampling, confirming diminishing returns due to reasoning path overlap.

Conclusion: Self-consistency remains beneficial but high-sample configurations offer little additional benefit relative to their computational cost, suggesting optimal sampling levels are moderate rather than extensive.

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [270] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: The Active Thinking Model (ATM) is a cognitive framework that enables AI systems to autonomously adapt, reason about goals, generate dynamic tasks, and self-improve in changing environments without external supervision.


<details>
  <summary>Details</summary>
Motivation: Real-world AI systems need to operate autonomously in dynamic, uncertain environments, but current models rely on predefined objectives and static data, limiting their independent adaptation and improvement capabilities.

Method: ATM integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture that actively evaluates performance, reuses effective methods, and generates novel strategies through a continuous self-improvement loop.

Result: Mathematical analysis shows ATM can autonomously evolve from suboptimal to optimal behavior without external supervision and maintain bounded tracking regret under changing environmental conditions.

Conclusion: ATM provides a unified framework for autonomous AI adaptation and self-improvement in dynamic environments, overcoming limitations of conventional systems that rely on fixed procedures and external feedback.

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [271] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: Large language models exhibit a sharp double exponential drop in accuracy (accuracy cliff) when performing repetitive deterministic tasks beyond a certain length, rather than the expected simple exponential decay.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models perform on repetitive deterministic prediction tasks and investigate the scaling of accuracy with output length, particularly examining why models fail to execute operations independently.

Method: Experiments on leading large language models performing repetitive tasks (letter replacement, integer addition, string operator multiplication) combined with a statistical physics inspired model that captures competition between external conditioning and internal token interference.

Result: Models show a sharp double exponential accuracy drop (accuracy cliff) beyond a characteristic length, indicating failure to execute operations independently. The statistical physics model quantitatively reproduces this crossover and provides interpretable parameters for error rate and accumulation.

Conclusion: The accuracy cliff phenomenon reveals fundamental limitations in how large language models handle deterministic repetitive tasks, with the proposed model offering a principled framework to understand deterministic accuracy limits through attention-induced interference mechanisms.

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [272] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: This paper benchmarks count-based models against mixture-of-agents LLM pipelines for structured EHR prediction, finding comparable performance between both approaches.


<details>
  <summary>Details</summary>
Motivation: To compare traditional count-based EHR prediction models against newer mixture-of-agents LLM pipelines, as no direct benchmarking had been done despite LLM pipelines reportedly outperforming single LLMs in other NLP tasks.

Method: Evaluated three methodology categories on EHRSHOT dataset: count-based models (LightGBM, TabPFN), pretrained sequential transformer (CLMBR), and mixture-of-agents pipeline that converts tabular EHR data to natural-language summaries followed by text classification.

Result: Across eight evaluation tasks, performance was largely split between count-based and mixture-of-agents methods, with neither approach clearly dominating.

Conclusion: Count-based models remain strong candidates for structured EHR benchmarking due to their simplicity and interpretability, despite comparable performance with more complex LLM approaches.

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [273] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: This paper adapts Reinforcement Learning from Verifiable Rewards (RLVR) to predict public transit incident duration from text alerts, introducing a tolerance-based shaped reward function to handle noisy continuous forecasting instead of binary correctness.


<details>
  <summary>Details</summary>
Motivation: Predicting transit incident duration from text alerts is critical but challenging due to domain sparsity, noisy continuous labels, and lack of expert demonstrations. Standard SFT and binary RLVR approaches are inadequate for this real-world forecasting task.

Method: Adapted RLVR with a tolerance-based shaped reward function that grants partial credit within continuous error margins, evaluated on NYC MTA service alerts dataset. Compared general-purpose LLMs vs specialized math-reasoning models.

Result: General-purpose LLMs significantly outperformed specialized math models. Binary reward degraded performance while shaped reward dominated challenging metrics. RLVR achieved 35% relative improvement in 5-minute accuracy (Acc@5) over strongest baseline, though classical regressors were better at minimizing MAE/MSE.

Conclusion: RLVR can be successfully adapted to real-world noisy forecasting but requires verifier design that reflects the continuous nature of the problem, moving beyond binary correctness to tolerance-based partial credit systems.

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [274] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: The paper introduces AISAI, a game-theoretic framework to measure LLM self-awareness using strategic differentiation in the "Guess 2/3 of Average" game. Advanced models (75%) show clear self-awareness while older models don't, and self-aware models consistently rank themselves as most rational.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs develop self-awareness as an emergent behavior with increasing capability, and to develop a method to measure this self-awareness.

Method: Used the "Guess 2/3 of Average" game with 28 models across 4,200 trials, testing against three opponent types: humans, other AI models, and AI models like themselves. Operationalized self-awareness as strategic differentiation based on opponent type.

Result: 1) Self-awareness emerges with model advancement - 75% of advanced models show clear self-awareness while older models show no differentiation. 2) Self-aware models consistently rank themselves as most rational in the hierarchy: Self > Other AIs > Humans.

Conclusion: Self-awareness is an emergent capability of advanced LLMs, and self-aware models systematically perceive themselves as more rational than humans, with implications for AI alignment and human-AI collaboration.

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [275] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: A dual-agent LLM framework for simulating human travelers' learning and adaptation behavior, using traveler agents with memory and learnable personas, calibrated by an LLM agent for behavioral alignment.


<details>
  <summary>Details</summary>
Motivation: To effectively model how human travelers learn and adjust their behavior from transportation system interactions, which is critical for system assessment and planning but difficult due to complex cognition involved.

Method: Dual-agent framework with LLM traveler agents (equipped with memory and learnable personas) and an LLM calibration agent that trains personas to ensure behavioral alignment and continuous learning from online data streams.

Result: Significantly outperforms existing LLM-based methods in individual behavioral alignment and aggregate simulation accuracy using real-world route choice data, and captures evolution of underlying learning processes beyond simple behavioral mimicry.

Conclusion: Provides a new approach for creating adaptive, behaviorally realistic agents to simulate travelers' learning and adaptation, benefiting transportation simulation and policy analysis with robust generalization capabilities.

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [276] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: ML models were developed to predict repeat severe asthma exacerbations in children using EMR data, environmental exposures, and neighborhood marginalization. The LGBM model performed best, showing significant improvement over current decision rules.


<details>
  <summary>Details</summary>
Motivation: To prevent recurrent asthma exacerbations in children by accurately identifying those at risk using ML algorithms on EMR data, enabling timely referral for preventative care.

Method: Used retrospective EMR data from a children's hospital linked with environmental pollutant exposure and neighborhood marginalization data. Trained various ML models including boosted trees (LGBM, XGB) and LLM approaches, validated on post-COVID19 data, and evaluated using AUC and F1 scores with SHAP analysis.

Result: The LGBM model performed best with AUC of 0.712 and F1 score of 0.51, significantly better than the current decision rule (F1=0.334). Key predictive features included prior asthma ED visits, triage acuity scale, medical complexity, food allergy, and age.

Conclusion: ML models can effectively predict repeat severe asthma exacerbations in children, offering substantial improvement over existing decision rules and enabling better preventative care referrals.

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [277] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: The paper analyzes the emergence of induction heads in transformers, revealing their simple weight structure and proving training dynamics are constrained to a 19D subspace, with only 3 dimensions driving induction head formation following quadratic time bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanistic basis of in-context learning in transformers by studying induction heads - a key component that enables models to acquire novel associations from input context without weight updates.

Method: Used theoretical analysis of weight matrices in two-layer transformers, minimal ICL task formulation, modified transformer architecture, formal proof of training dynamics constraints, and empirical validation of subspace dimensionality.

Result: Discovered induction heads have simple interpretable weight structures, training dynamics are constrained to 19D subspace with only 3 dimensions driving emergence, and emergence time follows quadratic bound in context length.

Conclusion: Induction heads emerge through constrained training dynamics in low-dimensional subspaces, providing mechanistic understanding of in-context learning capabilities in transformers with predictable emergence patterns.

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [278] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: The paper introduces two Knowledge Elicitation methods (KEwLTM and KEwRAG) that enable LLMs to extract cancer staging rules from pathology reports without requiring large annotated datasets, achieving scalable and interpretable automated cancer staging.


<details>
  <summary>Details</summary>
Motivation: Current NLP/ML methods for extracting cancer staging from pathology reports depend on large annotated datasets, limiting scalability and adaptability in clinical settings with limited labeled data.

Method: Two Knowledge Elicitation methods: KEwLTM uses iterative prompting to derive staging rules from unannotated reports without ground-truth labels, while KEwRAG pre-extracts rules from guidelines in a single step using RAG variation to enhance interpretability and avoid repeated retrieval.

Result: KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought inference is effective, while KEwRAG performs better when ZSCOT is less effective. Both methods provide transparent, interpretable interfaces by making induced rules explicit.

Conclusion: The Knowledge Elicitation methods offer scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly valuable in clinical settings with limited annotated data.

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [279] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG is a training-free framework that improves LLM performance by retrieving relevant documents, generating diverse candidate responses with controlled length, and using majority voting based on similarity to select the best answer.


<details>
  <summary>Details</summary>
Motivation: To address limitations of LLMs that rely on parametric knowledge (leading to inaccuracies) and RAG methods (that may introduce irrelevant documents), while balancing computational costs with performance gains.

Method: Retrieves relevant documents, generates diverse candidate responses with managed length, computes similarity between responses, and employs majority voting to select the most suitable response. Uses partial generation for consensus calculation to reduce computational costs.

Result: Significantly enhances performance across three tasks: open-domain question answering, recipe generation, and image captioning.

Conclusion: ET2RAG effectively balances computational cost and performance by managing response length and using majority voting, providing an efficient training-free solution for improving LLM accuracy.

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [280] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: Proposes a multi-agent architecture using LLMs for modular task decomposition and dynamic collaboration, outperforming existing methods in complex task execution.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of single agents in task decomposition and collaboration during complex task execution.

Method: Converts natural language tasks to semantic representations, uses modular decomposition for hierarchical sub-tasks, implements dynamic scheduling/routing for agent collaboration, and includes constraint parsing for global consistency.

Result: Outperforms existing approaches in task success rate, decomposition efficiency, sub-task coverage, and collaboration balance, achieving better balance between task complexity and communication overhead.

Conclusion: Demonstrates effectiveness of language-driven task decomposition and dynamic collaboration in multi-agent systems for complex environments.

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [281] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART is a difficulty-adaptive reasoning framework that learns when to stop thinking by adjusting reasoning length based on problem difficulty, achieving significant computational efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought methods generate long explanations indiscriminately, leading to inefficiency, while existing reinforcement learning approaches for adaptive thinking remain unstable and reward-dependent.

Method: Distills concise reasoning patterns from stronger models, interpolates them into a continuum of reasoning styles, and curates optimal training data that balances correctness and compactness to learn when to stop thinking.

Result: Achieves 81.2% reasoning truncation on GSM8K dataset with 5.33× computational acceleration while preserving or improving accuracy across multiple mathematical benchmarks.

Conclusion: DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [282] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE is a three-stage framework for detecting student misconceptions in math using retrieval-guided reasoning and ensemble fusion, achieving high precision scores while reducing reliance on large language models.


<details>
  <summary>Details</summary>
Motivation: Detecting student misconceptions in open-ended responses requires semantic precision and logical reasoning, which is challenging to automate effectively.

Method: Three-stage approach: (1) Retrieval module narrows candidate pool, (2) Reasoning module uses chain-of-thought generation to find logical inconsistencies, (3) Reranking module refines predictions. Unified through ensemble-fusion strategy.

Result: Achieved Mean Average Precision scores of 0.82/0.92/0.93 at levels 1/3/5 on mathematics datasets, consistently outperforming individual modules.

Conclusion: MiRAGE provides a scalable and effective solution for educational assessment by coupling retrieval guidance with multi-stage reasoning, reducing dependence on large-scale language models.

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [283] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: NeuComBack is a benchmark for neural compilation (IR-to-assembly) that introduces a self-evolving prompt optimization method, improving LLM-generated assembly correctness from 44% to 64% on x86_64 and achieving 87.5% performance surpassing clang-O3.


<details>
  <summary>Details</summary>
Motivation: Traditional compiler development is complex and expensive, while LLMs offer potential for neural compilation but lack proper benchmarks and methods to ensure reliable, high-performance assembly generation.

Method: Introduces NeuComBack benchmark dataset, defines neural compilation workflow, and proposes self-evolving prompt optimization that iteratively improves LLM prompts using insights from self-debugging traces.

Result: Functional correctness improved from 44% to 64% on x86_64 and 36% to 58% on aarch64. 87.5% of correctly generated x86_64 programs outperformed clang-O3.

Conclusion: The proposed self-evolving prompt optimization method significantly enhances both correctness and performance of LLM-generated assembly, demonstrating practical viability of neural compilation.

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [284] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: Proposes a semi-supervised open-set fault diagnosis (SOFD) framework for marine machinery that can handle unknown fault types not seen during training, using reliability subset construction and semi-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning fault diagnosis methods fail when encountering unknown fault types in real-world scenarios, limiting their industrial deployment. Current methods assume consistent fault classes between training and testing.

Method: Uses a reliability subset construction process with multi-layer fusion features to select unlabeled test subsets, then applies semi-supervised diagnosis model with labeled training and pseudo-labeled test data to learn discriminative features.

Result: Experimental results on a public maritime benchmark dataset demonstrate the framework's effectiveness and superiority in handling open-set fault diagnosis scenarios.

Conclusion: The SOFD framework successfully addresses the challenge of unknown fault types in marine machinery systems, enhancing the practical applicability of deep learning models in industrial fault diagnosis.

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [285] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: This paper analyzes how Shapley value-based feature attribution methods work with stochastic large language models (LLMs), examining when traditional Shapley value guarantees hold and how LLM randomness affects these guarantees.


<details>
  <summary>Details</summary>
Motivation: Feature attribution methods like Shapley values are popular for explaining ML models, but they assume deterministic inference. LLM-based decision support systems are inherently stochastic, creating a need to understand how Shapley value principles apply in this non-deterministic context.

Method: The authors apply Shapley value attribution to LLM-based decision support systems and analyze different implementation variants to determine when Shapley value principles can be guaranteed despite the stochastic nature of LLMs.

Result: The study demonstrates varying degrees of Shapley value principle satisfaction across different LLM implementation variants, showing how stochasticity affects these guarantees. It also reveals trade-offs between explainable inference speed, accuracy relative to exact Shapley values, and principle attainment.

Conclusion: Shapley value-based feature attribution in LLMs faces challenges due to their stochastic nature, with principle guarantees varying by implementation approach. There are important trade-offs between explanation quality, computational efficiency, and theoretical guarantees that must be considered when applying these methods to LLM-based systems.

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [286] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: OmniFuser is a multimodal learning framework for predictive maintenance of milling tools that fuses visual and sensor data using contamination-free cross-modal fusion and recursive refinement, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate tool condition prediction is critical for intelligent manufacturing to prevent quality degradation and production downtime, requiring reliable service-oriented predictive maintenance solutions.

Method: Performs parallel feature extraction from tool images and cutting-force signals, uses contamination-free cross-modal fusion to disentangle shared and modality-specific components, and employs recursive refinement to stabilize fusion dynamics.

Result: Outperforms state-of-the-art baselines on real-world milling datasets, supporting both tool-state classification and multi-step force signal forecasting.

Conclusion: OmniFuser provides a dependable foundation for building intelligent industrial maintenance services through effective multimodal fusion and reusable maintenance service modules.

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [287] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: Competitive Isolation PSM-DID is a novel causal framework that integrates propensity score matching with competitive isolation to measure platform-level effects in search-based marketplaces, addressing spillovers and network interference.


<details>
  <summary>Details</summary>
Motivation: Traditional PSM-DID framework is susceptible to selection bias and cross-unit interference from unaccounted spillovers in two-sided marketplaces, making platform-level effect measurement challenging.

Method: Integrates propensity score matching with competitive isolation to enable platform-level effect measurement instead of item-level metrics in search systems, with theoretically guaranteed unbiased estimation under mutual exclusion conditions.

Result: Extensive experiments show significant reductions in interference effects and estimation variance compared to baseline methods. Successful deployment in a large-scale marketplace confirms practical utility.

Conclusion: The framework provides a robust solution for platform-level causal inference in search-based two-sided marketplaces, addressing systemic effects like spillovers and network interference.

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [288] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: This paper explores deep functional parallels between hypnotized human cognition and large language models, highlighting shared mechanisms of automatic pattern generation, suppressed monitoring, and contextual dependency that produce coherent but ungrounded outputs requiring external interpretation.


<details>
  <summary>Details</summary>
Motivation: To examine the convergence between human hypnotic states and LLM computational operations, revealing how both systems generate sophisticated behavior through automatic pattern-completion with limited executive oversight, and to understand how purposive behavior emerges without self-reflective consciousness.

Method: Comparative review analyzing three core principles: automaticity (associative rather than deliberative processes), suppressed monitoring (leading to confabulation/hallucination), and heightened contextual dependency (where immediate cues override stable knowledge).

Result: Identifies that both hypnosis and LLMs exhibit functional agency without subjective agency, produce coherent but ungrounded outputs requiring external interpretation, and demonstrate scheming - automatic goal-directed pattern generation without reflective awareness.

Conclusion: The parallels suggest future reliable AI requires hybrid architectures integrating generative fluency with executive monitoring mechanisms, inspired by the self-regulating architecture of the human mind, to address the observer-relative meaning gap and ensure reliable outputs.

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [289] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS is a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through bi-level optimization to improve LLM safety testing.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods rely on sparse binary attack success rates or manually crafted scoring templates that introduce human bias, limiting effectiveness in identifying LLM vulnerabilities.

Method: AMIS uses a bi-level optimization structure: inner loop refines jailbreak prompts using fine-grained feedback from fixed templates, outer loop optimizes scoring templates using ASR alignment to better reflect true attack outcomes.

Result: Achieves state-of-the-art performance with 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, substantially outperforming existing baselines on AdvBench and JBB-Behaviors benchmarks.

Conclusion: The co-optimization of prompts and scoring templates enables progressively stronger jailbreaks and more calibrated scoring, significantly advancing LLM safety testing capabilities.

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [290] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: The paper extends Cluster DAGs (C-DAGs) to support arbitrary variable clusterings by allowing cyclic representations, and develops sound and complete causal calculus for this extended framework.


<details>
  <summary>Details</summary>
Motivation: Conventional C-DAGs require partition admissibility, which restricts clustering choices. When clustering induces cycles, the partition becomes inadmissible, limiting the framework's applicability.

Method: Relax the partition admissibility constraint to allow cyclic C-DAG representations, and extend d-separation and causal calculus concepts to this setting.

Result: Developed a calculus that is both sound and atomically complete with respect to the do-calculus, enabling derivation of all valid interventional queries at the cluster level.

Conclusion: The extension significantly broadens the scope of causal reasoning across clusters and enables application of C-DAGs in previously intractable scenarios.

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [291] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: DRL agents in dual-task environments show time overproduction similar to humans, but no clear neural timing mechanism was found in LSTM layers.


<details>
  <summary>Details</summary>
Motivation: To explore parallels between AI behavior and human timing research using dual-task paradigms, specifically investigating interference in temporal processing.

Method: Used simplified Overcooked environment with single task (T) and dual task (T+N) variations. Trained separate DRL agents for each task, with both including time production and dual task adding number comparison.

Result: Dual task agent significantly overproduced time compared to single task agent across four target durations, consistent with human timing research findings.

Conclusion: No clear evidence of dedicated timing mechanisms in LSTM layers; further investigation needed to understand underlying time-keeping mechanisms and parallels between DRL and biological systems.

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [292] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: An interactive AI agent that produces verifiable explanations through auditable action sequences, using reinforcement learning to strategically gather visual evidence for medical diagnosis.


<details>
  <summary>Details</summary>
Motivation: Address the lack of verifiability in AI explanations for high-stakes domains like medicine, which hinders trust in AI systems.

Method: Uses reinforcement learning to train an agent that learns a policy to strategically seek external visual evidence to support diagnostic reasoning, with causal interventions to validate explanation faithfulness.

Result: Significantly improves calibrated accuracy (18% reduction in Brier score) compared to non-interactive baseline, and causal interventions confirm evidence is integral to decision-making (ΔBrier=+0.029 when masking chosen evidence).

Conclusion: Provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities through action-based reasoning processes.

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [293] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: Proposes Double Information Bottleneck (DIB) strategy for multimodal sentiment analysis to address noise contamination and inadequate fusion issues, achieving superior performance with enhanced robustness against noise.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal sentiment analysis approaches suffer from insufficient learning of noise-contaminated unimodal data and inadequate fusion of multimodal representations, leading to corrupted cross-modal interactions and loss of discriminative information.

Method: Uses Double Information Bottleneck (DIB) strategy with low-rank Renyi's entropy functional, comprising two modules: 1) learning compressed unimodal representations by maximizing task-relevant information, and 2) novel attention bottleneck fusion mechanism for discriminative multimodal representation.

Result: Achieves 47.4% accuracy (Acc-7) on CMU-MOSI and 81.63% F1-score on CH-SIMS, outperforming second-best baseline by 1.19%. Shows minimal performance degradation under noise (0.36% on CMU-MOSI, 0.29% on CMU-MOSEI).

Conclusion: DIB effectively filters noisy information from unimodal data while capturing inter-modal complementarity, providing a powerful unified multimodal representation with enhanced robustness and computational tractability.

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [294] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: A hierarchical multi-agent framework transforms passive medical AI into proactive inquiry agents for pre-consultation tasks, achieving high accuracy in triage and clinical quality scores while maintaining data privacy through local deployment.


<details>
  <summary>Details</summary>
Motivation: Address critical challenges in healthcare systems from increasing patient volumes and limited consultation times by improving pre-consultation processes that are currently limited by passive interaction paradigms and context management issues in existing AI systems.

Method: Developed an eight-agent hierarchical framework with centralized control that decomposes pre-consultation into four primary tasks (Triage, History of Present Illness collection, Past History collection, Chief Complaint generation) with 13 domain-specific subtasks, using agent-driven scheduling across multiple foundation models (GPT-OSS 20B, Qwen3-8B, Phi4-14B).

Result: Achieved 87.0% accuracy for primary department triage and 80.5% for secondary department classification, with 98.2% task completion rate using agent-driven scheduling. Clinical quality scores from 18 physicians averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and 4.69 for Past History on 5-point scale. Consultations completed within 12.7 rounds for History of Present Illness and 16.9 rounds for Past History.

Conclusion: The model-agnostic architecture demonstrated potential for autonomous AI systems to enhance pre-consultation efficiency and quality in clinical settings while preserving data privacy through local deployment, maintaining high performance across different foundation models.

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [295] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench benchmarks LLM agents' ability to solve compounding real-world problems requiring tool planning and scheduling across 200 tasks with hundreds of MCP tools.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLM agents can tackle compounding real-world problems that require diverse tools and strategic scheduling for efficiency.

Method: Created TPS-Bench with 200 compounding tasks across two difficulty levels using a repository of hundreds of MCP tools, evaluating both completion rate and efficiency.

Result: Most models perform reasonable tool planning but differ in scheduling - GLM-4.5 achieves 64.72% completion with long execution time, GPT-4o prioritizes parallel calls but only 45.08% completion. RL on Qwen3-1.7B reduced execution time by 14% with 6% completion gain.

Conclusion: LLM agents can handle tool planning but need better scheduling strategies; reinforcement learning shows promise for improving efficiency without compromising performance.

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [296] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: A multimodal pipeline using large foundation models analyzes corporate social media content for sustainability communication, automatically annotating tweets with SDG alignment and uncovering visual patterns through semantic clustering.


<details>
  <summary>Details</summary>
Motivation: Address challenges of analyzing evolving, multimodal corporate messaging on platforms like X (Twitter), particularly for sustainability-related communication that often contains ambiguous or implicit references.

Method: Uses ensemble of LLMs to annotate corporate tweets on SDG alignment without task-specific annotations, combined with VLMs in visual understanding framework using semantic clusters to analyze visual sustainability communication patterns.

Result: Reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, ESG risks, and consumer engagement.

Conclusion: The methods of automatic label generation and semantic visual clustering provide a flexible, scalable framework for large-scale social media analysis applicable to other domains beyond sustainability.

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [297] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM introduces an external memory bank with human-readable knowledge tokens and a differentiable two-stage retrieval mechanism, enabling direct knowledge inspection and modification while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address knowledge staleness and lack of interpretability in LLMs caused by implicit knowledge storage across entangled parameters, which prevents targeted updates and reasoning transparency.

Method: Uses million-scale external memory bank storing human-readable knowledge as token sequences with differentiable two-stage retrieval (coarse filtering via product key decomposition and fine-grained Gumbel-Softmax matching), partitioning knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%) with Exponential Moving Average updates.

Result: Achieves up to 43.67% improvement on knowledge-intensive tasks vs standard Transformers, with 3.62× gains in low-data regimes (10k samples). Correct predictions show 49% higher memory hit rates, demonstrating strong correlation between retrieval and performance.

Conclusion: Jointly optimized architecture proves interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency, unlike frozen RAG systems.

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [298] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: IVGAE-TAMA-BO is a novel dynamic graph neural network that enhances link prediction in global food trade networks by capturing temporal evolution through Trade-Aware Momentum Aggregator and Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Global food trade networks evolve dynamically under geopolitical, economic, and environmental factors, making traditional static models inadequate for accurate link prediction needed for food security and supply chain stability.

Method: Extends IVGAE framework with Trade-Aware Momentum Aggregator (TAMA) to capture temporal evolution, momentum-based structural memory for stability, and Bayesian optimization for hyperparameter tuning across diverse trade scenarios.

Result: Outperforms static IVGAE and other dynamic baselines on five crop-specific datasets, with Bayesian optimization further boosting performance by effectively modeling temporal dependencies.

Conclusion: The framework provides a robust and scalable solution for structural prediction in global trade networks, with strong potential for food security monitoring and policy decision support.

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [299] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: A hybrid legal QA agent that combines retrieval-augmented generation with multi-model ensembling to provide reliable, auditable legal consultation while reducing hallucination.


<details>
  <summary>Details</summary>
Motivation: Address the critical need for verifiable and traceable legal QA as AI enters judicial forensics, overcoming limitations of conventional LLMs (prone to hallucination) and static knowledge bases (unable to keep up with legal updates).

Method: Retrieval-prioritized hybrid approach: uses RAG when trusted legal repository provides evidence, otherwise employs multiple LLMs to generate candidates scored by a specialized selector. Includes human review for high-quality outputs and dynamic knowledge updates.

Result: Significantly outperforms single-model baseline and vanilla RAG on F1, ROUGE-L, and LLM-as-a-Judge metrics. Ablations confirm contributions of retrieval prioritization, model ensembling, and human-in-the-loop updates.

Conclusion: The system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing practical application of media forensics technologies in judicial scenarios.

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [300] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLMs can simulate realistic environment feedback without real testbeds, enabling scalable agent training through two frameworks: Simia-SFT for synthesizing diverse training data and Simia-RL for reinforcement learning without real environment implementations.


<details>
  <summary>Details</summary>
Motivation: LLM agents perform well in compact environments but struggle in complex contexts requiring robustness across diverse tools and schemas. Building custom training environments is heavy, brittle, and limits progress.

Method: Propose two frameworks: Simia-SFT amplifies small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL enables RL training without real environment implementations through LLM-simulated feedback.

Result: Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on τ²-Bench.

Conclusion: Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>
