<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [cs.RO](#cs.RO) [Total: 34]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: An explainable AI system combining a CNN classifier and Vision-Language Model achieves 96.5% accuracy in detecting AI-generated images, with fast inference suitable for edge devices, while providing artifact localization and semantic explanations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of verifying visual authenticity due to increasingly realistic AI-generated imagery by creating an interpretable detection system.

Method: Combines a lightweight convolutional classifier (Faster-Than-Lies) with Vision-Language Model (Qwen2-VL-7B), uses autoencoder-based reconstruction error maps for artifact localization, and categorizes 70 visual artifacts into 8 semantic groups.

Result: Achieves 96.5% accuracy on extended CiFAKE dataset with adversarial perturbations, maintains 175ms inference time on 8-core CPUs, and generates explainable text for detected anomalies.

Conclusion: Demonstrates feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery, with potential applications in forensics, industrial inspection, and social media moderation.

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [2] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: CountFormer is a transformer-based framework that uses DINOv2 features and positional embeddings for class-agnostic object counting, achieving state-of-the-art performance on structurally complex scenes.


<details>
  <summary>Details</summary>
Motivation: Humans can count objects by perceiving visual repetition and structural relationships, but existing counting models struggle with complex shapes, internal symmetry, or overlapping components.

Method: Built on CounTR architecture, replaces visual encoder with self-supervised DINOv2 foundation model, incorporates positional embedding fusion, and uses lightweight convolutional decoder to produce density maps.

Result: Achieves performance comparable to state-of-the-art methods on FSC-147 dataset, with superior accuracy on structurally intricate or densely packed scenes.

Conclusion: Integrating foundation models like DINOv2 enables counting systems to approach human-like structural perception, advancing toward truly general and exemplar-free counting.

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [3] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Cl√©ment Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: A novel framework using fixed cameras and deep learning for continuous monitoring of floating debris in rivers, with geometric modeling for object size estimation.


<details>
  <summary>Details</summary>
Motivation: Floating anthropogenic debris in rivers negatively impacts biodiversity, water quality, and human activities like navigation and recreation, requiring effective monitoring solutions.

Method: Utilizes fixed in-situ cameras with deep learning models for debris detection and quantification, plus geometric modeling using camera characteristics for object size estimation from 2D images.

Result: Identified optimal deep learning models for accuracy and speed under complex conditions, demonstrated importance of dataset protocols including negative images and temporal leakage considerations.

Conclusion: Feasible to develop robust, low-cost automated monitoring systems using projective geometry with regression corrections for urban aquatic environments.

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [4] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: RareFlow is a physics-aware super-resolution framework for remote sensing imagery that maintains geometric fidelity and semantic accuracy under out-of-distribution conditions, using dual-conditioning and uncertainty quantification to prevent feature hallucination.


<details>
  <summary>Details</summary>
Motivation: Super-resolution for remote sensing imagery often fails under out-of-distribution conditions, producing visually plausible but physically inaccurate results, especially for rare geomorphic features captured by diverse sensors.

Method: Uses dual-conditioning architecture with Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Includes multifaceted loss function for spectral/radiometric consistency and stochastic forward pass for uncertainty quantification.

Result: In blind evaluations, geophysical experts rated outputs approaching ground truth fidelity, significantly outperforming state-of-the-art baselines with nearly 40% reduction in FID and superior perceptual metrics.

Conclusion: RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift.

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [5] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: A training-free method for 3D scene generation that repurposes text-to-3D object diffusion models as modular tile generators, enabling scalable synthesis of large, coherent scenes through multi-tile denoising and seamless blending.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene generation methods are limited to single objects, require domain-specific training, or lack full 360-degree viewability, creating a need for more flexible and scalable approaches.

Method: Reformulates scene generation as a multi-tile denoising problem using overlapping 3D regions independently generated by text-to-3D object diffusion models, with seamless blending via weighted averaging.

Result: Enables scalable synthesis of large, coherent scenes with local semantic control, diverse scene layouts, efficient generation, and flexible editing without scene-level datasets or retraining.

Conclusion: Establishes a simple yet powerful foundation for general-purpose, language-driven 3D scene construction that inherits generalization capabilities from object-level priors with minimal heuristics.

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [6] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: LHT-CLIP is a training-free framework that improves CLIP for semantic segmentation by exploiting visual discriminability across layers, attention heads, and tokens through three techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement.


<details>
  <summary>Details</summary>
Motivation: CLIP models struggle with semantic segmentation due to misalignment between image-level pre-training and pixel-level dense prediction needs. Prior methods inherit global alignment bias from preceding layers, leading to suboptimal performance.

Method: Three training-free techniques: 1) Semantic-spatial reweighting to restore visual discriminability, 2) Selective head enhancement using consistently discriminative attention heads, 3) Abnormal token replacement based on sparse activation patterns.

Result: Achieves state-of-the-art performance on 8 semantic segmentation benchmarks across diverse scenarios without additional training, auxiliary networks, or extensive hyperparameter tuning.

Conclusion: LHT-CLIP effectively bridges the gap between CLIP's pre-training and segmentation requirements, demonstrating practical deployment potential through systematic exploitation of existing model capabilities.

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [7] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride is a pipeline for generating coherent scene-level captions in instructional videos without manual segmentation, using adaptive frame sampling and multimodal reasoning to improve temporal coherence and educational value.


<details>
  <summary>Details</summary>
Motivation: Scene-level captioning in instructional videos enhances learning by aligning visual cues with textual guidance, but existing methods often fail to capture temporal structure, leading to incoherent captions that undermine educational intent.

Method: Uses adaptive frame sampling and multimodal windowing to capture key transitions, employs multimodal chain-of-thought for action-object pairs, and refines them with dynamic stride window selection to balance temporal context and redundancy.

Result: Outperforms strong baselines (VLLaMA3, GPT-4o) on both N-gram metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore), producing more temporally coherent and informative captions.

Conclusion: DynaStride shows promising direction for improving AI-powered instructional content generation by effectively integrating visual semantics and temporal reasoning in scene-level captions.

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [8] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: TurboPortrait3D enhances 3D portrait generation by combining image-to-avatar pipelines with diffusion models to refine noisy renders while maintaining 3D consistency and low latency.


<details>
  <summary>Details</summary>
Motivation: Existing image-to-3D models for portraits produce visual artifacts and fail to preserve subject identity, while diffusion models lack 3D awareness and are computationally expensive.

Method: Uses a feedforward image-to-avatar pipeline to generate initial 3D representations, then refines noisy renders with a single-step diffusion model conditioned on input images for multi-view consistency. Includes pre-training on synthetic multi-view data and fine-tuning on real images.

Result: Qualitatively and quantitatively outperforms current state-of-the-art methods for portrait novel-view synthesis while being time-efficient.

Conclusion: Image-space diffusion models can significantly enhance 3D portrait generation quality while maintaining 3D-awareness and low latency, achieving better identity preservation and detail than existing approaches.

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [9] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: PlanarGS enhances 3D Gaussian Splatting (3DGS) for indoor scenes by incorporating planar priors and geometric supervision to address ambiguous geometry in low-texture regions.


<details>
  <summary>Details</summary>
Motivation: 3DGS struggles with ambiguous geometry in large, low-texture indoor scenes due to reliance on photometric loss alone.

Method: Uses Language-Prompted Planar Priors (LP3) with vision-language segmentation, cross-view fusion, and geometric priors. Adds planar consistency and geometric supervision terms to optimize Gaussians.

Result: Significantly outperforms state-of-the-art methods on standard indoor benchmarks, achieving accurate and detailed 3D surface reconstruction.

Conclusion: PlanarGS effectively addresses 3DGS limitations in indoor scenes through planar and geometric priors, enabling high-fidelity reconstruction.

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [10] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,Jo√£o Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: AIRe is an adaptive training scheme for implicit neural representations that uses neuron pruning to reduce redundancy and input frequency densification to improve capacity, achieving better size-quality trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current methods for implicit neural representations require manual frequency selection and architecture tuning through heavy hyperparameter optimization, which is inefficient and suboptimal.

Method: AIRe employs neuron pruning to identify and remove less-contributory neurons via targeted weight decay and structured pruning, followed by input frequency densification in underfitting spectrum regions.

Result: Experiments on images and SDFs show that AIRe reduces model size while maintaining or improving reconstruction quality compared to existing methods.

Conclusion: AIRe provides an effective adaptive training approach that automatically refines INR architectures during optimization, eliminating the need for extensive hyperparameter tuning while achieving superior performance.

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [11] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: Neural USD is a framework for precise object editing in generative models using hierarchical scene representation inspired by Universal Scene Descriptor (USD), enabling per-object control over appearance, geometry, and pose.


<details>
  <summary>Details</summary>
Motivation: Current controllable generative models often cause unintended global changes when trying to edit specific objects in scenes, lacking precise and iterative object editing capabilities.

Method: Introduces Neural USD with structured hierarchical scene representation, accommodates diverse signals with minimal constraints, and applies fine-tuning to disentangle control signals for appearance, geometry, and pose.

Result: The framework enables iterative and incremental workflows with per-object control, demonstrating effective disentanglement of control signals for precise editing.

Conclusion: Neural USD represents a significant step forward in addressing precise object editing challenges in generative modeling through structured hierarchical scene representation.

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [12] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision is a novel image guardrail system that integrates human-like reasoning to enhance adaptability and transparency, achieving state-of-the-art performance while being 16x faster than GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Traditional image guardrail models have limitations including misclassification due to pure feature-based learning without semantic reasoning, inability to adapt to emerging threats without costly retraining, and lack of transparency.

Method: The approach includes an effective data collection and generation framework, policy-following training pipeline, customized loss function, and diverse QA generation and training strategy. It dynamically aligns with evolving safety policies at inference time without retraining.

Result: SafeVision outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster. It achieves state-of-the-art performance on different benchmarks.

Conclusion: SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats, addressing limitations of traditional models.

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [13] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: A framework that brings chain-of-thought reasoning to chest X-ray interpretation, enabling transparent stepwise reasoning similar to clinicians while maintaining competitive classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for medical image analysis are opaque and lack the transparent, stepwise reasoning that clinicians rely on for diagnosis and decision-making.

Method: Two-stage training: reasoning-style supervised fine-tuning followed by reinforcement learning with verifiable rewards over X-ray abnormalities. Combines high-fidelity visual encoding with explicit reasoning traces.

Result: Achieves competitive multi-label classification in out-of-distribution evaluation. In reader studies with expert radiologists, reasoning traces increased confidence, supported error auditing, and reduced report finalization time.

Conclusion: The approach enables trustworthy, explainable AI in medical imaging where reasoning quality is as critical as prediction quality, supporting safer human-AI collaboration and clinical auditability.

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [14] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: Proposes joint prediction of multiple Fourier components for arbitrary-scale super-resolution to improve quality and efficiency over existing recurrent methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods predict Fourier components one by one using recurrent neural networks, leading to performance degradation and inefficiency due to independent prediction.

Method: Predicting multiple Fourier components jointly instead of one by one.

Result: Improves both quality and efficiency in arbitrary-scale super-resolution.

Conclusion: Joint prediction of multiple components is more effective than independent prediction for super-resolution tasks.

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [15] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo is a comprehensive benchmark for egocentric AI assistants that evaluates memory, understanding, and cross-memory reasoning across 14+ hours of synchronized multi-modal data in realistic streaming scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate AI assistant capabilities in isolation, lack realistic streaming scenarios, or only support short-term tasks, failing to capture the real-world requirements of egocentric AI assistants.

Method: Created a dataset with over 14 hours per participant of synchronized egocentric video, audio, and text across four domains, aligned on a unified timeline with human-refined visual narrations and speech transcripts. Defines 12 diagnostic subtasks across three core capabilities.

Result: Contains 3,291 human-verified QA items spanning multiple question formats, evaluated in streaming settings with proposed metrics for real-time accuracy and memory persistence.

Conclusion: TeleEgo provides a realistic and comprehensive evaluation framework to advance the development of practical AI assistants that can process multi-modal inputs, respond in real time, and retain long-term memory.

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [16] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: Proposes AdvBlur, a novel diabetic retinopathy classification method that uses adversarial blurred images and dual-loss functions to improve domain generalization across different datasets and imaging conditions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for diabetic retinopathy detection struggle with robustness due to distributional variations from different acquisition devices, demographic disparities, and imaging conditions.

Method: Integrates adversarial blurred images into the dataset and employs a dual-loss function framework to address domain generalization challenges.

Result: Achieves competitive performance compared to state-of-the-art domain generalization DR models on unseen external datasets, with comprehensive evaluations across multiple datasets.

Conclusion: AdvBlur effectively mitigates the impact of unseen distributional variations and demonstrates strong domain generalization capabilities for diabetic retinopathy classification.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [17] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning M√ºller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich M√§chler,Jan Egger*

Main category: cs.CV

TL;DR: The SEG.A challenge introduced a large public dataset for aortic vessel tree segmentation, showing that 3D U-Net architectures with ensemble methods and custom post-processing achieved the best performance.


<details>
  <summary>Details</summary>
Motivation: Automated analysis of aortic vessel trees from CT angiography has clinical potential but was hindered by lack of shared high-quality data.

Method: Created a large multi-institutional dataset and benchmarked automated algorithms on hidden test sets, with optional surface meshing tasks for computational simulations.

Result: Deep learning methods dominated, with 3D U-Net architectures performing best. Ensemble methods significantly outperformed individual models, and performance was linked to algorithmic design and training data characteristics.

Conclusion: The challenge established a new performance benchmark and provides a lasting resource to drive future innovation toward clinically translatable tools.

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [18] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench is the first benchmark for evaluating foundation models on Mars-related tasks using orbital and surface imagery, addressing the lack of standardized evaluation frameworks in Mars science.


<details>
  <summary>Details</summary>
Motivation: Foundation models have shown strong generalization in many domains but their application to Mars science remains limited due to the absence of standardized benchmarks and evaluation frameworks.

Method: Created Mars-Bench with 20 datasets spanning classification, segmentation, and object detection tasks focused on key geologic features (craters, cones, boulders, frost), providing standardized datasets and baseline evaluations using various pre-trained models.

Result: Results suggest Mars-specific foundation models may offer advantages over general-domain counterparts, motivating further exploration of domain-adapted pre-training.

Conclusion: Mars-Bench establishes a standardized foundation for developing and comparing machine learning models for Mars science, with all data, models, and code publicly available.

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [19] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: APT is a black-box framework that uses LLMs to generate human-readable adversarial suffixes for text-to-image models, bypassing safety filters through dual-evasion strategy and achieving high transferability.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming methods for T2I models require white-box access, use inefficient per-prompt optimization, and generate meaningless prompts that are easily blocked by filters.

Method: Uses alternating optimization-finetuning pipeline between adversarial suffix optimization and LLM fine-tuning, with dual-evasion strategy: perplexity-based filtering constraint and banned-token penalties.

Result: Demonstrates excellent red-teaming performance with human-readable, filter-resistant prompts, and superior zero-shot transferability to unseen prompts and commercial APIs.

Conclusion: APT effectively exposes critical vulnerabilities in T2I models through human-readable adversarial prompts that bypass safety mechanisms.

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [20] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: Residual Networks (ResNet) with skip connections overcome vanishing gradient problems in deep CNNs, enabling training of very deep networks with better accuracy and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Training very deep convolutional neural networks is challenging due to vanishing gradient problems that prevent effective learning in deep layers.

Method: ResNet uses skip connections (shortcuts) that bypass intermediate layers, allowing gradients to flow directly through the network and enabling training of networks with hundreds of layers.

Result: On CIFAR-10 dataset, ResNet-18 achieved 89.9% accuracy compared to 84.1% for traditional deep CNN of similar depth, with faster convergence and more stable training.

Conclusion: Residual Networks with skip connections effectively solve the vanishing gradient problem, enabling successful training of very deep neural networks with improved performance.

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [21] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA is a one-stage parameter-efficient fine-tuning method that uses nonlinear kernel functions in low-rank decomposition and adaptive bi-level sparsity allocation to achieve state-of-the-art performance with significantly reduced memory usage compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current sparse tuning methods have two main limitations: they use gradient information to locate task-relevant weights which overlooks parameter adjustments during fine-tuning, and they require storing all weight matrices in the optimizer leading to high memory usage.

Method: SNELLA uses a one-stage approach that selectively updates weight matrices by adding them to another sparse matrix merged by two low-rank learnable matrices with nonlinear kernel functions. It also employs an adaptive bi-level sparsity allocation mechanism that allows weights to compete across and inside layers based on importance scores in an end-to-end manner.

Result: SNELLA achieves SOTA performance with 1.8% higher Top-1 accuracy on FGVC benchmark (91.9% vs 90.1%) compared to SPT-LoRA, and reduces memory usage by 31.1%-39.9% across models with parameter scales from 86M to 632M on classification, segmentation, and generation tasks.

Conclusion: SNELLA successfully overcomes the limitations of current sparse tuning methods by providing a more efficient one-stage approach that achieves better performance with significantly reduced memory consumption through innovative low-rank decomposition with nonlinear kernels and adaptive sparsity allocation.

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [22] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: COLA is a training-free framework that uses optimal transport to restore cross-modal alignment between image and text features in CLIP models under adversarial attacks, improving robustness without compromising clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP are vulnerable to adversarial perturbations due to misalignment between text and image features, which gets amplified under attacks. Existing methods overlook these feature gaps.

Method: COLA projects adversarial image embeddings onto a subspace spanned by class text features to filter distortions, then models images/texts as distributions over augmented views and refines alignment via optimal transport with integrated subspace projection.

Result: Extensive evaluations on 14 benchmarks show COLA improves robustness significantly, with average 6.7% improvement on ImageNet variants under PGD attacks while maintaining high clean accuracy.

Conclusion: COLA effectively addresses adversarial misalignment in VLMs through optimal transport-based cross-modality alignment, providing training-free robustness enhancement compatible with existing fine-tuned models.

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [23] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: BOB is a fine-tuning strategy for T2I models that extracts class-agnostic attributes (background, pose) from real examples, conditions on them during fine-tuning, and marginalizes them out during generation to improve synthetic data quality for fine-grained classification.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning T2I models with few real examples for synthetic dataset generation can cause overfitting and reduce diversity, limiting effectiveness for classification tasks.

Method: Extract class-agnostic attributes from real examples, explicitly condition T2I model on these attributes during fine-tuning, and marginalize them out during generation to preserve generative prior and reduce overfitting.

Result: BOB achieves state-of-the-art performance, outperforming DataDream by 7.4% on Aircraft dataset and performing better than using 10 real images in 3 out of 4 benchmarks with only 5 real images augmented with synthetic data.

Conclusion: BOB effectively mitigates overfitting in T2I fine-tuning, preserves model diversity, and significantly improves synthetic data quality for fine-grained classification tasks.

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [24] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: OmniText is a training-free generalist framework for Text Image Manipulation (TIM) that addresses limitations of existing text inpainting methods by enabling text removal, style control, and preventing duplicated letters through self-attention inversion and cross-attention redistribution.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based text inpainting methods have three key limitations: inability to remove text, lack of style control over rendered text, and tendency to generate duplicated letters, which hinders their broader applicability to TIM tasks.

Method: Uses self-attention inversion for text removal to reduce text hallucinations, redistributes cross-attention to reduce text hallucination, and introduces novel loss functions (cross-attention content loss and self-attention style loss) in a latent optimization framework for controllable inpainting.

Result: OmniText achieves state-of-the-art performance across multiple TIM tasks and metrics, comparable with specialist methods, and introduces OmniText-Bench benchmark dataset for evaluating diverse TIM tasks.

Conclusion: OmniText is the first generalist method capable of performing diverse TIM tasks including text removal, rescaling, repositioning, and insertion/editing with various styles, demonstrating superior performance over other text inpainting methods.

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [25] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: The paper discovers a positive correlation between interpretability and classifiability in pre-trained vision models, proposes an Inherent Interpretability Score (IIS) to quantify interpretability, and shows that improving classifiability also enhances interpretability.


<details>
  <summary>Details</summary>
Motivation: Pre-trained visual models prioritize classifiability but lack clear understanding of whether they can simultaneously achieve high interpretability and classifiability, which is crucial for widespread applications requiring interpretable representations.

Method: Proposes Inherent Interpretability Score (IIS) that quantifies representation interpretability by measuring information loss and the ratio of interpretable semantics that can be captured by interpretations.

Result: Surprisingly discovered positive correlation between interpretability and classifiability - representations with higher classifiability provide more interpretable semantics. Fine-tuning with interpretability maximization further improves classifiability, and predictions based on interpretations show less accuracy degradation.

Conclusion: Practitioners can unify improvements in both interpretability and classifiability for pre-trained vision models, as these two properties are positively correlated rather than conflicting objectives.

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [26] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: UHKD is a knowledge distillation framework that uses frequency domain transformations to enable effective knowledge transfer between heterogeneous teacher-student model architectures, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods struggle with architectural diversity between teacher and student models, especially when using intermediate features, due to semantic discrepancies. Most methods focus only on logits space and degrade in heterogeneous scenarios.

Method: Proposes Unified Heterogeneous Knowledge Distillation (UHKD) using Fourier transform to capture global feature information in frequency domain. Includes Feature Transformation Module for teacher features and Feature Alignment Module for student features with multi-level matching. Uses joint objective combining MSE on intermediate features and KL divergence on logits.

Result: Experiments on CIFAR-100 and ImageNet-1K show gains of 5.59% and 0.83% respectively over the latest method, demonstrating effectiveness in unifying heterogeneous representations.

Conclusion: UHKD effectively addresses architectural heterogeneity in knowledge distillation by leveraging frequency domain representations, enabling efficient utilization of visual knowledge across different model architectures.

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [27] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo is a large-scale multi-view RGB-D video dataset of canine movements with 1.2k sequences from 10 dogs, addressing limitations of existing datasets. The authors introduce a three-stage optimization pipeline for motion recovery and establish four benchmark settings for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing dog motion datasets lack multi-view and real 3D data, have limited scale and diversity. DogMo aims to overcome these limitations to advance research in dog motion recovery.

Method: Three-stage instance-specific optimization pipeline: 1) coarse alignment, 2) dense correspondence supervision, 3) temporal regularization. Fits SMAL model to motion sequences. Dataset includes 1.2k sequences from 10 dogs with multi-view RGB-D data.

Result: Created DogMo dataset with rich variation in motion and breed. Established four motion recovery benchmark settings (monocular/multi-view, RGB/RGB-D). Developed optimization pipeline for accurate motion recovery.

Conclusion: DogMo provides a principled foundation for advancing dog motion recovery research and opens new directions at the intersection of computer vision, computer graphics, and animal behavior modeling.

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [28] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: ETC is a training-free framework that accelerates diffusion models by reusing model outputs with error control and trend consistency, achieving 2.65x speedup over FLUX with minimal quality degradation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have excellent generative quality but suffer from slow iterative sampling. Existing training-free acceleration methods ignore denoising trends and lack error control, causing trajectory deviations and result inconsistencies.

Method: ETC introduces (1) a consistent trend predictor that projects historical denoising patterns into stable future directions across multiple approximation steps, and (2) a model-specific error tolerance search mechanism that identifies transition points from semantic planning to quality refinement to derive corrective thresholds.

Result: ETC achieves 2.65x acceleration over FLUX with only -0.074 SSIM score degradation in consistency, demonstrating significant speed improvement while maintaining high quality.

Conclusion: The proposed ETC framework effectively accelerates diffusion models by leveraging trend consistency and error-aware mechanisms, providing a practical solution for fast diffusion sampling without compromising generation quality.

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [29] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: A training-free framework that uses LLMs to generate explicit layouts and object-centric VLM reranking to improve text-to-image model compositionality while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Modern text-to-image models struggle with compositionality issues like accurate object counts, attributes, and spatial relations despite their realism.

Method: Combines object-centric approach with self-refinement: uses LLMs to synthesize explicit layouts from prompts, injects layouts into generation process, and employs object-centric VLM to iteratively rerank candidates for prompt alignment.

Result: Achieves stronger scene alignment with prompts compared to recent text-to-image models while preserving aesthetic quality.

Conclusion: The framework successfully improves layout faithfulness in text-to-image generation through explicit layout-grounding and self-refine-based inference-time scaling.

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [30] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: VC4VG is a caption optimization framework for text-to-video generation that analyzes caption requirements, provides design methodology, and introduces a benchmark with T2V-specific metrics.


<details>
  <summary>Details</summary>
Motivation: High-quality video-text pairs are crucial for text-to-video generation, but strategies for optimizing video captions specifically for T2V training remain underexplored.

Method: Analyze caption content from T2V perspective, decompose essential elements for video reconstruction, propose principled caption design methodology, and construct VC4VG-Bench benchmark with fine-grained, multi-dimensional metrics.

Result: Extensive T2V fine-tuning experiments show strong correlation between improved caption quality and video generation performance, validating the framework's effectiveness.

Conclusion: VC4VG provides an effective framework for optimizing video captions for text-to-video generation, with released benchmark tools and code to support further research.

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [31] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: A systematic framework for autonomous driving scene understanding using Vision-Language Models, featuring task-specific prompting, visual assembly, and optimized inference parameters, achieving over 70% accuracy on both clean and corrupted data.


<details>
  <summary>Details</summary>
Motivation: To enhance Vision-Language Models' performance on safety-critical autonomous driving tasks by addressing challenges in perception, prediction, planning, and corruption detection across diverse question types.

Method: Four-component framework: 1) Mixture-of-Prompts router for question classification, 2) Task-specific prompts with coordinate systems and reasoning techniques, 3) Visual assembly module for multi-view image composition, 4) Optimized inference parameters per task.

Result: Achieved 70.87% average accuracy on Phase-1 (clean data) and 72.85% on Phase-2 (corrupted data) using Qwen2.5-VL-72B model.

Conclusion: Structured prompting and spatial grounding significantly improve VLM performance on autonomous driving tasks, demonstrating the effectiveness of systematic framework design for safety-critical applications.

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [32] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: GenTrack is a novel multi-object tracking method that combines stochastic and deterministic approaches to handle varying numbers of targets, using PSO with fitness measures and social interactions to improve tracking accuracy and reduce ID switches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-object tracking including unknown and time-varying numbers of targets, maintaining target identity consistency, handling nonlinear dynamics, and improving performance with weak/noisy detectors during occlusions.

Method: Hybrid tracking using stochastic and deterministic approaches, PSO with fitness measures to guide particles, integration of social interactions among targets, comprehensive state and observation model with space consistency, appearance, detection confidence, track penalties, and social scores.

Result: Superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with reduced ID switches and track loss especially during occlusions.

Conclusion: GenTrack provides an effective solution for robust multi-object tracking with publicly available source code, offering three variants for flexible implementation and setting a new baseline for visual MOT systems.

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [33] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: UAP-SAM2 is a universal adversarial attack method designed to target SAM2 (Segment Anything Model 2) by addressing two key challenges: prompt dependency and semantic entanglement across frames, achieving superior performance over existing attacks.


<details>
  <summary>Details</summary>
Motivation: SAM2's robustness against adversarial attacks remains unexplored, and existing attacks on SAM may not transfer effectively due to architectural differences in SAM2, particularly regarding prompt guidance and frame-to-frame semantic consistency.

Method: The method uses a target-scanning strategy to divide frames into regions with random prompts to reduce prompt dependency, and a dual semantic deviation framework that distorts semantics within frames and disrupts consistency across consecutive frames to optimize a Universal Adversarial Perturbation (UAP).

Result: Extensive experiments on six datasets across two segmentation tasks show that UAP-SAM2 significantly outperforms state-of-the-art attacks by a large margin, demonstrating its effectiveness against SAM2.

Conclusion: UAP-SAM2 successfully addresses the unique challenges of attacking SAM2, providing a robust universal adversarial attack method that effectively compromises SAM2's segmentation capabilities across various prompts and frames.

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [34] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: A visual multi-object tracking method combining stochastic particle filtering with deterministic association for consistent tracking under nonlinear dynamics and varying target numbers, with particle swarm optimization for better sampling and novel identity preservation during occlusions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of maintaining identifier consistency in multi-object tracking with unknown and time-varying target numbers under nonlinear dynamics, where traditional methods struggle with track divergence during occlusions and interactions.

Method: Joint stochastic-deterministic approach: particle filter with PSO optimization using motion consistency, appearance similarity, and social-interaction cues; deterministic association with cost matrix considering spatial consistency, detection confidence, and track penalties; smooth state updating with identity preservation; velocity regression for trend-seed velocities.

Result: Experimental results show superior performance compared to state-of-the-art trackers, with the method working effectively for both pre-recorded videos and live camera streams.

Conclusion: The proposed hybrid approach successfully maintains identifier consistency in challenging tracking scenarios with nonlinear dynamics and varying target numbers, demonstrating improved performance over existing methods while being flexible for different video sources.

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [35] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: CLFSeg is an encoder-decoder framework that combines fuzzy logic with convolutional layers to improve polyp and cardiac segmentation, addressing uncertainty and boundary ambiguity while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional CNN models have limited generalizability, robustness, and inability to handle uncertainty in medical image segmentation, which affects performance for early cancer detection and treatment planning.

Method: Proposes CLFSeg framework with Fuzzy-Convolutional (FC) module that aggregates convolutional layers and fuzzy logic to identify local/global features while minimizing uncertainty and noise. Uses binary cross-entropy with dice loss to handle class imbalance and focus on tiny/boundary regions.

Result: Exceptional performance on four public datasets (CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, ACDC), surpassing existing SOTA methods and effectively focusing on relevant anatomical regions of interest.

Conclusion: CLFSeg improves segmentation performance while ensuring computational efficiency, making it a potential solution for real-world medical diagnostic scenarios.

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [36] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: MC-SJD is a training-free, lossless parallel decoding framework that accelerates autoregressive visual generation by extending Speculative Jacobi Decoding (SJD) with coupling-based token sampling to improve acceptance rates.


<details>
  <summary>Details</summary>
Motivation: Autoregressive modeling for visual generation suffers from slow inference speed due to per-token generation requiring thousands of steps, limiting practical adoption despite its strong generation capabilities.

Method: Extends SJD with MC-SJD, an information-theoretic approach using coupling to maximize probability of sampling identical draft tokens across iterations while preserving lossless property, requiring only single-line modification to existing algorithm.

Result: Achieves up to ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, with no degradation in output quality.

Conclusion: MC-SJD provides substantial performance gains for AR visual generation through simple yet effective coupling-based approach that maintains lossless decoding while dramatically improving inference speed.

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [37] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: ID¬≤Face is a training-centric face anonymization framework that uses a conditional diffusion model with identity-masked learning to disentangle identity from non-identity attributes, enabling direct anonymization without inference-time optimization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based anonymization methods rely on inference-time interventions that cause distribution shifts and entangle identity with non-identity attributes, degrading visual quality and data utility.

Method: Uses a conditional diffusion model with identity-masked learning, an Identity-Decoupled Latent Recomposer with Identity VAE, bidirectional latent alignment, and an Identity-Guided Latent Harmonizer with soft-gating. Includes Orthogonal Identity Mapping to suppress identity leakage.

Result: ID¬≤Face outperforms existing methods in visual quality, identity suppression, and utility preservation.

Conclusion: The proposed training-centric approach effectively disentangles identity and non-identity features, enabling high-quality anonymization without inference-time optimization.

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [38] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: SCOPE is a novel visual token pruning method for MLLMs that jointly optimizes saliency and coverage to preserve semantic completeness while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods focus only on saliency (attention scores), leading to semantic incompleteness in selected tokens despite computational savings.

Method: Proposes SCOPE score that integrates saliency with token-coverage gain, iteratively selecting tokens based on both their importance and ability to cover semantic relationships with other tokens.

Result: Extensive experiments on vision-language benchmarks using LLaVA-1.5 and LLaVA-Next show consistent outperformance over prior pruning approaches.

Conclusion: SCOPE effectively balances computational efficiency with semantic preservation by considering both saliency and coverage in visual token selection.

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [39] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: This paper introduces the first event-based microsaccade dataset using simulated eye movements and evaluates spiking neural networks for microsaccade classification, achieving ~90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional microsaccade studies using eye trackers or frame-based analysis are costly, limited in scalability and temporal resolution. Event-based sensing offers high-speed, low-latency alternative for capturing fine-grained spatiotemporal changes.

Method: Used Blender to render high-fidelity eye movement scenarios simulating microsaccades (0.5-2.0¬∞ angular displacement, 7 classes), converted to event streams using v2e. Evaluated with Spiking-VGG11/13/16 models and proposed Spiking-VGG16Flow (optical-flow-enhanced variant) in SpikingJelly.

Result: Models achieved around 90% average accuracy, successfully classifying microsaccades by angular displacement independent of event count or duration.

Conclusion: Demonstrates potential of spiking neural networks for fine motion recognition and establishes benchmark for event-based vision research. Dataset, code, and trained models will be publicly available.

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [40] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes Lipschitz-regularized object detection (LROD) to address the functional mismatch between image restoration and detection networks, improving detection robustness in adverse conditions like haze and low light.


<details>
  <summary>Details</summary>
Motivation: Traditional cascade frameworks that use image restoration as pre-processing for object detection suffer from functional mismatch, where restoration networks perform smooth transformations while detectors have discontinuous decision boundaries, causing instability and sensitivity to minor perturbations.

Method: The authors analyze the functional differences through Lipschitz continuity and propose LROD framework that integrates image restoration directly into detector's feature learning. They implement this as LR-YOLO, which harmonizes Lipschitz continuity of both tasks during training.

Result: Extensive experiments on haze and low-light benchmarks show that LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy compared to traditional approaches.

Conclusion: The proposed Lipschitz-regularized framework effectively addresses the functional mismatch between restoration and detection networks, providing a more stable and robust solution for object detection in adverse conditions.

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [41] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: DeshadowMamba introduces a selective state space model (Mamba) with CrossGate modulation and ColorShift regularization for shadow removal, achieving state-of-the-art performance by preserving structural integrity and chromatic consistency.


<details>
  <summary>Details</summary>
Motivation: Existing attention-based shadow removal models mix illumination cues from irrelevant regions, causing distorted structures and inconsistent colors. The authors revisit shadow removal from a sequence modeling perspective to address these limitations.

Method: Proposed DeshadowMamba uses Mamba (selective state space model) with CrossGate directional modulation to inject shadow-aware similarity into input gates, and ColorShift regularization with contrastive learning to suppress color contamination.

Result: Extensive experiments on public benchmarks show DeshadowMamba achieves state-of-the-art visual quality and strong quantitative performance in shadow removal.

Conclusion: The proposed method successfully adapts sequence modeling to shadow removal requirements, achieving robust structural integrity and chromatic consistency through directional state transitions and color-aware regularization.

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [42] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: UtilGen is a utility-centric data augmentation framework that optimizes synthetic data generation for downstream tasks using task feedback, achieving 3.87% average accuracy improvement over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Most data augmentation methods focus on visual quality (fidelity/diversity) but neglect task-specific requirements, which vary across different tasks and network architectures.

Method: Uses a weight allocation network to evaluate task-specific utility of synthetic samples, then iteratively refines generation via dual-level optimization: model-level (tailors generative model) and instance-level (adjusts prompt embeddings and initial noise).

Result: Achieved superior performance on 8 benchmark datasets with 3.87% average accuracy improvement over SOTA, producing more impactful and task-relevant synthetic data.

Conclusion: The paradigm shift from visual characteristics-centric to task utility-centric data augmentation is effective, as demonstrated by UtilGen's consistent performance improvements across diverse datasets.

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [43] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: A training-free one-shot attribution method using image resynthesis to identify the source of synthetic images, outperforming existing techniques in few-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Synthetic image source attribution is challenging in data scarcity conditions, requiring few-shot or zero-shot classification capabilities.

Method: Generate a prompt describing the image, resynthesize it with all candidate sources, and attribute to the model producing the closest resynthesis in feature space.

Result: The proposed resynthesis method outperforms state-of-the-art few-shot approaches when only a few training samples are available.

Conclusion: The method is effective for synthetic image attribution in data-scarce conditions, and the new dataset provides a valuable benchmark for future few-shot and zero-shot methods.

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [44] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: ViPER is a self-bootstrapping framework that enhances fine-grained visual perception in VLMs through a coarse-to-fine progressive learning approach with self-critiquing and self-prediction, achieving significant performance gains while maintaining generalizability.


<details>
  <summary>Details</summary>
Motivation: Address the critical bottleneck of limited fine-grained visual perception in Vision-Language Models (VLMs) caused by scarcity of high-quality data and limitations of existing methods like SFT (compromises general capabilities) and RFT (prioritizes textual reasoning over visual perception).

Method: Two-stage task structuring visual perception as coarse-to-fine progressive process; ViPER framework with self-critiquing and self-prediction; synergistic integration of image-level and instance-level reconstruction with two-stage reinforcement learning strategy; closed-loop training paradigm using internally synthesized data.

Result: Applied to Qwen2.5-VL family, produced Qwen-Viper series with average gain of 1.7% on seven comprehensive benchmarks across various tasks and up to 6.0% on fine-grained perception; consistently superior performance across different vision-language scenarios while maintaining generalizability.

Conclusion: ViPER enables self-improvement in perceptual capabilities and provides concrete evidence for the reciprocal relationship between generation and understanding, representing a breakthrough for developing more autonomous and capable VLMs.

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [45] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: The paper explores prompt learning as an efficient adaptation strategy for few-shot remote sensing scene classification, demonstrating that it consistently outperforms traditional baselines like zero-shot CLIP and linear probes.


<details>
  <summary>Details</summary>
Motivation: Remote sensing applications face challenges due to limited labeled data and high annotation costs across diverse domains. Direct application of vision-language models like CLIP is suboptimal due to domain gaps and need for task-specific semantic adaptation.

Method: Systematically evaluated several prompt learning methods: Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. Compared against zero-shot CLIP with hand-crafted prompts and linear probe on frozen CLIP features.

Result: Prompt learning consistently outperforms both baselines in few-shot scenarios across multiple benchmark datasets. Prompting with Self-Regulating Constraints achieved the most robust cross-domain performance in cross-dataset generalization tests.

Conclusion: Prompt learning serves as a scalable and efficient solution for bridging domain gaps in satellite and aerial imagery, providing a strong foundation for future research in remote sensing applications.

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [46] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: A novel switching Dual-Student architecture with Loss-Aware Exponential Moving Average for semi-supervised medical image segmentation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in teacher-student frameworks caused by strong correlation and unreliable knowledge transfer between teacher and student networks in semi-supervised medical image segmentation.

Method: Introduces a switching Dual-Student architecture that selects the most reliable student at each iteration, and a Loss-Aware Exponential Moving Average strategy to dynamically ensure the teacher absorbs meaningful information from students.

Result: Extensively evaluated on 3D medical image segmentation datasets, the method outperforms state-of-the-art semi-supervised methods.

Conclusion: The plug-and-play framework effectively improves segmentation accuracy under limited supervision by enhancing dual-student collaboration and preventing error reinforcement.

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [47] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: W2-Net addresses the overlooked challenge in Referring Expression Counting (REC) where annotation points focus on class-representative locations, causing models to neglect attribute information. It introduces dual-query mechanism and Subclass Separable Matching to improve subclass discrimination and counting accuracy.


<details>
  <summary>Details</summary>
Motivation: Current REC approaches suffer from annotation bias where points are placed on class-representative locations (e.g., heads), forcing models to focus on class-level features while ignoring attribute information from other visual regions (e.g., legs for 'walking'). This fundamental challenge has been overlooked.

Method: W2-Net decouples REC into 'what to count' and 'where to see' via dual-query mechanism: what-to-count (w2c) queries localize objects, while where-to-see (w2s) queries seek attribute-specific visual regions. Also introduces Subclass Separable Matching (SSM) with repulsive force to enhance inter-subclass separability during label assignment.

Result: Significantly outperforms state-of-the-art on REC-8K dataset: reduces counting error by 22.5% (validation) and 18.0% (test), and improves localization F1 by 7% and 8% respectively.

Conclusion: W2-Net effectively addresses the annotation bias problem in REC by explicitly decoupling object localization and attribute feature extraction, achieving substantial improvements in both counting accuracy and localization precision through its dual-query mechanism and novel matching strategy.

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [48] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mah√©,St√©phanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg is a lightweight, modular framework that converts research-grade stroke lesion segmentation models into deployable clinical applications with equivalent performance to original PyTorch pipelines.


<details>
  <summary>Details</summary>
Motivation: Deep learning frameworks like nnU-Net achieve state-of-the-art performance but are difficult to deploy clinically due to heavy dependencies and monolithic design.

Method: Decouples preprocessing, inference, and postprocessing: preprocessing uses Anima toolbox with BIDS-compliant outputs, inference uses ONNX Runtime with Float16 quantization (reducing model size by ~50%), and provides both GUI and CLI interfaces distributed as Python scripts and standalone Windows executable.

Result: On 300 sub-acute and chronic stroke subjects, segmentation performance was equivalent to original PyTorch pipeline (Dice difference <10^-3).

Conclusion: High-performing research pipelines can be successfully transformed into portable, clinically usable tools while maintaining equivalent performance.

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [49] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: Proposes MLSN, a luminance-aware multi-scale network for polarization image fusion that handles complex lighting environments through dynamic luminance injection, global-local feature fusion, and brightness enhancement.


<details>
  <summary>Details</summary>
Motivation: Polarization image fusion combines S0 and DOLP images to reveal surface properties, but faces challenges with inherent contrast differences in polarized images and complex luminance environments.

Method: Uses multi-scale spatial weight matrix with brightness branch for dynamic luminance injection, windowed self-attention for global-local feature fusion, and Brightness-Enhancement module for nonlinear luminance correction.

Result: Outperforms state-of-the-art methods on MSP, PIF and GAND datasets with MS-SSIM and SD metrics higher than average by 8.57%-63.53%. Also introduces MSP dataset with 1000 polarized image pairs.

Conclusion: MLSN effectively handles complex lighting in polarization image fusion through luminance-aware architecture and achieves superior performance compared to existing methods.

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [50] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergstr√∂m,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: This paper systematically studies how radiology reports can be used during pre-training and fine-tuning to improve medical image classification, finding that text-based pre-training helps when labels are well-represented in text but can be detrimental otherwise, and that fine-tuning with reports can provide significant benefits.


<details>
  <summary>Details</summary>
Motivation: Radiology reports contain rich expert annotations but require manual work from radiologists. The paper aims to determine when and how these reports can be leveraged during training to improve image-only classification, especially for tasks where labels are weakly associated with text.

Method: Conducted systematic study of using radiology reports during both pre-training and fine-tuning across diagnostic and prognostic tasks (e.g., 12-month readmission), varying training set sizes. Compared explicit image-text alignment approaches.

Result: Found that: (1) Pre-training with reports helps when labels are well-represented in text but can be detrimental otherwise; (2) Fine-tuning with reports can lead to significant improvements and sometimes has larger impact than pre-training method.

Conclusion: Provides actionable insights into when and how to leverage privileged text data for medical image classifiers, highlighting gaps in current research and showing that fine-tuning with reports can be more impactful than pre-training in certain settings.

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [51] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mah√©,Elise Bannier,St√©phanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: REFLECT, a flow-based generative model, is evaluated for unsupervised detection of focal and non-lesional abnormalities in post-stroke patients, showing that training on healthy controls improves detection performance compared to training on lesion-free stroke patient data.


<details>
  <summary>Details</summary>
Motivation: Post-stroke MRI reveals secondary structural changes like atrophy and ventricular enlargement that are poorly captured by supervised segmentation methods but serve as important imaging biomarkers for recovery and outcome.

Method: Used REFLECT, a flow-based generative model, for unsupervised anomaly detection. Trained two models: one on lesion-free slices from stroke patients (ATLAS) and another on healthy controls (IXI). Performance was evaluated using dual-expert annotations and Free-Response ROC analysis for anomaly maps.

Result: The IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43) compared to the ATLAS-trained model.

Conclusion: Training on fully healthy anatomy improves the modeling of normal variability, enabling broader and more reliable detection of structural abnormalities in post-stroke patients.

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [52] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: A sensor-free approach using satellite imagery and machine learning to monitor reservoir volume in Lebanon, achieving over 95% accuracy in water segmentation and less than 1.5% volume estimation error.


<details>
  <summary>Details</summary>
Motivation: Sustainable management of Qaraaoun Reservoir requires reliable monitoring despite frequent sensor malfunctions and limited maintenance capacity in Lebanon.

Method: Integrates open-source satellite imagery (Sentinel-2, Landsat), a new water segmentation index, and Support Vector Regression machine learning to estimate reservoir volume from surface area without ground measurements.

Result: Water segmentation aligns with ground truth for >95% of shoreline; SVR model achieves <1.5% error of full capacity and R¬≤ >0.98; generates 50 years of time-series data.

Conclusion: The method provides robust, cost-effective, sensor-independent monitoring that can be replicated for other water bodies and supports climate change research.

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [53] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive evaluation framework for explainable AI (XAI) methods in semantic segmentation, addressing the gap in evaluation strategies for this specific computer vision task.


<details>
  <summary>Details</summary>
Motivation: There's a need for transparency and trust in AI models, especially in safety-critical domains. While XAI evaluation has progressed for classification tasks, evaluation strategies for semantic segmentation remain underexplored despite its spatial and contextual complexities.

Method: The authors propose a systematic evaluation framework specifically designed for XAI in semantic segmentation, using pixel-level evaluation strategies and carefully designed metrics to account for spatial and contextual task complexities.

Result: Simulation results using class activation mapping (CAM)-based XAI schemes demonstrate the efficiency, robustness, and reliability of the proposed methodology.

Conclusion: The framework contributes to advancing transparent, trustworthy, and accountable semantic segmentation models by providing fine-grained interpretability insights.

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [54] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: DCIC-sgp is a learned image compression framework that uses self-generated priors to separate global structures from local textures, achieving better rate-distortion performance and reducing geometric deformation artifacts at low bitrates.


<details>
  <summary>Details</summary>
Motivation: Current learned image compression methods struggle to model complex correlation structures in natural images, particularly the entanglement of invariant global structures with transient local textures, leading to severe geometric deformation at low bitrates.

Method: The framework uses functional decomposition with self-generated priors that first encode structural backbone information, then holistically modulate the entire compression pipeline. This deep conditioning allows the analysis transform to focus on residual high-entropy details, effectively disentangling information streams.

Result: The method substantially mitigates geometric deformation artifacts at low bitrates and achieves significant BD-rate reductions of 14.4%, 15.7%, and 15.1% against VVC test model VTM-12.1 on Kodak, CLIC, and Tecnick datasets respectively.

Conclusion: The deeply-conditioned approach with self-generated priors effectively disentangles information streams in image compression, leading to superior performance and reduced artifacts compared to conventional methods.

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [55] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: Video Diffusion Models (VDMs) pretrained on spatiotemporal data show better data efficiency and compositional understanding than LLMs in visual tasks, suggesting VDMs as promising visual foundation models.


<details>
  <summary>Details</summary>
Motivation: LLMs have succeeded in language tasks with little supervision but struggle in the visual domain with compositional understanding and sample efficiency. The authors investigate if VDMs can bridge this gap.

Method: The authors compare pretrained LLMs and VDMs equipped with lightweight adapters on various visual tasks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata.

Result: VDMs demonstrate higher data efficiency than LLMs across all benchmarks, indicating stronger inductive biases for structure and dynamics from spatiotemporal pretraining.

Conclusion: Video pretraining provides inductive biases that support progress toward effective visual foundation models, outperforming language-based approaches in visual reasoning tasks.

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [56] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: This paper proposes a deep learning-based system for detecting and segmenting three types of tea leaf diseases (Red Rust, Helopeltis, Red spider mite) using object detection models and custom damage area calculation.


<details>
  <summary>Details</summary>
Motivation: To develop an automated system for identifying tea leaf diseases caused by pests and pathogens, and to quantify the damaged area on leaves for better disease management.

Method: Evaluated SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for object detection, and used Mask R-CNN for instance segmentation with a custom method to calculate damaged leaf portions.

Result: Faster R-CNN ResNet50 V1 performed better with 25% mAP compared to SSD MobileNet V2's 20.9% mAP. Both models showed low precision and recall values on IOU range 0.50:0.95.

Conclusion: Faster R-CNN ResNet50 V1 is more effective for tea leaf disease detection than SSD MobileNet V2, and the custom segmentation method enables quantification of disease damage areas.

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [57] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavou√©*

Main category: cs.CV

TL;DR: Kineo is a fully automatic, calibration-free pipeline for markerless motion capture from unsynchronized, uncalibrated consumer RGB cameras that simultaneously calibrates cameras and reconstructs 3D keypoints at metric scale with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing calibration-free motion capture approaches suffer from high computational cost and reduced reconstruction accuracy, while traditional methods require precise camera calibration that limits accessibility for non-experts and in-the-wild captures.

Method: Leverages 2D keypoints from off-the-shelf detectors with confidence-driven spatio-temporal keypoint sampling and graph-based global optimization to simultaneously calibrate cameras (including distortion coefficients) and reconstruct 3D keypoints and dense scene point maps. Introduces pairwise reprojection consensus score for reliability quantification.

Result: Substantial improvements over prior methods: reduces camera translation error by 83-85%, camera angular error by 86-92%, and world mean-per-joint error by 83-91%. Processes multi-view sequences faster than their duration in specific configurations (36min for 1h20min footage).

Conclusion: Kineo provides an efficient, accurate, and accessible solution for calibration-free motion capture that outperforms state-of-the-art methods while being practical for real-world applications, with open-source release to promote adoption.

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [58] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: Decoupled MeanFlow converts pretrained flow models into flow map models without architectural changes, enabling high-quality image generation in 1-4 steps with over 100x faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing denoising generative models require many steps due to discretization error, and flow map training typically requires incompatible architectural modifications with pretrained models.

Method: A decoding strategy that conditions final blocks of diffusion transformers on subsequent timesteps, converting flow models to flow maps without architectural changes, combined with enhanced training techniques.

Result: Achieved 1-step FID of 2.16 on ImageNet 256x256 and 2.12 on 512x512, surpassing prior art. With 4 steps, achieved FID of 1.51 and 1.68, nearly matching flow model performance.

Conclusion: Training flow models first then converting them to flow maps is more efficient than training flow maps from scratch, enabling high-quality fast sampling with minimal architectural overhead.

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [59] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: NeuralRTI improves reflectance transformation imaging quality but is computationally expensive. DisK-NeuralRTI uses knowledge distillation to reduce computational costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Traditional RTI methods (PTM, HSH) struggle with complex reflectance fields using few coefficients, causing artifacts. NeuralRTI provides superior quality but is computationally expensive for interactive relighting on limited hardware.

Method: Proposed DisK-NeuralRTI uses knowledge distillation to reduce computational costs of NeuralRTI while maintaining rendering quality, addressing the limitation of expensive custom decoder networks.

Result: The method successfully reduces computational costs compared to NeuralRTI while preserving rendering quality, making interactive relighting feasible on limited hardware.

Conclusion: Knowledge distillation enables efficient NeuralRTI rendering without sacrificing quality, making high-quality reflectance transformation imaging more accessible.

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [60] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Latent Sketchpad is a framework that enhances Multimodal Large Language Models (MLLMs) by adding visual planning capabilities through internal visual scratchpads, enabling models to interleave textual reasoning with visual latent generation.


<details>
  <summary>Details</summary>
Motivation: MLLMs excel at visual understanding but struggle with complex scenarios requiring visual planning and imagination. Inspired by human sketching as visual thinking, the framework aims to equip MLLMs with generative visual thought capabilities without compromising reasoning ability.

Method: Integrates visual generation directly into MLLMs' native autoregressive reasoning process using two components: Context-Aware Vision Head for autoregressive visual representation production, and pretrained Sketch Decoder for rendering visual latents into human-interpretable images.

Result: Experiments on MazePlanning dataset show Latent Sketchpad delivers comparable or superior reasoning performance to backbone MLLMs, and generalizes across distinct frontier MLLMs including Gemma3 and Qwen2.5-VL.

Conclusion: By extending textual reasoning to visual thinking, the framework opens new opportunities for richer human-computer interaction and broader applications in multimodal AI.

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [61] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: OSWorld-MCP is the first comprehensive benchmark for evaluating multimodal agents' tool invocation, GUI operation, and decision-making abilities in real-world computer environments, addressing the gap in fair assessment of tool usage capabilities.


<details>
  <summary>Details</summary>
Motivation: Past evaluations focused mainly on GUI interaction skills while overlooking tool invocation abilities enabled by Model Context Protocol (MCP), creating unfair comparisons between agents with integrated tools and those evaluated only on GUI interaction.

Method: Developed an automated code-generation pipeline to create tools and combined them with curated existing tools, resulting in 158 high-quality tools across 7 common applications that were manually validated for functionality, applicability, and versatility.

Result: MCP tools improved task success rates (e.g., from 8.3% to 20.4% for OpenAI o3, from 40.1% to 43.3% for Claude 4 Sonnet), but even the strongest models had relatively low tool invocation rates (only 36.3%), indicating significant room for improvement.

Conclusion: OSWorld-MCP provides a fair and comprehensive benchmark that deepens understanding of multimodal agents' tool usage capabilities and sets a new standard for evaluating performance in complex, tool-assisted environments.

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [62] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: A deep learning method using Kolmogorov-Arnold Networks with Gaussian Radial Basis Functions to correct scatter artifacts in cone-beam CT by modeling the rotational symmetry of scatter distribution in the projection domain.


<details>
  <summary>Details</summary>
Motivation: CBCT suffers from scatter artifacts during data acquisition, which cause CT value bias and reduced tissue contrast, degrading diagnostic accuracy in reconstructed images.

Method: The method models point scatter function using Gaussian RBFs embedded into KAN layers, leveraging rotational symmetry of scatter distribution in projection domain and KAN's nonlinear mapping capabilities for learning high-dimensional scatter features.

Result: Experimental validation on synthetic and real-scan data shows the model effectively corrects scatter artifacts in reconstructed images and outperforms current methods in quantitative metrics.

Conclusion: The proposed deep learning approach successfully addresses CBCT scatter artifacts by combining physical prior knowledge of scatter distribution with KAN's powerful function mapping capabilities, achieving superior correction performance.

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [63] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: Proposes a dual-branch CNN for face forgery detection using spatial and frequency domain features with adaptive fusion and unified loss function, achieving strong performance across multiple forgery types.


<details>
  <summary>Details</summary>
Motivation: Address the growing threat of realistic face forgery techniques (face swapping, attribute editing, diffusion-based synthesis) used for misinformation, identity fraud, and defamation, highlighting the urgent need for robust detection methods.

Method: Dual-branch CNN with RGB branch for semantic information and frequency branch for high-frequency artifacts, using channel attention for adaptive feature fusion and FSC Loss (focal loss + supervised contrastive loss + frequency center margin loss) for enhanced class separability.

Result: Achieves strong performance on DiFF benchmark across four forgery types (text-to-image, image-to-image, face swap, face edit) and outperforms average human accuracy.

Conclusion: The proposed method demonstrates effectiveness in detecting various face forgery types and has potential to contribute to safeguarding AI ecosystems against visual forgery attacks.

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [64] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0 is a comprehensive behavioral dataset capturing pathologists' visual search and decision-making during cancer diagnosis from whole-slide images, including eye-tracking, mouse interactions, and diagnostic decisions from 19 pathologists interpreting 397 WSIs.


<details>
  <summary>Details</summary>
Motivation: Pathologists' diagnostic accuracy averages only 70% and adding a second pathologist doesn't improve consistency. The field lacks behavioral data to explain diagnostic errors and inconsistencies in WSI interpretation.

Method: Collected 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data from 19 pathologists interpreting 397 WSIs using an ecologically valid testbed called PTAH.

Result: Recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. Created a comprehensive dataset that captures the full diagnostic workflow dynamics.

Conclusion: PathoGaze1.0 provides valuable behavioral data that can help explain diagnostic errors and inconsistencies, and could improve training for both pathologists and AI systems supporting human experts.

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [65] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: Proposes Group Relative Attention Guidance (GRAG), a method to enable continuous and fine-grained control over editing intensity in Diffusion-in-Transformer models by reweighting token delta values, achieving smoother control than Classifier-Free Guidance.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods based on Diffusion-in-Transformer models lack effective control over editing degree, limiting customization capabilities.

Method: Analyzes MM-Attention mechanism in DiT models, identifies bias vectors as inherent editing behavior, and proposes GRAG to reweight token delta values to modulate model focus between input image and editing instruction.

Result: GRAG can be integrated with minimal code (4 lines), consistently enhances editing quality, and provides smoother and more precise control over editing degree compared to Classifier-Free Guidance.

Conclusion: GRAG enables continuous and fine-grained control over editing intensity without tuning, improving editing quality and control precision in Diffusion-in-Transformer based image editing.

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [66] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: SAGE is a zero-shot method for video transitions that combines structural guidance with generative synthesis to create smooth, semantically consistent transitions between diverse clips without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current video transition methods struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, creating a need for content-aware and visually coherent transitions.

Method: SAGE combines structural guidance (line maps and motion flow) with generative synthesis in a zero-shot approach, drawing on artistic workflows like aligning silhouettes and interpolating salient features.

Result: Extensive experiments show SAGE outperforms both classical and generative baselines (FILM, TVG, DiffMorpher, VACE, GI) on quantitative metrics and user studies for transitions between diverse clips.

Conclusion: SAGE provides an effective zero-shot solution for creating smooth, semantically consistent video transitions that bridge diverse clips with large temporal or semantic gaps.

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [67] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: MIC-BEV is a Transformer-based BEV perception framework for infrastructure-based multi-camera 3D object detection that supports variable cameras and shows strong robustness under sensor degradation.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based detection models underperform in infrastructure scenarios due to multi-view setups, diverse camera configurations, degraded visual inputs, and various road layouts.

Method: Uses a graph-enhanced fusion module to integrate multi-view image features into BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. Also introduces M2I synthetic dataset for training.

Result: Achieves state-of-the-art performance in 3D object detection on both M2I and RoScenes datasets, with strong robustness under challenging conditions including extreme weather and sensor degradation.

Conclusion: MIC-BEV demonstrates potential for real-world deployment in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy.

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [68] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: Vision Transformers naturally develop object binding capabilities during self-supervised pretraining, with over 90% accuracy in determining whether two patches belong to the same object, particularly in models like DINO, MAE, and CLIP.


<details>
  <summary>Details</summary>
Motivation: To investigate whether object binding - the brain's ability to group features into coherent objects - naturally emerges in pre-trained Vision Transformers, challenging the view that ViTs lack this capability.

Method: Used similarity probes to decode 'IsSameObject' property from patch embeddings across ViT layers, comparing self-supervised (DINO, MAE, CLIP) and ImageNet-supervised models, and performed ablation studies.

Result: Object binding emerges reliably in self-supervised ViTs with over 90% accuracy, but is markedly weaker in ImageNet-supervised models. IsSameObject is encoded in a low-dimensional subspace and actively guides attention.

Conclusion: Object binding is not an architectural artifact but an ability acquired through specific pretraining objectives, challenging the view that ViTs lack object binding and showing symbolic knowledge emerges naturally in connectionist systems.

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [69] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: ProMoE is a Mixture-of-Experts framework for Diffusion Transformers that uses a two-step router with explicit routing guidance to address the limitations of applying MoE to visual tokens, achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Existing attempts to apply Mixture-of-Experts (MoE) to Diffusion Transformers (DiTs) have yielded limited gains due to fundamental differences between language and visual tokens. Visual tokens exhibit spatial redundancy and functional heterogeneity, which hinders expert specialization in vision MoE.

Method: ProMoE features a two-step router with explicit routing guidance: (1) conditional routing partitions image tokens into conditional and unconditional sets based on functional roles, and (2) prototypical routing refines assignments of conditional tokens using learnable prototypes based on semantic content. It also includes a routing contrastive loss to enhance intra-expert coherence and inter-expert diversity.

Result: Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives.

Conclusion: The proposed ProMoE framework effectively addresses the challenges of applying MoE to vision tasks by providing explicit semantic guidance through prototypical routing, which is crucial for vision MoE and enables superior performance compared to existing methods.

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [70] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: URSA is a discrete generative modeling framework that bridges the performance gap with continuous approaches for scalable video generation through iterative global refinement of discrete tokens with specialized designs for efficiency.


<details>
  <summary>Details</summary>
Motivation: Discrete approaches for video generation lag behind continuous-space methods due to error accumulation and long-context inconsistency issues.

Method: URSA formulates video generation as iterative global refinement of discrete spatiotemporal tokens using Linearized Metric Path and Resolution-dependent Timestep Shifting mechanisms, plus asynchronous temporal fine-tuning for unified task handling.

Result: URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods on challenging video and image generation benchmarks.

Conclusion: URSA successfully bridges the performance gap between discrete and continuous approaches for scalable video generation, enabling efficient high-resolution synthesis and long-duration generation with fewer inference steps.

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [71] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: Generative View Stitching (GVS) enables collision-free camera-guided video generation by sampling entire sequences in parallel with future conditioning, overcoming limitations of autoregressive models that fail with predefined camera trajectories.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video diffusion models cannot incorporate future conditioning from predefined camera trajectories, leading to collisions and generation collapse when following complex camera paths.

Method: Proposes GVS sampling algorithm that extends diffusion stitching methods to video generation, works with any off-the-shelf video model trained with Diffusion Forcing, and introduces Omni Guidance for temporal consistency and loop-closing mechanisms.

Result: GVS achieves stable, collision-free, frame-to-frame consistent camera-guided video generation that successfully handles complex predefined camera paths including impossible geometries like the Impossible Staircase.

Conclusion: GVS provides a practical solution for camera-guided video generation that maintains coherence across long sequences while faithfully following complex camera trajectories, demonstrating superior performance over autoregressive approaches.

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [72] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: RoboOmni is a framework for robotic manipulation that uses multimodal LLMs to infer user intentions from contextual cues like speech, sounds, and visual information rather than explicit commands.


<details>
  <summary>Details</summary>
Motivation: Current VLA models rely on explicit instructions, but real-world human-robot collaboration requires proactive intention inference from natural interactions like dialogue and environmental cues.

Method: Proposes RoboOmni framework with Perceiver-Thinker-Talker-Executor architecture using end-to-end omni-modal LLMs. Fuses auditory and visual signals spatiotemporally for intention recognition and supports direct speech interaction. Built OmniAction dataset with 140k episodes for training.

Result: RoboOmni outperforms text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance in both simulation and real-world experiments.

Conclusion: The work demonstrates the effectiveness of cross-modal contextual instructions for enabling robots to proactively infer human intentions and collaborate more naturally.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>


### [73] [Motivating Students' Self-study with Goal Reminder and Emotional Support](https://arxiv.org/abs/2510.23860)
*Hyung Chan Cho,Go-Eum Cha,Yanfu Liu,Sooyeon Jeong*

Main category: cs.RO

TL;DR: Social robots can effectively support college students during self-study by providing goal reminders and emotional support, improving focus, productivity, and engagement compared to just physical presence.


<details>
  <summary>Details</summary>
Motivation: To explore how social robots can assist students in self-studying contexts, which has not been extensively investigated despite extensive research on social robots in learning tasks.

Method: Exploratory Wizard-of-Oz study comparing robotic support behaviors (goal reminders and emotional support) against a control condition with only physical presence, measuring students' perceived focus, productivity, and engagement.

Result: Participants in goal reminder and emotional support conditions reported greater ease of use, with goal reminder condition showing higher willingness for future use. Satisfaction correlated with perceiving robot as social other, which predicted goal achievement.

Conclusion: Socially assistive robots have potential to support self-study through both functional (goal reminders) and emotional engagement, with perception of robot as social other being key to effectiveness.

Abstract: While the efficacy of social robots in supporting people in learning tasks
has been extensively investigated, their potential impact in assisting students
in self-studying contexts has not been investigated much. This study explores
how a social robot can act as a peer study companion for college students
during self-study tasks by delivering task-oriented goal reminder and positive
emotional support. We conducted an exploratory Wizard-of-Oz study to explore
how these robotic support behaviors impacted students' perceived focus,
productivity, and engagement in comparison to a robot that only provided
physical presence (control). Our study results suggest that participants in the
goal reminder and the emotional support conditions reported greater ease of
use, with the goal reminder condition additionally showing a higher willingness
to use the robot in future study sessions. Participants' satisfaction with the
robot was correlated with their perception of the robot as a social other, and
this perception was found to be a predictor for their level of goal achievement
in the self-study task. These findings highlight the potential of socially
assistive robots to support self-study through both functional and emotional
engagement.

</details>


### [74] [Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped](https://arxiv.org/abs/2510.23902)
*Jans Solano,Diego Quiroz*

Main category: cs.RO

TL;DR: A recovery-aware visual-inertial navigation system for low-cost wheeled quadrupeds that combines depth camera perception with deep reinforcement learning for robust locomotion and autonomous fall recovery across diverse terrains.


<details>
  <summary>Details</summary>
Motivation: Many state-of-the-art wheeled-legged robots rely on costly actuators and sensors, and lack integrated fall-recovery capabilities, especially for wheeled-legged morphologies. This work aims to lower deployment barriers for autonomous navigation in budget-constrained platforms.

Method: Leverages vision-based perception from a depth camera and deep reinforcement learning policies for robust locomotion and autonomous recovery from falls across diverse terrains.

Result: Simulation experiments show agile mobility with low-torque actuators over irregular terrain and reliable recovery from external perturbations and self-induced failures. Also demonstrates goal-directed navigation in structured indoor spaces with low-cost perception.

Conclusion: This approach successfully lowers the barrier to deploying autonomous navigation and robust locomotion policies in budget-constrained robotic platforms by combining cost-effective hardware with intelligent recovery capabilities.

Abstract: Wheeled-legged robots combine the efficiency of wheels with the obstacle
negotiation of legs, yet many state-of-the-art systems rely on costly actuators
and sensors, and fall-recovery is seldom integrated, especially for
wheeled-legged morphologies. This work presents a recovery-aware
visual-inertial navigation system on a low-cost wheeled quadruped. The proposed
system leverages vision-based perception from a depth camera and deep
reinforcement learning policies for robust locomotion and autonomous recovery
from falls across diverse terrains. Simulation experiments show agile mobility
with low-torque actuators over irregular terrain and reliably recover from
external perturbations and self-induced failures. We further show goal directed
navigation in structured indoor spaces with low-cost perception. Overall, this
approach lowers the barrier to deploying autonomous navigation and robust
locomotion policies in budget-constrained robotic platforms.

</details>


### [75] [Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments](https://arxiv.org/abs/2510.23928)
*Raman Jha,Yang Zhou,Giuseppe Loianno*

Main category: cs.RO

TL;DR: Proposes an adaptive keyframe selection method for 3D scene reconstruction that combines error-based selection using photometric/SSIM errors with momentum-based threshold adjustment, outperforming traditional static selection methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the data bottleneck in real-time perception for dynamic environments by dynamically curating the most informative frames, enabling high-quality 3D world representations from compressed data streams for scalable robot learning.

Method: Integrates two modules: error-based selection using photometric and SSIM errors, and momentum-based update that dynamically adjusts keyframe selection thresholds based on scene motion dynamics.

Result: Significant improvements over traditional static keyframe selection strategies, with consistent performance gains across Spann3r and CUT3R 3D reconstruction networks, validated through extensive ablation studies.

Conclusion: Represents a meaningful advancement toward adaptive perception systems that can dynamically respond to complex and evolving visual scenes in dynamic environments.

Abstract: In this paper, we propose an adaptive keyframe selection method for improved
3D scene reconstruction in dynamic environments. The proposed method integrates
two complementary modules: an error-based selection module utilizing
photometric and structural similarity (SSIM) errors, and a momentum-based
update module that dynamically adjusts keyframe selection thresholds according
to scene motion dynamics. By dynamically curating the most informative frames,
our approach addresses a key data bottleneck in real-time perception. This
allows for the creation of high-quality 3D world representations from a
compressed data stream, a critical step towards scalable robot learning and
deployment in complex, dynamic environments. Experimental results demonstrate
significant improvements over traditional static keyframe selection strategies,
such as fixed temporal intervals or uniform frame skipping. These findings
highlight a meaningful advancement toward adaptive perception systems that can
dynamically respond to complex and evolving visual scenes. We evaluate our
proposed adaptive keyframe selection module on two recent state-of-the-art 3D
reconstruction networks, Spann3r and CUT3R, and observe consistent improvements
in reconstruction quality across both frameworks. Furthermore, an extensive
ablation study confirms the effectiveness of each individual component in our
method, underlining their contribution to the overall performance gains.

</details>


### [76] [A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons](https://arxiv.org/abs/2510.23954)
*Pejman Kheradmand,Behnam Moradkhani,Raghavasimhan Sankaranarayanan,Kent K. Yamamoto,Tanner J. Zachem,Patrick J. Codd,Yash Chitalia,Pierre E. Dupont*

Main category: cs.RO

TL;DR: A Cosserat rod-based framework for modeling tendon-actuated concentric tube robots with n tubes and m_i tendons per tube, achieving <4% tip prediction error.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a complete mechanical model for tendon-actuated concentric tube mechanisms that combine advantages of both tendon-driven and concentric tube robots while overcoming their limitations.

Method: Proposed a Cosserat rod-based framework that models n concentric tubes with m_i tendons per tube, allowing tube twisting and elongation while enforcing shared centerline for bending.

Result: Validated with two-tube and three-tube assemblies achieving tip prediction errors <4% of robot's total length, and applied to existing robots with maximum tip deviations around 5% of total length.

Conclusion: The model provides a foundation for accurate shape estimation and control of advanced tendon-actuated concentric tube robots.

Abstract: Tendon-actuated concentric tube mechanisms combine the advantages of
tendon-driven continuum robots and concentric tube robots while addressing
their respective limitations. They overcome the restricted degrees of freedom
often seen in tendon-driven designs, and mitigate issues such as snapping
instability associated with concentric tube robots. However, a complete and
general mechanical model for these systems remains an open problem. In this
work, we propose a Cosserat rod-based framework for modeling the general case
of $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \{1,
\ldots, n\}$. The model allows each tube to twist and elongate while enforcing
a shared centerline for bending. We validate the proposed framework through
experiments with two-tube and three tube assemblies under various tendon
routing configurations, achieving tip prediction errors $<4\%$ of the robot's
total length. We further demonstrate the model's generality by applying it to
existing robots in the field, where maximum tip deviations remain around $5\%$
of the total length. This model provides a foundation for accurate shape
estimation and control of advanced tendon-actuated concentric tube robots.

</details>


### [77] [Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping](https://arxiv.org/abs/2510.23963)
*Hiroki Ishikawa,Kyosuke Ishibashi,Ko Yamamoto*

Main category: cs.RO

TL;DR: A soft robot finger with adaptive-twist deformation that can wrap around objects and grasp them from dense arrangements using a variable stiffness mechanism controlled by a single actuation source.


<details>
  <summary>Details</summary>
Motivation: To enable soft hands to grasp individual objects from densely packed groups by inserting fingers into narrow gaps and maintaining twisted deformation for secure grasping.

Method: Proposed a variable stiffness mechanism that adaptively changes stiffness with pressure, conducted FEA to determine design parameters, and developed a soft finger prototype.

Result: Successfully demonstrated grasping various objects through wrapping, with the finger capable of deep insertion into limited gaps and maintaining appropriate grasping force.

Conclusion: The adaptive-twist soft finger with variable stiffness mechanism enables effective grasping by wrapping in dense object arrangements using single actuation.

Abstract: This paper presents a soft robot finger capable of adaptive-twist deformation
to grasp objects by wrapping them. For a soft hand to grasp and pick-up one
object from densely contained multiple objects, a soft finger requires the
adaptive-twist deformation function in both in-plane and out-of-plane
directions. The function allows the finger to be inserted deeply into a limited
gap among objects. Once inserted, the soft finger requires appropriate control
of grasping force normal to contact surface, thereby maintaining the twisted
deformation. In this paper, we refer to this type of grasping as grasping by
wrapping. To achieve these two functions by a single actuation source, we
propose a variable stiffness mechanism that can adaptively change the stiffness
as the pressure is higher. We conduct a finite element analysis (FEA) on the
proposed mechanism and determine its design parameter based on the FEA result.
Using the developed soft finger, we report basic experimental results and
demonstrations on grasping various objects.

</details>


### [78] [A Survey on Collaborative SLAM with 3D Gaussian Splatting](https://arxiv.org/abs/2510.23988)
*Phuc Nguyen Xuan,Thanh Nguyen Canh,Huu-Hung Nguyen,Nak Young Chong,Xiem HoangVan*

Main category: cs.RO

TL;DR: This survey reviews multi-robot collaborative SLAM using 3D Gaussian Splatting (3DGS), analyzing architectures, core components, datasets, and future research directions.


<details>
  <summary>Details</summary>
Motivation: 3DGS enables real-time, high-fidelity rendering ideal for robotics, but its use in multi-robot systems introduces challenges in global consistency, communication management, and heterogeneous data fusion.

Method: Systematically categorizes approaches by architecture (centralized, distributed) and analyzes core components including multi-agent consistency, communication efficiency, Gaussian representation, semantic distillation, fusion and pose optimization, and real-time scalability.

Result: Provides comprehensive analysis of current approaches and identifies key performance metrics through dataset summaries and evaluation frameworks.

Conclusion: Identifies critical open challenges and future directions including lifelong mapping, semantic association and mapping, multi-model robustness, and bridging the Sim2Real gap.

Abstract: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity rendering, ideal for robotics. However,
its use in multi-robot systems introduces significant challenges in maintaining
global consistency, managing communication, and fusing data from heterogeneous
sources. We systematically categorize approaches by their architecture --
centralized, distributed -- and analyze core components like multi-agent
consistency and alignment, communication-efficient, Gaussian representation,
semantic distillation, fusion and pose optimization, and real-time scalability.
In addition, a summary of critical datasets and evaluation metrics is provided
to contextualize performance. Finally, we identify key open challenges and
chart future research directions, including lifelong mapping, semantic
association and mapping, multi-model for robustness, and bridging the Sim2Real
gap.

</details>


### [79] [VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion](https://arxiv.org/abs/2510.23997)
*Stanley Wu,Mohamad H. Danesh,Simon Li,Hanna Yurchyk,Amin Abyaneh,Anas El Houssaini,David Meger,Hsiu-Chin Lin*

Main category: cs.RO

TL;DR: VOCALoco is a modular skill-selection framework that dynamically adapts locomotion strategies for legged robots by evaluating pre-trained policies for safety and energy efficiency on complex terrains like staircases.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of end-to-end deep reinforcement learning approaches in legged robot locomotion, which lack safety and interpretability when generalizing to novel terrains.

Method: A modular framework that evaluates pre-trained locomotion policies by predicting execution safety and energy consumption (cost of transport) over a planning horizon, enabling dynamic selection of safe and efficient policies based on local terrain perception.

Result: Empirical evaluation on staircase locomotion tasks shows VOCALoco achieves improved robustness and safety during stair ascent and descent compared to conventional end-to-end DRL policies, validated in both simulated and real-world scenarios with a quadrupedal robot.

Conclusion: VOCALoco provides a safer and more interpretable alternative to end-to-end DRL for legged robot locomotion, successfully demonstrating improved performance on complex terrain navigation tasks.

Abstract: Recent advancements in legged robot locomotion have facilitated traversal
over increasingly complex terrains. Despite this progress, many existing
approaches rely on end-to-end deep reinforcement learning (DRL), which poses
limitations in terms of safety and interpretability, especially when
generalizing to novel terrains. To overcome these challenges, we introduce
VOCALoco, a modular skill-selection framework that dynamically adapts
locomotion strategies based on perceptual input. Given a set of pre-trained
locomotion policies, VOCALoco evaluates their viability and energy-consumption
by predicting both the safety of execution and the anticipated cost of
transport over a fixed planning horizon. This joint assessment enables the
selection of policies that are both safe and energy-efficient, given the
observed local terrain. We evaluate our approach on staircase locomotion tasks,
demonstrating its performance in both simulated and real-world scenarios using
a quadrupedal robot. Empirical results show that VOCALoco achieves improved
robustness and safety during stair ascent and descent compared to a
conventional end-to-end DRL policy

</details>


### [80] [Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model](https://arxiv.org/abs/2510.24029)
*Andrew Gerstenslager,Bekarys Dukenbaev,Ali A. Minai*

Main category: cs.RO

TL;DR: The paper proposes extending Boundary Vector Cells (BVCs) to 3D by incorporating vertical angular sensitivity, enabling more accurate spatial localization in complex environments while maintaining performance in simpler 2D-like scenarios.


<details>
  <summary>Details</summary>
Motivation: Current BVC models are limited to 2D environments and suffer from spatial ambiguities due to horizontal symmetries, making them inadequate for real-world 3D navigation.

Method: Incorporates vertical angular sensitivity into BVC framework and processes LiDAR data to capture vertical contours, allowing disambiguation of locations that would be indistinguishable in 2D representations.

Result: In environments with minimal vertical variation, the 3D model matches 2D baseline performance; in complex 3D environments, it produces substantially more distinct place fields and significantly reduces spatial aliasing.

Conclusion: Adding vertical dimension to BVC-based localization significantly enhances navigation and mapping in real-world 3D spaces while maintaining performance in simpler scenarios.

Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of
vertebrates that encode environmental boundaries at specific distances and
allocentric directions, playing a central role in forming place fields in the
hippocampus. Most computational BVC models are restricted to two-dimensional
(2D) environments, making them prone to spatial ambiguities in the presence of
horizontal symmetries in the environment. To address this limitation, we
incorporate vertical angular sensitivity into the BVC framework, thereby
enabling robust boundary detection in three dimensions, and leading to
significantly more accurate spatial localization in a biologically-inspired
robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby
disambiguating locations that would be indistinguishable under a purely 2D
representation. Experimental results show that in environments with minimal
vertical variation, the proposed 3D model matches the performance of a 2D
baseline; yet, as 3D complexity increases, it yields substantially more
distinct place fields and markedly reduces spatial aliasing. These findings
show that adding a vertical dimension to BVC-based localization can
significantly enhance navigation and mapping in real-world 3D spaces while
retaining performance parity in simpler, near-planar scenarios.

</details>


### [81] [SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration](https://arxiv.org/abs/2510.24052)
*Jongsuk Kim,Jaeyoung Lee,Gyojin Han,Dongjae Lee,Minki Jeong,Junmo Kim*

Main category: cs.RO

TL;DR: SynAD is a framework that enhances end-to-end autonomous driving models using synthetic data by designating ego vehicles in multi-agent scenarios, projecting path-level scenarios onto maps, and integrating synthetic data with real-world data through a novel training strategy.


<details>
  <summary>Details</summary>
Motivation: Real-world driving datasets limit scenario diversity for training end-to-end autonomous driving models. Synthetic scenario generation can enrich training data but hasn't been effectively applied to E2E AD due to lack of designated ego vehicles and sensor inputs in synthetic scenarios.

Method: Designates agent with most comprehensive driving information as ego vehicle in multi-agent synthetic scenarios, projects path-level scenarios onto maps, uses Map-to-BEV Network to derive bird's-eye-view features without sensor inputs, and develops training strategy to integrate map-based synthetic data with real driving data.

Result: SynAD effectively integrates all components and notably enhances safety performance in autonomous driving models.

Conclusion: SynAD bridges synthetic scenario generation and E2E AD, paving the way for more comprehensive and robust autonomous driving models.

Abstract: Recent advancements in deep learning and the availability of high-quality
real-world driving datasets have propelled end-to-end autonomous driving.
Despite this progress, relying solely on real-world data limits the variety of
driving scenarios for training. Synthetic scenario generation has emerged as a
promising solution to enrich the diversity of training data; however, its
application within E2E AD models remains largely unexplored. This is primarily
due to the absence of a designated ego vehicle and the associated sensor
inputs, such as camera or LiDAR, typically provided in real-world scenarios. To
address this gap, we introduce SynAD, the first framework designed to enhance
real-world E2E AD models using synthetic data. Our method designates the agent
with the most comprehensive driving information as the ego vehicle in a
multi-agent synthetic scenario. We further project path-level scenarios onto
maps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view
features without relying on sensor inputs. Finally, we devise a training
strategy that effectively integrates these map-based synthetic data with real
driving data. Experimental results demonstrate that SynAD effectively
integrates all components and notably enhances safety performance. By bridging
synthetic scenario generation and E2E AD, SynAD paves the way for more
comprehensive and robust autonomous driving models.

</details>


### [82] [Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation](https://arxiv.org/abs/2510.24055)
*Xiucheng Zhang,Yang Jiang,Hongwei Qing,Jiashuo Bai*

Main category: cs.RO

TL;DR: A framework combining Language-Conditioned Visual Representation (LCVR) and Language-conditioned Mixture-of-Experts Density Policy (LMoE-DP) addresses perceptual ambiguity and task conflict in multitask robotic manipulation via imitation learning, achieving 79% average success rate.


<details>
  <summary>Details</summary>
Motivation: Perceptual ambiguity and task conflict limit multitask robotic manipulation via imitation learning, as visually similar tasks can confuse models and different tasks may interfere with each other during training.

Method: Proposes LCVR to resolve perceptual ambiguities by grounding visual features with language instructions, and LMoE-DP using sparse expert architecture with gradient modulation to specialize in distinct action distributions and mitigate task conflict.

Result: LCVR boosts ACT and Diffusion Policy success rates by 33.75% and 25% respectively. The full framework achieves 79% average success, outperforming advanced baseline by 21% on real-robot benchmarks.

Conclusion: Combining semantic grounding through language-conditioned visual representation and expert specialization via mixture-of-experts enables robust and efficient multi-task manipulation in robotics.

Abstract: Perceptual ambiguity and task conflict limit multitask robotic manipulation
via imitation learning. We propose a framework combining a Language-Conditioned
Visual Representation (LCVR) module and a Language-conditioned
Mixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual
ambiguities by grounding visual features with language instructions, enabling
differentiation between visually similar tasks. To mitigate task conflict,
LMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal
action distributions, stabilized by gradient modulation. On real-robot
benchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion
Policy (DP) success rates by 33.75% and 25%, respectively. The full framework
achieves a 79% average success, outperforming the advanced baseline by 21%. Our
work shows that combining semantic grounding and expert specialization enables
robust, efficient multi-task manipulation

</details>


### [83] [Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition](https://arxiv.org/abs/2510.24067)
*Tianyi Ding,Ronghao Zheng,Senlin Zhang,Meiqin Liu*

Main category: cs.RO

TL;DR: Distributed multi-robot exploration planning using topological maps for balanced area partition and task allocation in obstacle-dense environments.


<details>
  <summary>Details</summary>
Motivation: Address collaborative multi-robot autonomous online exploration with balanced exploration area partition and task allocation among mobile robots in non-convex environments.

Method: Novel topological map structure for spatial connectivity and global exploration completeness; distributed weighted topological graph Voronoi algorithm for balanced graph space partitions; local planner for optimizing visitation sequences and generating motion trajectories.

Result: Significant improvements in exploration efficiency, completeness, and workload balance across robot team compared to state-of-the-art methods.

Conclusion: The proposed approach effectively solves distributed multi-robot exploration with theoretical guarantees for consensus convergence and equitable partitions, demonstrating superior performance in complex environments.

Abstract: This work addresses the collaborative multi-robot autonomous online
exploration problem, particularly focusing on distributed exploration planning
for dynamically balanced exploration area partition and task allocation among a
team of mobile robots operating in obstacle-dense non-convex environments.
  We present a novel topological map structure that simultaneously
characterizes both spatial connectivity and global exploration completeness of
the environment. The topological map is updated incrementally to utilize known
spatial information for updating reachable spaces, while exploration targets
are planned in a receding horizon fashion under global coverage guidance.
  A distributed weighted topological graph Voronoi algorithm is introduced
implementing balanced graph space partitions of the fused topological maps.
Theoretical guarantees are provided for distributed consensus convergence and
equitable graph space partitions with constant bounds.
  A local planner optimizes the visitation sequence of exploration targets
within the balanced partitioned graph space to minimize travel distance, while
generating safe, smooth, and dynamically feasible motion trajectories.
  Comprehensive benchmarking against state-of-the-art methods demonstrates
significant improvements in exploration efficiency, completeness, and workload
balance across the robot team.

</details>


### [84] [Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition](https://arxiv.org/abs/2510.24069)
*Sangmin Kim,Hajun Kim,Gijeong Kim,Min-Gyu Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: A phase-based trajectory optimization method for legged robots that ensures dynamic feasibility and friction cone constraints by leveraging linear differential equations and B√©zier polynomials.


<details>
  <summary>Details</summary>
Motivation: To generate reliable motion for legged robots by simultaneously computing robot paths and contact sequences while accurately considering dynamics in the optimization formulation.

Method: Uses superposition properties of linear differential equations to decouple translational dynamics for each contact point, employs B√©zier polynomials to derive analytical relationships between position and force, and exploits convex closure properties to ensure friction cone constraints.

Result: The framework successfully generates dynamically reliable motions with various gait sequences for legged robots, validated using a quadruped robot model.

Conclusion: The proposed trajectory optimization approach effectively ensures dynamic feasibility and friction cone constraints throughout the entire trajectory, enabling reliable motion generation for legged robots.

Abstract: To generate reliable motion for legged robots through trajectory
optimization, it is crucial to simultaneously compute the robot's path and
contact sequence, as well as accurately consider the dynamics in the problem
formulation. In this paper, we present a phase-based trajectory optimization
that ensures the feasibility of translational dynamics and friction cone
constraints throughout the entire trajectory. Specifically, our approach
leverages the superposition properties of linear differential equations to
decouple the translational dynamics for each contact point, which operates
under different phase sequences. Furthermore, we utilize the differentiation
matrix of B{\'e}zier polynomials to derive an analytical relationship between
the robot's position and force, thereby ensuring the consistent satisfaction of
translational dynamics. Additionally, by exploiting the convex closure property
of B{\'e}zier polynomials, our method ensures compliance with friction cone
constraints. Using the aforementioned approach, the proposed trajectory
optimization framework can generate dynamically reliable motions with various
gait sequences for legged robots. We validate our framework using a quadruped
robot model, focusing on the feasibility of dynamics and motion generation.

</details>


### [85] [ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring](https://arxiv.org/abs/2510.24108)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Jingde Chen,Nadine Chang,Maying Shen,Jingyu Song,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: ZTRS is the first end-to-end autonomous driving framework that eliminates imitation learning entirely, using only reinforcement learning with raw sensor inputs while achieving state-of-the-art performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving approaches have limitations: Imitation Learning suffers from sub-optimal expert demonstrations and covariate shift, while Reinforcement Learning typically works only with low-dimensional symbolic inputs rather than raw sensor data.

Method: ZTRS combines offline reinforcement learning with Exhaustive Policy Optimization (EPO), a policy gradient variant designed for enumerable actions and rewards, enabling direct learning from high-dimensional sensor inputs without imitation learning.

Result: ZTRS achieves state-of-the-art performance on Navhard benchmark and outperforms IL-based baselines on HUGSIM, demonstrating strong performance across Navtest, Navhard, and HUGSIM benchmarks.

Conclusion: ZTRS successfully bridges the gap between sensor-rich inputs and RL training, proving that end-to-end autonomous driving can be achieved without imitation learning while maintaining robust performance across diverse scenarios.

Abstract: End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

</details>


### [86] [PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI](https://arxiv.org/abs/2510.24109)
*Wenbin Ding,Jun Chen,Mingjia Chen,Fei Xie,Qi Mao,Philip Dames*

Main category: cs.RO

TL;DR: The paper proposes a novel embodied agent framework using Vision-Language Models (VLMs) for intelligent robotic manipulation, achieving 28% higher task success rates compared to LLM+CLIP approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLM-based embodied agents in planning and executing complex natural language control tasks online, and to better serve human welfare through Human-centered AI.

Method: A three-module framework: human-robot voice interaction, vision-language agent (with task planner, instruction converter, and feedback evaluator), and action execution module.

Result: Achieved 28% higher average task success rate in both simulated and real environments compared to LLM+CLIP approaches.

Conclusion: The proposed VLM-based embodied agent framework significantly improves execution success rates for high-level natural language instruction tasks in robotics.

Abstract: The rapid advancement of Large Language Models (LLMs) has marked a
significant breakthrough in Artificial Intelligence (AI), ushering in a new era
of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
welfare and needs, thereby placing higher demands on the intelligence level of
robots, particularly in aspects such as natural language interaction, complex
task planning, and execution. Intelligent agents powered by LLMs have opened up
new pathways for realizing HAI. However, existing LLM-based embodied agents
often lack the ability to plan and execute complex natural language control
tasks online. This paper explores the implementation of intelligent robotic
manipulating agents based on Vision-Language Models (VLMs) in the physical
world. We propose a novel embodied agent framework for robots, which comprises
a human-robot voice interaction module, a vision-language agent module and an
action execution module. The vision-language agent itself includes a
vision-based task planner, a natural language instruction converter, and a task
performance feedback evaluator. Experimental results demonstrate that our agent
achieves a 28\% higher average task success rate in both simulated and real
environments compared to approaches relying solely on LLM+CLIP, significantly
improving the execution success rate of high-level natural language instruction
tasks.

</details>


### [87] [LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation](https://arxiv.org/abs/2510.24118)
*Haotian Zhou,Xiaole Wang,He Li,Fusheng Sun,Shengyu Guo,Guolei Qi,Jianghuan Xu,Huijing Zhao*

Main category: cs.RO

TL;DR: LagMemo is a visual navigation system that uses language 3D Gaussian Splatting memory to handle multi-modal, open-vocabulary goal queries and multi-goal navigation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address practical demands for multi-modal, open-vocabulary goal queries and multi-goal visual navigation, overcoming limitations of classical methods restricted to single-goal, single-modality, and closed set goal settings.

Method: Constructs a unified 3D language memory using language 3D Gaussian Splatting during exploration, then queries the memory for task goals, predicts candidate goal locations, and uses local perception-based verification to dynamically match and validate goals during navigation.

Result: LagMemo's memory module enables effective multi-modal open-vocabulary goal localization and outperforms state-of-the-art methods in multi-goal visual navigation. Also created GOAT-Core benchmark for fair evaluation.

Conclusion: LagMemo provides an effective solution for practical multi-modal, open-vocabulary multi-goal visual navigation by leveraging language-enhanced 3D memory and dynamic goal verification.

Abstract: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo

</details>


### [88] [Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames](https://arxiv.org/abs/2510.24194)
*Ev Zisselman,Mirco Mutti,Shelly Francis-Meretzki,Elisei Shafer,Aviv Tamar*

Main category: cs.RO

TL;DR: Behavioral cloning with "blindfolded" experts who lack full task information generalizes better than cloning fully-informed experts, especially with fewer demonstrations.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in behavioral cloning for sequential decision-making by limiting the demonstrator's task information, forcing non-trivial exploration.

Method: Hide some task information from demonstrators, creating "blindfolded" experts who must explore to solve tasks. Clone their behavior and compare with fully-informed experts.

Result: Blindfolded expert cloning generalizes better to unseen tasks than fully-informed cloning in real-world robot peg insertion and Procgen videogames. Theoretical analysis shows generalization error scales with ‚àö(I/m), where I is task information available.

Conclusion: Cloning blindfolded experts improves generalization, especially with fewer demonstrated tasks, as demonstrated by experiments and theoretical analysis.

Abstract: Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home

</details>


### [89] [Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors](https://arxiv.org/abs/2510.24257)
*Ziqi Ma,Changda Tian,Yue Gao*

Main category: cs.RO

TL;DR: HMAMP uses adversarial motion priors to learn human-style manipulation skills, enabling robots to perform tasks like hammering with motion patterns similar to humans.


<details>
  <summary>Details</summary>
Motivation: To develop robots that can interact with humans more naturally by manipulating objects and tools in a human-like manner.

Method: Uses adversarial networks to model tool/object manipulation dynamics and task aims, training a discriminator with real-world and simulation data to generate realistic human-like motion trajectories.

Result: HMAMP outperforms baseline methods on hammering tasks and demonstrates potential for real-world applications through successful robot arm hammering experiments.

Conclusion: HMAMP represents significant progress toward developing robots that can interact naturally with humans by learning human-style manipulation skills.

Abstract: In recent years, there has been growing interest in developing robots and
autonomous systems that can interact with human in a more natural and intuitive
way. One of the key challenges in achieving this goal is to enable these
systems to manipulate objects and tools in a manner that is similar to that of
humans. In this paper, we propose a novel approach for learning human-style
manipulation skills by using adversarial motion priors, which we name HMAMP.
The approach leverages adversarial networks to model the complex dynamics of
tool and object manipulation, as well as the aim of the manipulation task. The
discriminator is trained using a combination of real-world data and simulation
data executed by the agent, which is designed to train a policy that generates
realistic motion trajectories that match the statistical properties of human
motion. We evaluated HMAMP on one challenging manipulation task: hammering, and
the results indicate that HMAMP is capable of learning human-style manipulation
skills that outperform current baseline methods. Additionally, we demonstrate
that HMAMP has potential for real-world applications by performing real robot
arm hammering tasks. In general, HMAMP represents a significant step towards
developing robots and autonomous systems that can interact with humans in a
more natural and intuitive way, by learning to manipulate tools and objects in
a manner similar to how humans do.

</details>


### [90] [DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation](https://arxiv.org/abs/2510.24261)
*Jingyi Tian,Le Wang,Sanping Zhou,Sen Wang,Jiayi Li,Gang Hua*

Main category: cs.RO

TL;DR: DynaRend is a 3D-aware representation learning framework that learns geometry, semantics, and dynamics through masked reconstruction and future prediction using differentiable volumetric rendering, achieving improved robotic manipulation performance.


<details>
  <summary>Details</summary>
Motivation: Current approaches rely on 2D vision pretraining that focuses on static semantics or scene geometry, or use video prediction models emphasizing 2D dynamics, failing to jointly learn the geometry, semantics, and dynamics needed for effective manipulation.

Method: Pretrains on multi-view RGB-D video data using masked reconstruction and future prediction with differentiable volumetric rendering to learn unified triplane features that capture spatial geometry, future dynamics, and task semantics.

Result: Substantial improvements in policy success rate, generalization to environmental perturbations, and real-world applicability across diverse manipulation tasks on RLBench, Colosseum benchmarks and real-world experiments.

Conclusion: DynaRend effectively learns 3D-aware and dynamics-informed representations that can be transferred to downstream robotic manipulation tasks via action value map prediction, addressing key limitations of existing approaches.

Abstract: Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.

</details>


### [91] [Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation](https://arxiv.org/abs/2510.24315)
*Baozhe Zhang,Xinwei Chen,Qingcheng Chen,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: CoNi-OA is a real-time obstacle avoidance algorithm for UAV-UGV cooperative systems that uses single-frame LiDAR data to generate velocity modulation matrices, enabling collision-free navigation without global state estimation or obstacle prediction.


<details>
  <summary>Details</summary>
Motivation: CoNi-MPC framework enables UAV control using relative states but lacks environmental awareness for obstacle avoidance, creating safety challenges in cooperative air-ground tasks.

Method: Utilizes single-frame raw LiDAR data to generate modulation matrices that directly adjust quadrotor velocity, enabling real-time trajectory generation within UGV's non-inertial frame without obstacle modeling or prediction.

Result: Achieves real-time performance with computational demands under 5 ms per iteration, maintains safety in dynamic environments, and enables collision-free trajectory generation without global state estimation.

Conclusion: CoNi-OA provides an efficient, adaptable obstacle avoidance solution for UAV-UGV cooperation that works in both static and dynamic environments without requiring global states or obstacle prediction.

Abstract: CoNi-MPC provides an efficient framework for UAV control in air-ground
cooperative tasks by relying exclusively on relative states, eliminating the
need for global state estimation. However, its lack of environmental
information poses significant challenges for obstacle avoidance. To address
this issue, we propose a novel obstacle avoidance algorithm, Cooperative
Non-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for
UAV-UGV cooperative scenarios without reliance on global state estimation or
obstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data
from the UAV to generate a modulation matrix, which directly adjusts the
quadrotor's velocity to achieve obstacle avoidance. This modulation-based
method enables real-time generation of collision-free trajectories within the
UGV's non-inertial frame, significantly reducing computational demands (less
than 5 ms per iteration) while maintaining safety in dynamic and unpredictable
environments. The key contributions of this work include: (1) a
modulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV
cooperation in non-inertial frames without global states; (2) rapid, real-time
trajectory generation based solely on single-frame LiDAR data, removing the
need for obstacle modeling or prediction; and (3) adaptability to both static
and dynamic environments, thus extending applicability to featureless or
unknown scenarios.

</details>


### [92] [NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation](https://arxiv.org/abs/2510.24335)
*Mingyu Jeong,Eunsung Kim,Sehun Park,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: NVSim is a framework that automatically creates large-scale, navigable indoor simulators from common image sequences, addressing visual artifacts and enabling mesh-free traversability checking.


<details>
  <summary>Details</summary>
Motivation: To overcome the cost and scalability limitations of traditional 3D scanning methods for creating indoor navigation environments.

Method: Uses Floor-Aware Gaussian Splatting to handle visual artifacts on sparsely observed floors and introduces a mesh-free traversability checking algorithm that constructs topological graphs by analyzing rendered views.

Result: Successfully demonstrated the system's ability to generate valid, large-scale navigation graphs from real-world data.

Conclusion: NVSim provides an efficient and scalable solution for creating navigable indoor simulators from image sequences, eliminating the need for expensive 3D scanning.

Abstract: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

</details>


### [93] [Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance](https://arxiv.org/abs/2510.24457)
*Jorge Vicente-Martinez,Edgar Ramirez-Laboreo*

Main category: cs.RO

TL;DR: An optimal trajectory generation method for 3D overhead cranes using differential flatness that incorporates complex constraints like nonlinear friction and collision avoidance, enabling aggressive movements with payload swing constrained only at the endpoint.


<details>
  <summary>Details</summary>
Motivation: To enable fast and safe crane operations by addressing the limitations of existing methods that neglect important physical constraints like friction, which can lead to actuator saturation and collisions.

Method: Leverages differential flatness to directly incorporate complex physical and dynamic constraints including nonlinear friction and collision avoidance for both payload and rope, while allowing aggressive movements by constraining payload swing only at the final point.

Result: Comparative simulations validate the approach, showing that neglecting dry friction causes actuator saturation and collisions, while the proposed method enables fast and safe crane trajectories.

Conclusion: Friction modeling is a fundamental requirement for generating fast and safe crane trajectories, and the differential flatness-based approach effectively handles complex constraints to enable aggressive crane movements.

Abstract: This paper presents an optimal trajectory generation method for 3D overhead
cranes by leveraging differential flatness. This framework enables the direct
inclusion of complex physical and dynamic constraints, such as nonlinear
friction and collision avoidance for both payload and rope. Our approach allows
for aggressive movements by constraining payload swing only at the final point.
A comparative simulation study validates our approach, demonstrating that
neglecting dry friction leads to actuator saturation and collisions. The
results show that friction modeling is a fundamental requirement for fast and
safe crane trajectories.

</details>


### [94] [Supervisory Measurement-Guided Noise Covariance Estimation](https://arxiv.org/abs/2510.24508)
*Haoying Li,Yifan Peng,Junfeng Wu*

Main category: cs.RO

TL;DR: The paper presents a bilevel optimization method for estimating sensor noise covariances in state estimation, using a factorization approach that enables parallel computation and achieves higher efficiency than existing baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate sensor noise covariance specification is crucial for reliable state estimation but difficult in practice due to environmental variability and front-end preprocessing challenges.

Method: Formulates noise covariance estimation as bilevel optimization with Bayesian factorization, using invariant extended Kalman filter with state augmentation at lower level and derivative filter for analytical gradients at upper level.

Result: Experiments on synthetic and real-world datasets demonstrate that the proposed method achieves higher efficiency compared to existing baselines.

Conclusion: The bilevel optimization approach with Bayesian factorization effectively addresses noise covariance estimation challenges while maintaining computational efficiency.

Abstract: Reliable state estimation hinges on accurate specification of sensor noise
covariances, which weigh heterogeneous measurements. In practice, these
covariances are difficult to identify due to environmental variability,
front-end preprocessing, and other reasons. We address this by formulating
noise covariance estimation as a bilevel optimization that, from a Bayesian
perspective, factorizes the joint likelihood of so-called odometry and
supervisory measurements, thereby balancing information utilization with
computational efficiency. The factorization converts the nested Bayesian
dependency into a chain structure, enabling efficient parallel computation: at
the lower level, an invariant extended Kalman filter with state augmentation
estimates trajectories, while a derivative filter computes analytical gradients
in parallel for upper-level gradient updates. The upper level refines the
covariance to guide the lower-level estimation. Experiments on synthetic and
real-world datasets show that our method achieves higher efficiency over
existing baselines.

</details>


### [95] [Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems](https://arxiv.org/abs/2510.24515)
*Malintha Fernando,Petter √ñgren,Silun Zhang*

Main category: cs.RO

TL;DR: This paper extends the Team Orienteering Problem (TOP) to competitive multi-robot settings with Stochastic Prize-Collecting Games (SPCG), proposing ORS and FORL algorithms that achieve 87-95% optimality compared to TOP solutions.


<details>
  <summary>Details</summary>
Motivation: Existing TOP formulations assume cooperative robots and don't handle competitive scenarios in reward-scarce environments where robots have self-interested objectives.

Method: Proposed Stochastic Prize-Collecting Games (SPCG) with two algorithms: Ordinal Rank Search (ORS) for determining effective ranks in local neighborhoods, and Fictitious Ordinal Response Learning (FORL) for learning best-response policies against higher-ranked opponents.

Result: Empirical evaluations show ORS enables more scalable learning to large team sizes, FORL generalizes better to imbalanced prize distributions, and learned policies achieve 87-95% optimality compared to TOP solutions.

Conclusion: SPCG effectively extends TOP to competitive multi-robot settings, with the proposed algorithms enabling efficient learning and near-optimal performance in competitive routing scenarios.

Abstract: The Team Orienteering Problem (TOP) generalizes many real-world multi-robot
scheduling and routing tasks that occur in autonomous mobility, aerial
logistics, and surveillance applications. While many flavors of the TOP exist
for planning in multi-robot systems, they assume that all the robots cooperate
toward a single objective; thus, they do not extend to settings where the
robots compete in reward-scarce environments. We propose Stochastic
Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the
presence of self-interested robots operating on a graph, under energy
constraints and stochastic transitions. A theoretical study on complete and
star graphs establishes that there is a unique pure Nash equilibrium in SPCGs
that coincides with the optimal routing solution of an equivalent TOP given a
rank-based conflict resolution rule. This work proposes two algorithms: Ordinal
Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in
temporarily-formed local neighborhoods during the games' stages, and Fictitious
Ordinal Response Learning (FORL) to obtain best-response policies against one's
senior-rank opponents. Empirical evaluations conducted on road networks and
synthetic graphs under both dynamic and stationary prize distributions show
that 1) the state-aliasing induced by OR-conditioning enables learning policies
that scale more efficiently to large team sizes than those trained with the
global index, and 2) Policies trained with FORL generalize better to imbalanced
prize distributions than those with other multi-agent training methods.
Finally, the learned policies in the SPCG achieved between 87% and 95%
optimality compared to an equivalent TOP solution obtained by mixed-integer
linear programming.

</details>


### [96] [GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots](https://arxiv.org/abs/2510.24533)
*Yuan Shen,Yuze Hong,Guangyang Zeng,Tengfei Zhang,Pui Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: GeVI-SLAM is a gravity-enhanced stereo visual inertial SLAM system that addresses underwater robot challenges by using stereo depth estimation, gravity initialization, and a 4-DOF PnP solver for improved accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Underwater VI SLAM faces challenges due to visual degeneracy and insufficient IMU motion excitation, making accurate localization difficult in underwater environments.

Method: Leverages stereo camera depth estimation to eliminate scale estimation during IMU initialization, uses gravity initialization to decouple pitch/roll for 4-DOF PnP solving with minimal 3-point solver, and implements bias-eliminated estimator with provable consistency. Also refines 6-DOF pose while estimating IMU covariance for adaptive gravity prior weighting.

Result: Extensive experiments on simulated and real-world data show GeVI-SLAM achieves higher accuracy and greater stability compared to state-of-the-art methods.

Conclusion: The proposed gravity-enhanced approach with 4-DOF PnP solver and adaptive weighting effectively addresses underwater VI SLAM challenges, providing superior performance in accuracy and stability.

Abstract: Accurate visual inertial simultaneous localization and mapping (VI SLAM) for
underwater robots remains a significant challenge due to frequent visual
degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system
designed to address these issues. By leveraging the stereo camera's direct
depth estimation ability, we eliminate the need to estimate scale during IMU
initialization, enabling stable operation even under low acceleration dynamics.
With precise gravity initialization, we decouple the pitch and roll from the
pose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point
(PnP) problem for pose tracking. This allows the use of a minimal 3-point
solver, which significantly reduces computational time to reject outliers
within a Random Sample Consensus framework. We further propose a
bias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the
relative pose converges to the true value as the feature number increases. To
handle dynamic motion, we refine the full 6-DOF pose while jointly estimating
the IMU covariance, enabling adaptive weighting of the gravity prior. Extensive
experiments on simulated and real-world data demonstrate that GeVI-SLAM
achieves higher accuracy and greater stability compared to state-of-the-art
methods.

</details>


### [97] [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](https://arxiv.org/abs/2510.24554)
*Vignesh Kottayam Viswanathan,Yifan Bai,Scott Fredriksson,Sumeet Satpute,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: A hierarchical framework for robotic inspection that combines global view-planning with local view replanning to handle environment uncertainty in real-world settings like subterranean mines.


<details>
  <summary>Details</summary>
Motivation: Existing robotic inspection methods rely on known environment models, but discrepancies between models and actual site conditions (due to natural or human activities) can alter surface morphology or introduce obstructions, making inspection unreliable.

Method: The framework divides inspection into: (a) generating initial global view-plan for regions of interest using historical maps, and (b) local view replanning to adapt to current scene morphology. This preserves global coverage while enabling reactive adaptation.

Result: The approach was validated through real-world deployments in subterranean mines using quadrupedal robots, showing robust performance against environment uncertainty.

Conclusion: The hierarchical framework successfully enables robotic inspection systems to maintain global objectives while adapting locally to environmental changes, making inspection tasks more reliable in uncertain conditions.

Abstract: In this work, we present a hierarchical framework designed to support robotic
inspection under environment uncertainty. By leveraging a known environment
model, existing methods plan and safely track inspection routes to visit points
of interest. However, discrepancies between the model and actual site
conditions, caused by either natural or human activities, can alter the surface
morphology or introduce path obstructions. To address this challenge, the
proposed framework divides the inspection task into: (a) generating the initial
global view-plan for region of interests based on a historical map and (b)
local view replanning to adapt to the current morphology of the inspection
scene. The proposed hierarchy preserves global coverage objectives while
enabling reactive adaptation to the local surface morphology. This enables the
local autonomy to remain robust against environment uncertainty and complete
the inspection tasks. We validate the approach through deployments in
real-world subterranean mines using quadrupedal robot.

</details>


### [98] [Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots](https://arxiv.org/abs/2510.24571)
*Hongxu Zhao,Guangyang Zeng,Yunling Shao,Tengfei Zhang,Junfeng Wu*

Main category: cs.RO

TL;DR: A Unified Iterative Calibration (UIC) framework for underwater SLAM systems that jointly estimates extrinsic parameters and clock offsets between sensors using Maximum A Posteriori estimation with Gaussian Process motion priors.


<details>
  <summary>Details</summary>
Motivation: Existing Doppler Velocity Log (DVL) calibration methods are limited to specific sensor configurations, rely on oversimplified assumptions, and fail to jointly estimate translational extrinsics and time offsets for high-accuracy underwater SLAM performance.

Method: Proposes UIC framework using MAP estimation with GP motion prior for high-fidelity motion interpolation, alternating between GP-based motion state updates and gradient-based calibration variable updates, supported by provably consistent sequential initialization.

Result: UIC can be applied to IMU, cameras and other sensor modalities, with open-source DVL-camera calibration toolbox released. Validated through simulations and real-world tests.

Conclusion: The UIC framework successfully addresses underwater sensor calibration limitations and its components (GP priors for MAP calibration, reliable initialization) are broadly applicable to other multi-sensor calibration problems beyond underwater applications.

Abstract: The calibration of extrinsic parameters and clock offsets between sensors for
high-accuracy performance in underwater SLAM systems remains insufficiently
explored. Existing methods for Doppler Velocity Log (DVL) calibration are
either constrained to specific sensor configurations or rely on oversimplified
assumptions, and none jointly estimate translational extrinsics and time
offsets. We propose a Unified Iterative Calibration (UIC) framework for general
DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a
Gaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC
alternates between efficient GP-based motion state updates and gradient-based
calibration variable updates, supported by a provably statistically consistent
sequential initialization scheme. The proposed UIC can be applied to IMU,
cameras and other modalities as co-sensors. We release an open-source
DVL-camera calibration toolbox. Beyond underwater applications, several aspects
of UIC-such as the integration of GP priors for MAP-based calibration and the
design of provably reliable initialization procedures-are broadly applicable to
other multi-sensor calibration problems. Finally, simulations and real-world
tests validate our approach.

</details>


### [99] [Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning](https://arxiv.org/abs/2510.24584)
*J√∏rgen Anker Olsen,Lars R√∏nhaug Pettersen,Kostas Alexis*

Main category: cs.RO

TL;DR: Curriculum-based reinforcement learning framework for training precise jumping policies for robot 'Olympus', achieving 1.25m horizontal jumps with centimeter accuracy and 1.0m vertical jumps.


<details>
  <summary>Details</summary>
Motivation: To develop precise and high-performance jumping capabilities for robots, addressing the challenge of sparse rewards in jumping tasks and enabling versatile dynamic locomotion.

Method: Uses curriculum-based RL with reward densification using projectile motion laws, reference state initialization for faster exploration, and separate policies for vertical/horizontal jumps combined with walking policy.

Result: Achieved horizontal jumps up to 1.25m with centimeter accuracy and vertical jumps up to 1.0m, successfully crossing Sim2Real gap and enabling omnidirectional jumping with minor modifications.

Conclusion: The proposed framework effectively trains precise jumping policies, demonstrates superior performance over previous works, and enables versatile dynamic locomotion capabilities for robots.

Abstract: This paper presents a curriculum-based reinforcement learning framework for
training precise and high-performance jumping policies for the robot `Olympus'.
Separate policies are developed for vertical and horizontal jumps, leveraging a
simple yet effective strategy. First, we densify the inherently sparse jumping
reward using the laws of projectile motion. Next, a reference state
initialization scheme is employed to accelerate the exploration of dynamic
jumping behaviors without reliance on reference trajectories. We also present a
walking policy that, when combined with the jumping policies, unlocks versatile
and dynamic locomotion capabilities. Comprehensive testing validates walking on
varied terrain surfaces and jumping performance that exceeds previous works,
effectively crossing the Sim2Real gap. Experimental validation demonstrates
horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to
1.0 m. Additionally, we show that with only minor modifications, the proposed
method can be used to learn omnidirectional jumping.

</details>


### [100] [GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization](https://arxiv.org/abs/2510.24623)
*Nicolai Steinke,Daniel Goehring*

Main category: cs.RO

TL;DR: GroundLoc is a LiDAR-only localization system for mobile robots in large outdoor environments that uses BEV image projection and place recognition (R2D2 or SIFT) for map registration, achieving sub-50cm accuracy with minimal storage requirements.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient LiDAR-only localization system for large-scale outdoor environments that can work with various sensor types while maintaining high accuracy and low storage requirements.

Method: Projects LiDAR data into Bird's-Eye View images focusing on ground areas, then uses R2D2 network or SIFT for place recognition and keypoint selection to register with 2D raster image maps.

Result: Outperforms state-of-the-art methods on SemanticKITTI and HeLiPR datasets, achieves Average Trajectory Error below 50 cm on all Ouster OS2 128 sequences while meeting online runtime requirements.

Conclusion: GroundLoc provides an effective LiDAR-only localization solution that supports multiple sensor types, achieves high accuracy, and requires only 4 MB storage per square kilometer for prior maps.

Abstract: In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.

</details>


### [101] [Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder](https://arxiv.org/abs/2510.24671)
*Li Li,Tobias Brinkmann,Till Temmen,Markus Eisenbarth,Jakob Andert*

Main category: cs.RO

TL;DR: A Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model is proposed for generating realistic multi-agent traffic scenarios in roundabouts to support virtual testing of intelligent driving functions.


<details>
  <summary>Details</summary>
Motivation: Traditional road testing for intelligent driving functions is time-consuming and expensive. Virtual testing offers better efficiency and reproducibility, but roundabouts with complex interactions remain underexplored.

Method: Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model that generates multi-agent traffic scenarios in roundabouts, with KPIs to evaluate interactive behavior.

Result: The model accurately reconstructs original scenarios and generates realistic, diverse synthetic scenarios. Latent space analysis shows partial disentanglement with interpretable effects on vehicle timing and velocity profiles.

Conclusion: The model effectively generates scenarios for validating intelligent driving functions involving multi-agent interactions and can augment data for development and improvement.

Abstract: With the increasing integration of intelligent driving functions into
serial-produced vehicles, ensuring their functionality and robustness poses
greater challenges. Compared to traditional road testing, scenario-based
virtual testing offers significant advantages in terms of time and cost
efficiency, reproducibility, and exploration of edge cases. We propose a
Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for
generating multi-agent traffic scenarios in roundabouts, which are
characterized by high vehicle dynamics and complex layouts, yet remain
relatively underexplored in current research. The results show that the
proposed model can accurately reconstruct original scenarios and generate
realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators
(KPIs) are employed to evaluate the interactive behavior in the generated
scenarios. Analysis of the latent space reveals partial disentanglement, with
several latent dimensions exhibiting distinct and interpretable effects on
scenario attributes such as vehicle entry timing, exit timing, and velocity
profiles. The results demonstrate the model's capability to generate scenarios
for the validation of intelligent driving functions involving multi-agent
interactions, as well as to augment data for their development and iterative
improvement.

</details>


### [102] [Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis](https://arxiv.org/abs/2510.24676)
*Jiaxuan Zhang,Yuquan Leng,Yixuan Guo,Chenglong Fu*

Main category: cs.RO

TL;DR: Using an inertial sensor on the sound ankle and genetic algorithm-optimized neural networks to predict thigh and knee joint angles for obstacle-crossing in powered transfemoral prosthetics, achieving high accuracy in gait phase estimation and low prediction errors.


<details>
  <summary>Details</summary>
Motivation: Amputees with powered transfemoral prosthetics face challenges navigating obstacles and complex terrain, requiring improved control methods for obstacle-crossing movements.

Method: Inertial sensor on sound ankle guides obstacle-crossing; genetic algorithm computes optimal neural network structure; gait progression prediction algorithm determines actuation angles for prosthetic knee motor.

Result: Method effectively eliminates noise interference when Gaussian noise standard deviation <1; achieves 100% accuracy in gait phase estimation under 150 Hz; thigh angle prediction error 8.71%; knee angle prediction error 6.78%.

Conclusion: The method accurately predicts gait progression and joint angles, offering significant practical value for obstacle negotiation in powered transfemoral prosthetics.

Abstract: For amputees with powered transfemoral prosthetics, navigating obstacles or
complex terrain remains challenging. This study addresses this issue by using
an inertial sensor on the sound ankle to guide obstacle-crossing movements. A
genetic algorithm computes the optimal neural network structure to predict the
required angles of the thigh and knee joints. A gait progression prediction
algorithm determines the actuation angle index for the prosthetic knee motor,
ultimately defining the necessary thigh and knee angles and gait progression.
Results show that when the standard deviation of Gaussian noise added to the
thigh angle data is less than 1, the method can effectively eliminate noise
interference, achieving 100\% accuracy in gait phase estimation under 150 Hz,
with thigh angle prediction error being 8.71\% and knee angle prediction error
being 6.78\%. These findings demonstrate the method's ability to accurately
predict gait progression and joint angles, offering significant practical value
for obstacle negotiation in powered transfemoral prosthetics.

</details>


### [103] [Fare: Failure Resilience in Learned Visual Navigation Control](https://arxiv.org/abs/2510.24680)
*Zishuo Wang,Joel Loo,David Hsu*

Main category: cs.RO

TL;DR: Fare is a framework for creating failure-resilient imitation learning policies that can automatically detect and recover from out-of-distribution scenarios without using explicit failure data.


<details>
  <summary>Details</summary>
Motivation: Imitation learning policies are effective for visual navigation but prone to unpredictable failures in out-of-distribution scenarios, requiring a solution that can both detect and automatically recover from failures.

Method: Fare embeds OOD-detection and recognition capabilities into IL policies without using explicit failure data, and pairs them with recovery heuristics. It identifies failure-causing factors by pinpointing image regions that trigger failure detections.

Result: Real-world experiments demonstrate that Fare enables failure recovery across two different policy architectures and supports robust long-range navigation in complex environments.

Conclusion: The Fare framework successfully creates failure-resilient IL policies that can automatically detect and recover from failures, enabling more robust visual navigation in challenging scenarios.

Abstract: While imitation learning (IL) enables effective visual navigation, IL
policies are prone to unpredictable failures in out-of-distribution (OOD)
scenarios. We advance the notion of failure-resilient policies, which not only
detect failures but also recover from them automatically. Failure recognition
that identifies the factors causing failure is key to informing recovery: e.g.
pinpointing image regions triggering failure detections can provide cues to
guide recovery. We present Fare, a framework to construct failure-resilient IL
policies, embedding OOD-detection and recognition in them without using
explicit failure data, and pairing them with recovery heuristics. Real-world
experiments show that Fare enables failure recovery across two different policy
architectures, enabling robust long-range navigation in complex environments.

</details>


### [104] [A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers](https://arxiv.org/abs/2510.24683)
*Caleb Escobedo,Nataliya Nechyporenko,Shreyas Kadekodi,Alessandro Roncone*

Main category: cs.RO

TL;DR: A framework for analyzing object-aware controllers that help robots avoid collisions, focusing on kinematics, motion profiles, and virtual constraints, with experimental verification and comparison of three controllers.


<details>
  <summary>Details</summary>
Motivation: Real-time control is essential for safe robot operation in dynamic environments with moving objects, requiring effective collision avoidance methods.

Method: Developed an analysis framework focusing on three design considerations (kinematics, motion profiles, virtual constraints) and used fundamental robot-obstacle experimental scenarios to verify robot behaviors.

Result: Comparison of three representative object-aware controllers revealed common deficiencies: lack of kinematic considerations, discontinuity in control points, and instability in movement profiles.

Conclusion: The framework can be used for future design, comparison, and benchmarking of obstacle avoidance methods in robotics.

Abstract: Real-time control is an essential aspect of safe robot operation in the real
world with dynamic objects. We present a framework for the analysis of
object-aware controllers, methods for altering a robot's motion to anticipate
and avoid possible collisions. This framework is focused on three design
considerations: kinematics, motion profiles, and virtual constraints.
Additionally, the analysis in this work relies on verification of robot
behaviors using fundamental robot-obstacle experimental scenarios. To showcase
the effectiveness of our method we compare three representative object-aware
controllers. The comparison uses metrics originating from the design
considerations. From the analysis, we find that the design of object-aware
controllers often lacks kinematic considerations, continuity of control points,
and stability in movement profiles. We conclude that this framework can be used
in the future to design, compare, and benchmark obstacle avoidance methods.

</details>


### [105] [Embodying Physical Computing into Soft Robots](https://arxiv.org/abs/2510.24692)
*Jun Wang,Ziyang Zhou,Ardalan Kahak,Suyi Li*

Main category: cs.RO

TL;DR: This perspective paper proposes a framework for embodying physical computing into soft robots using three strategies: analog oscillators, physical reservoir computing, and physical algorithmic computing to enable complex behaviors without traditional electronics.


<details>
  <summary>Details</summary>
Motivation: To advance soft robotics towards robustness and intelligence for everyday use by integrating soft computers and controllers, moving beyond traditional CMOS-based electronics.

Method: Proposes a framework for embodying physical computing into soft robots through three strategies: analog oscillators, physical reservoir computing, and physical algorithmic computing that encode inputs into mechanical computing kernels.

Result: The embodied computers enable soft robots to perform complex behaviors including coordinated locomotion with obstacle avoidance, payload weight and orientation classification, and programmable operation based on logical rules.

Conclusion: The paper presents a perspective framework for future development of physical computing in soft robotics, detailing working principles and surveying current state-of-the-art approaches.

Abstract: Softening and onboarding computers and controllers is one of the final
frontiers in soft robotics towards their robustness and intelligence for
everyday use. In this regard, embodying soft and physical computing presents
exciting potential. Physical computing seeks to encode inputs into a mechanical
computing kernel and leverage the internal interactions among this kernel's
constituent elements to compute the output. Moreover, such input-to-output
evolution can be re-programmable. This perspective paper proposes a framework
for embodying physical computing into soft robots and discusses three unique
strategies in the literature: analog oscillators, physical reservoir computing,
and physical algorithmic computing. These embodied computers enable the soft
robot to perform complex behaviors that would otherwise require CMOS-based
electronics -- including coordinated locomotion with obstacle avoidance,
payload weight and orientation classification, and programmable operation based
on logical rules. This paper will detail the working principles of these
embodied physical computing methods, survey the current state-of-the-art, and
present a perspective for future development.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [106] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: Proposed MSRANetV2, a CNN architecture with ResNet50V2 backbone enhanced with residual attention and SE blocks for colorectal cancer tissue classification, achieving exceptional performance (F1-score ~0.99) on two public datasets.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of cancer mortality, and conventional diagnostic methods like colonoscopy are subjective, time-consuming, and variable. Deep learning can enhance diagnostic precision and efficiency in digital pathology.

Method: MSRANetV2 uses ResNet50V2 backbone extended with residual attention mechanisms and squeeze-and-excitation blocks to extract deep semantic and fine-grained spatial features. It employs channel alignment and upsampling to fuse multi-scale representations.

Result: Achieved remarkable performance on CRC-VAL-HE-7K (Precision: 0.9884¬±0.0151, Recall: 0.9900¬±0.0151, F1: 0.9900¬±0.0145, AUC: 0.9999¬±0.00006, Accuracy: 0.9905¬±0.0025) and NCT-CRC-HE-100K datasets with similar high metrics. Grad-CAM visualizations enhanced interpretability.

Conclusion: MSRANetV2 is validated as a reliable, interpretable, and high-performing architectural model for classifying colorectal cancer tissues, demonstrating strong potential for improving diagnostic precision in digital pathology.

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS is a generalist game agent using unified keyboard-mouse action space that achieves state-of-the-art performance across various game domains through large-scale pre-training with 500B tokens.


<details>
  <summary>Details</summary>
Motivation: To create a generalist game agent that can operate across heterogeneous domains (OS, web, simulation games) using human-aligned native inputs rather than API- or GUI-based approaches, enabling large-scale continual pre-training.

Method: Uses unified keyboard-mouse action space, decaying continual loss to reduce causal confusion, and Sparse-Thinking strategy for balancing reasoning depth and inference cost. Pre-trained on 500B tokens with diverse trajectories and multimodal data.

Result: Achieves 2x success rate over previous SOTA on Minecraft tasks, close to human generality in unseen web 3D games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling confirms sustained improvements with cross-game and multimodal data.

Conclusion: Simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [108] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: AI's role in scientific problem-solving can displace disciplinary creativity, potentially diminishing the value of scientific pursuit.


<details>
  <summary>Details</summary>
Motivation: To examine how artificial intelligence affects disciplinary creativity in scientific fields, distinguishing between creative approaches and products.

Method: Drawing on philosophy of creativity concepts and analyzing two mathematical case studies to compare computational extensions versus AI-driven displacement of disciplinary creativity.

Result: While computation can extend disciplinary creativity, certain AI approaches displace it, altering the nature of scientific work.

Conclusion: AI's displacement of disciplinary creativity may diminish the value of scientific pursuit by changing how creative expertise is applied within disciplines.

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [109] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: ME-POMDPs extend standard POMDPs with discrete model uncertainty, representing multiple possible POMDPs. The paper generalizes them to AB-POMDPs, shows reduction methods, and develops exact/approximate algorithms for computing robust policies.


<details>
  <summary>Details</summary>
Motivation: To address scenarios where multiple domain experts disagree on problem modeling, requiring a single policy robust against all possible POMDP variations within a set.

Method: Generalize ME-POMDPs to AB-POMDPs with sets of initial beliefs, prove reduction theorems to simplify model variations, and develop exact and point-based approximate algorithms for robust policy computation.

Result: Successfully computed robust policies for standard POMDP benchmarks extended to multi-environment settings, demonstrating practical applicability of the approach.

Conclusion: ME-POMDPs and their generalization to AB-POMDPs provide a principled framework for handling model uncertainty, with effective algorithms for computing worst-case optimal policies in multi-environment scenarios.

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [110] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: A framework using test-time tuning enhances pre-trained transformers for de novo molecular structure generation directly from tandem mass spectra and molecular formulas, outperforming state-of-the-art methods by significant margins.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on database matching or multi-step pipelines with intermediate predictions, making identification challenging for compounds absent from reference databases.

Method: Leverages test-time tuning to enhance learning of pre-trained transformer models, enabling end-to-end de novo molecular structure generation directly from tandem mass spectra and molecular formulas.

Result: Surpasses state-of-the-art DiffMS by 100% on NPLIB1 and 20% on MassSpecGym benchmarks. Test-time tuning provides 62% relative performance gain over conventional fine-tuning on MassSpecGym.

Conclusion: The approach enables dynamic adaptation to novel spectra and generates structurally accurate molecular candidates even when predictions deviate from ground truth, providing valuable guidance for human interpretation and more reliable identification.

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [111] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Toma≈°ev,Tom Zahavy*

Main category: cs.AI

TL;DR: An AI system generates creative chess puzzles with aesthetic appeal and novel solutions, evaluated by world-renowned chess experts.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Generative AI can produce creative and novel outputs, specifically in the domain of chess puzzles.

Method: Developed an AI system to generate chess puzzles characterized by aesthetic appeal, novelty, counter-intuitive and unique solutions, then evaluated by chess experts.

Result: Three world-renowned chess experts (International Master Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler) selected their favorite AI-generated puzzles and explained their appeal based on creativity, challenge level, and aesthetic design.

Conclusion: The AI system demonstrates the ability to generate creative chess puzzles that are appreciated by domain experts for their aesthetic qualities and innovative solutions.

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [112] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: Current pathology foundation models fail to deliver expected breakthroughs in cancer diagnosis due to fundamental conceptual mismatches with tissue complexity, exhibiting low accuracy, poor robustness, and safety vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To understand why foundation models that revolutionized other domains are failing in computational pathology despite rapid adoption, by examining the underlying conceptual mismatches.

Method: Systematic evaluation and analysis of pathology foundation models to identify seven key causes of their shortcomings through conceptual examination of model assumptions versus tissue complexity.

Result: Identified seven interrelated causes of failure: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and fundamental design flaw in patch size.

Conclusion: Current pathology foundation models are conceptually misaligned with tissue morphology and require fundamental paradigm rethinking rather than incremental improvements.

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [113] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP is a hierarchical framework for LLMs that improves long-horizon reasoning through plan-ahead decomposition, structured context re-injection, and memory-efficient execution, achieving significant performance gains on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing sequential prompting methods suffer from context drift, goal information loss, and failure cycles, while hierarchical methods weaken cross-level continuity or have high runtime overhead.

Method: ReCAP combines three mechanisms: (1) plan-ahead decomposition - generate full subtask list, execute first item, refine remainder; (2) structured re-injection of parent plans - maintain consistent multi-level context during recursive return; (3) memory-efficient execution - bound active prompt for linear cost scaling with task depth.

Result: ReCAP achieves 32% gain on synchronous Robotouille and 29% improvement on asynchronous Robotouille under strict pass@1 protocol, substantially improving subgoal alignment and success rates on various long-horizon reasoning benchmarks.

Conclusion: ReCAP effectively aligns high-level goals with low-level actions, reduces redundant prompting, and preserves coherent context updates across recursion, making it a powerful framework for long-horizon reasoning tasks in LLMs.

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [114] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: LLM-based agents achieve near-optimal performance in decentralized multi-agent goal assignment using structured environmental information and deterministic conflict resolution.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coordinating multiple autonomous agents in shared environments under decentralized conditions without negotiation or iterative coordination.

Method: Agents independently generate ranked preferences over goals using structured environmental representations (grid visualizations and scenario data), then exchange rankings and use deterministic conflict-resolution rules for assignment.

Result: LLM-based agents with well-designed prompts and quantitative information achieve near-optimal makespans and consistently outperform traditional heuristics.

Conclusion: Language models show strong potential for decentralized goal assignment in multi-agent path planning, with information structure being a critical factor for success.

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [115] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,≈Åukasz StrƒÖk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: IBM developed CUGA, a generalist agent that achieves SOTA on benchmarks and shows promise in enterprise BPO settings, addressing scalability, safety, and governance requirements.


<details>
  <summary>Details</summary>
Motivation: Enterprises struggle to move AI agents from prototypes to production due to fragmented frameworks, slow development, and lack of standardized evaluation practices.

Method: CUGA uses a hierarchical planner-executor architecture with strong analytical foundations, evaluated on AppWorld, WebArena, and a custom BPO-TA benchmark with 26 tasks.

Result: CUGA achieved state-of-the-art performance on benchmarks and approached specialized agent accuracy in enterprise pilot while showing potential for reduced development time and cost.

Conclusion: Generalist agents like CUGA can operate at enterprise scale, but require addressing technical and organizational challenges to advance from research to production systems.

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [116] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Toma≈°ev,Tom Zahavy*

Main category: cs.AI

TL;DR: This paper introduces an RL framework with novel rewards to generate creative chess puzzles that are unique, counter-intuitive, diverse, and realistic, achieving 10x improvement in counter-intuitive puzzle generation.


<details>
  <summary>Details</summary>
Motivation: While Generative AI advances rapidly, generating truly creative, aesthetic, and counter-intuitive outputs remains challenging, particularly in the domain of chess puzzles.

Method: The authors benchmark Generative AI architectures and introduce an RL framework with novel rewards based on chess engine search statistics to enhance puzzle uniqueness, counter-intuitiveness, diversity, and realism.

Result: The RL approach dramatically increases counter-intuitive puzzle generation from 0.22% (supervised) to 2.5%, surpassing existing dataset rates (2.1%) and the best Lichess-trained model (0.4%). The puzzles meet novelty and diversity benchmarks, retain aesthetic themes, and are rated by human experts as more creative, enjoyable, and counter-intuitive than composed book puzzles.

Conclusion: The final outcome is a curated booklet of AI-generated puzzles acknowledged for creativity by three world-renowned experts, demonstrating that the approach can generate puzzles approaching classic compositions in quality.

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [117] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: This paper compares digital twin approaches for dynamical system modeling and control using a miniature greenhouse testbed, evaluating four predictive models and three control strategies across interpolation/extrapolation scenarios.


<details>
  <summary>Details</summary>
Motivation: To investigate the integration of physics-based, data-driven, and hybrid approaches with traditional and AI-driven controllers for digital twin applications in dynamical systems.

Method: Developed and compared four predictive models (Linear, PBM, LSTM, HAM) and three control strategies (MPC, RL, LLM-based control) using a miniature greenhouse test platform under interpolation and extrapolation scenarios.

Result: HAM provided the most balanced modeling performance across accuracy, generalization, and computational efficiency; LSTM achieved high precision but with greater resource cost. MPC delivered robust control performance, RL showed strong adaptability, and LLM-based controllers enabled flexible human-AI interaction.

Conclusion: Hybrid approaches like HAM offer balanced modeling performance, while different control strategies provide complementary strengths - MPC for robustness, RL for adaptability, and LLM-based control for human-AI interaction when combined with predictive tools.

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [118] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: This survey paper examines security risks specific to agentic AI systems powered by LLMs, providing a taxonomy of threats, evaluation benchmarks, and defense strategies for these autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems with planning, tool use, memory, and autonomy capabilities create new and amplified security risks that are distinct from traditional AI safety and conventional software security, requiring specialized analysis.

Method: The paper conducts a comprehensive survey that outlines a taxonomy of agentic AI threats, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives.

Result: The survey synthesizes current research on agentic AI security, identifying specific threats unique to these autonomous systems and providing frameworks for understanding and addressing them.

Conclusion: The paper aims to support the development of secure-by-design agent systems by highlighting open challenges and providing a foundation for future research in agentic AI security.

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [119] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: The paper proposes a novel training algorithm based on amortized variational inference to improve chain-of-thought reasoning in Large Vision-Language Models, addressing limitations of existing methods like SFT, PPO, and GRPO.


<details>
  <summary>Details</summary>
Motivation: Existing training algorithms for chain-of-thought reasoning in LVLMs don't generalize well across unseen tasks and rely heavily on biased reward models, limiting their interpretability and reliability.

Method: Reformulates reasoning as posterior inference using amortized variational inference, introduces diversity-seeking RL with sparse token-level rewards, and implements Bayesian inference-scaling with marginal likelihood for efficient rationale ranking.

Result: Empirically demonstrates enhanced performance on seven reasoning benchmarks, improving state-of-the-art LVLMs in effectiveness, generalization, and interpretability.

Conclusion: The proposed variational inference-based approach successfully addresses limitations of existing methods and significantly improves chain-of-thought reasoning capabilities in LVLMs.

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [120] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: A decentralized causal discovery framework using judo calculus that handles context-dependent causal effects across different regimes through sheaf theory and j-stability.


<details>
  <summary>Details</summary>
Motivation: Real-world causal effects depend on context (age, country, dose, genotype, etc.), requiring methods that can handle this regime dependence rather than assuming universal causal relationships.

Method: Uses judo calculus (j-stable causal inference with j-do-calculus in a topos of sheaves) combined with standard causal discovery methods (score-based, constraint-based, gradient-based) to formalize context dependence as local truth across regimes.

Result: Experimental results show computational efficiency from decentralized sheaf-theoretic approach and improved performance over classical causal discovery methods on synthetic and real-world datasets from biology and economics.

Conclusion: The judo calculus framework provides a constructive, decentralized approach to causal discovery that effectively handles regime-dependent causal effects with better efficiency and performance than traditional methods.

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [121] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: The paper proposes a new 'sign estimator' method for LLM alignment that replaces cross-entropy loss with binary classification loss, providing consistent ordinal alignment and reducing preference distortion compared to standard RLHF methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences and yield inconsistent estimates of population-average utility, which is a key measure of social welfare.

Method: The sign estimator replaces cross-entropy with binary classification loss in the aggregation step of pairwise comparison data, providing a simple yet provably consistent and efficient estimator under mild assumptions.

Result: In simulations using digital twins, the sign estimator reduced angular estimation error by nearly 35% and decreased disagreement with true population preferences from 12% to 8% compared to standard RLHF. It also outperformed panel data heuristics that explicitly model user heterogeneity.

Conclusion: The sign estimator offers a simple, consistent, and efficient solution for LLM alignment that substantially reduces preference distortion while maintaining implementation simplicity comparable to existing alignment pipelines.

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [122] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: This paper develops a conditioned deep learning model that incorporates individuals' social infrastructure resilience (SIR) and spatial context to predict post-disruption movement patterns using sparse individual-level data.


<details>
  <summary>Details</summary>
Motivation: Predicting individual movement shifts after disruptive events is challenging due to lack of SIR measures, insufficient capture of movement-spatial context interactions, and spatial sparsity of individual movement data.

Method: A conditioned deep learning model that incorporates individuals' SIR and captures complex relationships between movement patterns and local spatial context using large-scale sparse individual-level data.

Result: Experiments show that incorporating SIR and spatial context enhances prediction of post-event movement patterns, and the model captures divergent shifts among individuals with similar pre-event patterns but different SIR.

Conclusion: The conditioned model successfully predicts post-disruption movement patterns by accounting for individual SIR and spatial context, addressing key challenges in movement prediction.

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [123] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*ƒ∞brahim Oƒüuz √áetinkaya,ƒ∞. Esra B√ºy√ºktahtakƒ±n,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: This paper introduces two new LLM-discovered heuristics (EDDC and MDDC) for the single-machine total tardiness problem that outperform traditional methods and remain competitive with exact approaches on large instances.


<details>
  <summary>Details</summary>
Motivation: To leverage Large Language Models for discovering novel heuristics in combinatorial optimization, specifically for the NP-hard single-machine total tardiness problem where exact methods become computationally intractable for large instances.

Method: Developed two LLM-discovered heuristics (EDDC and MDDC) inspired by EDD and MDD rules, benchmarked using rigorous criteria including optimality gaps and solution time from MIP formulation, and compared against state-of-the-art heuristics and exact methods across various job sizes.

Result: For instances with more than 100 jobs, EDDC improves upon classic EDD rule and other algorithms, while MDDC consistently outperforms traditional heuristics and remains competitive with exact approaches, especially on larger instances up to 500 jobs.

Conclusion: Human-LLM collaboration can produce scalable, high-performing heuristics for NP-hard constrained combinatorial optimization problems, even under limited computational resources when properly configured.

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [124] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast is a cross-domain time series forecasting framework that decomposes series into seasonal and trend components, modeling them separately with specialized modules for better generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with domain-specific trend shifts and inconsistent periodic patterns when forecasting across heterogeneous time series domains, as they treat temporal series as undifferentiated sequences without explicitly decoupling structural components.

Method: OneCast decomposes time series into seasonal and trend components. Seasonal patterns are captured via lightweight projection with interpretable basis functions, while trends are encoded into discrete tokens via semantic-aware tokenizer and inferred through masked discrete diffusion mechanism.

Result: Extensive experiments across eight domains demonstrate that OneCast mostly outperforms state-of-the-art baselines in cross-domain time series forecasting.

Conclusion: The structured and modular approach of explicitly decoupling seasonal and trend components enables effective generalization across heterogeneous time series domains, addressing limitations of treating temporal series as undifferentiated sequences.

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [125] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer is a clustering-based log analysis chatbot that combines LLMs and ML algorithms to simplify cybersecurity log analysis, overcoming LLM limitations and achieving 39-68% performance improvements over existing chatbots.


<details>
  <summary>Details</summary>
Motivation: System log analysis is crucial for cybersecurity but challenging due to high costs, lack of expertise, and time constraints. Existing LLM-based solutions have limitations in context windows and structured text handling.

Method: Uses a modular architecture with router, log recognizer, log parser, and search tools. Combines LLMs with ML algorithms through clustering-based approach to address LLM limitations.

Result: Achieved 39-68% performance improvements over state-of-the-art LLM chatbots (ChatGPT, ChatPDF, NotebookLM) across four domain logs. Showed strong robustness with 93% reduction in interquartile range using ROUGE-1 scores.

Conclusion: LLMLogAnalyzer effectively enhances LLM capabilities for structured text analysis, improving accuracy and robustness for cybersecurity log analysis, making it valuable for both experts and non-technical users.

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [126] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: This study compares classical physics-based car following models (IDM, OVM, OVRV, CACC) with a machine learning approach (Random Forest) for modeling electric vehicle following behavior, finding that Random Forest significantly outperforms classical models across all gap scenarios.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of electric vehicles requires better understanding of their driving behavior to enhance traffic safety and develop smart driving systems, especially in mixed autonomy traffic environments.

Method: Used real-world EV following ICE vehicle data under varied driving conditions. Calibrated classical model parameters by minimizing RMSE, while Random Forest predicted acceleration using spacing, speed, and gap type as inputs.

Result: Random Forest achieved superior accuracy with RMSEs of 0.0046 (medium gap), 0.0016 (long gap), and 0.0025 (extra long gap). Among classical models, CACC performed best with RMSE of 2.67 for long gaps.

Conclusion: Machine learning models like Random Forest demonstrate significantly better performance than physics-based models for EV car following behavior, making them valuable for simulating EV behavior and analyzing mixed autonomy traffic dynamics.

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [127] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens is a transparent AI system for pathology that allows doctors to ask questions in plain English about tissue slides, provides structured reports with visual proofs, and focuses only on relevant tissue while ignoring background noise.


<details>
  <summary>Details</summary>
Motivation: To create a trustworthy AI system that doctors can trust by making it transparent and collaborative, rather than a black box, so pathologists can understand the AI's reasoning like consulting a colleague.

Method: Developed HistoLens which translates natural language questions into precise AI queries, provides structured reports with visual proofs (heatmaps showing exact cells/regions used for analysis), and trains the AI to focus only on patient tissue while ignoring background noise.

Result: Creates a workflow where pathologists remain in charge as experts, using the AI assistant to verify insights and make faster, more confident diagnoses through transparent collaboration.

Conclusion: HistoLens successfully bridges the trust gap in medical AI by providing transparency, visual explanations, and collaborative functionality that keeps the human expert in control while enhancing diagnostic confidence and efficiency.

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [128] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: OpsAgent is a lightweight, self-evolving multi-agent system for incident management that converts heterogeneous observability data into structured descriptions and uses multi-agent collaboration for transparent diagnostic inference, achieving state-of-the-art performance on the OPENRCA benchmark.


<details>
  <summary>Details</summary>
Motivation: Manual incident management is labor-intensive and error-prone with massive observability data, while existing automated approaches struggle with generalization, interpretability, and high deployment costs.

Method: Uses a training-free data processor to convert heterogeneous observability data into structured textual descriptions, employs multi-agent collaboration framework for transparent diagnostic inference, and implements dual self-evolution mechanism for continual capability growth.

Result: Comprehensive experiments on OPENRCA benchmark demonstrate state-of-the-art performance, showing OpsAgent is generalizable, interpretable, cost-efficient, and self-evolving.

Conclusion: OpsAgent is a practically deployable and sustainable solution for long-term operation in real-world cloud systems, addressing key limitations of existing incident management approaches.

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [129] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: Automated framework for generating high-difficulty multi-hop QA datasets that require complex reasoning over ambiguous, indirect cues, addressing the scarcity of training-ready data for retrieval-and-reasoning models.


<details>
  <summary>Details</summary>
Motivation: Current multi-hop QA datasets are scarce and mostly designed for evaluation, not training. Manual curation of non-trivially retrievable questions is costly and doesn't scale, creating a data bottleneck for training retrieval-and-reasoning agents.

Method: Three-step automated framework: (1) grows diverse evidence clusters using NLI-based relation typing and diversity-aware expansion, (2) applies reverse question construction to create oblique cues where isolated signals are underinformative but combined uniquely identify targets, (3) enforces quality with multi-model consensus filtering and structured constraint decomposition with evidence-based matching.

Result: Scalable process that produces complex, retrieval-resistant yet verifiable questions suitable for both SFT/RL training and challenging evaluation, reducing human curation effort while maintaining difficulty comparable to strong evaluation benchmarks.

Conclusion: The framework successfully addresses the critical data bottleneck for training high-capability retrieval-and-reasoning agents by automating the generation of training-ready multi-hop QA datasets that preserve the challenging characteristics of hard evaluation benchmarks.

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [130] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: BLM‚ÇÅ is a multimodal spatial foundation model that unifies digital and physical space operations with cross-embodiment generalization, outperforming existing model families by ~6% in digital tasks and ~3% in physical tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs have poor generalization across digital-physical spaces and embodiments, VLAs lack robust high-level reasoning, and ELLMs are constrained to digital space with poor physical world generalization. Unified models that work seamlessly across spaces and embodiments are missing.

Method: Two-stage training: Stage I injects embodied knowledge into MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module via intent-bridging interface that extracts high-level semantics from MLLM to guide control without fine-tuning the MLLM backbone.

Result: Single BLM‚ÇÅ instance outperforms four model families (MLLMs, ELLMs, VLAs, GMLMs) with ~6% gains in digital tasks and ~3% gains in physical tasks across evaluations on digital and physical benchmarks.

Conclusion: BLM‚ÇÅ successfully integrates cross-space transfer, cross-task learning, and cross-embodiment generalization, demonstrating robust performance across digital and physical environments while maintaining instruction following and reasoning capabilities.

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [131] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: UniPlanner is the first autonomous vehicle planning framework for multi-dataset integration, using three innovations to achieve unified cross-dataset learning while maintaining planning robustness.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning planning methods are confined to single-dataset training, limiting their robustness. The discovery that vehicular trajectory distributions and history-future correlations are consistent across datasets motivates multi-dataset integration.

Method: 1) HFTDN aggregates history-future trajectory pairs using historical trajectory similarity for cross-dataset planning guidance. 2) GFTM learns robust history-future correlations with gradient-free design to prevent shortcut learning. 3) S2D paradigm uses adaptive dropout for robust training and full prior utilization during inference.

Result: The framework achieves unified cross-dataset learning and maintains planning robustness while enabling knowledge transfer across different datasets.

Conclusion: UniPlanner successfully addresses the limitations of single-dataset training in autonomous vehicle planning by leveraging multi-dataset integration through three synergistic components, demonstrating consistent trajectory patterns across datasets can be effectively utilized for robust planning.

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [132] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: MGA is a memory-driven GUI agent that uses an "observe first, then decide" approach with structured memory to improve robustness and generalization in GUI interactions.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents suffer from error propagation due to dependence on historical trajectories and local exploration bias from "decision-first, observation-later" mechanisms that overlook critical interface cues.

Method: MGA reframes GUI interaction around "observe first, then decide" principle, modeling each step as an independent state with current screenshot, task-agnostic spatial information, and dynamically updated structured memory.

Result: Experiments on OSworld benchmarks, real desktop applications (Chrome, VSCode, VLC), and cross-task transfer show MGA achieves substantial gains in robustness, generalization, and efficiency compared to state-of-the-art baselines.

Conclusion: The memory-driven approach with structured representation enables more effective GUI agent performance by addressing key limitations of existing methods.

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [133] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow is an automated pipeline that discovers MCP servers, synthesizes training data, and trains models to improve LLMs' ability to use external tools from the Model Contextual Protocol ecosystem.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to utilize the rapidly expanding MCP ecosystem due to limited server coverage, costly manual curation, and lack of training support in existing research.

Method: An automated web-agent-driven pipeline for large-scale server discovery (1166 servers, 11536 tools), data synthesis (68733 instruction-function pairs, 6439 trajectories), and model training.

Result: MCP-Flow significantly outperforms prior work in scale and diversity, demonstrating superior MCP tool selection, function-call generation, and enhanced agentic task performance.

Conclusion: MCP-Flow provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments, with publicly available implementation.

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [134] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schm√∂cker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: The paper addresses the tie-breaking issue in MCTS when multiple actions from the same parent fall into the same abstract node, proposing and evaluating alternative intra-abstraction policies that outperform random tie-breaking.


<details>
  <summary>Details</summary>
Motivation: MCTS suffers from sample inefficiency, which can be improved using state/action abstractions to share information among nodes. However, current abstraction methods like pruned OGA don't properly handle cases where multiple actions from the same parent share an abstract node, requiring tie-breaking.

Method: The authors propose and empirically evaluate several alternative intra-abstraction policies to replace the implicit random tie-breaking used in state-of-the-art abstraction algorithms like pruned OGA.

Result: Several of the proposed intra-abstraction policies outperform the random policy across a majority of environments and parameter settings.

Conclusion: Alternative intra-abstraction policies can significantly improve MCTS performance when multiple actions from the same parent share abstract nodes, addressing a previously unnoticed limitation in existing abstraction algorithms.

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [135] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: The paper proposes Self-Indicator, a method that uses the rank of correlation matrices between input problems and output reasoning paths from LLMs to assess reasoning correctness without external resources.


<details>
  <summary>Details</summary>
Motivation: Existing methods for checking LLM outputs rely heavily on external resources like trained verifiers or elaborate prompts, leading to high computational overhead and domain-specific limitations.

Method: The method calculates the rank of correlation matrices between input problems and LLM reasoning paths, using this as an indicator of reasoning correctness. It then reweights candidate reasoning paths based on this indicator.

Result: Self-Indicator achieves over 75% accuracy in distinguishing correct reasoning paths from incorrect ones and improves accuracy on three reasoning benchmarks by more than 8% with minimal computational overhead.

Conclusion: The internal behaviors of LLMs contain reliable indicators of reasoning correctness, enabling effective self-verification without external resources.

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [136] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: A multi-agent framework using LLMs for claim verification through quantitative bipolar argumentation frameworks (QBAFs), with experiments showing improved forecasting accuracy when combining evidence from multiple agents.


<details>
  <summary>Details</summary>
Motivation: To improve judgmental forecasting by treating it as claim verification and leveraging multiple agents to provide diverse evidence and perspectives on future events.

Method: Proposed multi-agent framework with three types of LLM-powered agents: ArgLLM (existing QBAF approach), RbAM (relation-based argument mining from external sources), and RAG-ArgLLM (retrieval-augmented generation of arguments). Experiments used 2-3 agents with 6 different base LLMs on standard judgmental forecasting datasets.

Result: Combining evidence from multiple agents improved forecasting accuracy, especially with three agents, while providing explainable evidence combinations for claim verification.

Conclusion: Multi-agent frameworks with LLMs can effectively support claim verification in judgmental forecasting by combining diverse evidence sources and improving accuracy while maintaining explainability.

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [137] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: gLLMs like ChatGPT are revolutionizing communication research by outperforming human coders in content analysis at lower cost and time, but face seven key challenges that need systematic guidance for proper implementation.


<details>
  <summary>Details</summary>
Motivation: To address the underdeveloped integration of gLLMs into communication research methodology despite their potential for superior performance in content analysis tasks.

Method: Synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide addressing seven critical challenges.

Result: Identifies seven key challenges in gLLM-assisted content analysis: codebook development, prompt engineering, model selection, parameter tuning, iterative refinement, validation of reliability, and performance enhancement.

Conclusion: Provides a framework to make gLLM-based content analysis more accessible while ensuring adherence to disciplinary standards of validity, reliability, reproducibility, and research ethics.

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [138] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents is a multi-agent system that embeds Predictability-Computability-Stability (PCS) principles into LLM-driven data science workflows, outperforming existing end-to-end systems like AutoKaggle and DataInterpreter.


<details>
  <summary>Details</summary>
Motivation: Current LLM-driven data science systems rely solely on LLM reasoning without scientific guidance, limiting trustworthiness and robustness with real-world noisy datasets.

Method: Multi-agent system implementing modular workflow (data cleaning, feature engineering, modeling, evaluation) guided by PCS principles, incorporating perturbation analysis, unit testing, and model validation.

Result: Outperformed AutoKaggle and DataInterpreter on nine diverse datasets using DeepSeek-V3 and GPT-4o backends.

Conclusion: Successfully demonstrates feasibility of embedding PCS principles into LLM-driven data science automation to improve system performance and trustworthiness.

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [139] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: The paper introduces Brain-like Space, a unified geometric framework that maps AI models' intrinsic spatial attention organization to human brain networks, enabling comparison across different modalities and revealing a continuous arc-shaped geometry of brain-likeness.


<details>
  <summary>Details</summary>
Motivation: To understand whether artificial neural networks organize information similarly to the human brain, and to create a common ground for comparing AI models across different modalities (vision, language, multimodal) that is not bound to specific inputs or tasks.

Method: Developed Brain-like Space concept by mapping AI models' intrinsic spatial attention topological organization onto canonical human functional brain networks. Analyzed 151 Transformer-based models including large vision models, large language models, and large multimodal models.

Result: Uncovered a continuous arc-shaped geometry reflecting gradual increase in brain-likeness. Found that brain-likeness is shaped by pretraining paradigm (global semantic abstraction emphasis) and positional encoding schemes (facilitating cross-modal fusion), not just modality. Brain-likeness and downstream task performance are not identical.

Conclusion: Brain-like Space provides the first unified framework for situating, quantifying, and comparing intelligence across domains, revealing deep organizational principles that bridge machines and the brain.

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [140] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: Proposes a multi-agent AI ecosystem for N-of-1 medical decision support that moves beyond average patient models to provide personalized, transparent care for individual patients including those with rare conditions or underrepresented demographics.


<details>
  <summary>Details</summary>
Motivation: Current AI systems in medicine serve the average patient but fail at the margins - patients with rare variants, multimorbidity, or underrepresented demographics. This 'average patient fallacy' erodes equity and trust in medical AI.

Method: A multi-agent ecosystem where agents clustered by organ systems, patient populations, and analytic modalities use shared models and evidence synthesis tools. Results converge in a coordination layer that weighs reliability, uncertainty, and data density to create decision-support packets with risk estimates, confidence ranges, outlier flags, and linked evidence.

Result: Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in small groups, and risk-coverage trade-offs. The system provides transparent, equitable care centered on individual patients.

Conclusion: By moving from monolithic models to orchestrated intelligence, this approach aligns medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual, addressing computational demands, automation bias, and regulatory challenges through caching, consensus checks, and adaptive trials.

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [141] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: This paper addresses challenges in training Spiking Neural Networks (SNNs) for robotic control by analyzing surrogate gradient slopes and proposing a novel training approach using privileged guiding policies, achieving significant performance improvements in real-world drone control tasks.


<details>
  <summary>Details</summary>
Motivation: SNNs offer energy efficiency for robotics but face challenges: non-differentiable spiking neurons require surrogate gradients with unclear optimization properties, and stateful dynamics require sequence training hindered by limited sequence lengths in reinforcement learning.

Method: Systematically analyzed surrogate gradient slope settings and proposed a novel training approach using privileged guiding policies to bootstrap learning while exploiting online environment interactions with spiking policies, combined with adaptive slope schedules.

Result: Shallower slopes or scheduled slopes led to 2.1x improvement in RL performance. The combined method achieved 400 points average return in drone position control, substantially outperforming prior techniques like Behavioral Cloning and TD3BC which achieved at most -200 points.

Conclusion: This work advances both theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers, demonstrating successful application in real-world robotic systems.

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [142] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavraƒçiƒá*

Main category: cs.AI

TL;DR: Policy Cards are a machine-readable standard for defining operational, regulatory, and ethical constraints that AI agents must follow at runtime, enabling verifiable compliance and accountable autonomy.


<details>
  <summary>Details</summary>
Motivation: To address the need for practical mechanisms that integrate AI governance with engineering practice, ensuring autonomous agents follow required constraints and enabling distributed assurance in multi-agent ecosystems.

Method: Create a deployment-layer standard that defines a normative layer with allow/deny rules, obligations, evidentiary requirements, and mappings to assurance frameworks like NIST AI RMF, ISO/IEC 42001, and EU AI Act.

Result: Policy Cards can be automatically validated, version-controlled, and linked to runtime enforcement or continuous-audit pipelines, forming a foundation for distributed assurance.

Conclusion: Policy Cards provide a practical solution for verifiable compliance in autonomous agents, bridging high-level governance with hands-on engineering to enable accountable autonomy at scale.

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [143] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion is a novel LLM reasoning framework that enables dependency-aware query decomposition and logic-parallel content expansion to achieve both high efficiency and quality for real-time web applications.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning struggles to meet the dual requirements of high-quality complex reasoning and low-latency/high-throughput for interactive web services, creating a critical bottleneck.

Method: Orion decomposes queries into two phases: key point generation (retrieval-augmented few-shot prompting) and content parallel expansion (concurrent elaboration based on dependency graphs), with pipeline scheduling for cross-query parallelism.

Result: Orion achieves up to 4.33x higher token generation speed, 3.42x lower answer latency, and improves reasoning quality by up to 18.75% compared to baselines.

Conclusion: Orion successfully addresses the LLM reasoning bottleneck for web applications by enabling efficient parallel processing while maintaining logical consistency and improving reasoning quality.

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [144] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: APTBench is a new benchmark framework that converts real-world agent tasks into multiple-choice/text completion questions to evaluate agentic capabilities during LLM pre-training, focusing on planning and action abilities in software engineering and deep research scenarios.


<details>
  <summary>Details</summary>
Motivation: Current pre-training benchmarks focus on isolated static skills and fail to assess agentic capabilities, while agent benchmarks require multi-turn execution that base models can't handle, creating a gap in evaluating agentic potential during pre-training.

Method: APTBench converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models, focusing on core agentic abilities like planning and action across software engineering and deep research scenarios.

Result: APTBench provides more predictive signals of downstream agent performance compared to general-purpose benchmarks, while being significantly more lightweight and cost-effective than full-scale end-to-end agent evaluations after post-training.

Conclusion: APTBench addresses the critical need for evaluating agentic potential during pre-training, offering a practical framework to guide model training more effectively by bridging the gap between static skill benchmarks and complex agent evaluations.

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [145] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: MobileRisk-Live is a dynamic sandbox environment with safety detection benchmark for mobile agents, and OS-Sentinel is a hybrid framework combining formal verification and VLM-based contextual judgment to detect safety risks.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Model (VLM) agents operating mobile environments pose potential safety risks like system compromise and privacy leakage, but detecting these risks across complex mobile operational spaces remains underexplored.

Method: Introduced MobileRisk-Live benchmark with realistic trajectories and fine-grained annotations, and proposed OS-Sentinel framework that combines Formal Verifier for system-level violations with VLM-based Contextual Judge for contextual risk assessment.

Result: OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics, demonstrating superior safety detection performance.

Conclusion: The work establishes foundation for mobile agent safety research and provides critical insights for developing safer autonomous mobile agents.

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [146] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: This study compares logical and abstract reasoning abilities of multiple LLMs against human performance using custom reasoning questions, revealing significant gaps in LLMs' deduction capabilities.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs truly understand information and can perform logical inferences beyond linguistic tasks, advancing AI reasoning capabilities.

Method: Used eight custom-designed reasoning questions to test several LLMs (GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral, Perplexity, Sabi'a) and benchmarked results against human performance on the same tasks.

Result: Revealed significant differences between LLM and human performance, indicating areas where LLMs struggle with deduction and logical reasoning.

Conclusion: LLMs show limitations in logical and abstract reasoning compared to humans, highlighting the need for improved reasoning capabilities in artificial intelligence systems.

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [147] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: Law in Silico is an LLM-based agent framework that simulates legal societies with individual decision-making and institutional mechanisms, showing it can reproduce real-world crime trends and provide insights about legal system effectiveness.


<details>
  <summary>Details</summary>
Motivation: Real-world legal experiments are costly or infeasible, so simulating legal societies with AI provides an effective alternative for legal theory verification and development, as well as supporting legal administration.

Method: Introduces Law in Silico, an LLM-based agent framework for simulating legal scenarios with individual decision-making and institutional mechanisms of legislation, adjudication, and enforcement.

Result: Experiments comparing simulated crime rates with real-world data show LLM-based agents can largely reproduce macro-level crime trends and provide insights aligning with real-world observations. Micro-level simulations reveal that well-functioning, transparent, and adaptive legal systems better protect vulnerable individuals' rights.

Conclusion: LLM-based agents are effective for legal society simulation, capable of reproducing real-world crime patterns and demonstrating the importance of well-functioning legal systems for protecting vulnerable populations.

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [148] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: This paper introduces a pattern language with two architectural patterns for building actionable world models from structured data: DOM Transduction Pattern for simplifying web page complexity and Hypermedia Affordances Recognition Pattern for dynamically discovering web service capabilities.


<details>
  <summary>Details</summary>
Motivation: Software agents need actionable world models from structured data like DOM and web service descriptions, but face challenges with HTML verbosity and static API integrations that prevent adaptation to evolving services.

Method: Proposes two complementary architectural patterns: 1) DOM Transduction Pattern that distills verbose DOM into compact, task-relevant representations, and 2) Hypermedia Affordances Recognition Pattern that enables dynamic discovery and integration of unknown web service capabilities at runtime.

Result: The patterns provide a robust framework for engineering agents that can efficiently construct and maintain accurate world models from structured data sources.

Conclusion: This pattern language enables scalable, adaptive, and interoperable automation across the web and its extended resources by allowing agents to efficiently build and maintain actionable world models from complex structured data.

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [149] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: Proposes a two-stage pipeline for cost-efficient in-context learning that reduces LLM dependency by using cross-task examples for pseudo-labeling and graph-based label propagation.


<details>
  <summary>Details</summary>
Motivation: Collecting high-quality examples for new or challenging tasks is costly and labor-intensive, creating a need for more efficient ICL methods.

Method: Two-stage pipeline: 1) Use cross-task examples to prompt LLM for pseudo-labeling small target dataset, 2) Graph-based label propagation to spread labels to remaining examples without additional LLM queries.

Result: Experiments across five tasks show strong performance while significantly reducing labeling costs compared to traditional approaches.

Conclusion: The proposed method effectively combines cross-task supervision flexibility with LLM-free propagation scalability for cost-efficient in-context learning.

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [150] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: The paper proposes a data-centric paradigm for deploying Generative AI in healthcare, repositioning the medical data ecosystem as the foundational substrate to support GenAI systems through effective data processing pipelines and knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: GenAI promises transformative opportunities in healthcare but requires understanding of what can and cannot be achieved. Current deployments need better integration with healthcare tasks and data management.

Method: Proposes repositioning the medical data ecosystem as the foundational substrate for GenAI systems, with semantic vector search, contextual querying, and effective data processing pipelines to support both upstream model training and downstream clinical applications.

Result: The approach enables GenAI-powered operations that supply foundation models with high-quality multimodal data for pretraining and fine-tuning, while serving as a knowledge retrieval backend for task-specific inference via agentic layers.

Conclusion: The data-centric paradigm enables high-quality and effective deployment of GenAI for healthcare delivery by making the medical data ecosystem the core foundation for generative healthcare systems.

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [151] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT is a novel data synthesis framework that generates high-quality multi-turn function calling training data through environment-API graph interactions, advanced tool-query synthesis, and guided iterative chains, enabling smaller models to achieve state-of-the-art performance on function calling benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing data synthesis methods for function calling are insufficient for real-world environments due to challenges in targeted model training, tool architecture isolation, and multi-turn logical dependencies. High-quality multi-turn training data is critical for developing advanced AI systems' tool-using capabilities.

Method: FunReason-MT employs three key components: 1) Environment-API Graph Interactions to collect diverse high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify complex query construction, and 3) Guided Iterative Chain for sophisticated chain-of-thought generation.

Result: A 4B parameter model trained on FunReason-MT generated data achieved state-of-the-art performance on Berkeley Function-Calling Leaderboard (BFCLv3), outperforming most comparable-sized models and even closed-source models. Further improvements were demonstrated on BFCLv4.

Conclusion: FunReason-MT provides a reliable and robust framework for generating high-quality multi-turn function calling data, enabling effective agentic learning and addressing the structural deficiencies in existing data synthesis methods.

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [152] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: Foundation models (FMs) are transforming site-specific disease management in crops through vision-language models, enabling text-based symptom interpretation and interactive QA, with digital twins and reinforcement learning emerging for targeted spraying applications.


<details>
  <summary>Details</summary>
Motivation: To advance site-specific disease management (SSDM) in crops by leveraging foundation models that can integrate visual and textual data, interpret symptoms, reason about symptom-management relationships, and support interactive systems for growers.

Method: Reviewed approximately 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and analyzing their role in adaptive learning, reinforcement learning, and digital twin frameworks for targeted spraying.

Result: Key findings: FMs are gaining traction with surging literature in 2023-24; VLMs outpace LLMs with 5-10x increase in publications; RL and AL are still nascent for smart spraying; digital twins with RL can simulate targeted spraying; addressing sim-to-real gap is critical; human-robot collaboration remains limited.

Conclusion: Multi-modal FMs with real-time feedback will drive next-generation SSDM, with continued development needed in human-in-the-loop approaches and bridging the sim-to-real gap for practical field deployment.

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [153] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: OrchDAG is a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) to address multi-turn tool interactions, with experiments showing it provides a challenging benchmark and that graph-based rewards enhance RLVR training.


<details>
  <summary>Details</summary>
Motivation: Most existing work on agentic tool use overlooks the complexity of multi-turn tool interactions, creating a gap in understanding and benchmarking these complex interactions.

Method: Introduce OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) with controllable complexity, and propose a graph-based reward to enhance RLVR training.

Result: The dataset presents a challenging but solvable benchmark, and the proposed graph-based reward is effective when combined with GRPO-style algorithms.

Conclusion: Leveraging topological structure and data complexity is important for advancing multi-turn tool use capabilities in agentic systems.

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [154] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: A framework that integrates tool and document knowledge graphs to improve exemplar artifact generation through dependency modeling and deep-sparse integration.


<details>
  <summary>Details</summary>
Motivation: To enhance exemplar artifact generation by uncovering and exploiting dependencies among tools and documents, addressing the need for better tool-augmented reasoning and planning.

Method: Constructs tool knowledge graph from tool schemas using DeepResearch-inspired analysis, derives complementary knowledge graph from internal documents/SOPs, fuses both graphs, and uses deep-sparse integration to align structural tool dependencies with procedural knowledge.

Result: Experiments show the unified framework effectively models tool interactions and improves plan generation.

Conclusion: Linking tool graphs with domain knowledge graphs provides significant benefits for tool-augmented reasoning and planning.

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>
