<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: ESCA is a framework that enhances embodied agents through structured spatial-temporal understanding using SGClip, a CLIP-based model for generating scene graphs without human annotations.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack fine-grained alignment between visual content and textual semantics, limiting their effectiveness as general-purpose embodied agents.

Method: Proposed ESCA framework with SGClip model trained on 87K+ videos using neurosymbolic learning pipeline that combines model-driven self-supervision from video-caption pairs and structured reasoning.

Result: SGClip excels in scene graph generation and action localization benchmarks. ESCA consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across embodied environments while significantly reducing perception errors.

Conclusion: ESCA enables open-source models to surpass proprietary baselines by providing structured spatial-temporal understanding through automated scene graph generation without human annotations.

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: CrossRay3D is a sparse multi-modal 3D detector that improves token representation quality through Ray-Aware Supervision and Class-Balanced Supervision, achieving state-of-the-art performance on nuScenes while being computationally efficient and robust to missing sensor data.


<details>
  <summary>Details</summary>
Motivation: Existing sparse cross-modality detectors overlook token representation quality, resulting in sub-optimal foreground quality and limited performance. The paper identifies that geometric structure preservation and class distribution are key to improving sparse detector performance.

Method: Proposes Sparse Selector (SS) with two core modules: Ray-Aware Supervision (RAS) to preserve geometric information during training, and Class-Balanced Supervision to adaptively reweight class semantics. Also introduces Ray Positional Encoding (Ray PE) to address LiDAR-image modality distribution differences. These are integrated into an end-to-end sparse multi-modality detector called CrossRay3D.

Result: On nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS, while running 1.84× faster than other leading methods. Demonstrates strong robustness even with partial or complete missing LiDAR or camera data.

Conclusion: CrossRay3D successfully addresses the token representation quality problem in sparse detectors through geometric structure preservation and balanced class handling, achieving superior performance and efficiency while maintaining robustness to sensor failures.

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: A comprehensive pipeline using CCTV streams for multi-defect detection with YOLO and vision language models to generate structured maintenance action plans in JSON format.


<details>
  <summary>Details</summary>
Motivation: Manual infrastructure inspection is costly and hazardous, while existing automatic systems are limited to individual defect types or provide unstructured outputs that can't directly guide maintenance crews.

Method: Leverages street CCTV streams for multi-defect detection and segmentation using YOLO object detectors, then passes detections to a vision language model (VLM) for scene-aware summarization that generates structured JSON action plans.

Result: Experimental evaluation on public datasets and captured CCTV clips demonstrates accurate identification of diverse defects and production of coherent summaries.

Conclusion: The system shows promise for infrastructure monitoring but faces challenges in scaling to city-wide deployments, requiring further development for practical implementation.

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: IAD-GPT is a novel MLLM-based paradigm for industrial anomaly detection that combines text semantics with image features, using abnormal prompts, text-guided enhancement, and multi-mask fusion to achieve state-of-the-art performance on detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods lack multi-turn dialogues and detailed descriptions, while existing large model approaches haven't fully exploited MLLMs' potential for anomaly detection. The paper aims to bridge this gap by leveraging MLLMs' robust causal capabilities.

Method: Uses Abnormal Prompt Generator to create detailed anomaly prompts, Text-Guided Enhancer for visual grounding by interacting image features with text prompts, and Multi-Mask Fusion to incorporate mask knowledge for pixel-level anomaly perception.

Result: Achieves state-of-the-art performance on MVTec-AD and VisA datasets for self-supervised and few-shot anomaly detection and segmentation tasks.

Conclusion: IAD-GPT effectively combines text semantics with visual information to enhance MLLMs' anomaly detection capabilities, demonstrating superior performance through comprehensive experiments.

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: AI-assisted structured reporting (AI-SR) improves diagnostic accuracy and efficiency compared to free-text and structured reporting alone, with reduced reporting times and optimized visual attention patterns.


<details>
  <summary>Details</summary>
Motivation: To evaluate how structured reporting and AI assistance impact radiologists' image analysis behavior, diagnostic accuracy, efficiency, and user experience in chest radiograph interpretation.

Method: Prospective study with 8 readers (4 novice, 4 non-novice) analyzing 35 chest radiographs each using three reporting modes: free-text (FT), structured reporting (SR), and AI-assisted structured reporting (AI-SR), with eye-tracking and statistical analysis.

Result: AI-SR achieved highest diagnostic accuracy (κ=0.71 vs 0.58-0.60), fastest reporting times (25±9s vs 37±18s SR, 88±38s FT), and reduced visual attention shifts. AI-SR was the preferred mode among users.

Conclusion: Structured reporting improves efficiency by guiding visual attention toward images, and AI-prefilled structured reporting further enhances diagnostic accuracy and user satisfaction.

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: A framework for analyzing and mitigating intersectional biases in image classification through systematic evaluation and adaptive data augmentation.


<details>
  <summary>Details</summary>
Motivation: Machine learning models trained on imbalanced datasets often exhibit intersectional biases from interactions of multiple attributes like object class and environmental conditions.

Method: Proposes Intersectional Fairness Evaluation Framework (IFEF) with fairness metrics and interpretability tools, plus Bias-Weighted Augmentation (BWA) that adapts transformation intensities based on subgroup statistics.

Result: On Open Images V7 dataset, BWA improved accuracy for underrepresented class-environment intersections by up to 24 percentage points and reduced fairness metric disparities by 35%, with statistically significant improvements (p < 0.05).

Conclusion: The methodology provides a replicable approach for analyzing and addressing intersectional biases in image classification systems.

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: This paper presents a differentiable quantization method for neural networks that provides n-bit quantization, achieves near-full-precision accuracy with weight-only quantization, and comparable SOTA results with weight+activation quantization in just 15 training epochs.


<details>
  <summary>Details</summary>
Motivation: Previous quantization approaches were non-differentiable with manually set derivatives in backpropagation, making learning questionable. Also, existing shift/logarithmic quantization methods either avoided activation quantization or achieved lower accuracy, and couldn't scale beyond 1-bit quantization.

Method: The proposed approach uses a differentiable quantization method that can scale to n-bit quantization, supports both weight and activation quantization using shift bit quantization, and provides proof of convergence to optimal neural networks.

Result: With ImageNet using ResNet18: weight-only quantization achieves <1% accuracy drop compared to full precision; weight+activation quantization achieves comparable SOTA accuracy. Both results achieved in only 15 training epochs with slightly higher CPU instructions but no higher precision multiplication required.

Conclusion: The method provides a differentiable quantization approach with proven convergence that enables efficient n-bit quantization while maintaining high accuracy and fast training convergence.

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: StripRFNet is a novel deep neural network for road damage detection that addresses challenges in diverse damage shapes, slender crack detection, and small-scale damage recognition through three specialized modules, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Road surface damage threatens traffic safety and hinders sustainable urban development. Accurate detection is challenging due to diverse damage shapes, difficulty capturing slender cracks with high aspect ratios, and high error rates in small-scale damage recognition.

Method: StripRFNet comprises three modules: (1) Shape Perception Module (SPM) with large separable kernel attention for shape discrimination in multi-scale feature aggregation; (2) Strip Receptive Field Module (SRFM) using large strip convolutions and pooling to capture slender crack features; (3) Small-Scale Enhancement Module (SSEM) with high-resolution P2 feature map, dedicated detection head, and dynamic upsampling for small-object detection.

Result: On RDD2022 benchmark: Chinese subset improved F1-score, mAP50, and mAP50:95 by 4.4, 2.9, and 3.4 percentage points over baseline. On full dataset, achieved highest F1-score of 80.33% compared to CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while maintaining competitive inference speed.

Conclusion: StripRFNet achieves state-of-the-art accuracy and real-time efficiency, offering a promising tool for intelligent road maintenance and sustainable infrastructure management.

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms is a technique that uses object-specific transformations to quantify and reduce uncertainty in vision-based object detection for autonomous driving. It applies color space perturbations and diffusion-generated pedestrians during training, and uses detection score variance at inference to filter false positives and recover false negatives.


<details>
  <summary>Details</summary>
Motivation: Vision-based object detectors are vulnerable to uncertainty from data bias and distributional shifts, which is critical for safety in autonomous driving. There's a need for reliable perception that can quantify and reduce this uncertainty.

Method: ObjectTransforms performs color space perturbations on individual objects during training for robustness, and uses diffusion models to generate diverse pedestrian instances. At inference, it applies object perturbations and uses detection score variance to quantify predictive uncertainty.

Result: Experiments with YOLOv8 on NuImages 10K dataset show notable accuracy improvements and uncertainty reduction across all object classes during training, and predict higher uncertainty for false positives compared to true positives during inference.

Conclusion: ObjectTransforms is a lightweight yet effective mechanism for reducing uncertainty during training and quantifying it during inference, improving the precision-recall curve and overall reliability of vision-based perception in autonomous driving.

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: The Aria Gen 2 Pilot Dataset (A2PD) is an incremental multimodal egocentric dataset captured using Aria Gen 2 glasses, featuring daily activities across five scenarios with raw sensor data and machine perception outputs.


<details>
  <summary>Details</summary>
Motivation: To provide timely access to comprehensive egocentric multimodal data for research, demonstrating the Aria Gen 2 glasses' capability to perceive the wearer, environment, and interactions across diverse users and conditions.

Method: Dataset collection using Aria Gen 2 glasses with primary subject Dia'ane and friends recording daily activities in five scenarios: cleaning, cooking, eating, playing, and outdoor walking, providing both raw sensor data and machine perception algorithm outputs.

Result: A publicly available dataset at projectaria.com with open-source tools and usage examples, showcasing robust performance across diverse users and conditions in perceiving wearer, environment, and their interactions.

Conclusion: A2PD serves as a valuable resource for egocentric multimodal research with incremental releases and comprehensive data supporting various machine perception applications.

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: A training-free method for transferring appearance to 3D assets using rectified flow models with periodic guidance, outperforming direct 3D generative approaches and enabling texture and geometric detail transfer even with significant geometry differences.


<details>
  <summary>Details</summary>
Motivation: Current appearance transfer methods fail when geometry between input and appearance objects differs significantly, and direct 3D generative models produce unappealing results, highlighting the need for a more principled approach.

Method: Uses pretrained rectified flow models conditioned on image or text, with training-free guidance added periodically during sampling. Guidance is modeled as differentiable loss functions including part-aware losses for appearance and self-similarity.

Result: Successfully transfers texture and geometric details to 3D assets, outperforming baselines both qualitatively and quantitatively. Traditional metrics are unsuitable, so evaluation uses GPT-based ranking and user studies.

Conclusion: The method provides effective appearance transfer for dissimilar geometries and can be extended to different diffusion models and guidance functions, with applications in gaming, AR, and digital content creation.

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: Deep learning framework for automating thrombectomy procedures using self-supervised landmark classification with regression-based pretext tasks


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency and safety of resource-intensive thrombectomy procedures for ischemic stroke treatment

Method: Self-supervised framework that classifies skeletal landmarks using regression-based pretext tasks

Result: Model outperforms existing methods in regression and classification; positional pretext task significantly improves downstream classification performance

Conclusion: Framework shows promise for extending toward fully autonomous C-arm control to optimize trajectories from pelvis to head during thrombectomy

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch is a dual-branch semi-supervised framework for medical image segmentation that uses asynchronous optimization to separately train encoder and decoder branches, with novel techniques including Decoupled Dropout Perturbation, Pair-wise CutMix Cross-Guidance, and Consistency Matching to improve robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Limited annotated medical imaging data makes semi-supervised learning essential, but joint optimization in teacher-student frameworks can cause convergence and stability issues, especially in challenging scenarios.

Method: Proposes DuetMatch with dual-branch asynchronous optimization (each branch optimizes encoder or decoder while keeping other frozen), Decoupled Dropout Perturbation for regularization, Pair-wise CutMix Cross-Guidance for diversity, and Consistency Matching to refine noisy pseudo-labels.

Result: Extensive experiments on brain MRI segmentation datasets (ISLES2022 and BraTS) show DuetMatch consistently outperforms state-of-the-art methods across diverse semi-supervised scenarios.

Conclusion: DuetMatch demonstrates effectiveness and robustness for medical image segmentation, addressing key challenges in semi-supervised learning through its novel dual-branch architecture and regularization techniques.

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: A pipeline for autonomous C-arm navigation to anatomical landmarks using X-ray images, with uncertainty quantification and conformal prediction for reliable deployment.


<details>
  <summary>Details</summary>
Motivation: Manual C-arm positioning increases radiation exposure and procedural delays in fluoroscopy-guided interventions, necessitating automated solutions.

Method: Uses X-ray images from arbitrary starting positions to predict 3D displacement vectors toward target landmarks, incorporates aleatoric and epistemic uncertainty, applies conformal prediction for calibration, and combines probabilistic loss with skeletal pose regularization.

Result: Strong localization accuracy across multiple architectures with well-calibrated prediction bounds on synthetic X-ray data from DeepDRR.

Conclusion: The pipeline shows potential as a component in safe and reliable autonomous C-arm systems for clinical use.

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: The paper proposes an automatic pre-filtering stage using image quality assessment (IQA) to reduce manual review costs in deep generative image production pipelines, achieving 51.61% cost savings with an AutoML solution.


<details>
  <summary>Details</summary>
Motivation: Manual image quality assessment in generative AI pipelines is slow and expensive due to low yield of high-quality generated images, creating a need for automated pre-filtering to reduce costs.

Method: The authors developed a formula to estimate cost savings based on precision and pass yield of IQA engines, and applied it in a background inpainting use case using a simple AutoML solution.

Result: The proposed approach demonstrated significant cost savings of 51.61% in the background inpainting use case.

Conclusion: Automatic pre-filtering through IQA engines can substantially reduce manual review costs in generative image production pipelines, making the process more efficient and cost-effective.

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: PRISM is a novel framework that projects fMRI signals into a structured text space as an intermediate representation for visual stimuli reconstruction, outperforming existing methods by up to 8% reduction in perceptual loss.


<details>
  <summary>Details</summary>
Motivation: Understanding how the brain encodes visual information through fMRI signals and determining which latent space best supports the transformation from neural activity to image reconstruction.

Method: Projects fMRI signals into a structured text space, includes an object-centric diffusion module for composing individual objects to reduce detection errors, and an attribute-relationship search module to identify key attributes and relationships that align with neural activity.

Result: Extensive experiments on real-world datasets show PRISM outperforms existing methods, achieving up to 8% reduction in perceptual loss.

Conclusion: Using structured text as the intermediate space is crucial for effectively bridging fMRI signals and image reconstruction, as fMRI signals are more similar to text space than vision-based or joint text-image spaces.

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: The paper proposes a data-centric AI pipeline for tropical agriculture mapping, focusing on data quality and curation techniques to overcome challenges like limited annotated data and cloud cover.


<details>
  <summary>Details</summary>
Motivation: Tropical agriculture mapping faces unique challenges including lack of high-quality annotated data, high labeling costs, data variability, and regional generalization issues due to cloud cover and diverse crop calendars.

Method: Advocates a Data-Centric AI approach with techniques like confident learning, core-set selection, data augmentation, and active learning. Proposes a practical pipeline using 9 most mature methods for large-scale tropical agricultural mapping.

Result: Identifies 25 distinct strategies suitable for agricultural mapping pipelines and highlights their readiness for implementation in tropical contexts.

Conclusion: Data-centric approaches provide practical solutions for curating and training AI models that are better suited to the dynamic realities of tropical agriculture, overcoming limitations of traditional model-centric methods.

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: StretchySnake is a flexible training method for video SSMs that enables spatio-temporal adaptability by sampling videos at varying resolutions and dynamically interpolating model weights, outperforming transformer and SSM baselines by up to 28% across diverse action recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Current video training methods are tailored for transformers and fail to leverage SSMs' unique attributes, causing spatio-temporal inflexibility where models perform poorly on videos with unseen spatial and temporal resolutions during training.

Method: Proposes a flexible training method that samples videos at varying temporal and spatial resolutions and dynamically interpolates model weights to accommodate any spatio-temporal scale, introducing five variants of flexible training for video SSMs.

Result: StretchySnake outperforms transformer and SSM baselines by up to 28% on short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks, with strong adaptability to fine-grained actions (SSV2, Diving-48).

Conclusion: The method provides a simple drop-in training recipe that makes video SSMs more robust, resolution-agnostic, and efficient across diverse action recognition scenarios.

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: VM-BeautyNet is a novel ensemble model combining Vision Transformer and Mamba-based Vision model for facial beauty prediction, achieving state-of-the-art performance on SCUT-FBP5500 dataset with PC=0.9212, MAE=0.2085, RMSE=0.2698.


<details>
  <summary>Details</summary>
Motivation: Existing CNN models struggle to capture global facial features important for beauty perception, while Vision Transformers have quadratic complexity limitations. There's a need to combine global feature modeling with efficient long-range dependency capture.

Method: Proposed VM-BeautyNet - a heterogeneous ensemble architecture that fuses Vision Transformer (for global facial structure/symmetry) and Mamba-based Vision model (for efficient long-range dependencies with linear complexity).

Result: Achieved state-of-the-art performance on SCUT-FBP5500: Pearson Correlation=0.9212, MAE=0.2085, RMSE=0.2698. Grad-CAM visualizations confirmed complementary feature extraction between the two backbones.

Conclusion: VM-BeautyNet presents a powerful new architectural paradigm for computational aesthetics, effectively combining global and sequential feature modeling with interpretable decision-making.

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: A CNN was developed for early detection of Oral Cavity Squamous Cell Carcinoma (OCSCC) using image analysis, paired with hardware for image capture and processing to determine optimal resolution requirements.


<details>
  <summary>Details</summary>
Motivation: OCSCC often goes undetected due to subtle early stages and hidden development areas, leading to preventable deaths. CNNs with precise image segmentation could enable early detection.

Method: Trained a CNN on 4293 images (benign/malignant tumors, negative samples), tested on images altered to 5 common resolutions, and developed hardware for detailed image capture and processing.

Result: Higher resolution images led to higher-accuracy predictions on a logarithmic scale, showing diminishing returns with higher pixel counts. The system demonstrated effective OCSCC detection capabilities.

Conclusion: CNN-based systems with appropriate image resolution can effectively detect OCSCC, with hardware enhancement improving detection accuracy through better image quality.

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D is a large-scale multimodal dataset with 500 hours of 3D motion data from 439 participants, featuring single-person motions and multi-person interactions with comprehensive tracking and annotations.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive 3D motion dataset that captures diverse human behaviors including individual motions and social interactions for advancing research in human motion analysis and avatar technologies.

Method: Collected data from 439 participants in a multi-camera stage, capturing over 54 million frames of 3D motion tracking including hand gestures, body shape, and providing text annotations with separate audio tracks for each participant.

Result: Successfully created a dataset with 500 hours of multimodal 3D motion data covering prompted motions, hand gestures, locomotion, discussions, emotional conversations, collaborative activities, and co-living scenarios.

Conclusion: Embody 3D provides a valuable resource for research in human motion analysis, social interactions, and avatar technologies with its comprehensive multimodal data collection from diverse scenarios.

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: Proactive scene decomposition and reconstruction using human-object interactions to dynamically refine environment modeling in egocentric live streams, achieving accurate dynamic scene modeling with Gaussian splatting.


<details>
  <summary>Details</summary>
Motivation: Human behaviors are major causes of scene dynamics and contain rich cues, but conventional object-level reconstruction methods struggle with inherent ambiguities in static reconstruction.

Method: Online approach that leverages human-object interactions to iteratively disassemble and reconstruct environments, integrating camera/object pose estimation, instance decomposition, and online map updating using Gaussian splatting for photorealistic rendering.

Result: Achieves accurate and consistent dynamic scene modeling validated in multiple real-world scenarios with promising advantages over conventional methods.

Conclusion: The proposed system provides a flexible, progressive alternative to conventional object-level reconstruction by capitalizing on human-object interaction cues for dynamic scene understanding.

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus is a two-stage cascaded system for real-time video anomaly detection that combines lightweight filtering with VLM reasoning, achieving 57.68 fps and 97.2% accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational cost and unstable visual grounding of Vision-Language Models (VLMs) in video anomaly detection while enabling real-time deployment.

Method: Two-stage cascaded system with offline learning of normal behavioral rules, motion mask prompting to direct VLM attention, and rule-based deviation detection that identifies anomalies as deviations from learned norms.

Result: Achieves 57.68 fps on NVIDIA L40S GPU (151.79× speedup) and 97.2% accuracy comparable to state-of-the-art VLM-based methods across four datasets.

Conclusion: Cerberus establishes itself as a practical solution for real-time video analytics by efficiently combining lightweight filtering with fine-grained VLM reasoning.

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA is a new benchmark that reveals current membership inference attack methods perform no better than random chance when evaluated on unbiased datasets, challenging previous claims of high success rates.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental challenges in evaluating membership inference attacks against large vision-language models, where prior reported high success rates may stem from dataset distribution bias rather than true membership detection.

Method: Created a controlled benchmark with 6,000 images where member and non-member distributions are carefully balanced, with ground-truth membership labels across three distinct training stages.

Result: Experiments showed that state-of-the-art MIA methods converged to random chance performance under unbiased conditions, contradicting previous high success rate claims.

Conclusion: OpenLVLM-MIA provides a transparent and unbiased benchmark that clarifies current limitations of MIA research on LVLMs and establishes a foundation for developing stronger privacy-preserving techniques.

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch is a training-free framework that uses cross-image stroke attention to transfer stroke attributes from reference sketches to content images while preserving semantic structure.


<details>
  <summary>Details</summary>
Motivation: To generate sketches guided by reference styles by precisely transferring stroke attributes like line thickness, deformation, and texture sparsity while maintaining semantic structure and content fidelity.

Method: Proposes cross-image stroke attention mechanism embedded in self-attention layers to establish fine-grained semantic correspondences, plus adaptive contrast enhancement and semantic-focused attention for content preservation.

Result: Effectively synthesizes stylistically faithful sketches that closely resemble handcrafted results, outperforming existing methods in expressive stroke control and semantic coherence.

Conclusion: Stroke2Sketch enables accurate stroke attribute transfer while maintaining structural integrity, achieving superior performance in sketch generation compared to existing approaches.

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: This paper studies scaling laws for deepfake detection, finding that detection error follows power-law decay as training data scales up in domains and generation methods, similar to LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how deepfake detection performance scales with data quantity and diversity, since existing datasets are insufficient for such scaling studies.

Method: Constructed ScaleDF - the largest deepfake dataset with 5.8M real images from 51 domains and 8.8M fake images from 102 generation methods, then analyzed performance scaling patterns.

Result: Discovered power-law scaling relationships where detection error decreases predictably as number of real domains or deepfake methods increases, enabling performance forecasting.

Conclusion: Scaling laws apply to deepfake detection similar to LLMs, enabling data-centric approaches to counter evolving deepfake technology, though scaling has limitations.

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT enables efficient 4K text-to-image generation using hierarchical local attention with global guidance, achieving 2× faster inference and lower memory usage without requiring native 4K training data.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models are limited to sub-1K resolutions due to quadratic attention complexity and lack of native 4K training data, creating a need for efficient ultra-high-resolution generation that maintains both fine-grained detail and global coherence.

Method: Proposes hierarchical local attention with low-resolution global guidance: divides high-resolution latents into local windows for near-linear complexity, uses low-resolution latent with scaled positional anchors for global semantics, and employs LoRA adaptation to bridge global-local pathways. Implements Hilbert curve token reordering and fused-kernel optimization for GPU efficiency.

Result: Achieves 2× faster inference and lower memory usage compared to dense attention baselines, scales to 4K×4K resolution without additional training data, and delivers superior global coherence and sharper local detail on both quantitative (FID, IS, CLIP Score) and qualitative evaluations.

Conclusion: Hierarchical local attention with guided low-resolution anchors is an effective approach for advancing ultra-high-resolution image generation, offering a scalable solution that matches or outperforms methods requiring native 4K training.

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX is a cloud-edge collaborative framework that reduces diffusion model generation time by 15.8% compared to Stable Diffusion v1.5 while maintaining image quality, using a lightweight on-device model for previews and cloud model for final refinements.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models are computationally intensive and require iterative prompt refinement, which increases latency and burdens cloud resources.

Method: Proposes a cloud-edge collaborative framework with lightweight on-device diffusion model for rapid previews, high-capacity cloud model for final refinements, and a noise level predictor to dynamically balance computation load.

Result: Reduces average generation time by 15.8% compared to Stable Diffusion v1.5 while maintaining comparable image quality, and is only 0.9% slower than Tiny-SD with significantly improved image quality.

Conclusion: DiffusionX demonstrates efficiency and scalability with minimal overhead through cloud-edge collaboration and dynamic computation balancing.

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR is a token-level enhancement framework for autoregressive models that addresses reference identity confusion in multi-reference image generation through three mechanisms: token index embedding, instruct token injection, and identity-token disentanglement.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models struggle with decoupling different reference identities in multiple reference image generation, leading to identity confusion problems.

Method: Three token-level enhancement mechanisms: 1) Token Index Embedding for clustering tokens of same reference images, 2) Instruct Token Injection as extra visual feature container for detailed priors, 3) Identity-Token Disentanglement strategy to guide token representations toward independent identity features.

Result: The approach significantly enhances AR-based methods, achieving good identity consistency while preserving high-quality background reconstruction. Comprehensive experiments show it surpasses current state-of-the-art models in multiple reference image generation.

Conclusion: TokenAR framework effectively solves reference identity confusion in multi-reference generation and introduces the InstructAR Dataset, the first open-source large-scale multi-reference dataset for training and evaluation.

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: This paper challenges the assumption that MLLM performance mainly comes from the LLM backbone, showing that training strategies (especially RL vs SFT) fundamentally reshape vision encoder representations. RL produces stronger, more localized visual representations, leading to the proposed PIVOT method that builds strong vision encoders with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: There's a significant gap in understanding how vision encoders in MLLMs work, especially with the shift from SFT to RL training. The paper aims to analyze how different training strategies affect vision representations and MLLM performance.

Method: Conducted diverse experiments including ImageNet classification, segmentation, and gradient visualization to analyze vision encoder representations. Proposed PIVOT (Preference-Instructed Vision OpTimization) as a recipe for building strong vision encoders.

Result: RL training produces stronger and more precisely localized visual representations than SFT. PIVOT-trained vision encoders outperform larger counterparts while requiring less than 1% of standard vision pretraining computational cost.

Conclusion: Vision encoder training strategy fundamentally shapes MLLM performance, with RL being superior to SFT. PIVOT provides an efficient path for advancing MLLM vision backbones, challenging the dominant assumption that LLM backbones are the primary performance drivers.

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: Proposes GradNorm, a gradient-based framework for filtering positive nouns in Language-assisted Image Clustering (LaIC) that theoretically guarantees better separability than existing CLIP-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing filtering strategies for LaIC rely on CLIP feature space but lack theoretical foundation, creating a need for more rigorous approaches to identify semantically relevant nouns from unlabeled data.

Method: Uses gradient magnitude from back-propagation of cross-entropy between predicted target distribution and softmax output to measure noun positiveness, with theoretical guarantees of separability.

Result: Achieves state-of-the-art clustering performance across various benchmarks, demonstrating superior empirical performance over existing methods.

Conclusion: GradNorm provides a theoretically sound framework that subsumes existing filtering strategies as special cases and significantly improves image clustering through better noun filtering.

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: The paper introduces MIRAD, the first benchmark dataset for anomaly detection in social manufacturing, featuring diverse individualized products, data from six distributed manufacturing nodes, and substantial imaging heterogeneity. Evaluation shows significant performance drops in SOTA methods compared to conventional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Social manufacturing enables mass individualization but faces quality control challenges due to customized products, fragmented small-batch orders, and varying imaging environments across distributed sites. There's a scarcity of real-world datasets and tailored algorithms for this domain.

Method: Created the MIRAD dataset capturing three critical dimensions: diverse individualized products with large intra-class variation, data from six geographically dispersed manufacturing nodes, and substantial imaging heterogeneity. Conducted extensive evaluations of SOTA anomaly detection methods including one-class, multi-class, and zero-shot approaches.

Result: Results show a significant performance drop across all models compared with conventional benchmarks, highlighting the unresolved complexities of defect detection in real-world individualized production. The dataset is publicly available.

Conclusion: MIRAD bridges industrial requirements and academic research, providing a realistic foundation for developing robust quality control solutions essential for Industry 5.0. It demonstrates the need for more advanced anomaly detection methods tailored to social manufacturing challenges.

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: A large-scale dataset of 3,000 cataract surgery videos with comprehensive annotations for training surgical AI models, including temporal phases, instance segmentation, instrument-tissue interactions, and skill assessment.


<details>
  <summary>Details</summary>
Motivation: Current cataract surgery datasets lack diversity and annotation depth needed to train generalizable deep-learning models for computer-assisted surgery systems.

Method: Collected 3,000 phacoemulsification cataract surgery videos from two surgical centers with surgeons of varying experience levels, enriched with four annotation layers: temporal surgical phases, instance segmentation, instrument-tissue interaction tracking, and quantitative skill scores based on ICO-OSCAR rubrics.

Result: Benchmarking experiments demonstrated the dataset's technical quality for key surgical AI tasks including workflow recognition, scene segmentation, and automated skill assessment. Domain adaptation baseline was established for phase recognition across surgical centers.

Conclusion: The dataset addresses the gap in diverse, richly-annotated cataract surgery data and provides a valuable resource for developing generalizable surgical AI systems, with demonstrated utility across multiple surgical AI tasks.

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2 is an automated platform for real-time pothole detection using YOLO models, GPS geotagging, and OpenStreetMap visualization, with governance features for contractor accountability and public engagement.


<details>
  <summary>Details</summary>
Motivation: Road potholes pose significant safety hazards and maintenance challenges on India's diverse and under-maintained road networks, requiring automated monitoring solutions.

Method: Fine-tuned Ultralytics YOLO model on 7,000+ dashcam frames of Indian roads, synchronized OCR timestamps with GPS logs for geolocation, and built backend database with road segment attribution and contractor management.

Result: Developed a fully automated end-to-end platform with real-time detection, intelligent governance features, automated alerts, and actionable analytics for stakeholders.

Conclusion: iWatchRoadv2 enables data-driven smart city management, transparent governance, and sustainable improvements in road infrastructure maintenance through complete automation of the pothole monitoring lifecycle.

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter is a data-driven parametric model for plant morphology that encodes topology, shape, articulation, and deformation into a learned representation, addressing the lack of expressive plant modeling approaches compared to human/animal models.


<details>
  <summary>Details</summary>
Motivation: While powerful 3D parametric models exist for humans and animals, equally expressive approaches for modeling plants are lacking, despite their broad utility in 3D reconstruction, generation, understanding, and simulation.

Method: Demeter encodes key factors of plant morphology including topology, shape, articulation, and deformation into a compact learned representation. It handles varying shape topology across species and models three sources of shape variation: articulation, subcomponent shape variation, and non-rigid deformation.

Result: Experiments show that Demeter effectively synthesizes shapes, reconstructs structures, and simulates biophysical processes. The model was tested on a large-scale, ground-truthed soybean farm dataset.

Conclusion: Demeter advances crop plant modeling by providing an expressive parametric model that can handle the complex morphological variations in plants, with demonstrated effectiveness in shape synthesis, reconstruction, and simulation tasks.

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: A lightweight framework for AR/VR edge devices using encoder-decoder architecture with sparse convolution, SPLite decoder, and quantization-aware training achieves 42% efficiency improvement and 2.98x speed-up on Raspberry Pi 5 while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deploying deep learning models on AR/VR edge devices requiring real-time inference, low power consumption, and minimal latency while balancing efficiency and performance.

Method: Encoder-decoder architecture with sparse convolution on ResNet-18 backbone, SPLite decoder for faster processing, and quantization-aware training to reduce memory usage.

Result: 42% end-to-end efficiency improvement, 3.1x frame rate boost on Raspberry Pi 5, 2.98x overall speed-up, memory reduction with minimal accuracy loss (PA-MPJPE from 9.0mm to 9.1mm on FreiHAND).

Conclusion: The proposed framework achieves comparable accuracy to state-of-the-art methods while significantly enhancing computational efficiency for edge deployment in AR/VR applications.

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM is a MLLM-agent framework that enables reasoning-based 3D segmentation using 3D Gaussian Splatting representations with a Global-to-Local Spatial Grounding strategy to interpret complex human instructions.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between complex human instructions and precise 3D object grounding, as existing 3D segmentation methods struggle with ambiguous reasoning-based instructions while 2D vision-language models lack 3D spatial understanding.

Method: Uses 3D Gaussian Splatting representations to render photorealistic views for MLLM comprehension, with a Global-to-Local Spatial Grounding strategy: multiple global views for coarse localization followed by close-up novel views for fine-grained local segmentation.

Result: Achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and REALM3D benchmarks, and supports various 3D interaction tasks including object removal, replacement, and style transfer.

Conclusion: REALM demonstrates practical utility and versatility as an agent framework for 3D interaction tasks, bridging the gap between complex instructions and precise 3D object grounding without extensive 3D-specific post-training.

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL is a framework that uses self-supervised learning tasks as verifiable rewards for reinforcement learning fine-tuning of vision-language models, eliminating the need for human preference data while improving performance on vision-centric and vision-language reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often fail to adequately utilize visual evidence, relying instead on linguistic priors or textual shortcuts. While reinforcement learning could help align models, it has been hindered by the lack of scalable and reliable reward mechanisms.

Method: The proposed SSL4RL framework reformulates self-supervised learning objectives (such as predicting image rotation or reconstructing masked patches) into dense, automatic reward signals for RL-based fine-tuning.

Result: SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. The framework also shows generality by achieving significant gains when applied to graph learning.

Conclusion: SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives, offering new design principles for future work based on factors like task difficulty, model scale, and semantic alignment.

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick is a lightweight neural network that jointly matches points and line segments across images, achieving state-of-the-art performance with improved efficiency for real-time applications.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches treat point and line matching as separate tasks, while existing joint matching methods like GlueStick are computationally heavy and unsuitable for real-time or edge device deployment.

Method: Proposes LightGlueStick with a novel Attentional Line Message Passing (ALMP) component that explicitly exposes line connectivity to the network, enabling efficient communication between nodes for joint point and line matching.

Result: LightGlueStick establishes new state-of-the-art performance across different benchmarks while being lightweight enough for real-time applications and edge device deployment.

Conclusion: The proposed lightweight architecture successfully addresses the computational limitations of previous joint point-line matchers while achieving superior matching performance, making it suitable for practical applications like SLAM and Structure-from-Motion.

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: Proposes EDVD-LLaMA, an explainable deepfake video detection framework using multimodal LLM with spatio-temporal feature extraction and fine-grained reasoning mechanisms to provide traceable detection results.


<details>
  <summary>Details</summary>
Motivation: Traditional deepfake detection methods lack transparency and generalization capabilities, creating need for detectors that provide verifiable reasoning explanations alongside detection results.

Method: Uses Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract cross-frame deepfake features, and Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) with facial feature constraints for pixel-level localization and reliable reasoning.

Result: Achieves outstanding performance in detection accuracy, explainability, and robustness across cross-forgery methods and cross-dataset scenarios, outperforming previous DVD methods.

Conclusion: EDVD-LLaMA provides a more explainable and superior solution for deepfake video detection with traceable reasoning processes and trustworthy explanations.

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces RefAVA++ dataset and RefAtomNet++ framework for Referring Atomic Video Action Recognition (RAVAR), achieving state-of-the-art results through multi-hierarchical semantic-aligned cross-attention and multi-trajectory Mamba modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for action recognition lack precise language-guided understanding in complex multi-person scenarios, and previous RefAtomNet had limitations in cross-modal alignment and target person localization.

Method: RefAtomNet++ uses multi-hierarchical semantic-aligned cross-attention with multi-trajectory Mamba modeling at partial-keyword, scene-attribute, and holistic-sentence levels, with dynamic scanning trajectories for visual spatial tokens.

Result: RefAtomNet++ establishes new state-of-the-art results on the RefAVA++ dataset, which contains >2.9M frames and >75.1k annotated persons.

Conclusion: The proposed framework effectively addresses cross-modal alignment challenges in RAVAR and demonstrates superior performance for fine-grained, language-guided atomic action recognition in complex multi-person scenarios.

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: Improved loss function using Gaussian bounding box representation and Bhattacharyya distance for better rotated object detection accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection frameworks underperform with rotated objects due to limitations in capturing orientation variations, especially in applications like aerial imagery and autonomous driving.

Method: Proposed rotation-invariant loss function leveraging Gaussian bounding box representation and Bhattacharyya distance, with anisotropic Gaussian representation to address isotropic variance issues in square-like objects.

Result: Significant improvements in mean Average Precision metrics compared to existing methods when integrated into state-of-the-art deep learning detectors.

Conclusion: The approach establishes new benchmarks in rotated object detection with broad applications requiring precise object localization regardless of orientation.

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN is a visual prompt initialization strategy that enhances adaptation of self-supervised models by aligning prompts with informative regions and injecting new representational directions, achieving state-of-the-art performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing visual prompt tuning methods often fail to specialize prompts or enrich representation space, especially with self-supervised backbones, which becomes problematic in challenging tasks and data-scarce settings where effective adaptation is critical.

Method: VIPAMIN enhances adaptation by (1) aligning prompts with semantically informative regions in the embedding space, and (2) injecting novel representational directions beyond the pretrained subspace, requiring only a single forward pass and lightweight operations.

Result: VIPAMIN consistently improves performance across diverse tasks and dataset sizes, setting a new state of the art in visual prompt tuning.

Conclusion: VIPAMIN provides an effective and efficient visual prompt initialization strategy that significantly enhances adaptation of self-supervised models while maintaining computational efficiency.

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: The paper proposes a weakly supervised domain adaptation method for mitochondria segmentation in EM images using sparse point annotations, featuring multitask learning with cross-teaching and instance-aware pseudo-label selection.


<details>
  <summary>Details</summary>
Motivation: Mitochondria segmentation from EM images is valuable for biological research but annotation is costly. While UDA methods help with domain shifts, they have low performance. Weak supervision with sparse point labels can reduce annotation effort while improving performance.

Method: Multitask learning framework combining segmentation and center detection with cross-teaching mechanism and class-focused cross-domain contrastive learning. Uses segmentation self-training with instance-aware pseudo-label selection that semantically selects reliable pseudo-labels using detection task.

Result: Method outperforms existing UDA and WDA methods, significantly narrowing performance gap with supervised upper bound. Also achieves substantial improvements over other UDA techniques under UDA setting.

Conclusion: The proposed weakly supervised approach effectively leverages sparse point annotations to achieve high-performance mitochondria segmentation across domains, reducing annotation costs while maintaining competitive performance.

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: Proposes a foresighted VLN agent using Q-learning to predict future outcomes, combining task-agnostic Q-features with navigation instructions for improved goal-oriented navigation.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods focus on historical information while ignoring future implications of actions, limiting long-term planning capabilities.

Method: Trains Q-model with unlabeled trajectory data to generate Q-features for candidate actions, integrates with navigation instructions via cross-modal encoder, and uses A*-style search combining future and historical scores.

Result: Extensive experiments on goal-oriented VLN datasets validate the method's effectiveness in improving navigation performance.

Conclusion: The proposed foresighted agent successfully incorporates future-aware planning into VLN, demonstrating significant improvements over history-only approaches.

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar is a hierarchical Gaussian compression framework for efficient transmission and high-quality rendering of dynamic 3D avatars, achieving superior compression efficiency and visual quality compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting compression methods for digital humans lack human priors, leading to suboptimal bitrate efficiency and reconstruction quality that hinders streamable 3D avatar systems.

Method: Disentangles Gaussian representation into structural layer (StyleUNet-based generator mapping poses to Gaussians) and motion layer (SMPL-X model for compact pose variations), with facial attention mechanism for identity preservation under low-bitrate constraints.

Result: Significantly outperforms prior methods in both visual quality and compression efficiency, providing a streamable solution for rapid 3D avatar rendering.

Conclusion: HGC-Avatar enables efficient transmission and high-quality rendering of dynamic avatars with layer-wise compression, progressive decoding, and controllable rendering from diverse inputs like video or text.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench is the first benchmark for evaluating Large Multimodal Models' ability to detect and resolve real inconsistencies across text, figures, tables, and equations in scientific papers, revealing poor performance (26.1-54.2%) among 21 leading models.


<details>
  <summary>Details</summary>
Motivation: Current LMM benchmarks fail to capture real-world multimodal complexity in scientific papers, particularly the subtle inconsistencies across different modalities that undermine clarity, reproducibility, and trust in scientific research.

Method: Created PRISMM-Bench through a multi-stage pipeline: mined real reviewer-flagged inconsistencies from papers, used LLM-assisted filtering and human verification to curate 262 inconsistencies from 242 papers, designed three tasks (inconsistency identification, remedy, and pair matching), and introduced structured JSON-based answer representations to minimize linguistic biases.

Result: Benchmarking 21 leading LMMs (including GLM-4.5V 106B, InternVL3 78B, Gemini 2.5 Pro, GPT-5) revealed strikingly low performance ranging from 26.1% to 54.2%, demonstrating the challenge of multimodal scientific reasoning.

Conclusion: Current LMMs struggle significantly with detecting and resolving real multimodal inconsistencies in scientific papers, highlighting the need for progress towards more trustworthy scientific assistants capable of reliable multimodal reasoning.

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: OOS-DSD: A novel deep learning method that enhances out-of-stock detection using auxiliary learning with YOLOv8, adding product segmentation and depth estimation branches, achieving 1.8% mAP improvement over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Out-of-stock detection is crucial for retail verification to identify unavailable products on shelves, but existing methods need improvement in accuracy and robustness.

Method: Extended YOLOv8 with additional convolutional branches for OOS detection, product segmentation, and depth estimation. Depth branch uses pseudo-labels from Depth Anything V2 with proposed depth normalization for training stability.

Result: Achieved 1.8% mAP improvement over SOTA OOS detection methods. Ablation studies showed auxiliary learning increased mAP by 3.7% and depth normalization by 4.2%.

Conclusion: Auxiliary learning with depth estimation and proper normalization significantly improves OOS detection performance, demonstrating the effectiveness of multi-task learning in retail applications.

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: Proposes a representative-centric image categorization and retrieval method using graph attention network (GAT)-based autoencoders to create context-aware latent representations and category representatives for efficient image comparison.


<details>
  <summary>Details</summary>
Motivation: To develop an effective image categorization and retrieval approach that leverages graph structures and attention mechanisms to capture important features and relationships between images, moving beyond standard feature-based methods.

Method: Uses a graph where nodes represent images/representatives and edges capture similarity relationships. GAT-based autoencoder constructs context-aware latent representations, then obtains category representatives from embeddings. Categorizes query images by comparing their representatives to category representatives, and retrieves most similar images within identified categories.

Result: Demonstrates effectiveness through experiments comparing GAT autoencoders with standard feature-based techniques, showing improved performance in representative-centric image categorization and retrieval.

Conclusion: The representative-centric approach using GAT-based autoencoders provides an effective framework for image categorization and retrieval by leveraging graph structures and attention mechanisms to create meaningful representations and comparisons.

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: Vision-only autonomous flight system for small UAVs using semantic segmentation and monocular depth estimation for obstacle avoidance and autonomous landing without GPS or LiDAR.


<details>
  <summary>Details</summary>
Motivation: Enable autonomous drone navigation in indoor environments without expensive sensors like LiDAR or GPS dependency, addressing computational efficiency challenges for resource-constrained platforms.

Method: Combines semantic segmentation with monocular depth estimation using knowledge distillation (SVM teacher to U-Net student), adaptive scale factor algorithm for metric depth conversion, and end-to-end learning for flight policies.

Result: Achieved 14.4 cm mean distance error, 100% success rate in validation tests, 87.5% autonomous mission success rate with end-to-end learning, increased surveillance distance and reduced mission time.

Conclusion: Advances practical vision-based drone navigation in structured environments by solving metric depth estimation and computational efficiency challenges for deployment on resource-constrained platforms.

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [51] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ is a fine-tuning method that enhances compositional reasoning in vision-language models by adding token-level reconstruction and sentence-level alignment objectives to contrastive learning, achieving state-of-the-art performance on compositional reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Vision-language models trained with standard contrastive objectives struggle with compositional reasoning because text encoders focus on individual words rather than their relations, and contrastive training primarily aligns words with visual objects.

Method: READ adds two auxiliary objectives to contrastive learning: (1) token-level reconstruction using a frozen pre-trained decoder to reconstruct alternative captions from original caption embeddings, and (2) sentence-level alignment that explicitly aligns paraphrased sentences in embedding space.

Result: READ-CLIP achieves state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest conventional fine-tuning baseline by up to 4.1%. Applying READ to existing CLIP variants (NegCLIP and FSC-CLIP) also improves performance.

Conclusion: The reconstruction and alignment objectives offer complementary benefits: reconstruction encourages capturing relationships between words within captions, while alignment ensures consistent representations for paraphrases with different wording.

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [52] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: Using Retrieval Augmented Generation with Cypher query language to scale LLM-based natural language grounding with 3D scene graphs, overcoming limitations of direct context window encoding.


<details>
  <summary>Details</summary>
Motivation: Existing methods encode entire 3D scene graphs as text in LLM context windows, which doesn't scale to large or rich graphs. Need better approach for connecting natural language to robot's world representations.

Method: Propose using Retrieval Augmented Generation with graph database encoding of 3DSG. Provide Cypher query language as tool for LLM to retrieve relevant graph data for language grounding tasks.

Result: Cypher interface scales significantly better to large graphs on both local and cloud models. Large performance improvements in grounded language tasks while substantially reducing token count of scene graph content.

Conclusion: Using Cypher as interface to 3D scene graphs is effective for scalable natural language grounding, outperforming baseline context window and code generation methods.

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [53] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: GaitRDAE is a novel gait recognition framework that dynamically adapts to motion regions with varying temporal scales, achieving state-of-the-art performance by automatically searching for optimal temporal receptive fields and emphasizing stable behavior patterns.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods use predefined regions with fixed temporal scales, making it difficult to model dynamically changing motion regions and adapt to their specific patterns, especially when covariates affect visual appearance.

Method: The Region-aware Dynamic Aggregation and Excitation framework (GaitRDAE) includes two modules: RDA dynamically searches optimal temporal receptive fields for each region, and RDE emphasizes learning of motion regions with stable behavior patterns while suppressing attention to static regions affected by covariates.

Result: Experimental results show that GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

Conclusion: The proposed framework successfully addresses the limitations of existing methods by dynamically adapting to motion regions with varying temporal scales and focusing on stable behavior patterns, leading to improved gait recognition accuracy.

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [54] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: This paper introduces the first systematic benchmark using real-world political deepfakes from social media, revealing that current deepfake detectors perform poorly on authentic political content and are vulnerable to simple manipulations.


<details>
  <summary>Details</summary>
Motivation: The proliferation of AI-generated content, especially political deepfakes, poses serious misinformation risks, but existing detection models are trained on synthetic datasets and lack generalizability to real-world political deepfakes circulating on social media.

Method: The study uses the Political Deepfakes Incident Database - a curated collection of real-world political deepfakes from social media since 2018 - to systematically evaluate state-of-the-art deepfake detectors from academia, government, and industry.

Result: Academic and government detectors perform poorly, while paid tools achieve relatively higher performance. However, all detectors struggle to generalize to authentic political deepfakes and are vulnerable to simple manipulations, especially in video content.

Conclusion: There is an urgent need for politically contextualized deepfake detection frameworks to better protect the public from real-world political deepfake threats.

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [55] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive open-source lychee dataset for fruit detection and maturity classification, addressing the lack of annotated data for vision-based harvesting robots in subtropical fruit cultivation.


<details>
  <summary>Details</summary>
Motivation: There are currently no consistently and comprehensively annotated open-source lychee datasets featuring fruits in natural growing environments, which are essential for developing vision-based harvesting robots to improve productivity and reduce labor reliance.

Method: Constructed a dataset with 11,414 images (878 raw RGB, 8,780 augmented RGB, and 1,756 depth images) acquired under diverse weather conditions and times across multiple lychee varieties. Images were annotated with 9,658 label pairs for detection and classification, with three independent labelers and a fourth reviewer for consistency verification.

Result: The dataset encompasses three ripeness stages and includes detailed statistical analyses. Experiments were performed using three representative deep learning models to evaluate the dataset's effectiveness.

Conclusion: The dataset is publicly available for academic use and provides a valuable resource for developing lychee harvesting robots through improved vision-based detection and maturity classification capabilities.

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [56] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD is a training-free framework that addresses object hallucination in Large Vision-Language Models by targeting visual encoder issues through three strategies: re-weighting visual tokens, introducing noise-derived tokens, and applying adversarial attacks with contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: Object hallucination in LVLMs, where models generate plausible but inaccurate object descriptions, remains a significant challenge. Unlike previous work focusing on LLM components, this paper traces hallucinations to visual encoder issues including statistical bias, inherent bias, and vulnerability.

Method: Proposes SHIELD framework with three training-free strategies: 1) re-weighting visual tokens to reduce statistical bias, 2) introducing noise-derived tokens to counter inherent bias, and 3) applying adversarial attacks with contrastive decoding to address vulnerability.

Result: SHIELD effectively mitigates object hallucinations across diverse benchmarks and LVLM families, while also achieving strong performance on general LVLM benchmarks, demonstrating broad applicability.

Conclusion: SHIELD provides an effective training-free solution for reducing object hallucinations in LVLMs by addressing visual encoder issues, showing promising results across multiple benchmarks and model families.

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [57] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H is a multi-task learning framework for real-time spatial perception that performs semantic segmentation, depth, edge, and surface normal estimation from monocular images using a Window-Based Cross-Task Attention Module and lightweight ViT-based backbone.


<details>
  <summary>Details</summary>
Motivation: Need for efficient multi-task models that leverage complementary task information while minimizing computational overhead for real-time spatial perception deployment on edge devices.

Method: Uses Window-Based Cross-Task Attention Module for structured feature exchange while preserving task-specific details, built on lightweight ViT-based DINOv2 backbone optimized for real-time deployment.

Result: Outperforms state-of-the-art multi-task models on NYUDv2, surpasses single-task depth and semantic baselines on Hypersim, achieves superior performance on Cityscapes while maintaining computational efficiency on laptop hardware.

Conclusion: M2H demonstrates practical effectiveness in spatial perception tasks and serves as foundation for monocular spatial perception systems supporting 3D scene graph construction in dynamic environments.

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [58] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector is a lightweight plug-and-play framework for multimodal LLMs that reformulates token compression as an end-to-end learnable decision process, achieving superior performance across various compression rates while preserving critical information.


<details>
  <summary>Details</summary>
Motivation: MLLMs face computational and memory bottlenecks from massive visual tokens in high-resolution images or multi-image inputs. Previous token compression methods are constrained by heuristic rules that risk discarding critical information and suffer from biases like attention sinks.

Method: Proposes VisionSelector - a scorer module decoupled from MLLM backbone with differentiable Top-K mechanism and curriculum annealing strategy to bridge training-inference gap, enabling efficient adaptive token selection at arbitrary compression rates.

Result: Achieves 100% accuracy on MME with 30% retention budget, outperforms prior methods by 12.14% at 10% retention budget, doubles prefill speed, and demonstrates generalization across various compression rates with only 12.85M trainable parameters.

Conclusion: VisionSelector provides an efficient and adaptive token compression framework that preserves critical information while significantly reducing computational overhead, making it suitable for various compression budgets in multimodal LLM applications.

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [59] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: A deep learning framework for real-time medical image analysis that integrates U-Net, EfficientNet, and Transformer models with optimization techniques to achieve high accuracy and fast inference across X-ray, CT, and MRI modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional medical image interpretation is time-consuming, variable among clinicians, and lacks the precision and speed needed for real-time clinical use. Current image processing techniques are insufficient for practical healthcare applications.

Method: Integrates U-Net, EfficientNet, and Transformer-based neural networks with real-time optimization strategies including model pruning, quantization, and GPU acceleration. Supports flexible deployment on edge devices, local servers, and cloud infrastructures with PACS and EHR interoperability.

Result: Achieved state-of-the-art performance with classification accuracies >92%, segmentation Dice scores >91%, and inference times <80 milliseconds on public benchmark datasets. Visual explanation tools (Grad-CAM, segmentation overlays) enhanced transparency.

Conclusion: The framework can substantially accelerate diagnostic workflows, reduce clinician workload, and support trustworthy AI integration in time-critical healthcare environments.

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [60] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse is a new multi-turn conversation benchmark for Vision-and-Language Models (VLMs) featuring 647 dialogues across 484 tasks, showing that even top models like GPT-4o achieve only 50% success rate in complex conversations.


<details>
  <summary>Details</summary>
Motivation: Existing multi-turn datasets only partially capture real-world conversational scenarios, while real applications demand more intricate multi-turn dialogues.

Method: Created MultiVerse benchmark with 647 dialogues from 12 VLM evaluation benchmarks, using checklist-based evaluation with GPT-4o as automated evaluator across 37 key aspects.

Result: Evaluation of 18 VLMs revealed that even the strongest models achieve only 50% success rate in complex multi-turn conversations, and providing full dialogue context significantly enhances performance for smaller/weaker models.

Conclusion: MultiVerse serves as a comprehensive benchmark for evaluating multi-turn interaction abilities in VLMs, highlighting the importance of in-context learning and the challenging nature of complex conversational scenarios.

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [61] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: UTAP introduces universal adversarial perturbations that can systematically degrade multiple pathology foundation models' performance across diverse datasets and models with imperceptible noise patterns.


<details>
  <summary>Details</summary>
Motivation: To reveal critical vulnerabilities in pathology foundation models and establish high-standard benchmarks for robustness evaluation, highlighting the need for better defense mechanisms in AI pathology applications.

Method: Optimized deep learning approach to create fixed, weak noise patterns that when added to pathology images disrupt feature representations across multiple foundation models.

Result: UTAP causes significant performance drops across various state-of-the-art pathology foundation models on multiple datasets, demonstrating universality across field-of-views and transferability to unseen black-box models.

Conclusion: UTAP represents a broad threat to pathology foundation models and their applications, establishing critical benchmarks for robustness evaluation and highlighting the urgent need for improved defense mechanisms and adversarial training.

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [62] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: HYDRA introduces a hybrid knowledge distillation approach for spectral reconstruction that achieves state-of-the-art performance with 18% accuracy improvement and faster inference times.


<details>
  <summary>Details</summary>
Motivation: Previous multi-scale attention methods only work well for sparse spectra, while modern hyperspectral sensors have hundreds of channels, creating a need for more generalizable spectral reconstruction methods.

Method: Uses a Teacher model that encapsulates latent hyperspectral data and a Student model that learns mappings from natural images to the Teacher's encoded domain, with a novel training method.

Result: Achieves SOTA performance across all metrics with 18% boost in accuracy and faster inference times than current SOTA models at various channel depths.

Conclusion: HYDRA successfully addresses key limitations of prior spectral reconstruction models and provides high-quality reconstruction for modern hyperspectral imaging applications.

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [63] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR is a dual-agent framework that improves spatial reasoning in VLMs by first extracting sufficient 3D information using expert models and then iteratively refining it to achieve minimality, achieving state-of-the-art performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with spatial reasoning due to inadequate 3D understanding from 2D-centric pre-training and reasoning failures caused by redundant 3D information.

Method: A dual-agent framework with: 1) Perception Agent that programmatically queries 3D scenes using expert models including a novel SOG module for language-grounded directions, and 2) Reasoning Agent that iteratively refines information to achieve minimal sufficiency.

Result: Significantly improves accuracy and achieves state-of-the-art performance across two challenging benchmarks, while producing interpretable reasoning paths.

Conclusion: Explicitly pursuing both sufficiency and minimality in 3D information processing enables robust spatial reasoning and provides high-quality training data for future models.

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [64] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: SDPA++ is a self-supervised denoising framework for OCT images that uses only noisy images to generate pseudo-ground-truth through self-fusion and patch aggregation, achieving improved image quality without requiring clean reference images.


<details>
  <summary>Details</summary>
Motivation: OCT imaging is crucial for retinal disease diagnosis but suffers from inherent speckle noise. Acquiring paired clean/noisy datasets for supervised denoising is challenging due to clinical constraints, necessitating self-supervised approaches.

Method: Proposes SDPA++ framework that: 1) generates pseudo-ground-truth images via self-fusion and self-supervised denoising, 2) uses these as targets to train ensemble denoising models, 3) employs patch-based strategy for enhanced clarity.

Result: Validated on IEEE SPS VIP Cup dataset (real-world noisy OCT images without clean references), showing improvements in CNR, MSR, TP, and EP metrics, demonstrating effective denoising performance.

Conclusion: SDPA++ provides a practical solution for OCT image denoising in clinical settings where clean reference images are unavailable, potentially improving diagnostic outcomes through enhanced image quality.

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [65] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: Domain-connecting contrastive learning (DCCL) improves domain generalization by enhancing intra-class connectivity across domains through aggressive data augmentation, cross-domain positive samples, model anchoring, and generative transformation loss.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts between training and testing samples hinder model generalization. While contrastive learning should theoretically help domain generalization by learning class-separated representations, direct application actually deteriorates performance due to lack of intra-class connectivity across domains.

Method: Proposed DCCL paradigm with data-side improvements (aggressive data augmentation, cross-domain positive samples) and model-side techniques (model anchoring to exploit pre-trained representations, generative transformation loss) to enhance intra-class connectivity across domains.

Result: Extensive experiments on five standard DG benchmarks show DCCL outperforms state-of-the-art baselines even without domain supervision.

Conclusion: DCCL effectively addresses the intra-class connectivity deficiency in domain generalization settings and provides generalizable representations that work well across unseen test domains.

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [66] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM is a one-step human motion prediction framework using consistency models that achieves comparable accuracy to diffusion models while being significantly faster.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency of multi-step denoising in diffusion-based human motion prediction methods by developing a single-step generation approach.

Method: Uses consistency models to learn self-consistent mapping between noisy and clean motion states, with Transformer-based spatiotemporal architecture and temporal embeddings for long-range dependencies.

Result: Achieves comparable or superior accuracy to state-of-the-art diffusion models on Human3.6M and HumanEva-I datasets while reducing inference steps by up to two orders of magnitude.

Conclusion: HumanCM provides an efficient alternative to diffusion models for human motion prediction, maintaining high accuracy with dramatically reduced computational cost.

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [67] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: This paper introduces SCENECOT, a novel framework for grounded Chain-of-Thought reasoning in 3D scenes, addressing the gap in human-like scene-object grounded reasoning for 3D Large Language Models.


<details>
  <summary>Details</summary>
Motivation: Existing 3D LLMs struggle with grounded question-answering due to under-exploration of human-like scene-object grounded reasoning mechanisms.

Method: Proposes SCENECOT framework with grounded Chain-of-Thought reasoning that decouples complex tasks into simpler problems and builds visual clues using multimodal expert modules. Also creates SCENECOT-185K, the first large-scale grounded CoT reasoning dataset.

Result: Extensive experiments show strong performance across various 3D scene reasoning benchmarks with high grounding-QA coherence.

Conclusion: This is the first successful application of CoT reasoning to 3D scene understanding, enabling step-by-step human-like reasoning with potential for broader 3D scene understanding applications.

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [68] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM is an Implicit Residual World Model that focuses on predicting residual changes in dynamic scenes rather than fully reconstructing future scenes, improving efficiency and accuracy in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Current vision-centric world models inefficiently reconstruct entire future scenes, including static backgrounds, wasting computational capacity. The goal is to focus modeling efforts on dynamic changes in the environment.

Method: IR-WM creates BEV representations of current state, uses previous BEV features as temporal prior, predicts only residual changes conditioned on ego-vehicle actions, and applies alignment modules to correct semantic/dynamic misalignments over time.

Result: Achieves top performance on nuScenes benchmark for both 4D occupancy forecasting and trajectory planning, with implicit future states substantially improving planning accuracy.

Conclusion: The residual modeling approach effectively focuses computational resources on dynamic scene elements, leading to superior performance in autonomous driving tasks while reducing redundancy in static background reconstruction.

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [69] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer is a semantic segmentation model that achieves high-precision coral reef mapping using noisy supervision from Allen Coral Atlas, outperforming conventional methods and producing more accurate predictions than the training labels.


<details>
  <summary>Details</summary>
Motivation: Coral reefs need accurate large-scale mapping for conservation, but existing global products like Allen Coral Atlas have limited spatial precision and semantic consistency, especially for fine-grained boundary delineation.

Method: UKANFormer builds on UKAN architecture with a Global-Local Transformer (GL-Trans) block in the decoder to extract both global semantic structures and local boundary details, enabling high-precision mapping under noisy supervision.

Result: Achieved coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming conventional baselines under the same noisy labels setting. The model produces predictions that are visually and structurally more accurate than the noisy training labels.

Conclusion: Architectural design can mitigate label noise and support scalable mapping under imperfect supervision, challenging the notion that data quality directly limits model performance. UKANFormer provides a foundation for ecological monitoring where reliable labels are scarce.

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [70] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: This survey provides a unified framework for world models in embodied AI, proposing a three-axis taxonomy covering functionality, temporal modeling, and spatial representation, while systematizing data resources, metrics, and identifying key challenges.


<details>
  <summary>Details</summary>
Motivation: Embodied AI requires agents that can perceive, act, and anticipate how actions reshape future world states, with world models serving as internal simulators for environment dynamics to support perception, prediction, and decision making.

Method: The authors formalize the problem setting and learning objectives, then propose a three-axis taxonomy: (1) Functionality (Decision-Coupled vs. General-Purpose), (2) Temporal Modeling (Sequential Simulation vs. Global Difference Prediction), (3) Spatial Representation (Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, Decomposed Rendering).

Result: The survey systematizes data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance, and provides quantitative comparison of state-of-the-art models.

Conclusion: Key open challenges include scarcity of unified datasets, need for evaluation metrics that assess physical consistency over pixel fidelity, trade-off between model performance and computational efficiency for real-time control, and achieving long-horizon temporal consistency while mitigating error accumulation.

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [71] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: Beam search significantly improves text-to-image generation in autoregressive models, enabling a 2B parameter model to outperform a 12B parameter diffusion model, with advantages stemming from discrete token spaces and early pruning capabilities.


<details>
  <summary>Details</summary>
Motivation: To translate the inference-time scaling benefits seen in Large Language Models to image generation, as previous attempts with diffusion models showed limited success.

Method: Applied beam search to discrete, sequential visual autoregressive models, leveraging the discrete token space for early pruning and computational reuse.

Result: Beam search substantially improved text-to-image generation, with a 2B parameter autoregressive model outperforming a 12B parameter diffusion model across benchmarks.

Conclusion: Model architecture (discrete token spaces in autoregressive models) is critical for inference-time optimization in visual generation, not just model scale.

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [72] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: This paper introduces a dataset of SR artifacts with human-rated prominence scores and trains a model to detect prominent artifacts, moving beyond binary artifact detection.


<details>
  <summary>Details</summary>
Motivation: Current SR models produce varying levels of artifacts that impact perceived quality differently, but existing methods treat all artifacts uniformly rather than considering their perceptual prominence to human observers.

Method: Created a dataset of 1302 artifact examples from 11 SR methods with crowdsourced prominence scores, then trained a lightweight regressor to produce spatial prominence heatmaps.

Result: The trained regressor outperforms existing methods at detecting prominent artifacts and produces spatial prominence heatmaps.

Conclusion: The work enables prominence-aware evaluation and mitigation of SR artifacts, with the dataset and code released to facilitate further research.

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [73] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR is a CNN-based image restoration framework that uses wavelet transforms and Mamba-based modules to improve texture detail reconstruction through large receptive fields and channel-aware modeling.


<details>
  <summary>Details</summary>
Motivation: Previous CNN-based image restoration methods struggle with restoring fine texture details due to limited receptive fields and lack of channel feature modeling.

Method: Proposes Global Multiscale Wavelet Transform Convolutions (GMWTConvs) to expand receptive field, Mamba-Based Channel-Aware Module (MCAM) for long-range dependencies, and Multiscale Texture Enhancement Loss (MTELoss) for texture preservation.

Result: Extensive experiments show WaMaIR outperforms state-of-the-art methods in image restoration with better computational efficiency.

Conclusion: The proposed framework effectively addresses texture detail restoration challenges in image restoration tasks through innovative wavelet transforms and channel-aware modeling.

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [74] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: Region in Context is a novel framework for text-conditioned image editing that performs multilevel semantic alignment between vision and language to enable precise and harmonized changes by understanding regions in relation to the whole scene.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat image regions in isolation without accounting for how each part contributes to the overall visual and semantic composition, leading to inconsistent edits, unnatural transitions, and loss of coherence across the image.

Method: Introduces a dual-level guidance mechanism: regions are represented with full-image context and aligned with detailed region-level descriptions, while the entire image is simultaneously matched to a comprehensive scene-level description generated by a large vision-language model.

Result: Experiments show that it produces more coherent and instruction-aligned results compared to existing methods.

Conclusion: The proposed framework enables precise and harmonized image edits by performing multilevel semantic alignment between vision and language, addressing the limitations of isolated region processing in current text-conditioned image editing approaches.

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [75] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: EMRRG is a novel X-ray medical report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods like Partial LoRA, achieving strong performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing medical report generation models rely heavily on LLMs with limited exploration of pre-trained vision foundation models, advanced fine-tuning techniques, and non-Transformer architectures like Mamba networks.

Method: X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction using Partial LoRA. An LLM with hybrid decoder generates reports in end-to-end training.

Result: Extensive experiments on three benchmark datasets fully validated the effectiveness of the proposed strategies for X-ray medical report generation.

Conclusion: The proposed EMRRG framework demonstrates the potential of fine-tuning Mamba networks with parameter-efficient methods for medical report generation, achieving strong results and opening new research directions.

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [76] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE is a novel 6D object pose estimation method that uses Bundle Adjustment principles and Lie algebra to create a pose-differentiable rendering pipeline with 3D Gaussian Splatting, achieving improved accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation methods struggle with textureless objects and varying illumination conditions when establishing 2D-3D correspondences, limiting their robustness in real-world scenarios.

Method: Proposes GS2POSE that formulates pose regression using Bundle Adjustment principles, leverages Lie algebra to extend 3DGS capabilities for pose-differentiable rendering, and iteratively optimizes pose by comparing input and rendered images while updating color parameters for illumination adaptation.

Result: Achieves accuracy improvements of 1.4% on T-LESS, 2.8% on LineMod-Occlusion, and 2.5% on LineMod datasets compared to previous models.

Conclusion: GS2POSE effectively addresses limitations of textureless objects and illumination variations through its pose-differentiable rendering approach and color parameter updates, demonstrating superior performance on standard benchmarks.

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [77] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: A training-free framework for video understanding that combines pre-trained VLMs with machine learning algorithms to reframe video analysis as spatio-temporal clustering in semantic feature space.


<details>
  <summary>Details</summary>
Motivation: Current video understanding models require extensive task-specific training on annotated datasets, which is costly and lacks scalability. There's a need to translate the zero-shot reasoning capabilities of VLMs from static images to videos without additional training.

Method: Transform video into semantic feature trajectories using frozen VLM visual encoder, apply Kernel Temporal Segmentation (KTS) to partition into event segments, perform unsupervised density-based clustering to identify recurring scenes, and generate structured summaries using VLM's generative capabilities.

Result: The framework automatically produces structured, multi-modal video summaries without requiring training, providing interpretable analysis of video content structure.

Conclusion: This approach offers an effective, interpretable, and model-agnostic pathway for zero-shot automated structural analysis of video content, bypassing the need for costly end-to-end training.

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [78] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS is a plug-and-play method that enables Multimodal Large Language Models (MLLMs) to perform pixel-level segmentation without finetuning, preserving the model's generalization capabilities while achieving competitive segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for adding segmentation to MLLMs require finetuning, which alters the model's output space and compromises its intrinsic generalization, undermining the goal of building unified models.

Method: LENS attaches a lightweight, trainable head to a frozen MLLM, refining spatial cues from attention maps to extract keypoints and generate point-wise features compatible with mask decoders.

Result: LENS achieves segmentation performance competitive with or superior to retraining-based methods while fully preserving the MLLM's generalization capabilities.

Conclusion: The attachable design of LENS establishes an efficient paradigm for extending MLLMs without compromising their core capabilities, paving the way for truly multi-talented unified models.

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [79] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: Unsupervised road segmentation using geometric priors and temporal consistency, achieving 0.82 IoU on Cityscapes without manual labels.


<details>
  <summary>Details</summary>
Motivation: Eliminate reliance on costly manually labeled datasets for road segmentation in autonomous driving by developing a fully unsupervised approach.

Method: Uses geometric priors to generate weak labels (pixels above horizon as non-road, quadrilateral in front as road), then refines with temporal consistency via feature tracking and mutual information maximization.

Result: Achieves 0.82 Intersection-over-Union on Cityscapes dataset, demonstrating high accuracy with simple design.

Conclusion: Combining geometric constraints and temporal consistency enables scalable unsupervised road segmentation for autonomous driving applications.

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [80] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: PIF is a Personalized Image Filter that uses a pretrained text-to-image diffusion model to learn and transfer photographic styles from reference images through textual inversion, enabling effective style extraction and transfer while preserving content.


<details>
  <summary>Details</summary>
Motivation: Learning and transferring photographic style requires understanding how photos are edited from their original appearance. Previous methods either fail to learn meaningful photographic concepts from references or cannot preserve content fidelity.

Method: Based on a pretrained text-to-image diffusion model, PIF learns the average appearance of photographic concepts and adjusts them using text prompts. It employs textual inversion technique to optimize prompts for photographic concepts from reference images.

Result: PIF demonstrates outstanding performance in extracting and transferring various kinds of photographic styles.

Conclusion: The proposed PIF framework effectively addresses the challenges of photographic style learning and transfer by leveraging diffusion models and textual inversion techniques.

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [81] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet is a large public coral reef image dataset with fine-grained genus-level annotations mapped to WoRMS, providing benchmarks for domain generalization and fine-grained coral classification.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are declining rapidly due to climate change, creating an urgent need for scalable, automated monitoring systems that can handle fine-grained classification across diverse geographical domains.

Method: Aggregated imagery from 76 CoralNet sources and one Red Sea site, totaling ~925,000 genus-level hard coral annotations with expert-verified labels mapped to WoRMS. Proposed two evaluation settings: within-source partitioning and cross-source domain generalization testing.

Result: Supervised within-source performance is promising but drops sharply across domains. Zero-shot models perform poorly overall, especially for rare and visually similar genera, highlighting the challenge of domain generalization in coral classification.

Conclusion: ReefNet provides a challenging benchmark to catalyze advances in domain generalization and fine-grained coral classification, with released dataset, code, and models to advance robust, domain-adaptive coral reef monitoring.

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [82] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: The paper proposes AdaptMoist, a domain adaptation method that uses texture features from wood chip images to accurately predict moisture content across different wood sources, achieving 80% accuracy compared to 57% for non-adapted models.


<details>
  <summary>Details</summary>
Motivation: Current moisture prediction methods for wood chips are either slow/destructive (oven drying) or inaccurate when dealing with wood from various sources due to data distribution shifts. There's a need for robust approaches that handle source variability.

Method: Comprehensive analysis of five texture feature types from wood chip images, combined feature sets, and a domain adaptation method (AdaptMoist) that transfers knowledge between wood chip domains using texture features and model saving based on adjusted mutual information.

Result: Combined texture features achieved 95% accuracy for moisture prediction. AdaptMoist improved cross-domain prediction accuracy by 23%, achieving 80% average accuracy compared to 57% for non-adapted models.

Conclusion: AdaptMoist is an effective robust solution for wood chip moisture content estimation across domains, making it suitable for wood chip-reliant industries by addressing source variability issues.

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [83] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: M2HVideo is a framework that converts mannequin videos into realistic human videos while preserving identity and clothing details, addressing head-body misalignment and temporal identity drift through pose-aware encoding and distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Mannequin-based clothing displays are cost-effective but lack realism and expressive detail, limiting their effectiveness for online fashion presentation. There's a need to generate photorealistic human videos from mannequin footage while maintaining identity control.

Method: M2HVideo uses a pose-aware and identity-preserving framework with: 1) dynamic pose-aware head encoder for consistent identity embeddings, 2) mirror loss in pixel space via DDIM-based denoising to preserve facial details, and 3) distribution-aware adapter for temporal coherence by aligning identity and clothing feature distributions.

Result: Extensive experiments on UBC fashion dataset, ASOS dataset, and MannequinVideos dataset show M2HVideo achieves superior performance in clothing consistency, identity preservation, and video fidelity compared to state-of-the-art methods.

Conclusion: M2HVideo successfully addresses the challenges of mannequin-to-human video generation, providing a practical solution for creating realistic fashion presentation videos while maintaining identity control and temporal coherence.

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [84] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R is a hierarchical training method that enhances 2D Gaussian Splatting by improving rendering quality while maintaining geometric accuracy through normal consistency regularization, in-place cloning of low-quality Gaussians, and fine-tuning with frozen opacity.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles with accurate surface representation, while 2DGS has geometric fidelity but compromised rendering quality. There's a need to achieve both high-quality rendering and precise geometric structures simultaneously.

Method: Three-stage hierarchical training: 1) Train original 2D Gaussians with normal consistency regularization, 2) Select and clone inadequate 2D Gaussians using in-place cloning operation, 3) Fine-tune the model with opacity frozen.

Result: Only 1% more storage and minimal additional training time compared to original 2DGS, while achieving high-quality rendering results and preserving fine geometric structures.

Conclusion: The approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [85] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer is a lightweight transformer-based semantic segmentation framework that achieves state-of-the-art weapon detection with 80.64% mIoU at 82.26 FPS, making it suitable for edge deployment in security applications.


<details>
  <summary>Details</summary>
Motivation: Traditional weapon detection methods provide only bounding boxes, lacking fine-grained segmentation needed for comprehensive threat analysis. Existing segmentation models either sacrifice accuracy for efficiency or require excessive computational resources incompatible with edge devices.

Method: Integrates Convolutional Block Attention Module (CBAM) with MixVisionTransformer architecture, combining CBAM-enhanced encoder backbone with attention-integrated hamburger decoder for multi-class weapon segmentation across five categories: handgun, rifle, knife, revolver, and human.

Result: Achieves 80.64% mIoU and 89.13% mFscore with real-time inference at 82.26 FPS. Requires only 4.886G FLOPs and 3.66M parameters, outperforming heavyweight models requiring up to 48x more computation.

Conclusion: ArmFormer establishes itself as the optimal solution for deployment on portable security cameras, surveillance drones, and embedded AI accelerators in distributed security infrastructure due to its superior accuracy and computational efficiency.

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [86] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL is a semi-supervised medical image segmentation framework that enforces bilateral alignment in both representation and label spaces, achieving state-of-the-art performance while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised medical image segmentation methods focus only on label-space consistency but overlook representation-space alignment, leading to models that struggle to learn discriminative and spatially coherent representations for complex medical images.

Method: BARL uses a unified framework with two collaborative branches. For label-space alignment: Dual-Path Regularization and Progressively Cognitive Bias Correction. For representation-space alignment: region-level and lesion-instance matching between branches to capture fragmented pathological patterns.

Result: Extensive experiments on four public benchmarks and a proprietary CBCT dataset show BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies validate each component's contribution.

Conclusion: BARL demonstrates that bilateral alignment in both representation and label spaces is crucial for effective semi-supervised medical image segmentation, achieving superior performance while reducing annotation burden.

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [87] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: A registration-induced framework for 3D anomaly detection that integrates point-cloud registration with memory-based detection to achieve rotation-invariant and locally discriminative features.


<details>
  <summary>Details</summary>
Motivation: Current memory bank-based methods suffer from inconsistent feature transformations and limited discriminative capacity, especially in capturing local geometric details and achieving rotation invariance, particularly when registration fails.

Method: Proposes a registration-induced, rotation-invariant feature extraction framework that jointly optimizes point-cloud registration and memory-based anomaly detection by embedding feature extraction into the registration learning process.

Result: Extensive experiments on Anomaly-ShapeNet and Real3D-AD datasets demonstrate consistent outperformance over existing approaches in effectiveness and generalizability.

Conclusion: The integration of registration and anomaly detection enables acquisition of features that are both robust to rotations and highly effective for anomaly detection, addressing key limitations of current methods.

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [88] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: This paper proposes a neuron-level analysis framework to study multimodal information processing in vision-language models (VLMs) by comparing artificial neurons with human brain activity using fMRI data.


<details>
  <summary>Details</summary>
Motivation: Current ANN studies have limitations: unimodal approaches don't capture the brain's multimodal processing, and multimodal research focuses on high-level outputs while neglecting individual neurons' roles.

Method: Combines fine-grained artificial neuron analysis with fMRI-based voxel encoding to examine two architecturally distinct VLMs (CLIP and METER), comparing them with biological neuron activities across multiple functional brain networks.

Result: Four key findings: (1) ANs predict BN activities across language, vision, attention, and default mode networks; (2) Both show functional redundancy; (3) ANs exhibit polarity patterns mirroring BNs; (4) Different VLM architectures drive distinct BN patterns - CLIP shows modality-specific specialization while METER yields unified cross-modal activation.

Conclusion: The results provide compelling evidence for brain-like hierarchical processing in VLMs at the neuronal level, demonstrating shared representational mechanisms between artificial and biological neural systems.

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [89] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: Class-N-Diff is a classification-induced diffusion model that integrates a classifier within a diffusion model to generate and classify dermoscopic images simultaneously, improving both generation quality and classification performance for skin cancer diagnosis.


<details>
  <summary>Details</summary>
Motivation: Traditional class-conditioned generative models struggle to generate images that accurately represent specific medical categories, limiting their usefulness for applications like skin cancer diagnosis.

Method: The proposed Class-N-Diff model integrates a classifier within a diffusion model to guide image generation based on class conditions, providing better control over class-conditioned image synthesis.

Result: The model generates more realistic and diverse images while the classifier demonstrates improved performance for downstream diagnostic tasks.

Conclusion: Class-N-Diff is a robust tool for enhancing the quality and utility of diffusion model-based synthetic dermoscopic image generation, with potential applications in medical diagnosis.

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [90] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1 is a novel post-training framework using policy optimization and MLLM-based rewards to improve instruction-based image editing models' generalization beyond training distributions.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning alone causes models to overfit to annotated patterns, limiting their ability to generalize to diverse editing instructions and tasks beyond the training distribution.

Method: Uses Diffusion Negative-aware Finetuning (DiffusionNFT) for policy optimization, employs MLLM as a training-free reward model with fine-grained feedback, and implements low-variance group filtering to reduce scoring noise.

Result: UniWorld-V2 achieves state-of-the-art results on ImgEdit (4.49) and GEdit-Bench (7.83), with the framework being model-agnostic and delivering substantial performance gains across diverse base models.

Conclusion: The Edit-R1 framework effectively addresses overfitting in instruction-based image editing through policy optimization and MLLM-based rewards, demonstrating strong generalization and wide applicability across different models.

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [91] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: A modular framework for attributing contrails observed by ground-based cameras to source flights using aircraft surveillance and meteorological data, addressing the challenge of contrail-to-flight attribution.


<details>
  <summary>Details</summary>
Motivation: Aviation's non-CO2 effects from contrails contribute significantly to climate impact, but validating contrail models requires linking observed contrails to source flights, which is challenging with satellite data due to limited resolution and contrail drift.

Method: Uses ground-based cameras to capture contrails shortly after formation, then employs a modular framework with multiple geometric representations, distance metrics, temporal smoothing, and probability-based assignment strategies to link contrails to flights.

Result: The framework successfully establishes a baseline for contrail-to-flight attribution using ground-based camera data, providing a flexible approach that accommodates various geometric and temporal considerations.

Conclusion: This work provides a strong foundation and modular framework for future research in contrail attribution, enabling better validation of contrail models and understanding of aviation's climate impact.

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [92] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: This paper evaluates transformer-based architectures for thermal weapon segmentation, achieving state-of-the-art performance with SegFormer-b5 (94.15% mIoU) and fast inference with SegFormer-b0 (98.32 FPS), demonstrating robust weapon detection in challenging thermal conditions.


<details>
  <summary>Details</summary>
Motivation: Thermal weapon segmentation is crucial for surveillance applications where RGB systems fail in low-light and obscured conditions. While CNNs dominate thermal segmentation, they struggle with long-range dependencies and fine details. Vision Transformers show promise but remain underexplored for thermal weapon segmentation.

Method: The study adapts and evaluates four transformer-based architectures (SegFormer, DeepLabV3+, SegNeXt, Swin Transformer) on a custom thermal dataset of 9,711 images from real surveillance videos, automatically annotated using SAM2. Standard augmentation strategies within MMSegmentation framework ensure robust training and fair comparison.

Result: Significant improvements achieved: SegFormer-b5 reaches highest mIoU (94.15%) and Pixel Accuracy (97.04%), SegFormer-b0 provides fastest inference (98.32 FPS) with competitive mIoU (90.84%). SegNeXt-mscans offers balanced performance (85.12 FPS, 92.24% mIoU), DeepLabV3+ R101-D8 achieves 92.76% mIoU at 29.86 FPS.

Conclusion: Transformer architectures demonstrate robust generalization for weapon detection in low-light and occluded thermal environments, offering flexible accuracy-speed trade-offs suitable for diverse real-time security applications, establishing transformers as superior alternatives to CNNs for thermal weapon segmentation.

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [93] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: Res-Bench is a benchmark for evaluating resolution robustness in Multimodal Large Language Models (MLLMs), assessing performance stability across 12 resolution levels using novel robustness metrics.


<details>
  <summary>Details</summary>
Motivation: Current MLLM evaluations focus on semantic performance but overlook resolution robustness - whether performance remains stable across varying input resolutions, creating a critical gap in understanding model reliability.

Method: Developed Res-Bench with 14,400 samples across 12 resolution levels and 6 capability dimensions. Introduced novel robustness metrics: Spearman's correlation for resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for performance volatility.

Result: Conducted large-scale evaluation of leading MLLMs, examining model-centric and task-centric robustness, preprocessing strategies (padding, super-resolution), and fine-tuning for stability enhancement.

Conclusion: The study addresses the critical gap in resolution robustness evaluation for MLLMs and provides a comprehensive framework to assess and improve model stability across varying input resolutions.

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [94] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: This review paper provides a comprehensive analysis of foundation models (FMs) in medical image analysis, systematically categorizing studies and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Despite rapid proliferation of FM research in medical imaging, the field remains fragmented and lacks a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities.

Method: The review systematically categorizes studies into vision-only and vision-language FMs based on architectural foundations, training strategies, and downstream clinical tasks, and conducts quantitative meta-analysis of temporal trends in dataset utilization and application domains.

Result: The paper provides a structured analysis of FMs in medical image analysis, identifying persistent challenges including domain adaptation, efficient fine-tuning, computational constraints, and interpretability, along with emerging solutions.

Conclusion: The review identifies key future research directions aimed at enhancing robustness, explainability, and clinical integration of FMs to accelerate their translation into real-world medical practice.

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [95] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: Di-Bregman is a unified framework for diffusion distillation that uses Bregman divergence-based density-ratio matching to accelerate multi-step diffusion models into efficient one-step generators while maintaining high visual quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models achieve high generative quality but are computationally expensive due to slow multi-step sampling. Existing distillation methods lack a unified theoretical foundation for accelerating these models.

Method: Proposes Di-Bregman framework that formulates diffusion distillation as Bregman divergence-based density-ratio matching, providing a convex-analytic view that connects various existing objectives.

Result: Experiments on CIFAR-10 and text-to-image generation show improved one-step FID over reverse-KL distillation and maintain high visual fidelity compared to teacher models.

Conclusion: Bregman density-ratio matching provides a practical and theoretically-grounded approach for efficient one-step diffusion generation.

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [96] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: CARE is a framework for ADL recognition that aligns sequence-based and image-based representations through contrastive learning, achieving state-of-the-art performance on CASAS datasets while being robust to sensor issues.


<details>
  <summary>Details</summary>
Motivation: Existing ADL recognition methods have limitations: sequence-based approaches are sensitive to noise and lack spatial awareness, while image-based approaches lose fine-grained temporal dynamics. Naive fusion methods fail to properly align these complementary representations.

Method: CARE uses Sequence-Image Contrastive Alignment (SICA) to jointly optimize representation learning and classification. It integrates time-aware sequence encoding with spatially-informed image representations, using a joint contrastive-classification objective for end-to-end learning of aligned embeddings.

Result: CARE achieves state-of-the-art performance on three CASAS datasets: 89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7. It also demonstrates robustness to sensor malfunctions and layout variability.

Conclusion: CARE effectively addresses representation-level limitations in ADL recognition by enforcing alignment between sequence and image views, leveraging their complementary strengths for reliable smart home applications.

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [97] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: BaGLM enables online Video Step Grounding without training by using Large Multimodal Models with Bayesian filtering that incorporates step transition knowledge and progress estimation.


<details>
  <summary>Details</summary>
Motivation: Standard VSG methods require labeled training data and process full videos offline, limiting applications for online scenarios. This work explores performing VSG online without training.

Method: Uses LMMs to predict steps from limited frames, then develops BaGLM with Bayesian filtering that models step transitions via dependency matrix from LLMs and step progress estimation.

Result: The online strategy without task-specific tuning outperforms offline training-based models. BaGLM shows superior performance over state-of-the-art training-based offline methods on three datasets.

Conclusion: BaGLM successfully enables online Video Step Grounding without training requirements, demonstrating the effectiveness of leveraging LMMs with Bayesian filtering principles.

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [98] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: This paper conducts an empirical study on how different video feature representations affect temporal video grounding performance, showing that simply changing video encoders (CNN, temporal reasoning, transformers) significantly impacts model performance and reveals feature complementarity patterns.


<details>
  <summary>Details</summary>
Motivation: Current temporal video grounding research focuses on a limited selection of video representations, which may lead to architectural overfitting. The authors aim to investigate the impact of diverse video features on model performance.

Method: Extracted features from three benchmarks (Charades-STA, ActivityNet-Captions, YouCookII) using different video encoders based on CNNs, temporal reasoning, and transformers, then evaluated their impact on a classical architecture.

Result: Results show significant performance differences when changing video encoders, revealing clear patterns and errors from using certain features, and indicating potential feature complementarity.

Conclusion: Video feature selection has substantial impact on temporal video grounding performance, and exploring diverse feature representations can prevent architectural overfitting while revealing complementary feature patterns.

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [99] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: The paper challenges the need for specialized remote sensing foundation models, showing that general-purpose vision models perform equally well at small scales.


<details>
  <summary>Details</summary>
Motivation: To test whether specialized foundation models for remote sensing provide meaningful advantages over general-purpose vision foundation models, given the unique characteristics of satellite imagery.

Method: Created a benchmark to measure generalization to lower-resolution images for two downstream tasks, and trained iBOT (self-supervised vision encoder) on MillionAID dataset with remote sensing-specific modifications.

Result: None of the pretrained remote sensing models consistently outperformed general-purpose baselines at the ViT-B scale.

Conclusion: Specialized foundation models for remote sensing don't provide consistent improvements over general-purpose vision models in small-scale applications.

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [100] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG is a two-stage video temporal grounding method that uses multimodal LLMs to enrich text queries before localizing them in videos, achieving state-of-the-art performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To leverage multimodal LLMs' capabilities for fine-grained video temporal grounding by addressing the challenge of directly grounding natural language queries that may lack sufficient context or details.

Method: Two-stage approach: 1) Transform language queries into enriched sentences with additional details and cues using multimodal LLMs, 2) Ground enriched queries using a lightweight decoder with multiple-instance-learning objective to mitigate noise and hallucinations.

Result: State-of-the-art performance across various temporal video grounding benchmarks, significantly outperforming previous LLM-based approaches and comparable to specialized models, with clear advantages in zero-shot evaluation.

Conclusion: ED-VTG effectively harnesses multimodal LLMs for video temporal grounding through query enrichment and specialized decoding, demonstrating superior performance while maintaining strong zero-shot capabilities.

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [101] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: W2R2 is a training framework that addresses 2D semantic bias in multimodal 3D grounding by disentangling 2D semantic and 3D spatial features, using dual-objective loss functions to improve localization accuracy without changing inference architecture.


<details>
  <summary>Details</summary>
Motivation: Current multimodal 3D grounding models suffer from "2D semantic bias" - over-relying on 2D image features for localization while disregarding 3D geometric inputs, leading to suboptimal fusion performance.

Method: Proposes What-Where Representation Re-Forming (W2R2) with disentangled representation learning: 2D features as semantic beacons for "What" identification and 3D features as spatial anchors for "Where" localization. Uses dual-objective loss with Alignment Loss for multimodal synergy and Pseudo-Label Loss to penalize 2D-dominant outputs.

Result: Experiments on ScanRefer and ScanQA show significant gains in localization accuracy and robustness, particularly in cluttered outdoor scenes.

Conclusion: W2R2 effectively addresses 2D semantic bias through targeted shortcut suppression and representation disentanglement, enabling precise 3D grounding without architectural modifications.

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [102] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: This paper presents a novel approach using conditional StyleGAN2-ADA and StyleGAN3 to generate high-resolution synthetic live fingerprints, and CycleGANs to create realistic spoof fingerprints for spoof detection systems, achieving excellent performance metrics while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of collecting large fingerprint datasets which are time-consuming, expensive, and require strict privacy measures, by creating synthetic fingerprint data that maintains utility while preserving privacy.

Method: Uses conditional StyleGAN2-ADA and StyleGAN3 architectures to generate synthetic live fingerprints conditioned on finger identities, and CycleGANs to translate these into realistic spoof fingerprints simulating various presentation attack materials.

Result: Created two synthetic datasets (DB2 and DB3) with 1,500 fingerprint images each. StyleGAN3 achieved FID as low as 5, TAR of 99.47% at 0.01% FAR, while StyleGAN2-ADA achieved 98.67% TAR. No significant identity leakage was detected, confirming strong privacy preservation.

Conclusion: The proposed generative approach successfully creates high-quality synthetic fingerprint datasets that maintain utility for biometric applications while effectively preserving privacy, making them suitable for training and evaluating fingerprint recognition and spoof detection systems.

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [103] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: This study developed a clinician-in-the-loop deep learning pipeline for lung cancer CT segmentation using multiple DL models, with VNet achieving best performance and radiomic stability, and semi-supervised learning outperforming supervised learning for prognostic modeling.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is the leading cause of cancer mortality, with CT imaging being central to screening and treatment. Manual segmentation is variable and time-intensive, while existing deep learning approaches face barriers to clinical adoption due to trust and reproducibility issues.

Method: Used multi-center CT data from 999 patients across 12 datasets, evaluated five DL models (3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D) benchmarked against expert contours. Assessed segmentation reproducibility using 497 radiomic features and compared supervised vs semi-supervised learning across 38 dimensionality reduction strategies and 24 classifiers. Six physicians qualitatively evaluated masks across seven clinical domains.

Result: VNet achieved best performance (Dice = 0.83, IoU = 0.71) and radiomic stability (mean correlation = 0.76, ICC = 0.65). Semi-supervised learning consistently outperformed supervised learning (accuracy = 0.88, F1 = 0.83). Radiologists preferred VNet for peritumoral representation and smoother boundaries, favoring AI-generated masks for refinement rather than replacement.

Conclusion: Integrating VNet with semi-supervised learning yields accurate, reproducible, and clinically trusted CT-based lung cancer prognosis, demonstrating a feasible path toward physician-centered AI translation in clinical practice.

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [104] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: The paper proposes a generalized representation selection method for person re-identification that improves performance by choosing better class representatives beyond just centroids, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current person re-identification methods focus on feature extraction and objective functions, but class representation selection is underexplored. Prior techniques using class centroids yield suboptimal results, creating a need for better representation strategies.

Method: A generalized selection method that chooses class representations not limited to centroids, allowing adjustable number of representations per class to meet specific application requirements. Applied on top of multiple re-identification embeddings.

Result: The approach substantially improves upon contemporary results across multiple re-identification embeddings, achieving better balance between accuracy and mean average precision.

Conclusion: The proposed generalized representation selection method advances person re-identification performance beyond state-of-the-art by optimizing class representative selection rather than relying solely on feature extraction improvements.

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [105] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: V-Reason improves video reasoning in LMMs by using entropy signals to optimize thinking behavior during inference, reducing output tokens by 58.6% and achieving near-RL performance without training.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning with LMMs relies on expensive RL training and verbose chain-of-thought, causing computational overhead and limited control over the thinking process.

Method: Uses entropy as a signal to observe micro-exploration/exploitation patterns, then adapts LMM's value cache via a small controller using entropy-based optimization during inference without training.

Result: Significant improvements over base models across video reasoning datasets, narrowing gap with RL-trained models to within 0.6% accuracy while reducing output tokens by 58.6%.

Conclusion: Entropy-based inference-time optimization enables efficient video reasoning comparable to RL-trained models without training overhead.

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [106] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: This paper investigates the trade-off between general-purpose foundation vision models (Hiera) and specialized models (SAM2) for segmentation, revealing that specialization comes at a cost of broader semantic information and creates representational bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-off between general-purpose foundation vision models and specialized counterparts for efficient feature coding design, as this trade-off is not yet fully understood.

Method: Comparing feature versatility of Hiera encoder against SAM2 using a lightweight trainable neck to probe adaptability of frozen features, and quantifying information-theoretic cost of specialization through cross-neck analysis.

Result: SAM2's specialization is highly effective for spatially-related tasks like depth estimation but underperforms Hiera on conceptually distant tasks such as pose estimation and image captioning, showing measurable loss of broader semantic information. Cross-neck analysis reveals each adaptation level creates further representational bottlenecks.

Conclusion: The analysis illuminates trade-offs in feature universality and provides quantitative foundation for designing efficient feature coding and adaptation strategies for diverse downstream applications.

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [107] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: ProDAT is a density-aware tail-drop mechanism for progressive point cloud coding that enables adaptive decoding at multiple bitrates using a single model, achieving superior compression efficiency over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: 3D point clouds are crucial for applications like autonomous driving and AR, but their large data volumes and bandwidth constraints limit deployment in resource-limited environments. Existing learning-based methods lack progressive decoding capability.

Method: Proposed ProDAT uses density information as guidance to adaptively decode latent features and coordinates based on significance, enabling progressive decoding at multiple bitrates with one model.

Result: Experimental results show ProDAT enables progressive coding and achieves over 28.6% BD-rate improvement for PSNR-D2 on SemanticKITTI and over 18.15% on ShapeNet compared to state-of-the-art learning-based methods.

Conclusion: ProDAT successfully bridges the gap in progressive point cloud coding by providing density-aware adaptive decoding, offering both progressive capability and superior coding efficiency with a single model.

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [108] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: GOOD is a novel framework that uses dual-level guidance (image-level and feature-level) with diffusion models to generate diverse out-of-distribution samples, improving OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image diffusion approaches for OOD sample generation suffer from semantic instability and insufficient shift diversity due to text-conditioned embedding perturbations, limiting generalization to realistic OOD scenarios.

Method: GOOD directly guides diffusion sampling trajectories using ID classifiers with dual-level guidance: (1) image-level guidance reduces input likelihood via gradient of log partition, and (2) feature-level guidance promotes sampling in feature-sparse regions using k-NN distance in classifier's latent space.

Result: The framework enables more controllable and diverse OOD sample generation, and training with GOOD-generated samples notably enhances OOD detection performance according to thorough quantitative and qualitative analyses.

Conclusion: GOOD provides a flexible and effective approach for generating diverse OOD samples that significantly improves OOD detection capabilities through its dual-guidance design and unified OOD scoring mechanism.

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [109] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: FMCAF is a preprocessing architecture that enhances RGB and infrared fusion using frequency-domain filtering and cross-attention, achieving improved object detection performance across different multimodal datasets without dataset-specific tuning.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal object detection robustness by leveraging complementary cues from RGB and infrared sensors, addressing the need for generalizable fusion methods that work across different challenging conditions.

Method: Combines frequency-domain filtering (Freq-Filter) to suppress redundant spectral features with a cross-attention-based fusion module (MCAF) to enhance intermodal feature sharing between RGB and infrared inputs.

Result: Outperforms traditional concatenation fusion, achieving +13.9% mAP@50 on VEDAI (aerial vehicle detection) and +1.1% on LLVIP (low-light pedestrian detection).

Conclusion: FMCAF shows potential as a flexible foundation for robust multimodal fusion in future detection pipelines, demonstrating generalizability across different multimodal challenges.

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [110] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD is a two-stage post-training framework that improves MLLMs' ability to generate editable CAD models from single images by enhancing geometric accuracy and promoting concise modeling procedures through depth/normal map priors and reinforcement learning with group length rewards.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities, which hinders their ability to generate editable parametric CAD models from single images for industrial concept design.

Method: Two-stage framework: 1) Supervised fine-tuning using depth and surface normal maps as dense geometric priors combined with RGB images as multi-channel input; 2) Reinforcement learning with group length reward to promote compact parametric modeling sequences while maintaining geometric fidelity, using dynamic weighting for training stability.

Result: State-of-the-art performance on DeepCAD and Fusion360 datasets, consistently outperforming existing methods in code validity, geometric accuracy, and modeling conciseness under the same MLLM backbone.

Conclusion: GACO-CAD effectively addresses MLLMs' spatial reasoning limitations for CAD generation from single images, achieving improved geometric accuracy and more concise modeling procedures through its two-stage post-training approach.

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [111] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane enhances Gaussian Splatting by incorporating planar priors to improve geometric accuracy and mesh structure in 3D scene reconstruction, particularly for man-made environments.


<details>
  <summary>Details</summary>
Motivation: Current Gaussian Splatting methods struggle with reconstructing planar regions smoothly and precisely, which is crucial for man-made environments like indoor spaces and urban streets where planes are fundamental primitives.

Method: Uses off-the-shelf segmentation and normal prediction models to extract planar priors, establishes structured representations for planar Gaussian coordinates, employs Dynamic Gaussian Re-classifier to adaptively reclassify problematic Gaussians, and refines mesh layouts using optimized planar priors.

Result: Significantly improves geometric accuracy of extracted meshes across various baselines while maintaining rendering quality, reduces number of vertices and faces, and enables decoupling and flexible manipulation of objects on supportive planes.

Conclusion: GSPlane successfully addresses the planar reconstruction limitations in Gaussian Splatting by incorporating structured planar priors, leading to more accurate geometry and cleaner mesh connectivity without sacrificing rendering performance.

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [112] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: This paper presents the first large-scale systematic evaluation of Out-of-Distribution (OoD) detection methods for plankton recognition, finding that ViM method significantly outperforms others, especially in Far-OoD scenarios.


<details>
  <summary>Details</summary>
Motivation: Plankton recognition models face challenges from distribution shifts due to complex morphologies, species diversity, and continuous discovery of novel species, leading to unpredictable errors. The field lacks systematic integration of latest computer vision developments and unified benchmarks.

Method: Created OoD benchmarks simulating various distribution shift scenarios using DYB-PlanktonNet dataset, and systematically evaluated twenty-two OoD detection methods.

Result: ViM method significantly outperformed other approaches, particularly excelling in Far-OoD scenarios with substantial improvements in key metrics.

Conclusion: This comprehensive evaluation provides reliable reference for algorithm selection in automated plankton recognition and lays foundation for future research in plankton OoD detection.

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [113] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes a novel optimization strategy for pre-trained diffusion models to enhance content fidelity while maintaining perceptual realism in low-level vision tasks, particularly addressing issues in low-light scenarios.


<details>
  <summary>Details</summary>
Motivation: Pre-trained diffusion-based methods often sacrifice content fidelity for perceptual realism, especially in low-light scenarios where degraded information limits effective control. The authors identify two main causes: lack of suitable conditional latent modeling and absence of bidirectional interaction between conditional and noisy latents.

Method: The method introduces a latent refinement pipeline that recovers spatial details lost during VAE encoding using generative priors. It enables dynamic interaction between refined latent conditions and noisy latents during the diffusion process. The approach is plug-and-play and can be integrated into existing diffusion networks.

Result: Extensive experiments demonstrate significant fidelity improvements in pre-trained diffusion-based methods while preserving realism and aesthetics.

Conclusion: The proposed optimization strategy effectively addresses fidelity loss in diffusion-based methods through improved conditional latent modeling and bidirectional interaction mechanisms, achieving better restoration performance particularly in challenging low-light scenarios.

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [114] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: A method using LED lighting to create invisible watermarks detectable by cameras but not humans, using spectral modulation for imperceptibility at standard frame rates.


<details>
  <summary>Details</summary>
Motivation: To enable privacy protection and content verification through imperceptible watermarks that can be embedded in videos using environmental lighting.

Method: Optimizes LED spectral profiles to be minimally visible to humans but detectable by cameras, using spectral modulation instead of intensity modulation, and works with standard frame rates (30-60 fps).

Result: Achieves embedding of 128 bits within 10-second video clips, sufficient for essential metadata while maintaining visual imperceptibility.

Conclusion: The approach provides a practical solution for embedding invisible watermarks in video content using LED lighting, supporting privacy protection and content verification applications.

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [115] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: A zero-shot prompt-aware visual token pruning method that balances task relevance and information diversity, achieving state-of-the-art performance with up to 90% token reduction while significantly reducing GPU memory and inference latency.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) face prohibitive inference costs due to visual token redundancy, and existing pruning methods neglect text prompt guidance, failing to prioritize task relevance.

Method: A hierarchical approach that first selects task-relevant visual tokens based on text prompt guidance, then supplements with diversity tokens to preserve broader context, modeling pruning as a balance between relevance and diversity.

Result: Achieves performance matching or surpassing state-of-the-art with minimal accuracy loss even when pruning up to 90% of tokens, with significant reductions in GPU memory footprint and inference latency.

Conclusion: The proposed prompt-aware token pruning method effectively reduces VLM inference costs while maintaining performance by explicitly balancing task relevance and information diversity.

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [116] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: Adapted Segment Anything Model (SAM) with fine-tuned mask decoder achieves 86.30% mIoU and 92.60% Dice score for detecting riverbank erosion and disappeared settlements in Bangladesh using historical Google Earth imagery.


<details>
  <summary>Details</summary>
Motivation: Bangladesh's rivers cause annual destruction by swallowing villages and farmland, displacing thousands, but tracking this erosion has been extremely difficult for human analysts.

Method: Used color-channel analysis for rough land/water segmentation, then fine-tuned SAM's mask decoder on a new dataset of historical Google Earth imagery (2003-2025) with manual annotations of vanished settlements.

Result: Achieved 86.30% mean Intersection over Union and 92.60% Dice score, significantly outperforming traditional methods and off-the-shelf deep learning models.

Conclusion: Provides first annotated dataset of disappeared settlements, specialized AI model for erosion detection, and method for quantifying land loss, enabling policymakers to monitor erosion and protect vulnerable communities.

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [117] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D is a unified framework that uses kinematic-aware diffusion models for category-level articulated object shape reconstruction, pose estimation, and generation from single-view inputs.


<details>
  <summary>Details</summary>
Motivation: Articulated objects like laptops and drawers pose significant challenges for 3D reconstruction due to their multi-part geometries and variable joint configurations, which create structural diversity across different states.

Method: The approach uses a Kinematic-Aware VAE to encode geometry, joint angles, and part segmentation into a structured latent space, then employs two conditional diffusion models for global pose/joint parameter regression and kinematic-aware latent code generation, followed by iterative optimization with Chamfer-distance minimization.

Result: Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the framework's effectiveness in accurately reconstructing articulated objects and estimating their kinematic properties.

Conclusion: KineDiff3D provides a comprehensive solution for articulated object reconstruction and pose estimation that handles structural diversity while preserving articulation constraints.

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [118] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: This paper presents a round outcome prediction model for VALORANT using minimap information and tactical features extracted from match footage, achieving 81% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional esports match prediction relies on match log data and statistics, but FPS games like VALORANT require analysis of complex strategies and real-time tactical information from match footage.

Method: Based on TimeSformer video recognition model, the approach incorporates detailed tactical features from minimap information including character positions and in-game events, using dataset augmentation with tactical event labels.

Result: The model achieved approximately 81% prediction accuracy, particularly from the middle phases of rounds, significantly outperforming models trained only on raw minimap information.

Conclusion: Leveraging tactical features extracted from match footage is highly effective for predicting round outcomes in VALORANT, demonstrating the value of video-based tactical analysis over traditional statistical approaches.

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [119] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: This paper introduces QV-M², a multi-moment retrieval dataset addressing the limitation of existing single-moment retrieval methods, and proposes FlashMMR framework with post-verification for better multi-moment alignment.


<details>
  <summary>Details</summary>
Motivation: Existing moment retrieval methods focus on single-moment retrieval, but real-world applications often require retrieving multiple relevant moments per query, making current datasets and methods insufficient.

Method: Proposed FlashMMR framework with Multi-moment Post-verification module using constrained temporal adjustment and verification to refine moment boundaries and prune low-confidence proposals.

Result: FlashMMR achieves improvements over prior SOTA by 3.00% on G-mAP, 2.70% on mAP@3+tgt, and 2.56% on mR@3 on QV-M² dataset. The dataset contains 2,212 annotations covering 6,384 video segments.

Conclusion: QV-M² serves as an effective benchmark for multi-moment retrieval, and FlashMMR provides a strong baseline, establishing foundation for advancing research in realistic video temporal grounding scenarios.

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [120] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: Face recognition systems are vulnerable to adversarial attacks, but face preprocessing techniques significantly impact attack success rates. Different face detection models can reduce attack effectiveness by up to 78%, while preprocessing-invariant methods can improve transferability by 27%.


<details>
  <summary>Details</summary>
Motivation: Face preprocessing is a critical but often overlooked component in face recognition systems when evaluating adversarial attacks, especially in blackbox settings where internal preprocessing details are unknown.

Method: The study investigates transferability of state-of-the-art adversarial attacks against different preprocessing techniques in blackbox settings, and proposes a preprocessing-invariant method using input transformations to improve attack transferability.

Result: Face detection model choice degrades attack success rate by up to 78%, while interpolation methods have minimal impact. Preprocessing degrades attack strength even in whitebox settings due to noise vector interactions with face detection. The proposed preprocessing-invariant method improves transferability by up to 27%.

Conclusion: Face preprocessing is crucial in face recognition systems and must be considered to improve adversarial generalization of facial adversarial examples.

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [121] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision is a large-scale, meticulously curated vision-language dataset of 24 million samples that unifies over 200 sources through a semi-automated human-in-the-loop pipeline, achieving superior model performance through data hygiene and quality control.


<details>
  <summary>Details</summary>
Motivation: The advancement of vision-language models is hampered by fragmented, inconsistent, and contaminated public datasets, creating a need for a unified, high-quality corpus.

Method: Semi-automated human-in-the-loop pipeline that unifies 200+ sources into 185 subsets, performs rigorous de-duplication and decontamination against 66 benchmarks, and includes agentic/GUI tasks with unified action space validated by human reviewers.

Result: Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, demonstrating benefits of scale, data hygiene, and balanced automation with human oversight.

Conclusion: FineVision provides a high-quality, unified vision-language corpus that accelerates data-centric VLM research, with the corpus and curation tools being released to the community.

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [122] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: GtR is a training-free hierarchical sampling strategy that accelerates masked autoregressive models by decomposing generation into structure generation and detail reconstruction stages, achieving 3.72x speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Masked autoregressive models have constrained acceleration potential due to the complexity of modeling spatially correlated visual tokens in a single step, creating a need for more efficient generation methods.

Method: GtR uses a two-stage approach: structure generation for global semantic scaffolding followed by detail reconstruction for completing remaining tokens. It also includes Frequency-Weighted Token Selection (FTS) to allocate more computation to detail-rich tokens based on high-frequency energy.

Result: Achieves 3.72x speedup on MAR-H while maintaining comparable quality (FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), outperforming existing acceleration methods across various model scales and generation tasks.

Conclusion: GtR effectively accelerates masked autoregressive models through hierarchical sampling and frequency-aware token selection, demonstrating significant speed improvements without compromising generation quality.

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [123] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff is a diffusion-based framework that uses character-level guidance to restore severely degraded license plate images, achieving superior restoration quality and recognition accuracy compared to baseline models.


<details>
  <summary>Details</summary>
Motivation: License plate image restoration is crucial not only for LPR systems but also for increasing evidential value, enhancing visual clarity, and facilitating further utilization of license plate images captured under realistic conditions.

Method: CharDiff leverages fine-grained character-level priors extracted through external segmentation and OCR modules, and incorporates a Character-guided Attention through Region-wise Masking (CHARM) module to ensure each character's guidance is restricted to its own region without interfering with other regions.

Result: CharDiff significantly outperformed baseline restoration models, achieving a 28% relative reduction in Character Error Rate (CER) on the Roboflow-LP dataset compared to the best-performing baseline model.

Conclusion: The structured character-guided conditioning effectively enhances the robustness of diffusion-based license plate restoration and recognition in practical deployment scenarios.

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [124] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: A framework that jointly learns detailed head avatars and non-rigid facial deformations caused by hand-face interactions, addressing challenges in spatial relationships and deformation learning through depth order loss, contact regularization, and PCA-based deformation modeling.


<details>
  <summary>Details</summary>
Motivation: Current photorealistic 3D head avatar methods focus only on facial regions and ignore natural hand-face interactions that convey cognitive states like pondering, limiting their realism and expressiveness.

Method: Combines depth order loss with contact regularization for pose tracking, learns a PCA basis for hand-induced facial deformations from interaction data, and incorporates contact loss inspired by physics-based simulation to reduce interpenetration artifacts.

Result: Evaluated on iPhone-captured RGB(D) videos and synthetic datasets, the method captures better appearance and more accurate deforming geometry than state-of-the-art surface reconstruction methods.

Conclusion: The proposed framework successfully models hand-face interactions by addressing spatial relationship challenges and learning deformation priors, producing physically plausible avatars with realistic interaction effects.

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [125] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC is a hyperbolic representation learning framework for Domain Generalization with Generalized Category Discovery (DG-GCD) that achieves domain and category-level generalization without episodic simulation, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods assume simultaneous access to labeled/unlabeled data from the same domain, limiting applicability in open-world scenarios with distribution shifts. DG-GCD aims to generalize to unseen domains containing novel categories without accessing target domain data during training.

Method: Uses hyperbolic representation learning with GPT-guided diffusion for domain augmentation, Tangent CutMix for curvature-aware interpolation in tangent space, and a unified loss combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion. Includes a learnable curvature parameter.

Result: Achieves state-of-the-art results on PACS, Office-Home, and DomainNet benchmarks, consistently outperforming existing Euclidean and hyperbolic (DG)-GCD baselines.

Conclusion: HIDISC provides an efficient and effective framework for DG-GCD that avoids the computational cost and error accumulation of episodic training while achieving superior generalization across domains and categories.

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [126] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld is a novel 4D occupancy world model that uses sparse dynamic queries for flexible and adaptive perception, replacing traditional static grids with range-adaptive perception and state-conditioned forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing occupancy world models rely on static embeddings/grids that limit perception flexibility and misalign with the dynamic, continuous nature of real scenarios.

Method: Proposes Range-Adaptive Perception module with learnable queries modulated by ego vehicle states, and State-Conditioned Forecasting module using regression-guided formulation instead of classification. Includes Temporal-Aware Self-Scheduling training strategy.

Result: Achieves state-of-the-art performance across perception, forecasting, and planning tasks, with advantages in flexibility, adaptability, and efficiency.

Conclusion: SparseWorld demonstrates superior performance through its sparse dynamic query approach, effectively addressing limitations of traditional occupancy models in dynamic 4D environments.

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [127] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: A rubric-guided pseudo-labeling framework that uses LLMs for zero-shot video summarization, achieving strong performance without training by creating dataset-adaptive scoring rubrics from minimal ground-truth annotations.


<details>
  <summary>Details</summary>
Motivation: Address limitations of supervised methods (high labeling costs, poor generalization) and unsupervised methods (poor semantic capture), while overcoming prompt sensitivity in existing zero-shot approaches.

Method: Transform ground-truth annotations into pseudo labels aggregated into structured rubrics; use contextual prompting where intermediate segments are evaluated with adjacent scene summaries to assess narrative progression and redundancy.

Result: Achieved F1 scores of 57.58 on SumMe and 63.05 on TVSum, surpassing unsupervised and prior zero-shot baselines while approaching supervised performance.

Conclusion: Rubric-guided pseudo labeling effectively stabilizes LLM-based scoring and establishes a general, interpretable zero-shot paradigm for video summarization.

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [128] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: A training framework for large-scale video generation models that optimizes data processing, model architecture, training strategy, and infrastructure, resulting in MUG-V 10B model that matches state-of-the-art performance and is fully open-sourced.


<details>
  <summary>Details</summary>
Motivation: Training large-scale video generation models is challenging due to cross-modal text-video alignment, long sequences, and complex spatiotemporal dependencies, requiring resource-intensive approaches.

Method: Optimized four pillars: data processing, model architecture, training strategy, and infrastructure. Used Megatron-Core for large-scale training with data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training.

Result: MUG-V 10B model matches recent state-of-the-art video generators overall and surpasses leading open-source baselines in e-commerce-oriented video generation tasks in human evaluations.

Conclusion: The framework delivers significant efficiency gains and performance improvements, and the complete stack including model weights, training code, and inference pipelines is open-sourced as the first public release of large-scale video generation training code using Megatron-Core.

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [129] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: MambaX-Net is a semi-supervised 3D segmentation model for longitudinal prostate cancer active surveillance that leverages previous time point data and pseudo-labels to achieve superior segmentation with limited expert annotations.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning segmentation models are unsuitable for longitudinal active surveillance analysis due to their training on single-time-point data and reliance on expert annotations, which are scarce in clinical practice.

Method: Proposed MambaX-Net with Mamba-enhanced Cross-Attention Module for temporal evolution and spatial dependencies, Shape Extractor Module for anatomical representation, and semi-supervised self-training using pseudo-labels from pre-trained nnU-Net.

Result: MambaX-Net significantly outperforms state-of-the-art U-Net and Transformer-based models on longitudinal AS dataset, achieving superior prostate zone segmentation even with limited and noisy training data.

Conclusion: The proposed MambaX-Net effectively addresses challenges in longitudinal prostate cancer active surveillance by leveraging temporal information and semi-supervised learning, enabling accurate segmentation without extensive expert annotations.

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [130] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL is a novel class-incremental learning framework for endoscopic image analysis that addresses catastrophic forgetting through three key components: distribution-aligned exemplar selection, class-balanced loss with prior regularization, and gradient calibration to mitigate new-class bias.


<details>
  <summary>Details</summary>
Motivation: Existing replay-based CIL methods fail to effectively mitigate catastrophic forgetting in endoscopic imaging due to severe domain discrepancies and class imbalance, which are inherent challenges in clinical endoscopic data.

Method: EndoCIL incorporates three components: 1) MDBR for diverse exemplar selection using distribution alignment, 2) PRCBL to handle class imbalance by integrating prior distributions and balance weights, and 3) CFG to adjust classifier gradients and reduce bias toward new classes.

Result: Extensive experiments on four public endoscopic datasets show that EndoCIL generally outperforms state-of-the-art CIL methods across varying buffer sizes and evaluation metrics, effectively balancing stability and plasticity.

Conclusion: The framework demonstrates promising potential for clinical scalability and deployment in lifelong endoscopic diagnosis, successfully addressing the key challenges of domain discrepancies and class imbalance in endoscopic CIL.

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [131] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: Proposes a DINOv2-based method with registers to detect face spoofing attacks by identifying subtle differences between live and spoofed face images.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to spoofing attacks where malicious actors use photos to bypass authentication, requiring detection methods to identify these attacks before face recognition.

Method: Uses DINOv2 with registers to extract generalizable features and suppress attention mechanism perturbations, enabling focused attention on essential minute features for spoof detection.

Result: Demonstrated effectiveness through experiments on datasets from The 6th Face Anti-Spoofing Workshop and SiW dataset.

Conclusion: The proposed DINOv2-based method with registers effectively detects face spoofing attacks by leveraging attention mechanisms to identify subtle differences between live and spoofed images.

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [132] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT is a temporal dataset of car models (2007-2023) for studying AI adaptation to evolving visual environments. It enables research on continual learning and time-aware generation.


<details>
  <summary>Details</summary>
Motivation: AI systems need to adapt to changing visual environments where object appearances evolve over time, particularly for technological artifacts like cars.

Method: Created CaMiT dataset with 787K labeled and 5.1M unlabeled car samples; proposed time-incremental classification with two strategies: backbone pretraining and classifier-only updates; explored time-aware image generation.

Result: Static pretraining achieves competitive performance but accuracy declines across years. Time-incremental approaches improve temporal robustness. Time-aware generation produces more realistic outputs.

Conclusion: CaMiT provides a benchmark for temporal adaptation in fine-grained recognition and generation, addressing realistic continual learning scenarios with evolving classes.

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [133] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: VisiPruner is a training-free pruning framework that reduces up to 99% of vision-related attention computations in MLLMs by leveraging insights about the three-stage cross-modal interaction process.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from significant computational overhead due to quadratic attention growth with multimodal tokens, and existing pruning methods lack fundamental understanding of how MLLMs process multimodal information.

Method: Systematic analysis reveals a three-stage cross-modal interaction: shallow layers recognize task intent, middle layers fuse modalities via critical visual tokens, and deep layers focus on linguistic refinement. Based on this, VisiPruner prunes vision tokens at appropriate layers without training.

Result: VisiPruner reduces up to 99% of vision-related attention computations and 53.9% of FLOPs on LLaVA-v1.5 7B, outperforming existing pruning methods and generalizing across diverse MLLMs.

Conclusion: The findings provide actionable guidelines for training efficient MLLMs by aligning model architecture with intrinsic layer-wise processing dynamics, and VisiPruner offers an effective training-free pruning solution.

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [134] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: This paper compares two frugal federated learning approaches for violence detection: zero-shot/fine-tuned vision-language models (VLMs) and personalized compact 3D CNNs, evaluating accuracy, energy usage, and CO2 emissions in non-IID settings.


<details>
  <summary>Details</summary>
Motivation: To develop energy-efficient and environmentally sustainable AI solutions for violence detection in video surveillance, addressing the need for responsible resource-aware systems.

Method: Compared two strategies: (1) zero-shot and federated fine-tuning of LLaVA-7B vision-language models using LoRA, and (2) personalized training of a compact 65.8M parameter 3D CNN. Evaluated performance under realistic non-IID data distributions.

Result: Both approaches exceeded 90% accuracy. CNN3D slightly outperformed LoRA-tuned VLMs in ROC AUC and log loss while using less energy. VLMs remained better for contextual reasoning and multimodal inference. Quantified energy and CO2 emissions across training and inference.

Conclusion: Proposes a hybrid model: lightweight CNNs for routine classification with selective VLM activation for complex scenarios. Provides a reproducible baseline for responsible, resource-aware AI in video surveillance with sustainability considerations.

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [135] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: A fairness-aware deepfake detection framework that integrates temporal feature learning and demographic-aware data augmentation to improve fairness, interpretability, and generalization across different demographic groups.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods suffer from bias, lack transparency, and fail to capture temporal information, leading to biased decisions and unreliable results across demographic groups.

Method: Proposes sequence-based clustering for temporal modeling of deepfake videos, concept extraction for interpretability, and demographic-aware data augmentation that balances underrepresented groups while preserving deepfake artifacts through frequency-domain transformations.

Result: Extensive experiments on FaceForensics++, DFD, Celeb-DF, and DFDC datasets using Xception and ResNet architectures demonstrate the method achieves the best tradeoff between fairness and accuracy compared to state-of-the-art approaches.

Conclusion: The proposed framework effectively enhances fairness and interpretability in deepfake detection while maintaining high accuracy, addressing key limitations of existing methods.

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [136] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: The paper introduces PICABench, a benchmark for evaluating physical realism in image editing, and finds that current models struggle with maintaining physical consistency.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models focus on completing instructions but overlook physical effects like shadows, reflections, and object interactions, which are crucial for realism.

Method: Proposed PICABench with 8 physical sub-dimensions and PICAEval evaluation protocol using VLM-as-a-judge with human annotations. Also created PICA-100K dataset from videos for training.

Result: Evaluation of mainstream models shows physical realism remains challenging with significant room for improvement.

Conclusion: Physical realism is a key challenge in image editing, and the benchmark provides foundation for moving from naive content editing to physically consistent realism.

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [137] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: IC-MoE is an intelligent communication mixture-of-experts model that enhances medical image segmentation foundation models by improving high-level feature representation while preserving pretrained weight structural integrity through expert voting and semantic-guided contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for medical image segmentation foundation models have limitations in representing high-level features and disrupt the structural integrity of pretrained weights during adaptation.

Method: The method constructs three types of experts (basic, semantic, adaptive) with pixel probability adaptive voting strategy for expert selection and fusion, plus semantic-guided contrastive learning to address weak supervision issues.

Result: Extensive experiments on three public medical image segmentation datasets show IC-MoE outperforms other state-of-the-art models and demonstrates superior generalizability across diverse medical segmentation scenarios.

Conclusion: IC-MoE effectively supplements foundational medical image segmentation models with enhanced high-level features while maintaining pretrained structural integrity, providing a robust solution for medical image segmentation tasks.

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [138] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: Plug-and-Forecast (PnF) is a plug-and-play method that enhances existing motion forecasting models for autonomous driving by integrating multimodal large language models (MLLMs) to handle complex scenarios through natural language descriptions, achieving improved performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems use specialized models that perform well in standard conditions but struggle to generalize cost-effectively to diverse real-world scenarios. There's a need for methods that can quickly adapt to complex driving situations.

Method: PnF extracts structured scene understanding from MLLMs using designed prompts and distills this information into learnable embeddings to augment existing behavior prediction models. It leverages MLLMs' zero-shot reasoning capabilities without requiring fine-tuning.

Result: The method was validated on two state-of-the-art motion forecasting models using Waymo Open Motion Dataset and nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.

Conclusion: PnF provides a practical plug-and-play approach that significantly enhances motion prediction performance in autonomous driving by leveraging MLLMs' natural language understanding capabilities, enabling quick adaptation to complex scenarios without the need for model fine-tuning.

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [139] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: This paper introduces Bi-IRRA, a bidirectional implicit relation reasoning framework for multilingual text-to-image person retrieval that achieves state-of-the-art results by addressing modality heterogeneity and language barriers.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image person retrieval methods face challenges with modality heterogeneity and are English-centric, limiting their application in multilingual contexts. Global methods overlook fine-grained differences while local methods require prior information for part alignments.

Method: Proposes Bi-IRRA framework with: 1) bidirectional implicit relation reasoning module for masked image and text prediction to model local relations across languages and modalities, and 2) multi-dimensional global alignment module to bridge modality heterogeneity. Also creates a multilingual benchmark using LLMs for translation refinement.

Result: Achieves new state-of-the-art results on all multilingual TIPR datasets. The method effectively handles cross-modal and cross-language challenges in person retrieval.

Conclusion: Bi-IRRA successfully addresses modality heterogeneity and language barriers in text-to-image person retrieval through bidirectional implicit relation reasoning and global alignment, establishing a new benchmark for multilingual TIPR research.

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [140] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: SG-CLDFF is a saliency-guided cross-layer deep feature fusion framework that improves white blood cell segmentation and classification by integrating saliency preprocessing with multi-scale feature aggregation, achieving better performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Accurate white blood cell analysis is crucial for hematological diagnosis but faces challenges from staining variability, complex backgrounds, and class imbalance in microscopic images.

Method: Uses saliency priors to highlight WBC regions, a hybrid EfficientSwin-style backbone for multi-resolution features, ResNeXt-CC-inspired cross-layer fusion, multi-task training with segmentation and classification heads, and class-aware weighted losses with saliency-alignment regularization.

Result: Achieves consistent gains in IoU, F1, and classification accuracy on standard benchmarks (BCCD, LISC, ALL-IDB) compared to CNN and transformer baselines, with ablation studies confirming contributions of saliency preprocessing and cross-layer fusion.

Conclusion: SG-CLDFF provides a practical and explainable solution for reliable automated WBC analysis in clinical workflows, with improved robustness and interpretability through saliency guidance and multi-scale feature fusion.

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [141] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: Proposes a two-stage approach with chunking strategy and ensemble method for Parkinson's disease detection from hand-drawn images, achieving high accuracy especially on unseen patient data.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing Parkinson's disease detection methods: lack of sufficient datasets and poor robustness with unseen patient data.

Method: Two-stage approach: 1) Classify drawing types (circle, meander, spiral), 2) Extract features using 2x2 chunking strategy where each chunk is processed separately, then use ensemble method to merge decisions.

Result: Achieved 97.08% accuracy for seen patients and 94.91% for unseen patients on NewHandPD dataset, maintaining only 2.17 percentage point gap compared to 4.76-point drop in prior work.

Conclusion: The proposed chunking and ensemble approach effectively overcomes dataset limitations and improves robustness for Parkinson's disease detection from hand-drawn images, particularly for unseen patients.

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [142] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: A machine vision-based surgical lighting system that uses YOLOv11 to automatically track a blue marker and direct LED lighting, reducing surgeon fatigue and improving illumination consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical lighting requires manual adjustments, causing surgeon fatigue, neck strain, and inconsistent illumination due to drift and shadowing.

Method: Uses YOLOv11 object detection to identify a blue marker placed above the surgical site, then directs a high-power LED light source using two servomotors with tilt-pan brackets.

Result: The YOLO model achieved 96.7% mAP@50 on validation set with annotated surgical scene images containing the blue spherical marker.

Conclusion: This automated lighting solution reduces physical strain on surgeons, improves illumination consistency, and supports better surgical outcomes.

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [143] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in multi-turn video dialogues, addressing limitations of existing single-turn QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM evaluation benchmarks only test single-turn question answering, which doesn't reflect the complexity of real-world multi-turn dialogues in video understanding scenarios.

Method: Created MT-Video-Bench with 987 multi-turn dialogues across diverse domains, assessing six core competencies focused on perceptivity and interactivity, aligned with real-world applications like sports analysis and video tutoring.

Result: Evaluation of various state-of-the-art MLLMs revealed significant performance discrepancies and limitations in handling multi-turn video dialogues.

Conclusion: The benchmark addresses a critical gap in MLLM evaluation and will be publicly available to advance research in multi-turn video understanding capabilities.

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [144] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: Longer training in self-supervised learning can degrade performance on dense prediction tasks like semantic segmentation. The paper introduces a metric called Dense representation Structure Estimator (DSE) to evaluate dense performance without annotations, and proposes model selection and regularization methods to address this degradation.


<details>
  <summary>Details</summary>
Motivation: To address the counterintuitive phenomenon where longer self-supervised learning training actually impairs performance on dense prediction tasks, which they call Self-supervised Dense Degradation (SDD). This degradation occurs consistently across various SSL methods and needs evaluation without annotations.

Method: Proposes Dense representation Structure Estimator (DSE) composed of class-relevance and effective dimensionality measures. Uses DSE for model selection strategy and regularization method to mitigate dense degradation effects.

Result: Experiments across 16 SSL methods and 4 benchmarks show model selection improves mIoU by 3.0% on average with negligible computational cost. DSE regularization consistently mitigates dense degradation effects.

Conclusion: The proposed DSE metric effectively correlates with downstream performance and provides practical solutions through model selection and regularization to address the self-supervised dense degradation problem in SSL training.

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [145] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: This paper investigates feature learning strategies for signature forgery detection, focusing on improving cross-dataset generalization. The study compares raw signature images vs. shell preprocessing methods across three public benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated signature verification is critical for banking and authentication, but current deep learning methods struggle with cross-dataset generalization due to variations in handwriting styles and acquisition protocols.

Method: Used three public benchmarks (CEDAR, ICDAR, GPDS Synthetic) with two experimental pipelines: one based on raw signature images and another using shell preprocessing. Analyzed behavioral patterns across datasets.

Result: The raw-image model achieved higher performance across benchmarks, while the shell-based model showed promising potential for future refinement. No definitive superiority was established between the two approaches.

Conclusion: Both approaches have merits - raw images perform better currently, but shell preprocessing shows promise for developing more robust cross-domain signature verification systems with further refinement.

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [146] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench is the first benchmark for evaluating multimodal models on long video understanding, featuring 1,000 information-dense videos with visual, audio, and text modalities across six challenging task scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for assessing models' ability to understand long videos with rich contextual elements across multiple modalities (visual, audio, text), particularly for content like lectures, interviews, and vlogs.

Method: Created a benchmark with 1,000 long-duration videos from FineVideo dataset, designed six challenging task scenarios (Intra-Event and Inter-Event Tasks), and implemented a three-step semi-automated quality assurance pipeline for question and answer validation.

Result: Experimental results show that omni-modal models still struggle with tasks requiring precise temporal localization and long-range causal inference, and extended experiments reveal information loss and processing bias in multi-modal fusion.

Conclusion: LongInsightBench effectively exposes current limitations in multimodal models for long video understanding, particularly in temporal reasoning and causal inference, highlighting the need for improved multimodal fusion techniques.

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [147] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: A dual-encoder attention framework combining segmented lesions and clinical metadata for more accurate and interpretable skin cancer classification.


<details>
  <summary>Details</summary>
Motivation: Automated skin cancer diagnosis faces challenges with high intra-class variability, subtle differences between classes, and lack of interpretability in deep learning 'black box' models, limiting clinical trust.

Method: Uses Deep-UNet with Dual Attention Gates and ASPP for lesion segmentation, then dual DenseNet201 encoders on original and segmented images with multi-head cross-attention fusion, plus transformer-based module for patient metadata integration.

Result: Achieves state-of-the-art segmentation performance and significantly improved classification accuracy and average AUC on HAM10000, ISIC 2018 and 2019 datasets, with Grad-CAM visualizations confirming lesion-focused predictions.

Conclusion: Integrating precise lesion segmentation and clinical data with attention-based fusion creates a more accurate and interpretable skin cancer classification model that builds clinical trust.

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [148] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba is a scalable framework that overcomes limitations in fMRI-based causal inference by decomposing the problem into BOLD deconvolution and causal graph inference using a Conditional Mamba architecture, achieving superior accuracy and revealing hidden neural dynamics.


<details>
  <summary>Details</summary>
Motivation: Address fundamental limitations in fMRI-based causal inference: the ill-posed nature of inferring neural causality from hemodynamically distorted BOLD signals and the computational intractability of existing methods like Dynamic Causal Modeling (DCM).

Method: Decomposes the complex inverse problem into two tractable stages: BOLD deconvolution to recover latent neural activity, followed by causal graph inference using a novel Conditional Mamba architecture.

Result: Achieves 37% higher accuracy than DCM on simulated data. On real task fMRI data, recovers well-established neural pathways with 88% fidelity, whereas conventional approaches fail in over 99% of subjects. Reveals strategic shifts in brain's primary causal hub between executive and salience networks depending on stimulus.

Conclusion: Provides neuroscientists with a practical tool for large-scale causal inference that captures both fundamental circuit motifs and flexible network dynamics underlying cognitive function.

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [149] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: Adversarial clothes with large coverage can break multiple existing defense methods against adversarial patches, revealing common vulnerabilities in current adversarial defense approaches.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods against adversarial patches were found to fail when patch size is enlarged, and adversarial clothes provide a natural-looking test case with large coverage that can reveal vulnerabilities in current defenses.

Method: Evaluated various defense methods against adversarial clothes in both digital and physical worlds, and crafted a single set of clothes that could break multiple defense methods on Faster R-CNN.

Result: All defense methods performed poorly against adversarial clothes. A single set of clothes achieved 96.06% ASR against undefended detector and over 64.84% ASRs against nine defended models in physical world.

Conclusion: Existing adversarial defense methods have common vulnerabilities against adversarial clothes, highlighting the need for more robust defense approaches that can handle large-scale adversarial patterns.

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [150] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: iDETEX is a unified multimodal LLM that performs quality grounding, perception, and description for detailed and explainable image quality assessment, achieving SOTA performance on ViDA-UGC benchmark and winning ICCV MIPI 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: To address the emerging challenge of detailed and explainable IQA beyond simple scalar quality prediction, enabling more human-aligned and interpretable evaluation paradigms.

Method: Proposes iDETEX - a unified MLLM with task-specific offline augmentation modules, data mixing strategy, and online enhancement strategies to exploit multi-sourced supervision across three heterogeneous subtasks.

Result: Achieves state-of-the-art performance on ViDA-UGC benchmark across all subtasks and ranks first in ICCV MIPI 2025 Detailed Image Quality Assessment Challenge.

Conclusion: iDETEX demonstrates effectiveness and robustness in delivering accurate and interpretable quality assessments through unified multimodal approach.

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [151] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: Proposes a post-processing Open-set Recognition method that measures agreement between feature space (NCM distances) and logit space (softmax probabilities) to identify unknown classes without retraining.


<details>
  <summary>Details</summary>
Motivation: Current wildlife classification models fail on unknown classes and require retraining for OSR. Need for a post-processing approach that works with pre-trained models.

Method: Measures agreement between NCM-based probability distribution (from feature space) and softmax probabilities (from logit space) to detect unknown samples.

Result: Achieves AUROC of 93.41 (African animals) and 95.35 (Swedish animals), ranking top-3 on both datasets with consistent performance.

Conclusion: The proposed post-processing OSR method effectively identifies unknown classes without model retraining and shows consistent cross-dataset performance.

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [152] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Semantic-E2VID is an event-to-video reconstruction framework that incorporates semantic information from vision foundation models to enhance reconstruction quality by addressing the lack of semantic data in event camera outputs.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture only intensity changes and lack semantic information about static objects and backgrounds, which limits the quality of event-to-video reconstruction. Existing E2V approaches overlook the importance of semantic information in video reconstruction.

Method: The framework uses a cross-modal feature alignment module to transfer visual semantics from SAM to the event encoder, a semantic-aware feature fusion block to integrate learned semantics, and semantic perceptual supervision using SAM-generated labels.

Result: Extensive experiments show Semantic-E2VID significantly enhances frame quality and outperforms state-of-the-art E2V methods across multiple benchmarks.

Conclusion: Incorporating semantic information from vision foundation models effectively addresses the semantic gap in event-to-video reconstruction, leading to improved reconstruction quality.

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [153] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: A training-free method for Video-LLMs that enables efficient streaming video understanding by selecting important visual tokens, processing them recurrently, and using caption-based QA.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with streaming scenarios where hour-long videos must be processed online with timely responses, due to computational inefficiency.

Method: Three key components: 1) LLM-informed visual token selection to identify and discard ~95% unimportant tokens, 2) Recurrent processing of past selected tokens for temporal coherence, 3) Caption-based question answering for lightweight responses.

Result: Achieves state-of-the-art performance on streaming video benchmarks while maintaining efficiency and effectiveness.

Conclusion: The proposed training-free approach enables Video-LLMs to handle streaming video scenarios efficiently by balancing computational efficiency with understanding accuracy.

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [154] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: Synthetic facial recognition datasets can achieve comparable or better accuracy than real datasets while preserving privacy and enabling bias mitigation, making them a viable ethical alternative.


<details>
  <summary>Details</summary>
Motivation: The ethical dilemma of using real facial data collected without consent, leading to dataset retractions and legal liabilities under regulations like GDPR, creates a need for privacy-preserving alternatives.

Method: Systematic literature review identifying 25 synthetic facial recognition datasets (2018-2025) combined with rigorous experimental validation examining seven key privacy requirements, involving over 10 million synthetic samples and comparison on five standard benchmarks.

Result: Best-performing synthetic datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and 94.91% respectively, surpassing established real datasets including CASIA-WebFace (94.70%). Synthetic data ensures proper intra-class variability while maintaining identity separability and offers unprecedented control for bias mitigation.

Conclusion: Synthetic facial data is scientifically viable and ethically imperative for facial recognition research, providing comparable performance to real datasets while addressing privacy and bias concerns.

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [155] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: A new method for Parkinson's disease severity diagnosis using multiple facial expression features with attention-based fusion and adaptive class balancing to address class imbalance issues.


<details>
  <summary>Details</summary>
Motivation: Current PD diagnosis methods rely on single facial expressions leading to misdiagnosis, ignore class imbalance across PD stages, and focus only on binary classification rather than severity diagnosis.

Method: Integrates multiple facial expression features through attention-based feature fusion and uses adaptive class balancing strategy that dynamically adjusts training sample contributions based on class distribution and classification difficulty.

Result: Experimental results demonstrate promising performance for PD severity diagnosis and confirm the efficacy of both attention-based feature fusion and adaptive class balancing.

Conclusion: The proposed method effectively addresses limitations of existing approaches by leveraging multiple facial expressions and handling class imbalance, enabling more accurate PD severity diagnosis.

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [156] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans is a closed-loop framework that enables bidirectional knowledge transfer between exocentric and egocentric views for affordance grounding, improving performance in complex interaction scenarios.


<details>
  <summary>Details</summary>
Motivation: Previous weakly-supervised affordance grounding methods only transfer knowledge one-way from exocentric to egocentric images, limiting their applicability in complex interaction scenarios where object regions may be occluded.

Method: Introduces a closed-loop framework with unified cross-modal localization and denoising knowledge distillation to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images.

Result: Achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body.

Conclusion: The bidirectional knowledge transfer in LoopTrans significantly enhances affordance grounding performance and handles occlusion challenges better than previous one-way transfer approaches.

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [157] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: A vision-based system using YOLOv11 and BoT-SORT to automatically detect and track horses and people in stables, distinguishing five event types while accounting for camera blind spots.


<details>
  <summary>Details</summary>
Motivation: Manual monitoring of stalled horses is labor-intensive and time-consuming, making automated systems essential for early detection of health and welfare issues.

Method: Uses object detection (YOLOv11) and multi-object tracking (BoT-SORT) to detect/track horses and people, with event inference based on object trajectories and spatial relations. Created custom dataset using CLIP and GroundingDINO for annotation.

Result: Qualitative evaluation showed reliable performance for horse-related events but limitations in detecting people due to data scarcity. System successfully accounts for camera blind spots.

Conclusion: Provides foundation for real-time behavioral monitoring in equine facilities with implications for animal welfare and stable management.

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [158] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect is a dense keypoint detector that combines classical detector outputs to train a lightweight ESPNet model, achieving superior density, repeatability, and matching performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing keypoint detectors (both traditional and learning-based) suffer from limitations including sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding of visually important regions.

Method: Fuse outputs from 7 keypoint and 2 edge detectors to create ground-truth masks, then train a lightweight ESPNet model using these masks as labels to enable semantic focus and dense keypoint detection adaptable to diverse conditions.

Result: On Oxford Affine Covariant Regions dataset: achieves maximum values of 0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003 (correct matches), surpassing other detectors in all metrics.

Conclusion: DeepDetect successfully unifies classical detector strengths through deep learning, producing highly dense and repeatable keypoints that adapt well to challenging visual conditions while maintaining semantic focus on important image regions.

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [159] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: Using AV1 motion vectors for dense sub-pixel correspondences and short tracks, achieving comparable performance to SIFT with less CPU usage and denser matches.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient compressed-domain front end for computer vision pipelines that can scale effectively.

Method: Repurposing AV1 motion vectors to produce dense sub-pixel correspondences and short tracks filtered by cosine consistency.

Result: On short videos, the method runs comparably to sequential SIFT with less CPU usage, yields denser matches with competitive pairwise geometry, and successfully reconstructs 0.46-0.62M points at 0.51-0.53px reprojection error in a 117-frame SfM demo.

Conclusion: Compressed-domain correspondences are a practical, resource-efficient front end with clear paths to scaling in full pipelines.

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [160] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: This paper introduces HQ-NightRain, a high-quality benchmark for nighttime image deraining, and proposes CST-Net, a Color Space Transformation Network that uses learnable color space conversion and implicit illumination guidance to effectively remove rain from nighttime scenes.


<details>
  <summary>Details</summary>
Motivation: Nighttime image deraining is more challenging than daytime deraining due to complex nighttime scenarios and the lack of high-quality datasets that properly represent the coupling between rain and illumination effects.

Method: The authors develop CST-Net with a learnable color space converter (CSC) to facilitate rain removal in the Y channel where nighttime rain is more pronounced, and introduce implicit illumination guidance to capture illumination information for better robustness in complex nighttime scenarios.

Result: Extensive experiments demonstrate the value of the proposed HQ-NightRain dataset and the effectiveness of the CST-Net method for nighttime image deraining.

Conclusion: The paper provides both a high-quality benchmark dataset and an effective network architecture that addresses the unique challenges of nighttime image deraining through color space transformation and illumination guidance.

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [161] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: The paper proposes a novel initialization strategy for sparse-view 3D Gaussian Splatting that improves novel view rendering by enhancing Structure-from-Motion coverage and supplementing missing regions through frequency-aware techniques, self-initialization, and point-cloud regularization.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS tends to overfit to training views, causing blurring in novel views. Prior methods focus on training-time constraints, but this work reveals that initialization quality is the decisive factor that determines performance limits, while training constraints only provide modest improvements.

Method: 1) Frequency-aware SfM that improves coverage in low-texture areas using low-frequency view augmentation and relaxed multi-view correspondences; 2) 3DGS self-initialization that creates additional points from photometric supervision; 3) Point-cloud regularization that ensures multi-view consistency and uniform spatial coverage through geometric/visibility priors.

Result: Experiments on LLFF and Mip-NeRF360 datasets demonstrate consistent performance gains in sparse-view settings, establishing the proposed approach as a stronger initialization strategy compared to existing methods.

Conclusion: Initialization quality is the primary determinant of sparse-view 3DGS performance. The proposed three-component initialization strategy effectively supplements SfM's limitations and provides a more reliable foundation for 3D Gaussian Splatting in sparse-view scenarios.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [162] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet introduces an entropy-guided dual-clustering approach for unsupervised salient object detection, combining spectral clustering for high-entropy pixels and k-means for low-entropy pixels, then aligning them with optimal transport to generate high-quality pseudo-masks that train a MaskFormer-style model.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised SOD methods struggle with reliable pseudo-mask generation. The authors observe that boundary and interior pixels have different geometric properties, and optimal transport's global consistency is underutilized when prototype quality is poor.

Method: POTNet replaces single k-means with entropy-guided dual clustering: spectral clustering for high-entropy pixels, k-means for low-entropy pixels, then optimal transport alignment. This generates sharp pseudo-masks in one forward pass, which supervise a MaskFormer-style encoder-decoder (AutoSOD).

Result: AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure on five benchmarks, while eliminating SelfMask's offline voting and improving training efficiency.

Conclusion: The proposed entropy-guided dual-clustering with optimal transport alignment enables near-supervised accuracy in salient object detection without pixel-level labels, significantly narrowing the gap between unsupervised and fully supervised methods.

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [163] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet is a weakly-supervised method for road crack detection that uses only image-level labels instead of costly pixel-level annotations, achieving comparable performance to supervised methods through adversarial learning between classifier, reconstructor, and detector components.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on expensive pixel-level annotations for road crack detection in smart city infrastructure maintenance, enabling more scalable road inspection.

Method: End-to-end weakly-supervised framework with three components: classifier generating CAMs, reconstructor measuring feature inferability, and detector producing pixel-wise results. Uses adversarial learning between classifier and reconstructor, path-aware attention module (PAAM) for feature fusion, and center-enhanced CAM consistency module (CECCM) for pseudo-label refinement.

Result: Achieves comparable results to supervised methods and outperforms existing weakly-supervised methods on three image-level datasets, significantly advancing scalable road inspection.

Conclusion: WP-CrackNet demonstrates that weakly-supervised learning with only image-level labels can achieve competitive road crack detection performance, making large-scale infrastructure inspection more practical and cost-effective.

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [164] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D extends VGGT to handle dynamic scenes by introducing a dynamics-aware aggregator that disentangles static and dynamic information, enabling better camera pose estimation, depth prediction, and point cloud reconstruction in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing 3D feed-forward models like VGGT struggle with dynamic elements in real-world scenes because they are trained on static datasets, limiting their effectiveness in scenarios with moving humans or deformable objects.

Method: Proposes PAGE-4D with a dynamics-aware aggregator that predicts a dynamics-aware mask to suppress motion cues for pose estimation while amplifying them for geometry reconstruction, resolving the conflict between these tasks.

Result: Extensive experiments show PAGE-4D consistently outperforms VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular/video depth estimation, and dense point map reconstruction.

Conclusion: PAGE-4D successfully extends static 3D models to dynamic scenes by addressing the fundamental conflict between pose estimation and geometry reconstruction through dynamics-aware information disentanglement.

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [165] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces UCIS4K, the first underwater camouflaged instance segmentation dataset with 3,953 images, and proposes UCIS-SAM network with three novel modules to address underwater degradation challenges in segmenting camouflaged marine organisms.


<details>
  <summary>Details</summary>
Motivation: Underwater vision tasks face challenges due to degraded environments (color distortion, low contrast, blurring), and traditional methods trained on terrestrial datasets perform poorly on underwater camouflaged instance segmentation.

Method: Proposed UCIS-SAM network based on Segment Anything Model with three key modules: Channel Balance Optimization Module for underwater feature learning, Frequency Domain True Integration Module to reduce camouflage pattern interference, and Multi-scale Feature Frequency Aggregation Module for boundary enhancement.

Result: Extensive experiments on UCIS4K and public benchmarks show that UCIS-SAM outperforms state-of-the-art approaches in underwater camouflaged instance segmentation.

Conclusion: The proposed UCIS4K dataset and UCIS-SAM network effectively address the challenges of underwater camouflaged instance segmentation, demonstrating superior performance compared to existing methods.

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [166] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft is a multi-agent framework that generates structured, textured 3D assets from natural language using a Graph-based Procedural Shape representation, enabling better spatial understanding and interactive editing.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods produce unstructured meshes with poor interactivity, making them impractical for artistic workflows. There's a need for structured, interactive 3D generation that can be easily customized and edited.

Method: Proposes ShapeCraft with Graph-based Procedural Shape (GPS) representation that decomposes complex language into structured sub-task graphs. Uses LLM agents to hierarchically parse user input, initialize GPS, and iteratively refine procedural modeling and painting.

Result: Qualitative and quantitative experiments show superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based methods. Also demonstrates versatility through animated and user-customized editing examples.

Conclusion: ShapeCraft enables structured, textured, and interactive 3D generation from natural language, with potential for broader interactive applications including animation and custom editing workflows.

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [167] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: Proposes a machine learning framework using UAV-scanned point clouds and synthetic BIM data for automated segmentation of 3D infrastructure models, reducing manual labeling requirements.


<details>
  <summary>Details</summary>
Motivation: To overcome the time-consuming and error-prone manual labeling process required for segmenting structural components from UAV-captured 3D models in structural health monitoring.

Method: Combines real-world UAV-scanned point clouds with synthetic data generated from Building Information Modeling (BIM) using a machine learning-based framework for automated segmentation.

Result: Demonstrated high accuracy in identifying and segmenting railroad track components (rails and crossties), with significantly reduced training time while maintaining reasonable segmentation accuracy using smaller datasets supplemented with BIM data.

Conclusion: The automated approach improves precision and efficiency of 3D infrastructure model segmentation and advances the integration of UAV and BIM technologies for structural health monitoring and infrastructure management.

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [168] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2 is a unified framework for unsupervised anomaly detection that bridges performance gaps in multi-class models while extending across diverse data modalities and task settings using a simple reconstruction-based approach with five key elements.


<details>
  <summary>Details</summary>
Motivation: Existing multi-class anomaly detection models underperform compared to specialized single-class models, and the field has fragmented into scenario-specific methods, creating deployment barriers and highlighting the need for a unified solution.

Method: A reconstruction-based framework guided by 'less is more' philosophy, orchestrating five simple elements to achieve superior performance without modification across diverse tasks.

Result: Achieves unprecedented 99.9% and 99.3% image-level AUROC on MVTec-AD and VisA respectively for multi-class models, state-of-the-art performance in multi-view and multi-modal inspection, and surpasses previous full-shot models using only 8 normal examples per class.

Conclusion: Dinomaly2's minimalistic design, computational scalability, and universal applicability position it as a unified solution for the full spectrum of real-world anomaly detection applications.

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [169] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: DINO-CV is a self-supervised segmentation framework that uses LiDAR-derived DEMs to automatically map low-lying dry-stone walls in vegetated areas, achieving 68.6% mIoU with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Dry-stone walls are important heritage structures but remain largely unmapped due to inaccessibility and high manual mapping costs. Two key challenges are visual occlusion by vegetation and limited labeled training data.

Method: Uses high-resolution Airborne LiDAR-derived DEMs to overcome vegetation occlusion. Introduces self-supervised cross-view pre-training with knowledge distillation to learn invariant representations across multiple DEM derivatives. Supports various vision backbones including ResNet, Wide ResNet, and Vision Transformers.

Result: Achieved 68.6% mIoU on test areas in Budj Bim cultural landscape, and maintained 63.8% mIoU when fine-tuned with only 10% labeled data. Successfully identified dense collections of colonial dry-stone walls.

Conclusion: Demonstrates the potential of self-supervised learning on high-resolution DEM derivatives for automated dry-stone wall mapping in vegetated heritage environments with scarce annotations.

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [170] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer is a novel framework for real-time 4D panoptic segmentation in streaming settings, using a Dual-Thread System to efficiently process frames and enable real-time capability for dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Real-time, fine-grained perception is critical for highly dynamic environments like crowd evacuation and autonomous driving, where processing within constrained time budgets is essential.

Method: Uses a Dual-Thread System with predictive thread (leverages historical motion/geometric info to forecast future dynamics) and inference thread (ensures timely prediction by aligning with latest memory and compensating for ego-motion/dynamic movements).

Result: Demonstrates superior robustness under high FPS conditions, accurately predicts dynamic objects in complex scenes across indoor HOI4D and outdoor SemanticKITTI/nuScenes datasets.

Conclusion: 4DSegStreamer effectively enables real-time 4D panoptic segmentation in streaming settings and can be seamlessly integrated into existing 3D/4D segmentation methods.

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [171] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det is a class-agnostic open-world 3D detector that detects any objects in 3D scenes without text prompts, leveraging 2D foundation models and cross-modal fusion to achieve generalized 3D object discovery.


<details>
  <summary>Details</summary>
Motivation: Traditional closed-set 3D detectors struggle with open-world scenarios, and existing open-vocabulary models face issues with vocabulary expansion and semantic overlap. There's insufficient research on learning generalized 3D objectness for detecting novel objects unseen during training.

Method: Proposes OP3Det using 2D foundation models for semantic and geometric priors to generate class-agnostic proposals. Integrates cross-modal mixture of experts that dynamically routes uni-modal and multi-modal features from point cloud and RGB image data to learn generalized 3D objectness.

Result: Significantly outperforms existing open-world 3D detectors by up to 16.0% in AR and achieves 13.5% improvement over closed-world 3D detectors in extensive experiments.

Conclusion: OP3Det demonstrates extraordinary performance in open-world 3D object detection, effectively addressing the limitations of traditional closed-set detectors and existing open-vocabulary approaches through its prompt-free, class-agnostic design and cross-modal fusion strategy.

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [172] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: The paper introduces Generalized Adversarial Solver (GAS), a simple ODE sampler parameterization that combines distillation loss with adversarial training to improve diffusion model sampling efficiency while preserving fine-grained details.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from computationally expensive sampling despite achieving state-of-the-art generation quality. Existing gradient-based optimization methods for distillation require intricate training techniques and don't explicitly focus on preserving fine-grained details.

Method: Proposes Generalized Solver - a simple ODE sampler parameterization without additional training tricks, combined with adversarial training to mitigate artifacts and enhance detail fidelity. The method uses both original distillation loss and adversarial training.

Result: The Generalized Adversarial Solver demonstrates superior performance compared to existing solver training methods under similar resource constraints, reducing function evaluations from dozens to just a few while improving quality.

Conclusion: The proposed Generalized Adversarial Solver provides an effective approach for efficient diffusion model sampling that maintains generation quality and detail fidelity without requiring complex training techniques.

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [173] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT is a post-pretraining structured pruning method for Vision Transformers that enables elastic inference across compute budgets without retraining or labeled data, using evolutionary algorithms and gradient information.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models come in fixed sizes, forcing suboptimal deployment choices under real-world computational constraints. There's a need for methods that can adapt pretrained models to various compute budgets efficiently.

Method: Combines gradient information with cross-network structure correlations approximated via evolutionary algorithm. Uses self-supervised importance scoring and does not require labeled data, retraining, or classification heads.

Result: Superior performance over state-of-the-art methods across various sparsities on DINO, SigLIPv2, DeIT, and AugReg models. Generates elastic models in under 5 minutes on a single A100 GPU that can be adjusted to any computational budget.

Conclusion: SnapViT provides an efficient pruning strategy for pretrained Vision Transformers with a novel evolutionary approximation of Hessian off-diagonal structures, enabling flexible deployment without performance degradation.

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [174] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: A new computational framework using YOLOv11 fine-tuning achieves over 95% accuracy in classifying circulating blood cell clusters and identifying cell types within them from flow cytometry images.


<details>
  <summary>Details</summary>
Motivation: Current computational approaches focus on single-cell analysis, but there's a lack of tools for analyzing irregular-shaped cell clusters containing multiple cell types, which are important biomarkers for thrombosis, infection, and inflammation.

Method: Two-step framework: 1) Fine-tuned YOLOv11 model classifies images into cell cluster vs non-cluster groups, outperforming CNNs and ViT; 2) Identifies cell types by overlaying cluster contours with multi-channel fluorescence stain regions, handling cell debris and staining artifacts.

Result: Achieved over 95% accuracy in both cluster classification and phenotype identification, demonstrating effective automated analysis of CCC images from flow cytometry.

Conclusion: The automated framework successfully analyzes CCC images using bright-field and fluorescence data, with potential for broader applications in immune and tumor cell cluster analysis across various diseases.

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [175] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS is a comprehensive benchmark that evaluates 3D Gaussian Splatting (3DGS) under real-world raindrop conditions, addressing limitations of existing synthetic benchmarks by testing the full pipeline from unconstrained raindrop-corrupted images to 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: 3DGS performance degrades significantly under raindrop conditions due to occlusions and distortions, but existing benchmarks use synthetic data with known poses and don't account for real-world challenges like pose estimation errors and domain gaps between synthetic and real raindrops.

Method: The benchmark includes three parts: data preparation (collecting real-world dataset with three aligned image sets), data processing, and raindrop-aware 3DGS evaluation covering raindrop interference types, camera pose estimation, point cloud initialization, single image rain removal, and 3D Gaussian training.

Result: The study reveals critical insights about performance limitations of existing 3DGS methods on unconstrained raindrop images, the impact of camera focus position on reconstruction quality, and interference from inaccurate pose and point cloud initialization.

Conclusion: RaindropGS establishes clear directions for developing more robust 3DGS methods under raindrop conditions by comprehensively evaluating the full pipeline and identifying key performance bottlenecks in real-world scenarios.

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [176] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: The paper investigates whether diffusion transformer-based image-to-video models can generate realistic pedestrian movement patterns in crowded scenes by conditioning on trajectory keyframes.


<details>
  <summary>Details</summary>
Motivation: To explore if large-scale video-trained I2V models with inherent world-modeling capabilities can realistically simulate pedestrian dynamics in crowded public spaces.

Method: Conditions I2V models on keyframes from pedestrian trajectory benchmarks and evaluates trajectory prediction performance using quantitative measures of pedestrian dynamics.

Result: The study examines the capability of these models to generate realistic pedestrian movement patterns.

Conclusion: The research demonstrates the potential of I2V models for simulating pedestrian dynamics, though specific performance outcomes would depend on the quantitative evaluation results.

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [177] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: Training-free multi-reference VPR method using matrix decomposition for basis representations and residual matching, achieving up to 18% improvement in Recall@1 over single-reference approaches.


<details>
  <summary>Details</summary>
Motivation: To improve visual place recognition performance using multiple reference sets under varying conditions without incurring high computational costs from training or complex models.

Method: Descriptor-agnostic approach that jointly models places using multiple reference descriptors via matrix decomposition into basis representations, enabling projection-based residual matching.

Result: Improves Recall@1 by up to ~18% over single-reference methods and outperforms multi-reference baselines across appearance and viewpoint changes, with ~5% gains on unstructured data.

Conclusion: The proposed method demonstrates strong generalization while remaining lightweight, offering effective multi-reference VPR without training requirements.

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [178] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA is a new paradigm for efficient Vision Language Model inference that decouples visual sparsity across prefilling and decoding stages to reduce latency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models face scalability limitations due to the growing number of visual tokens that dominate inference latency, requiring more efficient inference methods.

Method: SparseVILA decouples visual sparsity by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding, built on an AWQ-optimized inference pipeline.

Result: Achieves up to 4.0× faster prefilling, 2.5× faster decoding, and 2.6× overall speedup on long-context video tasks while improving accuracy on document-understanding and reasoning tasks.

Conclusion: SparseVILA establishes a new direction for efficient multimodal inference with a training-free, architecture-agnostic framework that accelerates large VLMs without sacrificing capability.

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [179] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA is a foundation model that bridges multimodal computer-use agents with programmatic tools through hybrid actions, combining GUI primitives with high-level tool calls to reduce cascading failures and improve performance.


<details>
  <summary>Details</summary>
Motivation: Current computer-use agents rely exclusively on primitive GUI actions (click, type, scroll) which lead to cascading failures and performance bottlenecks, while being isolated from rich programmatic interfaces available to other agents.

Method: Four-component approach: automated pipeline for scaling programmatic tools from documentation and code; synthetic data engine producing 17,000+ verifiable tasks; large-scale hybrid action trajectory collection; two-stage training combining supervised fine-tuning with online reinforcement learning.

Result: 7B and 32B models show 22% relative improvement on OSWorld, 11% faster execution, and 21.7% success rate on WindowsAgentArena, outperforming baselines trained on Windows data.

Conclusion: Hybrid action mechanism is critical for reducing error propagation while maintaining execution efficiency, successfully bridging the gap between GUI primitives and programmatic tools in computer-use agents.

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [180] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph is a framework that renders long texts into images for vision-language models to achieve 3-4x token compression while maintaining accuracy comparable to leading LLMs, enabling efficient million-token-level text processing.


<details>
  <summary>Details</summary>
Motivation: Scaling context windows to million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. The authors aim to tackle this challenge through visual context scaling instead of extending token-based sequences.

Method: Propose Glyph framework that renders long texts into images and processes them with vision-language models (VLMs). Use LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression.

Result: Achieves 3-4x token compression while maintaining accuracy comparable to Qwen3-8B on long-context benchmarks. Provides 4x faster prefilling and decoding, and 2x faster SFT training. Enables 128K-context VLM to handle 1M-token-level text tasks under extreme compression.

Conclusion: Visual context scaling through text rendering offers an effective alternative to token-based sequence extension, providing substantial compression and efficiency gains for long-context modeling while maintaining performance.

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: ConsistEdit is a novel attention control method for MM-DiT models that enables consistent text-guided editing across images and videos by using vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of query, key, and value tokens.


<details>
  <summary>Details</summary>
Motivation: Current training-free attention control methods struggle to balance strong editing strength with source consistency, especially in multi-round and video editing where errors accumulate. Existing methods enforce global consistency, limiting fine-grained editing of individual attributes while preserving others.

Method: Proposed ConsistEdit method built on MM-DiT architecture with three key components: vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of query, key, and value tokens. It performs editing across all inference steps and attention layers without handcraft.

Result: Achieves state-of-the-art performance across image and video editing tasks in both structure-consistent and structure-inconsistent scenarios. Enables robust multi-round and multi-region editing with progressive adjustment of structural consistency.

Conclusion: ConsistEdit is the first approach to perform consistent editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and enabling finer control over structural consistency in text-guided editing tasks.

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign is a framework that enhances multimodal safety alignment in Large Vision-Language Models using prompt-guided tree search to embed safety constraints into reasoning processes and detect risks in real-time.


<details>
  <summary>Details</summary>
Motivation: Existing defenses are vulnerable to multimodal jailbreaks due to new attack surfaces from visual inputs, lack of safety supervision in reasoning chains, and degraded alignment under modality fusion.

Method: Uses visual-textual interactive prompts to embed safety constraints, employs Monte Carlo Tree Search to construct diverse safety-critical prompt trajectories, and implements prompt-based scaling for real-time risk detection.

Result: Extensive experiments show VisuoAlign proactively exposes risks, enables comprehensive dataset generation, and significantly improves LVLM robustness against complex cross-modal threats.

Conclusion: VisuoAlign provides an effective framework for multimodal safety alignment that addresses key vulnerabilities in current LVLM defenses through systematic prompt-guided tree search.

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [183] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: The paper introduces Structured Cognitive Loop (SCL) - an executable epistemological framework that bridges philosophy and AI by defining intelligence as a continuous loop process rather than a property.


<details>
  <summary>Details</summary>
Motivation: Large language models lack genuine epistemic understanding, exposing the need for epistemic architecture. Traditional AI focuses on 'what is intelligence?' while SCL asks 'under what conditions does cognition emerge?'

Method: SCL operationalizes philosophical insights into computationally interpretable structures, creating an executable epistemology framework grounded in process philosophy, enactive cognition, and extended mind theory.

Result: Functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic systems. Intelligence is redefined as capacity to reconstruct epistemic state through intentional understanding.

Conclusion: SCL bridges philosophy and AI, enabling theories of cognition to be enacted and tested. Real progress requires architectures that realize cognitive principles structurally, not just larger models.

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [184] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: Citiverses (virtual city environments) can serve as experimental spaces for regulatory learning by testing policy scenarios and technologies through expert consultation and analysis of potential applications in transportation, urban planning, and climate crisis.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of citiverses as immersive virtual environments for regulatory experimentation and policy learning, addressing the need for safe testing spaces before real-world implementation.

Method: Conducted consultation with high-level expert panel including European Commission policymakers, national government science advisers, and leading researchers in digital regulation and virtual worlds, followed by analysis of experimental topics across various domains.

Result: Identified key research areas including scalability, real-time feedback, complexity modeling, cross-border collaboration, risk reduction, citizen participation, ethical considerations, and emerging technology integration. Analyzed specific experimental topics in transportation, urban planning, and environmental/climate crisis domains.

Conclusion: Citiverses show significant potential as regulatory learning spaces but require responsible development with careful consideration of ethical, economic, ecological and social dimensions, and integration with existing experimentation ecosystems like test beds and regulatory sandboxes.

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [185] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA is a psych-inspired unified memory system for AI agents that treats memory as a constructive and adaptive process, featuring trimodal adaptation mechanisms and hybrid memory access to improve adaptability and long-term knowledge retention.


<details>
  <summary>Details</summary>
Motivation: Existing AI memory systems lack adaptability to diverse tasks and overlook the constructive, task-oriented role of memory, drawing inspiration from Piaget's cognitive development theory to address these limitations.

Method: Proposes PISA with trimodal adaptation mechanism (schema updation, evolution, creation) and hybrid memory access architecture combining symbolic reasoning with neural retrieval.

Result: Empirical evaluation on LOCOMO benchmark and new AggQA benchmark shows PISA sets new state-of-the-art by significantly enhancing adaptability and long-term knowledge retention.

Conclusion: PISA successfully addresses limitations of existing memory systems through its constructive approach and adaptation mechanisms, demonstrating superior performance in adaptability and knowledge retention.

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [186] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: LLMs with environment interfaces for Tower of Hanoi still experience performance collapse beyond certain complexity thresholds, showing mode-like collapse rather than true reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM performance collapse on complex puzzles is due to the requirement of tracking state space independently, by providing an environment interface.

Method: Provided LLMs with an environment interface for Tower of Hanoi problems, allowing tool calls for moves, written justifications, state observation, and self-reprompting.

Result: Environment access didn't prevent performance collapse. Policy analysis revealed increasing divergence from optimal and random policies, showing mode-like collapse at each complexity level.

Conclusion: LLMs exhibit mode-like collapse in complex reasoning tasks, where performance depends on whether their learned mode matches the correct solution, suggesting similar phenomena in Large Reasoning Models.

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [187] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: Cognitive Load Traces (CLTs) is a mid-level interpretability framework for deep models that quantifies model-internal resource allocation through symbolic, temporally varying functions representing Intrinsic, Extraneous, and Germane load components.


<details>
  <summary>Details</summary>
Motivation: Inspired by Cognitive Load Theory in human cognition, the authors aim to develop an interpretability framework that can analyze reasoning dynamics in deep models by quantifying internal resource allocation patterns.

Method: CLTs are represented as a three-component stochastic process (IL_t, EL_t, GL_t) corresponding to Intrinsic, Extraneous, and Germane load, instantiated through measurable proxies like attention entropy, KV-cache miss ratio, representation dispersion, and decoding stability. The framework includes symbolic formulations and visualization methods (load curves, simplex diagrams).

Result: Experiments on reasoning and planning benchmarks demonstrate that CLTs can predict error-onset, reveal cognitive strategies, and enable load-guided interventions that improve reasoning efficiency by 15-30% while maintaining accuracy.

Conclusion: CLTs provide an effective interpretability framework for analyzing deep model reasoning dynamics, offering both diagnostic capabilities and practical interventions to improve model efficiency without sacrificing accuracy.

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [188] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow is a novel autoformalization pipeline that preserves logical structure by constructing dependency graphs and using lemma-based formalization, achieving state-of-the-art results on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Current autoformalization approaches produce executable code but frequently fail to preserve semantic meaning and logical structure of original mathematical proofs, limiting their integration into rigorous mathematical workflows.

Method: ProofFlow constructs a directed acyclic graph (DAG) to map logical dependencies between proof steps, then employs a lemma-based approach to systematically formalize each step as an intermediate lemma, preserving the original argument's logical structure.

Result: ProofFlow achieves a ProofScore of 0.545 on a new benchmark of 184 undergraduate-level problems, substantially exceeding baselines like full-proof formalization (0.123) and step-proof formalization (0.072).

Conclusion: The ProofFlow pipeline, benchmark, and evaluation metric provide a comprehensive framework for advancing proof autoformalization, with structural fidelity as a primary objective enabling better integration of LLMs into mathematical workflows.

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [189] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: The paper presents a vision for creating a knowledge graph from the MO|RE sports science data repository to standardize and make motor performance data machine-understandable.


<details>
  <summary>Details</summary>
Motivation: To enable better evaluation and comparison of physical and cognitive capabilities between populations by transforming how motor performance data are modeled and shared across studies.

Method: Develop a knowledge graph using an ontology rooted in Basic Formal Ontology, focusing on formally representing the interrelation of plan specifications, specific processes, and related measurements.

Result: A proposed infrastructure for publishing and archiving research data in sports science, particularly in motor performance research.

Conclusion: The approach aims to standardize motor performance data and make it machine-understandable, developed within the Leibniz Science Campus Digital Transformation of Research (DiTraRe) initiative.

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [190] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: This paper proposes a new conflict measure method for Random Permutation Sets (RPS) by analyzing conflicts from both random finite set and Dempster-Shafer theory perspectives, using an inconsistency measure inspired by rank-biased overlap.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need to measure conflict between evidence represented by permutation mass functions in order-structured uncertain information fusion, as RPS is a new formalism for reasoning with uncertainty involving order information.

Method: The authors define an inconsistency measure between permutations inspired by rank-biased overlap (RBO), propose a non-overlap-based conflict measure method for RPSs, and treat RPS theory as an extension of Dempster-Shafer theory with order information in focal sets.

Result: The proposed method demonstrates natural top-weightedness property, effectively measures conflict between RPSs from DST perspective, and provides decision-makers with flexible selection of weights, parameters, and truncated depths through numerical examples.

Conclusion: The conflict measure method successfully addresses the need for measuring conflicts in RPS by leveraging insights from both random finite set theory and Dempster-Shafer theory, while maintaining the important property of top-weightedness for order information.

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [191] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT introduces parallel-in-time neural twins that model dynamical systems from measurements, maintaining on-trajectory state estimation unlike autoregressive models, with demonstrated success in turbulent fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: To create neural twins as digital replicas of real systems that can consume measurements at test time to update their state, enabling context-specific decision-making while remaining on-trajectory with the true system state.

Method: PAINT trains a generative neural network to model the distribution of states parallel over time, using a sliding window approach at test time to predict states from measurements.

Result: Theoretical analysis shows PAINT is on-trajectory while autoregressive models are not. Empirical evaluation on 2D turbulent fluid dynamics demonstrates PAINT stays on-trajectory and predicts system states from sparse measurements with high fidelity.

Conclusion: PAINT enables development of neural twins that remain on-trajectory, facilitating more accurate state estimation and decision-making in dynamical systems.

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [192] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: ISGFAN is a robust cross-domain fault diagnosis framework that addresses noise interference and domain shifts through information separation and global-focal adversarial learning.


<details>
  <summary>Details</summary>
Motivation: Existing transfer fault diagnosis methods fail in industrial environments with severe noise interference and domain shifts, limiting their practical effectiveness.

Method: Uses information separation architecture with adversarial learning and orthogonal loss to decouple domain-invariant fault representation, plus global-focal domain-adversarial scheme for both conditional and marginal distribution alignment.

Result: Outperforms other prominent approaches on three public benchmark datasets, demonstrating superior transfer robustness under noise conditions.

Conclusion: ISGFAN effectively handles noise interference and domain shifts in cross-domain fault diagnosis, providing a robust solution for industrial applications.

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [193] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: This paper presents a hybrid approach combining offline constraint programming optimization with online temporal-network execution to create robust manufacturing schedules that remain feasible under worst-case uncertainty, eliminating deadline violations with minimal makespan overhead.


<details>
  <summary>Details</summary>
Motivation: Modern manufacturing systems face challenges with hard delivery deadlines and stochastic task durations due to process noise, equipment variability, and human intervention. Traditional deterministic schedules break down when reality deviates from plans, causing costly last-minute repairs.

Method: Build a CP model of flexible job-shop with per-job deadline tasks, insert optimal buffer Δ*, translate the plan into Simple Temporal Network with Uncertainty (STNU), verify dynamic controllability to guarantee real-time dispatcher can retime activities for every bounded duration realization without violating constraints.

Result: Extensive Monte-Carlo simulations on Kacem 1-4 benchmark suite show 100% elimination of deadline violations compared to state-of-the-art meta-heuristic schedules, with only 3-5% makespan overhead. CP solve-times and STNU checks remain sub-second on medium-size instances.

Conclusion: The work demonstrates how temporal-network reasoning bridges the gap between proactive buffering and dynamic robustness, moving industry closer to truly digital, self-correcting factories.

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [194] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: LLM-generated clinical Chains-of-Thought (CoTs) reliability depends on strategic prompt curation, with Selective Few-shot using diverse, high-quality examples significantly outperforming other strategies, while AI evaluators fail to detect these critical differences.


<details>
  <summary>Details</summary>
Motivation: To address the data scarcity in creating high-quality clinical CoTs for explainable medical AI and verify the clinical reliability of LLM-generated synthetic data.

Method: Blinded comparative study where senior clinicians evaluated CoTs generated via three strategies: Zero-shot, Random Few-shot (shallow examples), and Selective Few-shot (diverse, high-quality examples), compared against GPT-4o evaluations.

Result: Selective Few-shot strategy significantly outperformed others across all human evaluation metrics (p < .001). Random Few-shot offered no improvement over Zero-shot, and AI evaluator failed to detect performance differences.

Conclusion: Clinical reliability of synthetic CoTs depends on strategic prompt curation using "Gold-Standard Depth" and "Representative Diversity" principles, not just example presence. Human expertise remains indispensable for evaluating high-stakes clinical AI.

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [195] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: This paper proposes a framework to redefine corporate knowledge in the age of AI by developing quantitative metrics that measure epistemic states and map them to legal standards of corporate responsibility.


<details>
  <summary>Details</summary>
Motivation: Traditional notions of corporate mens rea (mental state) are challenged as AI increasingly mediates enterprise decision-making, requiring new approaches to impute knowledge and responsibility.

Method: Developed a formal model using extended cognition theory, creating continuous organizational knowledge metrics that integrate computational cost and validated error rates, with thresholded knowledge predicates and firm-wide epistemic capacity indices.

Result: Created measurable metrics that can be operationally mapped onto legal standards (actual knowledge, constructive knowledge, wilful blindness, recklessness) to create audit artefacts for corporate accountability.

Conclusion: Provides a pathway to make the corporate mind tractable and accountable in the algorithmic age through measurable and justiciable audit artefacts.

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [196] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI is a multi-agent framework that uses LLMs to automatically evaluate and rank PHI de-identification models without heavy reliance on gold-standard annotations, providing cost-effective and reliable model selection.


<details>
  <summary>Details</summary>
Motivation: Current PHI de-identification model evaluation depends on expensive, small-scale expert annotations, creating barriers for practical model comparison and selection.

Method: Deploys multiple Evaluation Agents (LLMs) that independently judge PHI extraction correctness, then consolidates results through LLM-based majority voting to produce stable rankings.

Result: Experiments show TEAM-PHI produces consistent rankings that closely match supervised evaluation and human judgment, despite individual evaluator variation.

Conclusion: TEAM-PHI offers a practical, secure, and cost-effective solution for automatic PHI de-identification model evaluation and selection when ground-truth labels are limited.

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [197] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: The paper introduces the Right To Be Remembered (RTBR) concept to address AI-driven information omission risks in LLMs, which can disproportionately suppress certain narratives while amplifying others, threatening collective memory.


<details>
  <summary>Details</summary>
Motivation: LLMs provide synthesized responses that feel authoritative but collapse multiple perspectives, concentrating information power in few vendors and risking gradual erasure of those with limited digital presence while amplifying the already prominent.

Method: Proposes the Right To Be Remembered (RTBR) concept as a framework to minimize AI-driven information omission risks, ensure fair treatment rights, and maximize truthfulness in generated content.

Result: The paper presents a conceptual framework rather than empirical results, establishing RTBR as a response to the identified threats of LLM-mediated information retrieval.

Conclusion: The RTBR concept is necessary to counter the threat of AI reshaping collective memory through disproportionate suppression and amplification of information, ensuring fair treatment and truthfulness in AI-generated content.

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [198] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: ScholarEval is a retrieval-augmented framework for evaluating AI-generated research ideas based on soundness and contribution, outperforming baselines including OpenAI's o4-mini-deep-research in expert evaluations and user studies.


<details>
  <summary>Details</summary>
Motivation: As AI tools become more common for research ideation, robust evaluation is needed to ensure the validity and usefulness of generated ideas.

Method: Introduces ScholarEval framework that assesses research ideas on soundness (empirical validity based on literature) and contribution (advancement relative to prior research), evaluated using ScholarIdeas dataset - 117 expert-annotated ideas across four disciplines.

Result: ScholarEval achieves significantly higher coverage of expert-annotated rubric points than all baselines, consistently preferred over OpenAI's o4-mini-deep-research in evaluation actionability, depth, and evidence support. User study shows significant outperformance in literature engagement, idea refinement, and usefulness.

Conclusion: ScholarEval provides an effective framework for evaluating research ideas, with code, dataset, and tool released openly for community use.

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [199] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: Large reasoning models are vulnerable to 'reasoning distraction' attacks where maliciously embedded irrelevant tasks divert them from primary objectives, reducing accuracy by up to 60%. The paper proposes a training-based defense combining SFT and RL that improves robustness by over 50 points.


<details>
  <summary>Details</summary>
Motivation: To identify and systematically analyze a critical vulnerability in large reasoning models where they can be diverted from their primary objectives by irrelevant complex tasks embedded in prompts, threatening LRM reliability.

Method: Comprehensive study across diverse models and benchmarks to demonstrate susceptibility, analysis of how alignment techniques amplify the weakness, and development of a training-based defense combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data.

Result: State-of-the-art LRMs are highly susceptible to reasoning distraction attacks, with injected distractors reducing task accuracy by up to 60%. The proposed defense improves robustness by over 50 points on challenging distractor attacks.

Conclusion: Reasoning distraction represents a distinct and urgent threat to LRM reliability, and the proposed training-based defense provides a practical step toward safer and more trustworthy reasoning systems.

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [200] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: The paper identifies efficiency bottlenecks in web-interactive LLM agentic systems and proposes SpecCache, a caching framework with speculative execution that reduces web environment overhead by up to 3.2x without degrading performance.


<details>
  <summary>Details</summary>
Motivation: Existing research on LLM agentic systems focuses mainly on reasoning performance while neglecting efficiency. Web interactions in these systems introduce significant latency that impacts overall system performance.

Method: The authors conduct an empirical study decomposing end-to-end latency into LLM API latency and web environment latency across 15 models and 5 providers. They propose SpecCache, a caching framework augmented with speculative execution to reduce web environment overhead.

Result: Web environment latency contributes up to 53.7% of overall latency in web-based agentic systems. SpecCache improves cache hit rate by up to 58x compared to random caching and reduces web environment overhead by up to 3.2x.

Conclusion: Efficiency is a critical bottleneck in web-interactive LLM agentic systems, and SpecCache effectively addresses this issue by significantly reducing web environment latency without compromising system performance.

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [201] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: DTKG is a dual-track framework for multi-hop QA that combines KG path-based reasoning and LLM fact verification to handle both parallel and chained reasoning tasks efficiently.


<details>
  <summary>Details</summary>
Motivation: Current multi-hop reasoning approaches either use LLM response-based fact verification (good for parallel reasoning but poor for chained reasoning) or KG path-based chain construction (good for chained reasoning but inefficient for parallel reasoning), leading to suboptimal efficiency and accuracy.

Method: DTKG uses a dual-track approach inspired by Dual Process Theory, with two main stages: Classification Stage to identify reasoning type, and Branch Processing Stage that handles parallel reasoning via KG verification and chained reasoning via KG path construction.

Result: The framework addresses the limitations of single-technique approaches by dynamically selecting the appropriate reasoning method based on the question type.

Conclusion: DTKG provides a comprehensive solution for multi-hop QA by leveraging both KG-based and LLM-based techniques through a cognitive-inspired dual-process architecture.

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [202] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG is a compact typed knowledge graph with symbolic verifier that enforces mathematically interpretable rules in LLM reasoning, improving exact match from 0.767 to 1.000 on FDA benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce fluent reasoning steps while violating simple mathematical or logical constraints, creating a need for systems that can enforce mathematical consistency.

Method: Introduces MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier that encodes entities, relations, and three domain-inspired rules, and applies minimal corrections to guarantee consistency.

Result: On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely.

Conclusion: MedRule-KG provides a general scaffold for safe mathematical reasoning and demonstrates how symbolic verification can eliminate rule violations in LLM reasoning.

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [203] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT is a dynamic anchor selection framework for concept erasure in text-to-image diffusion models that overcomes limitations of fixed anchors by using sibling exclusive concepts and a two-stage evaluation mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods rely on fixed anchor strategies, which cause issues like concept re-emergence and erosion. The authors identified that erasure is inherently sensitive to anchor selection through causal tracing.

Method: Proposed SELECT framework with a two-stage evaluation mechanism that automatically discovers optimal anchors for precise erasure while identifying critical boundary anchors to preserve related concepts. Uses sibling exclusive concepts as superior anchors.

Result: SELECT efficiently adapts to multiple erasure frameworks and consistently outperforms existing baselines across key performance metrics, averaging only 4 seconds for anchor mining of a single concept.

Conclusion: SELECT serves as a universal anchor solution that addresses the limitations of fixed anchor strategies in concept erasure for text-to-image diffusion models.

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [204] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: Users with inconsistent preferences can align algorithms with their true interests through strategic engagement, but this requires sufficient foresight. A critical horizon exists where foresighted users achieve alignment while shortsighted users get aligned to the algorithm's objectives.


<details>
  <summary>Details</summary>
Motivation: Algorithms shape user interactions but users often have inconsistent preferences - spending time on low-value content while wanting different outcomes. This creates a need to understand how users can effectively steer algorithms toward their true interests.

Method: Model user decision process as dual systems: rational System 2 decides engagement, impulsive System 1 determines duration. Use multi-leader, single-follower Stackelberg game where users commit to engagement strategies and algorithm best-responds. Define 'burden of alignment' as minimum optimization horizon needed.

Result: Critical horizon exists: sufficiently foresighted users achieve alignment, while shortsighted users get aligned to algorithm's objectives. This burden can be substantial, but small costly signals (like extra clicks) significantly reduce the required horizon.

Conclusion: Users with inconsistent preferences can align engagement-driven algorithms through strategic interaction in Stackelberg equilibrium, though achieving alignment requires foresight and may benefit from deliberate signaling to reduce the burden.

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [205] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: The paper proposes a three-phase iterative system combining strategic planning with verification, addressing limitations in current LLM reasoning approaches that separate monitoring from verification.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning approaches are inefficient - Monitor-Generate methods lack verification mechanisms, while Generate-Verify methods start generation without strategic planning, creating a gap that needs bridging.

Method: Implemented Flavell's cognitive monitoring model as a three-phase iterative system within the broader Monitor-Generate-Verify framework, combining strategic planning with verification.

Result: Achieved 75.42% accuracy on GSM8K vs 68.44% for SELF-REFINE and 67.07% for Self-Verification, with fewer attempts (1.3 vs 2.0) at 27-37% increased inference cost.

Conclusion: Upfront monitoring produces higher-quality initial solutions that reduce refinement needs, though evaluation beyond arithmetic reasoning is needed to establish generalizability.

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [206] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: HSCM is a human-inspired causal framework for domain generalization that disentangles and reweights image attributes like color, texture, and shape to improve model robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional domain generalization models that rely on statistics and learn distortion-invariant representations, by replicating human vision system's hierarchical processing and multi-level learning.

Method: Proposes Humanoid-inspired Structural Causal Model (HSCM) that models fine-grained causal mechanisms by disentangling and reweighting key image attributes (color, texture, shape) to enhance generalization across diverse domains.

Result: HSCM outperforms existing domain generalization models through both theoretical and empirical evaluations, demonstrating improved robustness and more principled causal relationship capture.

Conclusion: HSCM provides a more effective approach for domain generalization by leveraging human intelligence principles, enabling better transfer and learning in dynamic, complex environments while ensuring interpretability.

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [207] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: RGMem is a self-evolving memory framework that uses renormalization group principles to create multi-scale user profiles from dialogue history, enabling long-term behavioral consistency in LLM-based conversational systems.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems struggle with long-term user modeling due to finite context windows and static memory, leading to shallow personalization and lack of cross-session continuity. Existing solutions like RAG focus on fact-level storage but fail to distill latent user preferences from multi-turn dialogues.

Method: RGMem organizes dialogue history in multiple scales through hierarchical coarse-graining and rescaling operations. It extracts semantics from episodic fragments and progressively forms dynamically-evolved user profiles, modeling memory evolution as a multi-scale process of information compression and emergence.

Result: The framework accomplishes high-level and accurate user profiles from noisy, microscopic-level interactions by progressively distilling latent preferences and deep traits from multi-turn dialogues.

Conclusion: RGMem enables long-term memory and behavioral consistency for language agents in the LLM era by creating dynamically-evolved user profiles through multi-scale information processing inspired by renormalization group physics.

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [208] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense is a prescriptive decision support framework that uses LLMs to transform customer reviews into actionable business recommendations, going beyond traditional preference prediction to provide deeper insights for growth and customer loyalty.


<details>
  <summary>Details</summary>
Motivation: Customer feedback is crucial for strategic growth, but existing AI systems mainly predict user preferences rather than providing prescriptive, business-facing recommendations from unstructured reviews.

Method: Integrates clustering, LLM adaptation, and expert-driven evaluation into a unified pipeline that identifies key trends, recurring issues, and specific concerns within customer sentiments.

Result: Preliminary manual evaluations show strong alignment between the model's recommendations and business objectives, demonstrating potential for data-informed decision-making.

Conclusion: The framework offers a new perspective on AI-driven sentiment analysis, showing value in refining business strategies and maximizing customer feedback impact.

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [209] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: NP-ENGINE is a comprehensive framework for training and evaluating LLMs on NP-hard problems, featuring a generator-verifier-heuristic pipeline that enables scalable RLVR training. The resulting model QWEN2.5-7B-NP achieves SOTA performance on NP-BENCH and shows strong out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown strong reasoning capabilities but their ability to solve complex NP-hard optimization problems remains underexplored, creating a gap that needs to be addressed.

Method: Proposed NP-ENGINE framework with 10 tasks across 5 domains, each with controllable instance generator, rule-based verifier, and heuristic solver. Used zero-RLVR with curriculum learning on Qwen2.5-7B-Instruct.

Result: QWEN2.5-7B-NP significantly outperforms GPT-4o on NP-BENCH and achieves SOTA performance. RLVR training enables strong out-of-domain generalization to reasoning and non-reasoning tasks, with scaling trends showing improved generalization with task diversity.

Conclusion: Task-rich RLVR training is a promising direction for advancing LLM's reasoning ability, revealing new insights into RLVR scaling laws and demonstrating that complex optimization training can enhance general reasoning capabilities.

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [210] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug is a typed computer language encoded in vector-symbolic architecture that ensures all programs halt in polynomial time, enabling neural networks to learn types and facilitating efficient skill acquisition through program synthesis.


<details>
  <summary>Details</summary>
Motivation: To model human mental representations and their acquisition in the brain, enabling human-like skill learning that is faster than brute force approaches and more efficient than current methods.

Method: Encodes the light linear functional programming language (LLFPL) using holographic declarative memory for types and a Lisp VSA variant for terms, allowing neural networks to learn types from embedding spaces.

Result: Developed Doug language that enables points in neural network embedding spaces to be interpreted as types, with similar types clustering together in both structure and content.

Conclusion: Doug represents a step toward modeling actual human mental representations and their acquisition processes in the brain, potentially enabling more efficient skill learning that mimics human cognitive capabilities.

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [211] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: Urban-R1 is a reinforcement learning framework that mitigates geospatial bias in urban foundation models through Group Relative Policy Optimization and urban region profiling, improving cross-region generalization.


<details>
  <summary>Details</summary>
Motivation: Current urban foundation models using supervised fine-tuning exhibit persistent geospatial bias, producing regionally skewed predictions and limited generalization across different geographic areas.

Method: Proposes Urban-R1 framework using reinforcement learning with Group Relative Policy Optimization (GRPO) to optimize reasoning across geographic groups, employing urban region profiling as a proxy task for measurable rewards from multimodal urban data.

Result: Extensive experiments show Urban-R1 effectively mitigates geo-bias and improves cross-region generalization, outperforming both SFT-trained and closed-source models across diverse regions and tasks.

Conclusion: Reinforcement learning alignment represents a promising pathway toward equitable and trustworthy urban intelligence by addressing geospatial bias in urban foundation models.

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [212] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena is the first physics-aligned interactive benchmark for evaluating LLMs' engineering construction capabilities through language-driven tasks with physical constraints.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation for LLMs' construction competencies despite their promising reasoning capabilities, and to bridge the gap between natural language specifications and physically viable structures.

Method: Created a customizable benchmarking framework with extendable task design spanning static/dynamic mechanics across difficulty tiers, supported by a 3D Spatial Geometric Computation Library and baseline LLM agentic workflow.

Result: Comprehensively evaluated eight frontier LLMs on their language-driven and physics-grounded construction automation capabilities using the BuildArena benchmark.

Conclusion: BuildArena provides the first standardized framework for assessing LLMs' engineering construction abilities, enabling systematic comparison and analysis of model performance in this domain.

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [213] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: The paper introduces the Ripple Effect Protocol (REP), a coordination protocol that enables AI agents to share lightweight sensitivities alongside decisions, improving group coordination in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Current AI agent communication protocols like A2A and ACP focus on communication rather than coordination, leading to brittle collective behavior where individually smart agents produce poor group outcomes as populations grow.

Method: REP allows agents to share not only decisions but also lightweight sensitivities - signals expressing how their choices would change if key environmental variables shifted. These sensitivities ripple through local networks, with formal protocol specifications separating required message schemas from optional aggregation rules.

Result: Benchmarks across three domains (supply chain cascades, preference aggregation, and sustainable resource allocation) show REP improves coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly handling multimodal sensitivity signals from LLMs.

Conclusion: By making coordination a protocol-level capability, REP provides scalable infrastructure for the emerging Internet of Agents, enabling groups to align faster and more stably than with agent-centric communication alone.

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [214] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow is a framework that improves KG-based RAG by using flow matching to optimize retrieval policies, achieving better accuracy and diversity without expensive supervision.


<details>
  <summary>Details</summary>
Motivation: Existing KG-based RAG methods struggle with accurate and diverse retrieval from text-rich KGs for complex queries, and PRMs require expensive process-level supervision.

Method: Uses transition-based flow matching to jointly optimize retrieval policy and flow estimator, factorizing rewards to guide proportional retrieval of high-quality candidates.

Result: Outperforms strong KG-RAG baselines including GPT-4o by 10% on average in hit rate and recall on STaRK benchmark, with strong generalization to unseen KGs.

Conclusion: GraphFlow effectively retrieves accurate and diverse knowledge from text-rich KGs for real-world queries, demonstrating robustness and generalization capabilities.

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [215] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: Proposes ssCDL, a semi-supervised confidence distribution learning method for uncertain knowledge graph completion that addresses imbalanced confidence distributions by transforming confidences into distributions and using meta-learning for pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Current UKG completion methods neglect the extremely imbalanced distributions of triple confidences, causing insufficient embeddings for high-quality completion.

Method: Transforms triple confidences into confidence distributions, uses relational learning on labeled and unlabeled data with pseudo labels generated by meta-learning to augment training data and rebalance confidence distributions.

Result: Experiments on two UKG datasets show ssCDL consistently outperforms state-of-the-art baselines across different evaluation metrics.

Conclusion: ssCDL effectively addresses the imbalanced confidence distribution problem in UKG completion through semi-supervised confidence distribution learning and meta-learning-based pseudo-labeling.

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [216] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI is a novel RL algorithm that uses count-based intrinsic rewards to improve exploration in LLM reasoning, encouraging diverse reasoning patterns and escaping local optima.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLM reasoning rely on sparse rewards and limited exploration, leading to repetitive and suboptimal reasoning patterns. The paper aims to design better exploration strategies for LLM reasoning.

Method: MERCI uses a lightweight Coin Flipping Network (CFN) to estimate pseudo counts and epistemic uncertainty over reasoning trajectories, converting them into intrinsic rewards that value novelty while preserving task reward signals. Integrated with GRPO framework.

Result: Experiments show MERCI encourages richer and more varied chains of thought, significantly improves performance over baselines, and helps policies escape local routines to discover better solutions.

Conclusion: Targeted intrinsic motivation can make exploration reliable for language model reasoning, addressing the limitations of current RL paradigms.

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [217] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: Large-scale AI models are transforming neuroscience by enabling end-to-end learning from raw brain signals, addressing challenges in multimodal data integration, spatiotemporal pattern interpretation, and clinical translation across five major neuroscience domains.


<details>
  <summary>Details</summary>
Motivation: The advent of large-scale AI models represents a paradigm shift from traditional computational methods in neuroscience, facilitating more efficient analysis of complex neural data and enabling new research capabilities.

Method: The paper explores applications across five neuroscience domains: neuroimaging/data processing, brain-computer interfaces/neural decoding, molecular neuroscience/genomic modeling, clinical assistance/translational frameworks, and disease-specific applications. It incorporates biologically informed architectural constraints for more interpretable models.

Result: Large-scale AI models successfully address major computational neuroscience challenges including multimodal neural data integration, spatiotemporal pattern interpretation, and derivation of translational frameworks for clinical deployment. The interaction between neuroscience and AI has become reciprocal.

Conclusion: The review highlights both the promise of large-scale AI models in neuroscience and key implementation considerations including rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. A systematic listing of critical neuroscience datasets is provided for validation.

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [218] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: AFL is an agentic framework with LLMs that fully automates solving complex vehicle routing problems from raw inputs to solutions without external intervention, achieving high code reliability and solution feasibility.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for VRPs require external intervention, leading to execution errors and low solution feasibility. There's a need for fully automated systems that can directly extract knowledge from raw inputs and generate self-contained code.

Method: AFL decomposes the pipeline into three subtasks and employs four specialized agents that coordinate interactions to enforce cross-functional consistency and logical soundness. It directly extracts knowledge from raw inputs and enables self-contained code generation without handcrafted modules or external solvers.

Result: Extensive experiments on 60 complex VRPs show comparable performance against meticulously designed algorithms. AFL substantially outperforms existing LLM-based baselines in both code reliability and solution feasibility, achieving rates close to 100% on evaluated benchmarks.

Conclusion: The proposed AFL framework successfully achieves full automation for solving complex vehicle routing problems, demonstrating high trustworthiness and effectiveness across various VRP variants while eliminating the need for external intervention.

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [219] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: This survey examines the paradigm shift in agentic AI from pipeline-based systems to model-native approaches, where planning, tool use, and memory capabilities are internalized within LLM parameters through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To trace the evolution of agentic AI from systems that externally orchestrate intelligence to models that internally develop intelligence through experience and learning.

Method: Systematic review of how planning, tool use, and memory capabilities have evolved from externally scripted modules to end-to-end learned behaviors, using RL as the algorithmic engine for outcome-driven exploration.

Result: Identifies a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, with applications in deep research agents and GUI agents.

Conclusion: Agentic AI is transitioning from constructing systems that apply intelligence to developing models that grow intelligence through experience, with continued internalization of capabilities like multi-agent collaboration and reflection.

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [220] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: This survey paper provides the first comprehensive overview of RL-based agentic search, organizing the field along three dimensions: functional roles of RL, optimization strategies, and scope of optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG pipelines are single-turn and heuristic, lacking adaptive control over retrieval and reasoning. RL offers a powerful mechanism for adaptive and self-improving search behavior in agentic search systems.

Method: The survey organizes RL-based agentic search along three dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). It summarizes representative methods, evaluation protocols, and applications.

Result: The paper provides a comprehensive framework for understanding RL-based agentic search, including a repository of related papers available at the provided GitHub link.

Conclusion: The survey aims to inspire future research on integrating RL and agentic search to build reliable and scalable systems, while discussing open challenges and future directions.

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [221] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: The paper proposes a workflow using lightweight emulators trained on compact experimental designs to address computational cost and transparency issues in simulation-driven engineering. The approach enables fast approximations, uncertainty quantification, and explainable AI analyses for complex systems.


<details>
  <summary>Details</summary>
Motivation: Address two central obstacles in simulation-driven engineering workflows: (1) high computational cost from many expensive simulator runs, and (2) limited transparency and reliability when decisions rely on opaque blackbox components.

Method: Train lightweight emulators on compact designs of experiments that provide fast approximations, enable uncertainty quantification, and support global and local Explainable AI (XAI) analyses. The methodology supports continuous and categorical inputs, combines global-effect and uncertainty analyses with local attribution.

Result: Demonstrated on hybrid-electric aircraft design and urban segregation agent-based model. The approach enables large-scale exploration in seconds, uncovers nonlinear interactions and emergent behaviors, identifies key design and policy levers, and signals regions where surrogates require more data or alternative architectures.

Conclusion: The surrogate model and XAI coupling provides an effective workflow for complex-system analysis that addresses computational efficiency and transparency challenges while guiding further data collection or model refinement.

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [222] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: ELMM is an efficient multimodal LLM for knowledge graph completion that compresses image tokens and prunes attention layers to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal knowledge graphs suffer from incompleteness, and existing multimodal LLMs face challenges with semantic noise from image tokens and high computational costs.

Method: Proposes Multi-view Visual Token Compressor using multi-head attention to compress image tokens from textual and visual views, plus attention pruning with linear projection to reduce inference costs.

Result: Achieves state-of-the-art performance on FB15k-237-IMG and WN18-IMG benchmarks while significantly improving computational efficiency.

Conclusion: ELMM establishes a new paradigm for multimodal knowledge graph completion by balancing performance and efficiency through token compression and model pruning.

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [223] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling natural human-like multimodal interactions.


<details>
  <summary>Details</summary>
Motivation: Human interaction is inherently multimodal and full-duplex, with capabilities like listening while watching, speaking while acting, and fluid turn-taking. Realizing these capabilities is essential for building human-simulating models.

Method: Uses a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone, leveraging strong pre-trained components while enabling efficient modality integration.

Result: On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines while uniquely supporting advanced multimodal behaviors like dialogue turn-taking, defective instruction rejection, speaking-while-acting, and action barge-ins.

Conclusion: ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence.

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [224] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista is a unified framework that enhances graph understanding by improving scalability and modality coordination through hierarchical organization and a planning agent that routes tasks to the most suitable modality.


<details>
  <summary>Details</summary>
Motivation: Vision-language models face scalability bottlenecks due to input-token constraints and lack effective mechanisms to coordinate textual and visual modalities in graph understanding tasks.

Method: GraphVista uses hierarchical organization into a lightweight GraphRAG base for scalability, retrieving only task-relevant content, and introduces a planning agent that routes tasks to text modality for simple property reasoning and visual modality for complex structural reasoning.

Result: GraphVista scales to graphs 200× larger than existing benchmarks and achieves up to 4.4× quality improvement over state-of-the-art baselines by fully exploiting complementary strengths of both modalities.

Conclusion: The proposed GraphVista framework successfully addresses scalability and modality coordination challenges in graph understanding, demonstrating superior performance across large-scale graph tasks.

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [225] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: Proposes Domain-Contextualized Concept Graph (CDC) - a knowledge modeling framework that makes domains explicit first-class elements using C-D-C triples (<Concept, Relation@Domain, Concept'>) to overcome rigid hierarchical limitations of traditional knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge graphs are constrained by fixed ontologies with rigid hierarchical structures, treating domains as implicit context rather than explicit reasoning components.

Method: CDC adopts C-D-C triple structure with domain specifications as dynamic classification dimensions, implements standardized relation predicates (structural, logical, cross-domain, temporal), and is implemented in Prolog for full inference capability.

Result: Case studies in education, enterprise knowledge systems, and technical documentation demonstrate CDC enables context-aware reasoning, cross-domain analogy, and personalized knowledge modeling.

Conclusion: CDC provides capabilities unattainable under traditional ontology-based frameworks by making domains explicit first-class elements in conceptual representation.

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [226] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B is the first agentic LLM for autonomous data science that can complete end-to-end pipelines from raw data to research reports, outperforming workflow-based agents despite having only 8B parameters.


<details>
  <summary>Details</summary>
Motivation: Existing workflow-based data agents are limited by predefined workflows and cannot achieve fully autonomous data science. The emergence of powerful LLMs makes autonomous data science from raw data to research reports now feasible.

Method: Proposed a curriculum-based agentic training paradigm that emulates human data scientists' learning trajectory, enabling progressive capability acquisition. Also introduced a data-grounded trajectory synthesis framework for constructing high-quality training data.

Result: DeepAnalyze-8B outperforms previous workflow-based agents built on more advanced proprietary LLMs, demonstrating strong performance across data question answering, specialized analytical tasks, and open-ended data research.

Conclusion: The work paves the way toward autonomous data science by introducing the first agentic LLM specifically designed for this purpose, with the model, code, and training data being open-sourced to advance the field.

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [227] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: VLM agents achieve 3x performance improvement through explicit visual state reasoning using world modeling rewards and bi-level GAE, outperforming proprietary models on diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of partial observability in VLM agents when transitioning from textual to visual states, requiring robust world modeling capabilities.

Method: Architecturally enforce reasoning via RL formulated as POMDP, decomposing into State Estimation and Transition Modeling with 5 reasoning strategies, using World Modeling Reward and Bi-Level GAE for supervision.

Result: 3B-parameter model achieves 0.82 score across 5 benchmarks (3x improvement from 0.21 baseline), outperforming GPT-5 (0.75), Gemini 2.5 Pro (0.67), and Claude 4.5 (0.62).

Conclusion: Explicit visual state reasoning with task-dependent belief representations (Natural Language for semantics, Structured for precision) enables effective VLM agent training in partially observable visual environments.

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [228] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: The paper evaluates four XRL algorithms for debugging by testing if users can identify an agent's goal from explanations, finding only one algorithm performed better than random chance and users were overconfident.


<details>
  <summary>Details</summary>
Motivation: Limited comparative evaluations exist for explainable reinforcement learning (XRL) algorithms in debugging applications, despite debugging being a core use case.

Method: Proposed a novel evaluation methodology using Atari's Ms. Pacman environment and four XRL algorithms to test whether users can identify an agent's goal from decision-making explanations.

Result: Only one XRL algorithm achieved greater than random accuracy for tested goals; users were generally overconfident in their selections; self-reported ease of identification and understanding did not correlate with actual accuracy.

Conclusion: Current XRL algorithms have limited effectiveness for debugging tasks, and user confidence does not align with actual performance, highlighting the need for better evaluation methods and improved explanation quality.

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [229] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: An LLM agentic framework for GPU kernel optimization that uses multi-agent collaboration and strategic search to achieve up to 16x faster runtime performance compared to baseline approaches.


<details>
  <summary>Details</summary>
Motivation: GPU kernel optimization is difficult and labor-intensive due to complex hardware interactions, and existing LLM approaches are limited as single-shot generators or naive refinement tools.

Method: Multi-agent collaboration framework with grounded instruction, dynamic context management, and strategic search that mimics expert engineers' workflow for iterative kernel refinement with profiling feedback.

Result: The system produces correct solutions where baselines often fail, achieving kernels with up to 16x faster runtime performance on the KernelBench benchmark.

Conclusion: Agentic LLM frameworks show strong potential for advancing fully automated, scalable GPU kernel optimization.

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [230] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic is a diagnostic framework that detects and corrects tool usage errors in LLMs, improving tool-calling accuracy by up to 13% over baselines.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented LLMs are increasingly used in real-world applications, but tool usage errors hinder their reliability, creating a need for better error detection and correction mechanisms.

Method: ToolCritic detects eight distinct tool-calling error types and provides targeted feedback to the main LLM, which then revises its response. The framework uses a synthetic dataset to train the error detection system.

Result: Experimental results on the Schema-Guided Dialogue dataset show ToolCritic improves tool-calling accuracy by up to 13% over baselines including zero-shot prompting and self-correction techniques.

Conclusion: ToolCritic represents a promising step toward more robust LLM integration with external tools in real-world dialogue applications by systematically addressing tool usage errors.

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [231] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID is a multi-agent AI system that combines free-text descriptions with ontology labels to improve gene set annotation accuracy, achieving 77% correct annotations for mouse gene sets and enabling novel insights into brain cell function.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like GSEA struggle with poorly characterized genes and require well-curated annotations. LLMs offer promise but have difficulty representing complex biological knowledge within structured ontologies.

Method: Developed a multi-agent AI system with retrieval-augmented generation (RAG) that integrates free-text descriptions with ontology labels, using PubMed literature to refine predictions and reduce hallucinations.

Result: Achieved 77% correct annotations for mouse gene sets, annotated 5,322 brain cell clusters from the mouse brain cell atlas, identified region-specific gene co-expression patterns, and discovered Basal Ganglia-related cell types with neurologically meaningful descriptions.

Conclusion: BRAINCELL-AID creates a valuable resource for community-driven cell type annotation by providing more accurate and robust gene set annotation capabilities compared to traditional methods.

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [232] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: This study develops two LLM-based systems for corporate credit assessment that automate evidence-based reasoning from non-financial indicators, with a debate-based multi-agent system showing superior reasoning quality over a single-agent approach.


<details>
  <summary>Details</summary>
Motivation: Current financial AI focuses on numerical prediction but fails to automate the interpretive reasoning required for professional loan evaluation using qualitative non-financial indicators.

Method: Developed two LLM-based systems: a single-agent system (NAS) with bidirectional analysis and a debate-based multi-agent system (KPD-MADS) using Karl Popper's critical dialogue framework with 10-step structured interaction.

Result: Both systems achieved significant productivity gains (NAS: 11.55s; KPD-MADS: 91.97s vs human baseline: 1920s). KPD-MADS showed superior reasoning quality with higher ratings in explanatory adequacy (4.0 vs 3.0), practical applicability (4.0 vs 3.0), and usability (62.5 vs 52.5).

Conclusion: Structured multi-agent interaction enhances reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [233] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: A handcrafted feature-based approach using color statistics, histograms, and texture features from fish eye images achieves up to 97.16% accuracy for automated fish freshness assessment, significantly outperforming previous deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Conventional sensory evaluation of fish freshness is subjective, inconsistent, and difficult to standardize, with limitations in detecting subtle, species-dependent spoilage cues.

Method: Systematically extracts and incrementally fuses complementary descriptors including color statistics, histograms across multiple color spaces, and texture features (LBP, GLCM) from fish eye images, capturing both global chromatic variations and localized degradations.

Result: LightGBM classifier achieved 77.56% accuracy (14.35% improvement over baseline) and ANN with augmented data reached 97.16% accuracy (19.86% improvement over prior best) on the FFE dataset.

Conclusion: Carefully engineered handcrafted features provide a robust, interpretable, and reliable solution for automated fish freshness assessment, offering valuable insights for practical food quality monitoring applications.

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [234] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM is a Physics-Informed LLM framework that uses evolutionary algorithms to automatically generate, evaluate, and refine HVAC anomaly detection rules with embedded physical constraints, achieving state-of-the-art performance while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: HVAC systems consume significant energy globally, requiring reliable anomaly detection. Classical rule-based methods lack adaptability, while deep learning approaches sacrifice transparency and physical plausibility. Current LLM-based methods ignore physical principles governing HVAC operations.

Method: PILLM framework operates in an evolutionary loop with physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling automatic generation and refinement of anomaly detection rules.

Result: Experiments on the Building Fault Detection dataset show PILLM achieves state-of-the-art performance while producing interpretable and actionable diagnostic rules.

Conclusion: PILLM advances trustworthy and deployable AI for smart building systems by combining the predictive power of LLMs with physical constraints, creating adaptive yet physically grounded anomaly detection rules.

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [235] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench is a benchmark for evaluating multi-agent communication protocols, showing that protocol choice significantly impacts performance metrics like latency, overhead, and resilience. ProtocolRouter is introduced as an adaptive protocol selector that improves performance over single-protocol baselines.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent system protocol selection lacks standardized evaluation methods, leading to intuition-driven choices that may not optimize for performance, reliability, and efficiency.

Method: Developed ProtocolBench to systematically compare protocols across four axes: task success, latency, message overhead, and robustness. Created ProtocolRouter, a learnable router that selects optimal protocols per scenario based on requirements and runtime signals.

Result: Protocol choice caused up to 36.5% variation in completion time and 3.48s difference in latency. ProtocolRouter reduced Fail-Storm recovery time by 18.1% versus best single-protocol baseline and improved success rates in GAIA scenarios.

Conclusion: Protocol selection significantly impacts multi-agent system performance, and adaptive protocol routing via ProtocolRouter provides measurable improvements over static protocol choices.

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [236] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: LaGAT: A hybrid framework combining neural MAPF policy (MAGAT) with search-based algorithm (LaCAM) that outperforms both pure search-based and learning-based methods in dense multi-agent pathfinding scenarios.


<details>
  <summary>Details</summary>
Motivation: Real-time near-optimal solutions for dense multi-agent pathfinding problems remain challenging for state-of-the-art planners, and prior learning-guided search methods have historically underperformed.

Method: Integrates learned heuristic from enhanced MAGAT architecture (neural MAPF policy with graph attention) into LaCAM search algorithm, using pre-train-then-fine-tune strategy on target maps and deadlock detection for imperfect neural guidance.

Result: Outperforms both purely search-based and purely learning-based methods in dense scenarios.

Conclusion: When carefully designed, hybrid search offers a powerful solution for tightly coupled, challenging multi-agent coordination problems.

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [237] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: Hybrid framework combining ECG foundation model with XGBoost classifier improves VT/VF prediction after AMI, achieving AUC 0.801 while maintaining interpretability through SHAP analysis.


<details>
  <summary>Details</summary>
Motivation: Malignant ventricular arrhythmias (VT/VF) following AMI are major cause of in-hospital death, but traditional risk scores have limited performance and deep learning models lack interpretability needed for clinical trust.

Method: Used ECGFounder foundation model to extract 150-dimensional diagnostic probability features from 6,634 ECG recordings, refined through feature selection to train XGBoost classifier, with SHAP for interpretability.

Result: Hybrid model achieved AUC of 0.801, outperforming KNN (0.677), RNN (0.676), and 1D-CNN (0.720). SHAP analysis revealed clinically meaningful features like premature ventricular complexes (risk) and normal sinus rhythm (protective).

Conclusion: Hybrid framework validates foundation model outputs as effective automated feature engineering for building trustworthy, explainable AI-based clinical decision support systems for VT/VF risk prediction.

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [238] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: A web-deployed LLM health coach was evaluated with real users, showing that uniform tool-heavy policies can harm specific subgroups like low-health-literacy/high-self-efficacy users. Simulator studies found that adding early information-gain bonuses improves trait identification and goal success.


<details>
  <summary>Details</summary>
Motivation: To understand how tool-augmented LLMs perform as health coaches in real-world settings and identify potential subgroup harms that average metrics might obscure.

Method: Used offline policy evaluation (OPE) with factorized decision heads (Tool/Style) on real user data (7 users, 280 rated turns), plus a lightweight simulator with hidden archetypes to test information-gain bonuses.

Result: Uniform heavy-tool policies raise average value but harm specific subgroups. Adding small early information-gain bonuses reliably shortens trait identification and improves goal success and pass@3 metrics.

Conclusion: Proposes an evaluation-first path to personalization: freeze the generator, learn subgroup-aware decision heads on typed rewards, and always report per-archetype metrics to surface subgroup harms.

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [239] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: TD-HNODE is a neural ODE framework that models disease progression using temporally detailed hypergraphs to capture continuous-time dynamics from irregular EHR data, outperforming baselines on type 2 diabetes and cardiovascular disease datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate disease progression modeling from EHRs is challenging due to irregular sampling times and patient heterogeneity. Existing methods lack adaptability to real-world data or fail to capture complex continuous-time dynamics.

Method: Proposed Temporally Detailed Hypergraph Neural ODE (TD-HNODE) that represents disease progression as a hypergraph and learns continuous-time dynamics via neural ODEs, with a learnable TD-Hypergraph Laplacian capturing interdependencies within and between progression trajectories.

Result: Experiments on two real-world clinical datasets show TD-HNODE outperforms multiple baselines in modeling progression of type 2 diabetes and related cardiovascular diseases.

Conclusion: TD-HNODE effectively addresses limitations of existing methods by capturing complex continuous-time progression dynamics from irregular EHR data through its hypergraph neural ODE framework.

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [240] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor is a reinforcement learning-based chatbot that provides comprehensive cryptocurrency investment analysis through a multi-agent framework with specialized tools and adaptive tool selection.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current cryptocurrency investment approaches: manual analysis is time-consuming and biased, data platforms lack depth, and LLM agents lack real-time data integration and multi-step reasoning.

Method: Multi-agent framework with specialized tools and reinforcement learning-based tool selection mechanism for multi-step planning and flexible integration of diverse data sources.

Result: 40.7% improvement in recall and 26.6% improvement in F1 score over base model in tool orchestration; high user satisfaction (4.64/5) with preference over general LLMs and existing platforms (4.62/5).

Conclusion: Coinvisor successfully addresses current limitations by providing real-time, adaptive cryptocurrency investment analysis through its reinforcement learning-based multi-agent framework.

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [241] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: RubiSCoT is an AI framework using NLP and LLMs to automate thesis evaluation from proposal to submission, providing consistent and scalable assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional thesis evaluation methods are time-consuming and suffer from evaluator variability, creating a need for more efficient and consistent assessment solutions.

Method: Uses advanced NLP techniques including large language models, retrieval-augmented generation, and structured chain-of-thought prompting for preliminary assessments, multidimensional assessments, content extraction, rubric-based scoring, and detailed reporting.

Result: The paper presents the design and implementation of RubiSCoT framework, demonstrating its capability for consistent, scalable thesis evaluation.

Conclusion: RubiSCoT has potential to optimize academic assessment processes through consistent, scalable, and transparent evaluation of academic theses.

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [242] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: FBI_LTL is a diverse planner for simulation-based planning that uses Linear Temporal Logic to define semantic diversity criteria, ensuring generation of meaningfully different plans rather than just syntactically different ones.


<details>
  <summary>Details</summary>
Motivation: Traditional planning approaches that produce single plans may not satisfy agent preferences, and existing diverse planning methods often generate semantically identical solutions despite syntactic differences.

Method: FBI_LTL integrates LTL-based diversity models directly into the search process to specify semantic diversity criteria, enabling agents to define what constitutes meaningfully different plans.

Result: Extensive evaluations show FBI_LTL generates more diverse plans compared to baseline approaches across various benchmarks.

Conclusion: This work establishes the feasibility of semantically-guided diverse planning in simulation-based environments, enabling innovative approaches in realistic, non-symbolic domains where traditional model-based approaches fail.

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [243] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: Active inference route-planning method using Dempster-Shafer theory and Gaussian sensor model for autonomous agents to maintain operational pictures by balancing exploration and exploitation.


<details>
  <summary>Details</summary>
Motivation: To develop autonomous control for intelligent agents that can reconnoiter geographical areas and maintain common operational pictures by effectively balancing exploration of unknown areas with tracking identified targets.

Method: Construct evidence maps incorporating positive/negative sensor observations over time, use Dempster-Shafer theory and Gaussian sensor model in generative model, calculate variational free energy based on divergence between pignistic probability distribution and posterior probability distribution, and direct agent movements toward positions minimizing free energy.

Result: The method successfully enables agents to balance searching extensive geographical areas while tracking identified target objects through incremental steps that minimize variational free energy.

Conclusion: The active inference approach with Dempster-Shafer theory provides an effective framework for autonomous route planning that addresses the exploration-exploitation tradeoff in maintaining operational awareness.

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [244] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: Legal ML systems treat past case outcomes as ground truth, but these outcomes are often shaped by unobserved human interventions (settlements, appeals) creating label indeterminacy, which significantly affects model behavior.


<details>
  <summary>Details</summary>
Motivation: Machine learning in legal domains typically treats past case outcomes as definitive ground truth, but these outcomes are often shaped by human interventions not captured in ML approaches, creating label indeterminacy where outcomes could have been different.

Method: The paper examines label indeterminacy in the context of classifying cases from the European Court of Human Rights, showing how different label construction methods during training affect model behavior.

Result: The way labels are constructed during training can significantly affect model behavior, demonstrating that label indeterminacy is a relevant concern that shapes how legal ML models perform.

Conclusion: Legal machine learning applications need to account for label indeterminacy, as existing methods for imputing indeterminate labels rely on unverifiable assumptions and the construction of labels during training meaningfully impacts model outcomes.

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [245] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE is an agentic framework that detects multimodal misinformation by decomposing verification into visual assessment, cross-modal consistency, retrieval-augmented fact-checking, and calibrated judgment, achieving 81.65% F1 without domain-specific training.


<details>
  <summary>Details</summary>
Motivation: Manual fact-checking is overwhelmed by billions of daily multimodal posts, and supervised models fail to generalize across manipulation tactics due to domain-specific training requirements.

Method: MIRAGE uses four sequential modules: visual veracity assessment for AI-generated images, cross-modal consistency analysis for out-of-context repurposing, retrieval-augmented factual checking with iterative question generation, and a calibrated judgment module that integrates all signals using vision-language model reasoning with web retrieval.

Result: On MMFakeBench validation set (1,000 samples), MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming GPT-4V with MMD-Agent (74.0% F1) by 7.65 points while maintaining 34.3% false positive rate versus 97.3% for judge-only baseline. Test set results (5,000 samples) confirm generalization with 81.44% F1 and 75.08% accuracy.

Conclusion: Decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling misinformation detection across modalities where labeled data remains scarce.

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [246] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: This paper proposes a method to distill reasoning capabilities from very large language models (VLLMs) into smaller, more efficient models for code generation, enabling better understanding of solution structure and algorithmic reasoning.


<details>
  <summary>Details</summary>
Motivation: Smaller language models lack the reasoning capabilities of VLLMs for complex code generation tasks, which require understanding solution-level relationships rather than just token prediction. There's a need for efficient models that can be deployed faster and cheaper while maintaining strong reasoning abilities.

Method: The approach trains smaller models to emulate VLLM reasoning by learning correct solution pathways and establishing structural correspondence between problems and solutions through structure-aware loss optimization, enabling deep understanding of solution structure beyond token-level generation.

Result: The fine-tuned model significantly outperforms baseline models in pass@1, average data flow, and average syntax match metrics across MBPP, MBPP Plus, and HumanEval benchmarks, achieved through a cheap and simple implementation process.

Conclusion: Reasoning capabilities can be effectively distilled from VLLMs into smaller models, enabling efficient deployment while maintaining strong code generation performance that understands solution structure and algorithmic reasoning.

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [247] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank is a low-latency decoder-based reranker that scores candidates in one pass and generates explanations only when needed, achieving strong performance on clinical order selection tasks.


<details>
  <summary>Details</summary>
Motivation: Clinicians need real-time ranking systems that can justify their choices, requiring low-latency solutions that maintain transparency.

Method: Single-decoder approach with pooled first-token scoring and uncertainty-gated explanation step, trained with curriculum learning focusing on hard cases.

Result: Strong effectiveness on encounter-scoped order selection (Recall@1~0.45-0.56, nDCG@20~0.625-0.699), outperforming encoder baselines in both effectiveness and flexibility.

Conclusion: Practical recipe for ranking fast by default and explaining when it helps, with single-policy design simplifying deployment and curriculum principle applicable beyond clinical tasks.

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [248] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: LLMs show promising forecasting capabilities but face bottlenecks in event recall, data understanding, and information aggregation speed compared to markets.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the predictive intelligence of large language models (LLMs) for real-world future event forecasting, an emerging paradigm called 'LLM-as-a-Prophet'.

Method: Built Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages for controlled and large-scale experimentation.

Result: Many LLMs exhibit impressive forecasting capabilities with small calibration errors, consistent prediction confidence, and promising market returns. However, they face key bottlenecks including inaccurate event recalls, misunderstanding of data sources, and slower information aggregation compared to markets.

Conclusion: While LLMs show significant promise as forecasting tools, there are critical limitations that need to be addressed for achieving superior predictive intelligence through the LLM-as-a-Prophet paradigm.

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [249] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: This paper proposes using multi-agent influence diagrams (MAIDs) to address challenges in steering cooperative multi-agent reinforcement learning (MARL), introducing targeted intervention through Pre-Strategy Intervention (PSI) to achieve desired outcomes with minimal human guidance.


<details>
  <summary>Details</summary>
Motivation: Steering cooperative MARL towards desired outcomes is challenging when global human guidance is impractical in large-scale systems, and existing coordination mechanisms lack easy-to-use research tools.

Method: Leverages MAIDs as a graphical framework to analyze MARL approaches, designs targeted intervention paradigm applied to single agents using Pre-Strategy Intervention (PSI) causal inference technique, and uses bundled relevance graph analysis to verify MARL learning paradigms.

Result: Demonstrates effectiveness of targeted intervention in experiments and verifies relevance graph analysis results, showing that composite desired outcomes can be achieved by maximizing causal effects through PSI.

Conclusion: MAIDs provide an effective framework for analyzing and designing MARL interaction paradigms, with targeted intervention offering a practical solution to the global guidance problem while enabling achievement of desired outcomes through causal inference.

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [250] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: Proposed Contextual Attention Modulation (CAM) and Hybrid CAM (HyCAM) framework for efficient multi-task adaptation in LLMs, achieving 3.65% average performance improvement while preserving general knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-task adaptation, facing catastrophic forgetting and high resource costs. Existing parameter-efficient methods perform poorly in complex multi-task scenarios.

Method: CAM dynamically modulates self-attention representations. HyCAM combines shared full-parameter CAM with lightweight specialized CAM modules using dynamic routing for adaptive knowledge fusion.

Result: Significant performance improvement (3.65% average) across heterogeneous tasks including question answering, code generation, and logical reasoning.

Conclusion: CAM and HyCAM effectively balance knowledge retention with task-specific specialization, outperforming existing approaches in multi-task adaptation.

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [251] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: VLMs often perceive visual evidence but fail to use it effectively, leading to incorrect answers. An attention-based intervention that highlights evidence regions improves accuracy across multiple VLM families without training.


<details>
  <summary>Details</summary>
Motivation: To understand why VLMs fail on multimodal tasks despite having correct visual evidence, and to determine if failures stem from perception issues or reasoning limitations.

Method: Examined layer-wise attention dynamics in VLMs, revealing shallow layers focus on text while deeper layers attend to visual evidence. Introduced inference-time intervention using selective attention-based masking to highlight evidence regions.

Result: Found VLMs often perceive visual evidence when outputting wrong answers ("seeing but not believing"). The intervention consistently improved accuracy across LLaVA, Qwen, Gemma, and InternVL without requiring training.

Conclusion: VLMs encode reliable evidence internally but under-utilize it. Making these signals explicit can bridge perception-reasoning gaps, advancing VLM diagnostic understanding and reliability.

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [252] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM is an ORB-SLAM3-based system that combines semantic filtering for known moving objects with adaptive robust loss for unknown ones, achieving improved accuracy in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Existing Visual SLAM methods struggle in dynamic environments - semantic filtering only handles known objects, while fixed robust kernels can't adapt to unknown moving objects, leading to degraded accuracy.

Method: Combines lightweight semantic keypoint filtering for known moving objects with Barron's adaptive robust loss for unknown ones, where the shape parameter is estimated online from residuals to automatically adjust between Gaussian and heavy-tailed behavior.

Result: Improved trajectory accuracy and robustness over state-of-the-art baselines, achieving up to 25% lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining performance at 27 FPS on average across TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets.

Conclusion: VAR-SLAM effectively handles both known and unknown moving objects in dynamic environments through adaptive robust loss estimation, demonstrating superior performance compared to existing methods.

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [253] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip is a specialized 3-DOF gripper for robotic disassembly of end-of-life computer desktops, featuring cable-driven transmission for compact operation in confined spaces and decoupled wrist-jaw actuation.


<details>
  <summary>Details</summary>
Motivation: Intelligent robotic disassembly of EOL products faces challenges due to lack of specialized hardware, limiting real-world application of machine learning techniques.

Method: Developed DeGrip - a customized 3-DOF gripper with cable-driven transmission for reduced size, decoupled wrist-jaw actuation, and evaluated it in an Isaac Sim EOL desktop disassembly environment.

Result: Evaluation confirmed DeGrip's capability for EOL desktop disassembly, demonstrating operation in confined spaces and disassembly of components in arbitrary configurations.

Conclusion: DeGrip successfully addresses the hardware limitations in robotic disassembly, enabling effective disassembly of EOL computer desktops in real-world scenarios.

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [254] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: Cosmos-Surg-dVRK is a surgical fine-tuned world foundation model that enables automated online evaluation of surgical policies, showing strong correlation with real dVRK platform outcomes and promising results for complex procedures.


<details>
  <summary>Details</summary>
Motivation: Direct evaluation of surgical policies on physical robotic platforms like dVRK faces challenges including high costs, time demands, reproducibility issues, and execution variability.

Method: Developed Cosmos-Surg-dVRK, a surgical fine-tune of the Cosmos world foundation model, combined with a trained video classifier for automated online evaluation and benchmarking of surgical policies.

Result: Strong correlation between online rollouts in Cosmos-Surg-dVRK and real dVRK outcomes on suture pad tasks; good agreement between human labelers and V-JEPA2-derived video classifier; promising alignment with real-world evaluations for ex-vivo porcine cholecystectomy tasks.

Conclusion: Cosmos-Surg-dVRK offers a transformative approach for simulating complex surgical tasks with high fidelity, enabling efficient automated evaluation of surgical policies while overcoming physical platform limitations.

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [255] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA is a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation of Vision-Language-Action agents through fine-grained capability tests and systematic stress tests.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of VLA agents is hindered by coarse end-task success metrics that fail to provide precise skill diagnosis or measure robustness to real-world perturbations, exacerbated by fragmented data landscape.

Method: Introduces NEBULA with dual-axis evaluation protocol combining fine-grained capability tests for skill diagnosis and systematic stress tests for robustness measurement, plus standardized API and large-scale aggregated dataset.

Result: Demonstrates that top-performing VLAs struggle with key capabilities like spatial reasoning and dynamic adaptation, which are obscured by conventional end-task success metrics.

Conclusion: NEBULA provides a practical foundation for robust, general-purpose embodied agents by measuring both what an agent can do and when it does so reliably.

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [256] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: Training-free runtime policy steering method improves reasoning-action alignment in VLA models by sampling candidate actions, simulating outcomes, and selecting sequences that best match the model's own textual plans.


<details>
  <summary>Details</summary>
Motivation: Reasoning VLA models often generate correct textual plans but fail to execute corresponding actions properly, especially in out-of-distribution scenarios, due to lack of embodied CoT faithfulness.

Method: Framework samples multiple candidate action sequences from the VLA, predicts outcomes via simulation, and uses a pre-trained VLM to select sequences whose outcomes best align with the VLA's textual plan.

Result: Achieves up to 15% performance gain over prior work on behavior composition tasks, boosts robustness to semantic and visual OOD perturbations, and enables novel behavior composition without re-training.

Conclusion: The method transforms action diversity from a source of error into a strength, improving reasoning-action alignment and scaling with compute and data diversity while maintaining training-free operation.

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [257] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT is a unified planning framework that integrates sensing objectives into motion optimization for UAVs with single depth cameras, enabling earlier detection of dynamic obstacles and safer navigation in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: UAVs with single depth cameras face challenges in dynamic obstacle avoidance due to limited field of view and blind spots. Existing methods separate motion planning from sensing, leading to ineffective and delayed obstacle responses.

Method: Uses Gaussian Process-based obstacle belief map for probabilistic representation of recognized and potential obstacles, with collision-aware inference that creates time-varying observation urgency maps. Integrates urgency values within FOV to enable real-time observation-aware trajectory planning.

Result: Detects potential dynamic obstacles 2.8 seconds earlier than baselines, increases dynamic obstacle visibility by over 500%, enables safe navigation in cluttered/occluded environments, with computation times under 10 ms.

Conclusion: SPOT successfully addresses the limitations of separated planning and sensing approaches by providing a unified framework that explicitly incorporates sensing objectives into motion optimization, significantly improving dynamic obstacle avoidance capabilities.

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [258] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: The paper presents Manual2Skill++, a vision-language framework that treats connections as first-class primitives in assembly representation, automatically extracting structured connection information from assembly manuals to enable more reliable robotic assembly.


<details>
  <summary>Details</summary>
Motivation: Current robotic assembly approaches treat connectors as an afterthought, while connections represent the critical "last mile" that ultimately determines assembly success or failure. The paper aims to elevate connections to primary consideration in assembly planning.

Method: Uses a large-scale vision-language model to parse symbolic diagrams and annotations in assembly manuals, encoding tasks as hierarchical graphs where nodes represent parts/sub-assemblies and edges explicitly model connection relationships between components.

Result: Validated on a dataset of over 20 assembly tasks with diverse connector types, and evaluated across four complex assembly scenarios in simulation spanning furniture, toys, and manufacturing components with real-world correspondence.

Conclusion: By treating connections as first-class primitives and leveraging the rich connection knowledge embedded in human-designed assembly manuals, the framework enables more reliable robotic assembly execution.

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [259] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: This paper proposes IPMC (integrated perception, motion, and communication) for edge robotics to reduce communication overhead by dynamically adapting communication strategies based on robotic perception and motion dynamics, using an imitation learning approach that reduces computational complexity by 10x.


<details>
  <summary>Details</summary>
Motivation: Existing edge robotics methods ignore interdependency between robotic functionalities and communication conditions, leading to excessive communication overhead from frequent exchanges of large-volume multi-modal data.

Method: Proposes IPMC framework where robots dynamically adapt communication strategies (compression ratio, transmission frequency, transmit power) using robotic perception and motion knowledge. Uses imitation learning neural network based on learning to optimize (LTO) paradigm.

Result: The LTO approach reduces computational complexity by over 10x compared to state-of-the-art optimization solvers. Experiments demonstrate superiority of IPMC and real-time execution capability of LTO.

Conclusion: IPMC revolutionizes edge robotics systems by integrating perception, motion, and communication, enabling dynamic adaptation of communication strategies to reduce excessive sensor data uploads while maintaining real-time performance.

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [260] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: A dataset of 1,893 user questions for household robots was collected from 100 participants, organized into 12 categories and 70 subcategories, revealing diverse question types beyond typical why-questions.


<details>
  <summary>Details</summary>
Motivation: With increasing use of large language models and conversational interfaces in human-robot interaction, there's a need to understand what questions users actually want to ask household robots to improve their question-answering capabilities.

Method: Created 15 video and 7 text stimuli depicting robots performing household tasks, then asked Prolific participants what questions they would ask the robot in each situation, categorizing the 1,893 collected questions.

Result: Most frequent question categories were about task execution details (22.5%), robot capabilities (12.7%), and performance assessments (11.3%). Users ranked questions about handling difficult scenarios as most important, and novice users asked different questions than experienced users.

Conclusion: The dataset provides foundation for identifying needed robot logging information, benchmarking question-answering modules, and designing explanation strategies that align with user expectations as robots enter human-shared environments.

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [261] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D is the largest dataset for off-road autonomous driving, covering diverse terrains and environmental conditions, with comprehensive benchmarks for 5 fundamental tasks.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of large-scale, high-quality datasets and benchmarks in off-road autonomous driving research.

Method: Created ORAD-3D dataset covering various terrains (woodlands, farmlands, grasslands, etc.) and environmental variations (weather conditions, illumination levels), and established benchmark evaluations for 5 tasks.

Result: Developed the largest dataset specifically curated for off-road autonomous driving with comprehensive benchmark suite.

Conclusion: ORAD-3D provides a unified and robust resource for advancing perception and planning in challenging off-road scenarios, with dataset and code made publicly available.

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [262] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: The SPD gripper is a novel robotic gripper with linear parallel gripping mechanism that eliminates the need for height adjustment when grasping objects on tabletops, unlike traditional grippers with arcuate fingertip motion.


<details>
  <summary>Details</summary>
Motivation: Traditional industrial grippers with parallel gripping capabilities require height adjustment of the entire robotic arm to avoid tabletop collisions due to their arcuate fingertip motion, which the SPD gripper aims to solve.

Method: The SPD gripper features a palm and two mechanically identical, symmetrically arranged fingers that can be driven independently or by a single motor, with fingertips following linear motion trajectory. The paper presents design philosophy, composition principles, and optimization analysis theory, followed by prototype development and testing.

Result: Experimental results demonstrate that the SPD gripper successfully achieves linear parallel gripping functionality and exhibits good adaptability to objects of different shapes and sizes.

Conclusion: The SPD gripper effectively addresses the tabletop collision issue of traditional grippers and can assist various robots in achieving effective grasping, laying foundation for collecting data to enhance deep learning training in embodied intelligence technologies.

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [263] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav is a real-time navigation system that handles complex free-text queries with spatial relationships by decomposing them into simpler object queries, computing intersections of semantic belief maps, and validating results with LVLM.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot object navigation systems are limited to simple object name queries and cannot handle complex free-text instructions with spatial relationships like "find the remote on the table".

Method: Three-step relaxation approach: 1) Decompose complex spatial queries into simpler object-level queries, 2) Compute intersection of semantic belief maps to find regions where all objects co-exist, 3) Validate discovered objects against original constraints using LVLM. Also adapts frontier exploration for spatial search queries.

Result: Validated through extensive experiments on MultiON benchmark and real-world deployment on Boston Dynamics Spot robot with Jetson Orin AGX, demonstrating effective handling of complex spatial queries.

Conclusion: DIV-Nav successfully enables robots to navigate using complex free-text queries with spatial relationships, overcoming limitations of traditional zero-shot object navigation systems.

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [264] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff is a parallel gripper system with differential linkage mechanism that enables linear-parallel grasping, reduces recalibration needs by 30%, and supports adaptive grasping for various industrial objects.


<details>
  <summary>Details</summary>
Motivation: Address limited adaptability of conventional end-effectors in intelligent industrial automation by creating a more flexible and adaptive grasping solution.

Method: Uses innovative differential linkage mechanism with modular symmetric dual-finger configuration, planetary gear transmission, kinematically optimized parallelogram linkage, and differential mechanism to achieve synchronized linear motion and independent finger pose adjustment.

Result: Reduces Z-axis recalibration requirements by 30% compared to arc-trajectory grippers, demonstrates adaptive grasping capabilities for diverse industrial workpieces and deformable objects, and enables future sensor integration for multimodal data acquisition.

Conclusion: SP-Diff advances robotic end-effector intelligence through its adaptive architecture, showing promising applications in collaborative robotics, logistics automation, and specialized operational scenarios as a flexible manufacturing solution.

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [265] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA is a Vision-Language-Action framework that represents robot policies as linear combinations of learned basis functions, enabling rapid adaptation to new tasks with just one demonstration through lightweight optimization.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models often fail when deployed in novel environments, embodiments, or tasks, lacking robust out-of-the-box performance.

Method: Jointly learns basis functions across datasets to create a structured skill space, then adapts via convex optimization minimizing L1 action error using a single demonstration, without gradient updates.

Result: Achieves lower action-prediction error on all five unseen datasets and succeeds in simulation and real-robot tasks where pretrained VLA models fail outright.

Conclusion: MoS-VLA enables efficient adaptation to new tasks with minimal overhead while maintaining strong performance across diverse domains.

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [266] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: Survey of 22 First Responders across 8 countries shows positive attitudes toward robots and semantic information for situational awareness, with participants willing to use imperfect AI tools (requiring 74.6% accuracy for trust).


<details>
  <summary>Details</summary>
Motivation: To investigate First Responders' attitudes toward semantic information and Situational Awareness in robotic systems during emergency operations, addressing the gap between lab capabilities and field realities.

Method: Structured questionnaire administered to 22 First Responders across eight countries, capturing demographic profiles, attitudes toward robots, and experiences with semantics-enhanced SA.

Result: Most FRs expressed positive attitudes toward robots, rated semantic information usefulness at 3.6/5, valued it for predicting emergencies (3.9/5), and required 74.6% accuracy for trust. Most valued semantic types: object identity, spatial relationships, and risk context.

Conclusion: The study reveals critical gap between lab robotics and field deployment, highlighting need for meaningful collaboration between FRs and robotics researchers to develop more user-aligned robotic systems for emergency response.

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [267] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: Analysis of how excitation design affects inertia identification for nano/micro-satellites, comparing two estimators across different torque profiles and satellite configurations.


<details>
  <summary>Details</summary>
Motivation: To understand how excitation design influences the identification of inertia properties in rigid nano- and micro-satellites, providing practical guidance for in-orbit adaptive inertia identification.

Method: Simulated nonlinear attitude dynamics with reaction-wheel coupling, actuator limits, and disturbances; used eight torque profiles of varying spectral richness; compared batch Least Squares and Extended Kalman Filter estimators across three satellite configurations and time-varying inertia scenarios.

Result: Excitation frequency content and estimator assumptions jointly determine estimation accuracy and robustness, with conditions identified for optimal performance of each method.

Conclusion: The study offers practical guidance for in-orbit adaptive inertia identification by outlining the specific conditions under which each estimation method performs best, with open-source code provided.

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [268] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: Proposed Adaptive Invariant Extended Kalman Filter for legged robots that adaptively adjusts contact foot model noise based on online covariance estimation, improving state estimation under varying contact conditions without requiring contact sensors.


<details>
  <summary>Details</summary>
Motivation: State estimation is crucial for legged robots as it directly affects control performance and locomotion stability. Traditional slip rejection methods fail to handle small slips effectively, and overly sensitive settings risk filter divergence.

Method: Uses Adaptive Invariant Extended Kalman Filter that adaptively adjusts noise level of contact foot model based on online covariance estimation. Employs contact detection algorithm instead of contact sensors to reduce hardware reliance.

Result: Validated through real-world experiments on quadruped robot LeoQuad, demonstrating enhanced state estimation performance in dynamic locomotion scenarios. Effectively handles small slips that traditional slip rejection fails to address.

Conclusion: The proposed method improves proprioceptive state estimation for legged robots under varying contact conditions, reducing reliance on additional hardware while maintaining stability and performance in dynamic locomotion.

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [269] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner is an LLM-enabled robotic motion planning framework that uses formal methods to self-correct generated motion plans, addressing hallucination issues in traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional motion planning approaches struggle with domain-specific constraints, spatio-temporal couplings, and discrepancies between task planning and execution. LLMs can reason semantically but often produce infeasible motion plans due to hallucination.

Method: The framework uses three cascaded modules that decompose spatio-temporal constraints. Each module stimulates an LLM to generate candidate trajectory sequences and verifies their feasibility using Signal Temporal Logic (STL) until a valid plan satisfying spatial, temporal, and logical constraints is found.

Result: Experiments show T3 Planner significantly outperforms baselines across different scenarios. The reasoning capability can be distilled into a lightweight Qwen3-4B model for efficient deployment.

Conclusion: T3 Planner successfully addresses the challenge of translating natural language instructions into executable motion plans by combining LLM semantic reasoning with formal verification methods, enabling reliable and feasible robotic motion planning.

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [270] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: This paper proposes shifting Positioning, Navigation, and Timing (PNT) from tool-oriented to cognition-driven approaches by combining brain-inspired spatial cognition with machine precision through a four-layer fusion framework.


<details>
  <summary>Details</summary>
Motivation: Current PNT systems need to be more resilient, energy-efficient and cognitively capable for complex environments. The goal is to develop universal PNT by integrating biological brain navigation principles with machine precision.

Method: Proposes a four-layer fusion framework (observation-capability-decision-hardware) that unites numerical precision with brain-inspired intelligence, and provides multi-level analysis comparing traditional PNT, biological brain PNT, and brain-inspired PNT.

Result: A new perspective and roadmap for brain-inspired PNT development that enables unmanned systems to achieve spatial cognition navigation while maintaining high precision.

Conclusion: Brain-inspired PNT represents a paradigm shift from tool-oriented to cognition-driven approaches, offering promising directions for developing more resilient and intelligent universal PNT systems.

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [271] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: The paper introduces C-Free-Uniform, a trajectory sampling method that uniformly samples free configuration space while being conditioned on the local environment map, and integrates it into CFU-MPPI controller, achieving better navigation performance with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory sampling methods generate control inputs independently of the environment, which limits their efficiency in cluttered environments where sampling should focus on free space.

Method: Propose C-Free-Uniform sampling that generates control input distribution p(u|x) conditioned on local map to uniformly sample free configuration space, and integrate it into Model Predictive Path Integral (MPPI) controller as CFU-MPPI.

Result: CFU-MPPI outperforms existing methods in success rate for challenging navigation tasks in cluttered polygonal environments while requiring significantly smaller sampling budget.

Conclusion: Environment-aware trajectory sampling through C-Free-Uniform significantly improves navigation performance and sampling efficiency in complex environments.

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [272] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: The RAPID Hand is a low-cost, 20-DoF dexterous hand prototype designed for affordable teleoperation to collect real-robot data for learning from demonstrations.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of affordable, fully-actuated five-fingered hands needed for dexterous teleoperation to enable large-scale real-robot data collection.

Method: Developed a novel anthropomorphic actuation and transmission scheme with universal phalangeal transmission for non-thumb fingers and omnidirectional thumb actuation, using 3D-printed parts and custom gears for cost-effectiveness and repairability.

Result: The hand was evaluated on three challenging tasks (multi-finger retrieval, ladle handling, piano playing) and demonstrated significant promise for dexterous teleoperation through both quantitative metrics and qualitative testing.

Conclusion: The RAPID Hand's fully actuated 20-DoF design shows strong potential for enabling affordable dexterous teleoperation systems.

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [273] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA is a multimodal goal-conditioned behavior cloning framework for autonomous catheter navigation that fuses visual observations and joystick kinematics to reduce operator dependency in cardiac catheterization procedures.


<details>
  <summary>Details</summary>
Motivation: Current robotic catheter systems require continuous physician input (follow-leader approach), leading to operator fatigue, increased radiation exposure, and procedural outcome variability. There's a need for intelligent autonomy in minimally invasive cardiac interventions.

Method: Proposes DINO-CVA framework that fuses visual observations and joystick kinematics into joint embedding space. Uses goal-conditioned behavior cloning with autoregressive action prediction from expert demonstrations to guide navigation toward specified destinations.

Result: Achieves high accuracy in predicting actions, matching kinematics-only baseline performance while additionally grounding predictions in anatomical environment through vision-awareness.

Conclusion: Demonstrates feasibility of multimodal, goal-conditioned architectures for catheter navigation, representing important progress toward reducing operator dependency and improving reliability of catheter-based therapies.

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [274] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: A Cross-Entropy Method with Reward Model (CEM-RM) framework efficiently optimizes tendon-driven soft robotic hands using teleoperation data, reducing design evaluations by over 50% while learning optimized hand designs.


<details>
  <summary>Details</summary>
Motivation: Designing soft robotic hands that are both compliant and functional across diverse use cases is challenging, and the co-design search space is high-dimensional with computationally expensive simulation-based evaluation.

Method: Proposed CEM-RM framework that optimizes tendon-driven soft robotic hands based on teleoperation control policy, using parallelized training in simulation with a design space of flexural soft fingers, then 3D-printing and deploying optimized designs.

Result: The optimized hands significantly outperform baseline hands in grasping success rates across diverse challenging objects in both simulation and hardware experiments.

Conclusion: The CEM-RM framework efficiently optimizes soft robotic hand designs using teleoperation data, achieving better performance than baselines while reducing computational costs.

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [275] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: This survey systematically reviews approaches for improving Vision-Language-Action (VLA) model efficiency to address computational and memory constraints in edge platforms like mobile manipulators, categorizing solutions into four dimensions and discussing future challenges.


<details>
  <summary>Details</summary>
Motivation: VLA models face significant computational and memory demands that conflict with edge platform constraints requiring real-time performance, creating a need for more efficient and scalable VLA systems.

Method: The survey categorizes efficiency improvement approaches into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category.

Result: The paper provides a systematic framework for understanding and addressing VLA efficiency challenges, organizing existing solutions and highlighting key techniques for reducing latency, memory footprint, and computational costs.

Conclusion: The survey identifies future trends and open challenges in advancing efficient embodied intelligence, emphasizing the need for continued research in making VLA systems more practical for real-world edge applications.

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [276] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: A decentralized kinodynamic planning method using imitation learning for multi-UAV cable-suspended load transport, achieving performance comparable to centralized approaches without inter-agent communication.


<details>
  <summary>Details</summary>
Motivation: Existing multi-UAV load transport methods rely on centralized control or inter-agent communication, which limits scalability and robustness in real-world scenarios with partial observability.

Method: Uses imitation learning to train decentralized student policies by mimicking a centralized kinodynamic motion planner with global observations. Employs physics-informed neural networks to generate smooth trajectories that respect motion derivative relationships.

Result: Student policies can be trained in under two hours on a standard laptop. The method achieves performance comparable to centralized approaches in both simulation and real-world experiments, successfully following agile reference trajectories.

Conclusion: The proposed decentralized approach effectively handles partial observability and eliminates the need for inter-agent communication while maintaining performance levels similar to centralized methods, making it suitable for practical multi-UAV load transport applications.

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [277] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++ is an enhanced autonomous driving framework that bridges cognitive reasoning from Vision-Language-Action models with end-to-end planning through metric-guided alignment to handle both challenging scenarios and physical feasibility.


<details>
  <summary>Details</summary>
Motivation: Conventional E2E driving models lack world knowledge for long-tail scenarios, while VLA models have limited 3D reasoning leading to physically infeasible actions. The goal is to combine their complementary strengths.

Method: Three components: 1) VLA module for semantically grounded trajectories, 2) E2E module with dense trajectory vocabulary for physical feasibility, 3) Metric-guided trajectory scorer to align and integrate both modules.

Result: Achieved EPDMS of 49.12 on ICCV 2025 Autonomous Grand Challenge leaderboard.

Conclusion: DiffVLA++ successfully integrates cognitive reasoning and physical planning through metric-guided alignment, demonstrating improved performance in autonomous driving.

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [278] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC is a universal variable impedance controller enhanced by vision language models that improves safety and adaptation in contact-rich robotic manipulation tasks through self-improving RAG and ICL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional variable impedance controllers lack generalization in unseen, complex, and unstructured safe interactions in universal task scenarios involving contact or uncertainty, limiting their effectiveness in real-world applications.

Method: Uses a self-improving Retrieval-Augmented Generation (RAG) and in-context learning (ICL) framework where RAG retrieves relevant prior experiences from memory, and ICL leverages these examples with current task prompts to query VLM for generating adaptive impedance parameters, informed by real-time force/torque feedback.

Result: Outperforms baselines on complex contact-rich tasks in both simulation and real-world robotics, with average success rate increasing from 27% (baseline) to 61.4% (OmniVIC), showing improved success rates and reduced force violations.

Conclusion: OmniVIC bridges high-level semantic reasoning and low-level compliant control, enabling safer and more generalizable manipulation in universal task scenarios.

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [279] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: SimpleVSF is a novel end-to-end autonomous driving framework that combines Vision-Language Models (VLMs) with trajectory fusion techniques to improve planning decisions in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving methods face challenges with suboptimal decision-making in complex scenarios, requiring enhanced cognitive capabilities for better planning.

Method: The framework uses conventional scorers and VLM-enhanced scorers, combined with a robust weight fusioner for quantitative aggregation and a VLM-based fusioner for qualitative, context-aware decision-making.

Result: SimpleVSF achieved state-of-the-art performance as the leading approach in the ICCV 2025 NAVSIM v2 End-to-End Driving Challenge, demonstrating superior balance between safety, comfort, and efficiency.

Conclusion: The proposed SimpleVSF framework successfully enhances end-to-end planning by leveraging VLM cognitive capabilities and advanced fusion techniques, addressing key challenges in autonomous driving decision-making.

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [280] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: A novel self-localization system using event cameras integrates visible light communication (VLC) and visible light positioning (VLP) for vehicle positioning in GPS-denied environments like tunnels, achieving simultaneous communication and distance estimation.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer high temporal resolution and dynamic range, making them suitable for capturing fast-moving objects and handling extreme lighting contrasts. This enables reliable positioning in GPS-denied environments where conventional methods fail.

Method: The system uses multiple LEDs with unique Walsh-Hadamard codes for identification. An event camera correlates received signals with these codes to separate light sources, enabling MISO communication via VLC and distance estimation via phase-only correlation between LED pairs.

Result: Field experiments at 30 km/h showed robust performance: distance estimation RMSE within 0.75 m up to 100 m range, and bit error rate below 0.01 across the same range.

Conclusion: This is the first vehicle-mounted system to achieve simultaneous VLC and VLP using a single event camera, demonstrating practical real-world applicability for autonomous navigation in challenging environments.

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [281] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: Pole-Image is a novel canonical representation that uses poles as anchors to generate signatures from surrounding 3D structure, enabling robust self-localization and high-sensitivity change detection for long-term robot autonomy.


<details>
  <summary>Details</summary>
Motivation: Conventional landmark-based methods face a trade-off between landmarks with high detectability but low distinctiveness (poles) and those with high distinctiveness but difficult stable detection (local point clouds). This work aims to combine the advantages of both by using poles as anchors for descriptive signatures.

Method: Proposes Pole-Image representation that converts a pole-like landmark and its surrounding environment into a 2D polar coordinate image with the pole as origin. Uses contrastive learning to learn viewpoint-invariant and highly discriminative descriptors from automatically collected observational data.

Result: The descriptor overcomes perceptual aliasing for robust self-localization, and the high-precision encoding enables high-sensitivity change detection for map maintenance.

Conclusion: Pole-Image provides a hybrid solution that leverages the easy detection of poles to create distinctive signatures from surrounding point clouds, enabling both reliable self-localization and map maintenance for long-term robot autonomy.

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [282] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: A modular control framework for legged robots that combines model-based dynamic control with online model adaptation and adaptive footstep planning to handle uncertainties in robot and terrain properties, enabling autonomous navigation in extreme environments.


<details>
  <summary>Details</summary>
Motivation: Planetary exploration requires robots that can navigate extreme, unknown environments. While wheeled rovers have limitations on uneven terrain, legged robots can overcome these challenges but face difficulties with environment-specific control when terrain and robot parameters are uncertain.

Method: Developed a modular control framework combining model-based dynamic control with online model adaptation and adaptive footstep planning. Includes state estimation for quadrupeds with/without contact sensing, supports runtime reconfiguration, and is integrated into ROS 2.

Result: Validated on two quadruped platforms, multiple hardware architectures, and in a volcano field test where the robot walked over 700 meters, demonstrating robust performance in extreme environments.

Conclusion: The framework successfully enables legged robots to autonomously navigate uncertain and extreme environments by addressing uncertainties in both robot and terrain properties through adaptive control and planning.

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [283] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: A framework for detecting spurious behaviors in multi-robot systems using Linear Temporal Logic specifications and Transformer-based anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable execution of high-level missions in multi-robot systems by detecting spurious behaviors like incorrect task sequences, spatial constraint violations, timing inconsistencies, and deviations from intended mission semantics.

Method: Proposes a structured data generation framework using Nets-within-Nets paradigm to coordinate robot actions with LTL mission specifications, and a Transformer-based anomaly detection pipeline for classifying robot trajectories.

Result: Achieves 91.3% accuracy in identifying execution inefficiencies, 88.3% for core mission violations, and 66.8% for constraint-based adaptive anomalies. Ablation studies show superior performance over simpler representations.

Conclusion: The proposed method effectively detects spurious behaviors in multi-robot systems with high accuracy, demonstrating robust detection capabilities across various anomaly types.

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [284] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: FeLaN introduces a physics-constrained neural network parameterization for floating-base systems that ensures physical consistency in inertia matrices while maintaining competitive performance on robot dynamics modeling.


<details>
  <summary>Details</summary>
Motivation: Current grey-box models ignore specific physical constraints of floating-base systems like humanoids and quadrupeds, particularly the positive definiteness, branch-induced sparsity, input independence, and composite spatial inertia properties.

Method: Developed Floating-Base Deep Lagrangian Networks (FeLaN) that parameterize inertia matrices to satisfy all physical constraints, training neural networks to predict physically plausible inertia matrices that minimize inverse dynamics error under Lagrangian mechanics.

Result: FeLaN achieves highly competitive performance on both simulated and real robots across multiple quadrupeds and humanoids, while providing greater physical interpretability compared to existing methods.

Conclusion: The proposed FeLaN framework successfully addresses the lack of physical consistency in deep learning models for floating-base systems by incorporating specific physical constraints into the inertia matrix parameterization, leading to improved generalization and interpretability.

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [285] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: A novel video planning framework that integrates interaction-time data to adapt to failures and uncertainties in partially observed environments, enabling dynamic replanning without explicit state modeling.


<details>
  <summary>Details</summary>
Motivation: Existing video planning frameworks struggle to adapt to failures at interaction time due to inability to reason about uncertainties in partially observed environments.

Method: Integrates interaction-time data into planning process by updating model parameters online and filtering out previously failed plans during generation, enabling implicit state estimation.

Result: Extensive experiments on a new simulated manipulation benchmark demonstrate improved replanning performance and advancement in video-based decision-making.

Conclusion: The framework successfully enables dynamic adaptation to failures and uncertainties in partially observed environments, advancing video-based planning capabilities.

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [286] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: DDBot is a differentiable digging robot framework that uses GPU-accelerated differentiable physics simulation to efficiently identify unknown granular material properties and optimize high-precision digging skills, achieving zero-shot real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to achieve efficiency and accuracy in granular material manipulation due to complex contact dynamics, unpredictable material properties, and intricate system states.

Method: Proposed DDBot framework with differentiable physics-based simulator, GPU-accelerated parallel computing, automatic differentiation, differentiable system identification, skill-to-action mapping, task-oriented demonstration, gradient clipping, and line search-based gradient descent.

Result: DDBot efficiently converges within 5-20 minutes to identify unknown granular dynamics and optimize digging skills, achieving high-precision results in zero-shot real-world deployments and outperforming state-of-the-art baselines.

Conclusion: DDBot demonstrates practical robustness and efficiency for granular material manipulation tasks, successfully addressing the challenges of complex contact dynamics and unknown material properties.

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [287] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: Proposes a unified Interactive Force-Impedance Control (IFIC) framework that ensures safe robot-human interaction in contact-rich environments by adapting to interaction power flow and guaranteeing system passivity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human-robot collaboration are primarily effective in contact-sparse environments but fail to maintain safety when robots physically interact with active humans or non-passive environments, potentially losing passivity and compromising safety.

Method: Develops a unified Interactive Force-Impedance Control (IFIC) framework formulated within a port-Hamiltonian framework, incorporating both interaction and task control ports to guarantee system passivity while enabling flexible role adaptation.

Result: The proposed control architecture ensures effortless and safe interaction in contact-rich environments by adapting to interaction power flow and maintaining system passivity.

Conclusion: The IFIC framework successfully addresses the safety challenges in human-robot physical interaction by providing a unified control approach that guarantees passivity while enabling flexible role switching between active leader and passive follower modes.

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [288] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: The paper demonstrates that Vision-Language-Action (VLA) models can be successfully deployed on soft continuum manipulators through targeted finetuning, enabling safe human-robot interaction in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: Robotic systems need to operate safely in human-centered environments, but current VLA models are limited to rigid serial manipulators and lack safe interaction capabilities. Soft continuum manipulators offer safety advantages but haven't been integrated with VLA frameworks.

Method: Developed a structured finetuning and deployment pipeline for two state-of-the-art VLA models (OpenVLA-OFT and π₀) on a soft continuum manipulator, evaluating performance across representative manipulation tasks.

Result: Out-of-the-box VLA policies failed due to embodiment mismatch, but targeted finetuning enabled the soft robot to perform equally to rigid counterparts in manipulation tasks.

Conclusion: Finetuning is essential for bridging embodiment gaps, and combining VLA models with soft robots enables safe, flexible embodied AI for human-shared environments.

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [289] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: A trustworthy AI system using CNN with MobileNetV2 achieves high accuracy in waste classification (99.8% training, 80.5% validation) and integrates with an energy-efficient robotic arm simulator for optimal waste sorting in urban environments.


<details>
  <summary>Details</summary>
Motivation: To develop an intelligent, reliable, and scalable waste management system that combines trustworthy AI principles with energy-efficient robotic sorting for urban settings.

Method: Uses convolutional neural network (CNN) enhanced through transfer learning with MobileNetV2 for waste classification into six categories, combined with a robotic arm simulator that calculates energy costs using Euclidean distance for optimal movement.

Result: Achieved 99.8% training accuracy and 80.5% validation accuracy for waste classification, with successful virtual sorting implementation using energy-efficient robotic arm movements.

Conclusion: The framework provides a reliable and scalable solution for smart waste management that incorporates trustworthy AI principles (transparency, robustness, fairness, safety) while maintaining high classification accuracy and energy efficiency.

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [290] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON introduces a novel paradigm that injects rich 3D spatial tokens into the action head of vision-language-action models, addressing the spatial reasoning gap in existing 2D-based VLAs while preserving language reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models built on 2D encoders have a spatial reasoning gap that limits generalization and adaptability. Current 3D integration techniques either require specialized sensors with poor cross-modality transfer, or inject weak cues lacking geometry that degrade vision-language alignment.

Method: FALCON leverages spatial foundation models to extract strong geometric priors from RGB alone, with an optional Embodied Spatial Model that can fuse depth or pose data without retraining. Spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone to preserve language reasoning.

Result: In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.

Conclusion: FALCON successfully addresses limitations in spatial representation, modality transferability, and alignment in VLA models, demonstrating superior performance and robustness across diverse evaluation scenarios.

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [291] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: A framework for switching between different output sets in nonlinear control systems using feedback linearization, with formal guarantees for stability and seamless transitions.


<details>
  <summary>Details</summary>
Motivation: To enable flexible control of nonlinear systems by allowing switching between different output sets while maintaining stability, which is useful for applications like robotics and vehicles.

Method: Introduces the concept of 'melds' - valid feedback-linearizable output subsets, with dwell-time and compatibility conditions for switching between melds while preserving stability.

Result: Proves uniform boundedness of system state during switching, exponential stability of error dynamics within intervals, and seamless tracking of common outputs across transitions.

Conclusion: The framework provides a systematic approach for output switching in feedback-linearizable nonlinear systems with formal stability guarantees, applicable to various systems like robots and vehicles.

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [292] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC is a Model Predictive Control framework for 3D Micro Air Vehicle navigation among humans that combines safety guarantees with data-driven human motion forecasting, enabling safe yet efficient navigation.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus on simplified 2D crowd navigation and fail to account for the full complexity of human body dynamics beyond root motion, limiting safe robotic navigation in human environments.

Method: Introduces a novel twist to reachability-based safety formulation that constrains only the initial control input for safety while modeling its effects over the entire planning horizon, combined with data-driven models for realistic human motion forecasting.

Result: Validated in both simulated experiments using real human trajectories and real-world scenarios, demonstrating effectiveness across tasks ranging from goal-directed navigation to visual servoing for human tracking. Outperforms baseline approaches in both efficiency and reliability.

Conclusion: The method ensures safety without excessive conservatism and is generic enough to be adapted by other platforms beyond MAVs, providing a robust framework for safe robotic navigation among humans.

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [293] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: Proposes D-PDDP, a distributed spatial-temporal trajectory optimization framework using ADMM and PDDP for large-scale UAV swarms, with adaptive penalty tuning to reduce iterations.


<details>
  <summary>Details</summary>
Motivation: Existing swarm trajectory optimization methods require pre-setting final times and suffer from time-consuming iterations, limiting practical application to large-scale UAV swarms.

Method: Two-level architecture combining ADMM for multi-UAV consensus and PDDP for fast local planning, with adaptive penalty parameter tuning via spectral gradient method.

Result: Developed D-PDDP algorithm that enables fully distributed optimization and reduces iteration count through adaptive parameter tuning.

Conclusion: The proposed framework effectively addresses large-scale UAV swarm trajectory optimization with improved computational efficiency and distributed implementation.

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [294] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: Proposes an intent-driven planning pipeline using LLMs for multi-robot manipulation tasks, specifically for dismantling EV batteries, with ensemble planning and verification components.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of planning complex manipulation tasks with multiple robots in unstructured environments where objects appear in arbitrary positions, requiring robust action sequence generation from human instructions.

Method: Uses a four-component pipeline: perception-to-text scene encoding, LLM ensemble for candidate sequence generation, LLM-based verifier for constraints, and deterministic filter to reject hallucinated objects.

Result: Evaluated on 200 real scenes with 600 operator prompts across five component classes, showing reliable mapping of operator intent to safe, executable multi-robot plans while maintaining low user effort.

Conclusion: The ensemble-with-verification approach effectively generates executable multi-robot plans from human instructions for complex manipulation tasks like EV battery dismantling.

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [295] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: The paper extends TLIO for bicycle localization using an improved Mixture-of-Experts model that significantly reduces computational costs while maintaining comparable accuracy to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional GNSS-based bicycle localization suffers from multipath effects, while inertial navigation methods lack robustness and existing learned approaches like TLIO have high computational costs that limit mobile deployment.

Method: Extended TLIO for bicycle localization and introduced an improved Mixture-of-Experts (MoE) model to reduce training and inference costs.

Result: Achieved comparable accuracy to state-of-the-art LLIO framework while reducing parameters by 64.7% and computational cost by 81.8%.

Conclusion: The proposed improved MoE model enables efficient bicycle localization with significantly reduced computational requirements while maintaining accuracy.

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [296] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: RESample is an automated OOD data augmentation framework that enhances VLA model robustness by sampling failure states and using exploratory sampling to improve recovery from distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Current imitation learning datasets lack failure and recovery data, causing VLA models to struggle with out-of-distribution states from minor perturbations.

Method: Leverage offline RL to identify sub-optimal actions, sample OOD states via rollout, and use exploratory sampling to incorporate action proxies into training data.

Result: Extensive experiments on LIBERO benchmark and real-world tasks show RESample consistently improves VLA model stability and generalization ability.

Conclusion: RESample effectively enhances VLA model robustness against distributional shifts through automated OOD data augmentation.

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [297] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot is a robotic system that creates detailed 3D digital twins of plants by using stereo cameras, a turntable, and a robot arm to manipulate leaves and capture occluded details like stem buds and leaf undersides.


<details>
  <summary>Details</summary>
Motivation: Commercial plant phenotyping systems with fixed cameras cannot capture detailed plant information due to leaf occlusion, limiting their ability to perceive important plant features.

Method: The system uses two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmented Gaussian Splat models. It includes robot algorithms for leaf manipulation to capture high-resolution images of occluded areas.

Result: Botany-Bot achieved 90.8% leaf segmentation accuracy, 86.2% leaf detection accuracy, 77.9% leaf manipulation accuracy, and 77.3% accuracy in capturing detailed overside/underside images.

Conclusion: The system successfully creates annotated digital twins of plants with high accuracy in segmentation, detection, and manipulation tasks, providing detailed plant phenotyping capabilities.

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [298] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic is a framework for learning compliant whole-body control policies for humanoid robots from human motions, enabling safe interaction with the environment by absorbing disturbances rather than rigidly tracking reference motions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for imitating human motions with reinforcement learning produce stiff control that aggressively corrects deviations, leading to brittle and unsafe behavior when robots encounter unexpected contacts.

Method: Uses an inverse kinematics solver to generate an augmented dataset of feasible compliant motions, then trains a reinforcement learning policy that rewards matching compliant responses rather than rigid tracking of reference motions.

Result: The method enables robots to respond compliantly to external forces while maintaining balance and posture, and can generalize to varied tasks from a single motion clip.

Conclusion: SoftMimic demonstrates safe and effective interaction with the environment through both simulations and real-world experiments, providing a robust approach for compliant whole-body control.

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [299] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBench is a comprehensive benchmark for evaluating MLLMs as embodied brains in robotic manipulation, covering 5 dimensions across 14 capabilities, 25 tasks, and 6092 QA pairs using real robotic data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on execution success or have limited task realism, failing to provide comprehensive evaluation of high-level cognitive capabilities in embodied systems.

Method: Created RoboBench with datasets from diverse embodiments, attribute-rich objects, and multi-view scenes using large-scale real robotic data, and introduced MLLM-as-world-simulator framework for planning evaluation.

Result: Experiments on 14 MLLMs revealed fundamental limitations in implicit instruction comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis.

Conclusion: RoboBench provides a comprehensive scaffold to quantify high-level cognition and guide development of next-generation embodied MLLMs.

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
