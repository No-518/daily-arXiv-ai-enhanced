<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.AI](#cs.AI) [Total: 29]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus-1.5 is a unified GUI agent family (2B, 8B, 30B-A3B variants) achieving SOTA performance on GUI automation benchmarks through mid-training, online RL, and model merging techniques.


<details>
  <summary>Details</summary>
Motivation: GUI agents struggle to balance broad generality with strong task performance; existing solutions lack robustness for real-world applications across diverse digital environments.

Method: Three key advances: 1) Mid-training with 10B tokens across 30+ datasets for GUI semantics foundation; 2) Online RL with full-trajectory rollouts for long-horizon navigation; 3) Unified agent via model merging of domain-specific models (grounding, web, mobile).

Result: Establishes new SOTA: ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), AndroidWorld (77.6%); demonstrates robust navigation across Chinese mobile apps in real-world scenarios.

Conclusion: UI-Venus-1.5 successfully addresses the generality-performance tradeoff in GUI agents, providing a unified solution for robust real-world applications across diverse digital environments.

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Agent Banana: hierarchical agentic planner-executor framework for professional image editing with Context Folding and Image Layer Decomposition to address over-editing, multi-turn consistency, and high-resolution workflow challenges.


<details>
  <summary>Details</summary>
Motivation: Address three persistent challenges in instruction-based image editing: (1) editors often over-edit beyond user intent, (2) existing models are single-turn while multi-turn edits degrade object faithfulness, and (3) evaluation at ~1K resolution misaligns with real workflows using ultra high-definition images (e.g., 4K).

Method: Propose Agent Banana, a hierarchical agentic planner-executor framework with two key mechanisms: Context Folding (compresses long interaction histories into structured memory for stable long-horizon control) and Image Layer Decomposition (performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs).

Result: On HDD-Bench (new high-definition benchmark with 4K images), Agent Banana achieves best multi-turn consistency and background fidelity (IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while competitive on instruction following. Also performs well on standard single-turn editing benchmarks.

Conclusion: Agent Banana advances reliable, professional-grade agentic image editing and its integration into real workflows by addressing key challenges in multi-turn consistency, object faithfulness, and high-resolution editing through hierarchical planning and structured memory mechanisms.

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

</details>


### [3] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: SemanticMoments uses temporal statistics from pre-trained semantic models to understand motion in videos, outperforming existing methods on new motion-focused benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing video representation methods rely too much on static appearance and scene context rather than motion dynamics, while traditional motion inputs like optical flow lack semantic understanding of high-level motion.

Method: SemanticMoments - a simple, training-free method that computes temporal statistics (higher-order moments) over features from pre-trained semantic models to capture motion dynamics.

Result: Outperforms existing RGB, flow, and text-supervised methods across new SimMotion benchmarks (synthetic + real-world annotated data), showing better motion understanding and disentanglement from appearance.

Conclusion: Temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding, addressing the gap in current video representation approaches.

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [4] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: A framework for automatically detecting and extracting personal names from news videos with transparent, auditable processing, outperforming generative methods in reliability despite slightly lower accuracy.


<details>
  <summary>Details</summary>
Motivation: Growing volume of video news content requires transparent methods for extracting on-screen information, but variability in graphical layouts and design patterns makes manual indexing impractical.

Method: Proposes an interpretable, modular extraction pipeline with a curated corpus of annotated news frames, designed for deterministic and auditable operation, evaluated against generative multimodal methods.

Result: Pipeline achieves 95.8% mAP@0.5 for graphical element localization, balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability. Generative methods achieve higher raw accuracy (F1: 84.18% vs 77.08%) but lack transparency.

Conclusion: Establishes a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in news media, highlighting the trade-off between deterministic auditability and stochastic inference.

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [5] [Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images](https://arxiv.org/abs/2602.09155)
*Ahmed Rahu,Brian Shula,Brandon Combs,Aqsa Sultana,Surendra P. Singh,Vijayan K. Asari,Derrick Forchetti*

Main category: cs.CV

TL;DR: Machine learning can detect subtle histological features in low-grade adenomas that predict future colorectal cancer risk.


<details>
  <summary>Details</summary>
Motivation: Despite screening, many patients with low-grade adenomas still develop CRC later. Current histology can't fully identify high-risk patients, creating need for better predictive tools.

Method: Use convolutional neural networks (CNNs) to analyze whole-slide images of low-grade tubular adenomas for subtle features predictive of CRC risk.

Result: Not specified in abstract (study investigates whether CNNs can detect predictive features).

Conclusion: Machine learning offers objective, comprehensive analysis of histology to identify high-risk patients for tailored surveillance and prevention.

Abstract: Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.

</details>


### [6] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: Proposes ASQL Conditioner, a zero-shot scene graph-based method for text-to-image synthesis that uses soft visual guidance to improve compositional abilities while maintaining text-image alignment.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with semantic fidelity and structural coherence when processing complex prompts with multiple objects, attributes, and spatial relationships. Existing approaches using rigid layout constraints limit compositional flexibility and diversity.

Method: Introduces a zero-shot, scene graph-based conditioning mechanism with ASQL (Attribute-Size-Quantity-Location) Conditioner. Uses a lightweight language model to generate soft visual guidance and guides diffusion-based generation through inference-time optimization.

Result: Enables maintenance of text-image alignment while supporting lightweight, coherent, and diverse image synthesis. Improves compositional abilities of existing models without rigid constraints.

Conclusion: Scene graph-based conditioning with soft visual guidance provides a promising approach to enhance text-to-image synthesis for complex prompts, offering better compositional flexibility and diversity compared to rigid layout-based methods.

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [7] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: A lightweight CNN-RNN model forecasts foot center-of-pressure and time-of-impact from shank-mounted RGB-D camera data during stair ascent transitions, achieving reasonable accuracy with real-time performance.


<details>
  <summary>Details</summary>
Motivation: While computer vision is used for environmental classification in gait analysis, predicting how the foot will contact changing environments (like stair transitions) is underexplored but crucial for anticipatory control in assistive systems.

Method: Used a CNN-RNN model trained on data from 8 subjects wearing an RGB-D camera on their right shank and instrumented insoles while stepping onto stairs. The model forecasts anterior-posterior foot center-of-pressure and time-of-impact within a 250ms window before foot-strike.

Result: Achieved COP mean absolute errors of 29.42mm, 26.82mm, and 23.72mm at 150ms, 100ms, and 50ms forecast horizons respectively. TOI errors were 21.14ms, 20.08ms, and 17.73ms at the same horizons. The lightweight model runs at 60 FPS on consumer hardware.

Conclusion: Forecasting COP and TOI from visual data is feasible with a lightweight model, enabling real-time anticipatory control for assistive systems during environmental transitions like stair ascent.

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [8] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: VLM-UQBench: A benchmark for modality-specific uncertainty quantification in vision-language models, revealing current methods' limitations in detecting fine-grained, instance-level ambiguity.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is crucial for safe VLM deployment, but current methods lack ability to localize uncertainty to specific sources (image, text, or cross-modal misalignment).

Method: Created VLM-UQBench with 600 real-world samples from VizWiz, curated into uncertainty subsets, plus scalable perturbation pipeline with visual, textual, and cross-modal perturbations. Proposed two metrics to evaluate UQ sensitivity and correlation with hallucinations.

Result: Existing UQ methods show modality-specific specialization and VLM dependence; modality uncertainty co-occurs with hallucinations but current scores provide weak risk signals; UQ methods fail to detect subtle instance-level ambiguity despite rivaling chain-of-thought on overt ambiguity.

Conclusion: Significant gap exists between current UQ practices and the fine-grained, modality-aware uncertainty needed for reliable VLM deployment, highlighting need for better uncertainty localization methods.

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [9] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: IR-SIS: An iterative refinement system for surgical image segmentation that uses natural language descriptions and clinician feedback to adaptively refine segmentations.


<details>
  <summary>Details</summary>
Motivation: Existing surgical segmentation methods are limited to predefined categories, produce one-shot predictions without refinement, and lack clinician interaction mechanisms.

Method: Uses fine-tuned SAM3 for initial segmentation, Vision-Language Model for instrument detection and quality assessment, and agentic workflow for adaptive refinement strategies with natural language feedback.

Result: Achieves state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements.

Conclusion: Establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [10] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: Modulation-based text conditioning in diffusion transformers is unnecessary for basic performance but can be repurposed as guidance for controllable improvements.


<details>
  <summary>Details</summary>
Motivation: To investigate whether modulation-based text conditioning is necessary in diffusion transformers, given recent approaches discard it in favor of attention-only mechanisms, and explore if it can provide any performance advantages.

Method: Analyze conventional usage of pooled text embeddings, then repurpose them as guidance for controllable shifts toward desirable properties without additional training.

Result: Pooled embeddings contribute little to overall performance in conventional usage, but when used as guidance, they provide significant gains across text-to-image/video generation and image editing tasks.

Conclusion: Modulation-based text conditioning is not necessary for basic performance but can be effectively repurposed as training-free guidance for controllable improvements across diverse diffusion tasks.

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [11] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: X-Mark is a sample-specific clean-label watermarking method for chest X-ray copyright protection that generates unique perturbations in salient regions using a conditional U-Net, ensuring watermark efficacy, robustness against dynamic scaling, and diagnostic quality preservation.


<details>
  <summary>Details</summary>
Motivation: Medical imaging datasets are valuable for training deep learning models but face unauthorized use issues. Existing watermarking methods designed for natural images don't work well for medical images due to dynamic scaling, high resolution, limited visual diversity, and the need to preserve diagnostic quality.

Method: X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each chest X-ray sample. It employs a multi-component training objective with Laplacian regularization to penalize high-frequency perturbations and achieve scale-invariance. Ownership verification is performed in black-box settings by detecting characteristic behaviors in suspicious models.

Result: Extensive experiments on CheXpert dataset show X-Mark achieves 100% watermark success rate (WSR) and reduces probability of false positives in Ind-M scenario by 12%. The method demonstrates resistance to potential adaptive attacks while preserving diagnostic quality.

Conclusion: X-Mark provides an effective solution for medical image copyright protection that addresses the unique challenges of medical imaging, offering robust watermarking that preserves diagnostic quality and enables reliable ownership verification in black-box settings.

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [12] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: Deep multi-modal method predicts patient hospitalization risk using wound variables and images, with transfer learning for wound assessment and healing trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Hospitalization drives high wound care costs, often due to delayed treatment, patient non-compliance, or comorbidities causing wound deterioration. Early risk prediction could prevent hospitalizations and reduce clinician diagnosis time.

Method: Deep multi-modal approach combining wound variables and wound images. Uses transfer learning-based wound assessment solution to predict both wound variables from images and healing trajectories.

Result: Proposed model can predict patient's risk of hospitalization confidently by collectively analyzing wound variables and images. Can detect wound complexities early that might affect healing process.

Conclusion: Novel deep multi-modal model enables early detection of wound complications, potentially preventing hospitalizations and reducing clinician workload in wound diagnosis.

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [13] [GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification](https://arxiv.org/abs/2602.09318)
*Lin-Guo Gao,Suxing Liu*

Main category: cs.CV

TL;DR: GAFRNet is a graph attention and fuzzy rule network for breast cancer histopathology classification that addresses limited annotations and interpretability issues in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning for breast cancer histopathology classification suffers from performance degradation with limited annotations and lacks interpretability (blackbox nature), hindering clinical integration.

Method: GAFRNet constructs similarity-driven graph representations to model intersample relationships, uses multihead graph attention to capture complex relational features, and incorporates a differentiable fuzzy-rule module that encodes topological descriptors (node degree, clustering coefficient, label consistency) into explicit, human-understandable IF-THEN diagnostic logic.

Result: Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) show GAFRNet consistently outperforms state-of-the-art methods across multiple magnifications and classification tasks.

Conclusion: GAFRNet demonstrates superior generalization and practical utility as a reliable, interpretable decision-support tool for weakly supervised medical image analysis, providing transparent reasoning that mimics medical expert deduction.

Abstract: Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>


### [14] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: The paper evaluates deep learning models (CNNs and transformers) for bladder cancer classification, finding ConvNext has limited generalization (~60% accuracy), ViTs show better calibration, and no single model provides optimal interpretability for both in-distribution and out-of-distribution samples.


<details>
  <summary>Details</summary>
Motivation: Vision transformers and CNNs perform well on natural images but may not generalize to medical imaging where abnormalities are small, motivating evaluation of these models for bladder cancer classification tasks.

Method: Three evaluation approaches: 1) Standard classification using 13 models (4 CNNs, 8 transformer-based), 2) Calibration analysis to assess model confidence calibration, and 3) GradCAM++ for interpretability evaluation. Used test time augmentation and conducted ~300 experiments on a multicenter bladder cancer dataset.

Result: ConvNext series showed limited generalization ability (~60% accuracy). ViTs demonstrated better calibration effects compared to ConvNext and Swin Transformer series. No single model provided optimal interpretability - ConvNext worked better for in-distribution samples, while ViT variants were better for out-of-distribution sample interpretation.

Conclusion: No one-size-fits-all solution exists for interpretable bladder cancer classification. Model selection should consider the specific use case: ConvNext for in-distribution samples, ViT variants for out-of-distribution interpretation, with ViTs showing superior calibration properties.

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [15] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: Kyrtos methodology for automatic recognition and analysis of charts with curves in technical documents, using clustering for line-segment recognition and converting to attributed graphs and natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: Deep understanding of technical documents requires accurate analysis of multiple modalities (graphics, tables, diagrams, text) and their associations. There's a need for automatic recognition and analysis of charts with curves in technical documents to extract valuable knowledge.

Method: Two-part approach: 1) Recognition processing using clustering-based method to identify middle-points delimiting line-segments that construct curves. 2) Analysis processing that parses extracted line-segments to capture behavioral features (direction, trend), converts segments' relations into attributed graphs, expresses graph relations into natural language text, and facilitates conversion into Stochastic Petri-net graphs.

Result: Extensive evaluation demonstrates accuracy of Kyrtos' recognition and analysis methods by measuring structural similarity between input chart curves and Kyrtos-generated approximations for charts with multiple functions.

Conclusion: Kyrtos methodology successfully enables automatic recognition and analysis of charts with curves in technical documents, preserving structural characteristics through attributed graphs and natural language descriptions, ultimately facilitating conversion to Stochastic Petri-net graphs for functional representation.

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [16] [Impact of domain adaptation in deep learning for medical image classifications](https://arxiv.org/abs/2602.09355)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: Domain adaptation improves medical image analysis across multiple scenarios including multi-modality data, noise robustness, federated learning, interpretability, and calibration.


<details>
  <summary>Details</summary>
Motivation: To explore how domain adaptation techniques can address various challenges in medical image analysis, including handling multi-modality data, noisy environments, federated learning constraints, improving model interpretability for clinical use, and enhancing classifier calibration.

Method: Used 10 deep learning models to simulate common DA techniques across four medical image datasets, testing scenarios including multi-modality, noisy data, federated learning, interpretability analysis with GradCAM++, and classifier calibration.

Result: DA with ResNet34 improved brain tumor classification by 4.7%; reduced Gaussian noise impact with ~3% accuracy increase; limited FL improvement (~0.3%); enhanced interpretability via GradCAM++; and reduced calibration error by ~2% compared to CNN alone.

Conclusion: Domain adaptation provides significant benefits for medical image analysis across multiple dimensions including performance, noise robustness, interpretability, and calibration, though its impact in federated learning settings appears limited.

Abstract: Domain adaptation (DA) is a quickly expanding area in machine learning that involves adjusting a model trained in one domain to perform well in another domain. While there have been notable progressions, the fundamental concept of numerous DA methodologies has persisted: aligning the data from various domains into a shared feature space. In this space, knowledge acquired from labeled source data can improve the model training on target data that lacks sufficient labels. In this study, we demonstrate the use of 10 deep learning models to simulate common DA techniques and explore their application in four medical image datasets. We have considered various situations such as multi-modality, noisy data, federated learning (FL), interpretability analysis, and classifier calibration. The experimental results indicate that using DA with ResNet34 in a brain tumor (BT) data set results in an enhancement of 4.7\% in model performance. Similarly, the use of DA can reduce the impact of Gaussian noise, as it provides $\sim 3\%$ accuracy increase using ResNet34 on a BT dataset. Furthermore, simply introducing DA into FL framework shows limited potential (e.g., $\sim 0.3\%$ increase in performance) for skin cancer classification. In addition, the DA method can improve the interpretability of the models using the gradcam++ technique, which offers clinical values. Calibration analysis also demonstrates that using DA provides a lower expected calibration error (ECE) value $\sim 2\%$ compared to CNN alone on a multi-modality dataset.

</details>


### [17] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: DBiSL is a differentiable bidirectional synergistic learning framework for semi-supervised medical image segmentation that enables online bidirectional cross-task collaboration between segmentation and regression tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis faces scarcity of high-quality labeled data due to high annotation costs and need for clinical expertise. Current dual-task collaborative learning methods are limited to unidirectional interactions (regression-to-segmentation) and cannot exploit online bidirectional cross-task collaboration.

Method: Proposes Differentiable Bidirectional Synergistic Learning (DBiSL) framework that integrates four SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Enables seamless bidirectional interaction between segmentation and regression tasks through differentiable transformations.

Result: Achieves state-of-the-art performance on two benchmark datasets. The framework demonstrates superior performance compared to existing semi-supervised learning methods for medical image segmentation.

Conclusion: DBiSL provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL. Offers a generic multitask learning framework applicable to broader computer vision applications beyond medical imaging.

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [18] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: Benchmark shows current image-to-3D foundation models struggle with single-slice medical reconstruction due to depth ambiguity, with SAM3D performing best but all showing moderate overlap metrics.


<details>
  <summary>Details</summary>
Motivation: 3D medical imaging is crucial but expensive and slow; image-to-3D models could help by reconstructing 3D from 2D, but it's unclear if natural image priors transfer to medical data.

Method: Zero-shot benchmark of five state-of-the-art image-to-3D models (SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, TripoSG) across six medical datasets and two natural datasets using voxel overlap and point cloud distance metrics.

Result: All models show moderate voxel overlap on medical data, indicating depth reconstruction failure. SAM3D achieves strongest topological similarity, while others tend to oversimplify reconstructions.

Conclusion: Single-slice medical reconstruction has limits due to planar nature of 2D data causing depth ambiguity, motivating multi-view approaches for reliable medical 3D inference.

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [19] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: K-Sort Eval: A VLM-based evaluation framework using posterior correction and dynamic matching for efficient, reliable assessment of visual generative models.


<details>
  <summary>Details</summary>
Motivation: Current human evaluation methods (Arena platforms) are costly and time-consuming, limiting scalability. While VLMs offer a promising alternative, their hallucinations and biases compromise alignment with human preferences, and static evaluation approaches are inefficient.

Method: Proposes K-Sort Eval framework with: 1) (K+1)-wise free-for-all comparisons where VLMs rank new models against existing ones, 2) Posterior correction that adaptively adjusts Bayesian updating based on VLM-human consistency, and 3) Dynamic matching strategy balancing uncertainty and diversity for efficient evaluation.

Result: Extensive experiments show K-Sort Eval delivers results consistent with human K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both efficiency and reliability.

Conclusion: K-Sort Eval provides a scalable, human-aligned evaluation framework for visual generative models that overcomes limitations of both costly human evaluations and unreliable VLM-based methods through posterior correction and dynamic matching.

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [20] [LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging](https://arxiv.org/abs/2602.09413)
*Xinyu Wang,Ke Deng,Fei Dou,Jinbo Bi,Jin Lu*

Main category: cs.CV

TL;DR: LARV is a layer-wise adaptive rescaling veneer that plugs into any task-vector merger to boost performance by addressing layer heterogeneity in vision transformers, suppressing shallow-layer interference while amplifying deeper task-specific features.


<details>
  <summary>Details</summary>
Motivation: Existing task-vector merging methods treat all layers uniformly, overlooking the strong layer-wise heterogeneity in large vision transformers where shallow layers are sensitive to interference while deeper layers encode stable task-specific features.

Method: LARV is a training-free, data-free, merger-agnostic layer-wise adaptive rescaling veneer that assigns per-layer scales to each task vector before aggregation. It uses simple deterministic schedules based on layer proxies to suppress shallow-layer interference and amplify deeper-layer alignment.

Result: LARV consistently improves all task-vector baselines across 8/14/20-task settings on FusionBench with Vision Transformers. For example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14.

Conclusion: LARV turns model merging into a robust, layer-aware procedure rather than a uniform one, suppressing shallow-layer interference while modestly amplifying deeper, task-stable features, and is orthogonal to base mergers with negligible cost.

Abstract: Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>


### [21] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: Operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters, establishing deterministic stability inequalities, Lipschitz bounds, and nonasymptotic concentration estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical framework for analyzing stability and statistical properties of high-dimensional nonlinear inverse problems with block-structured parameters, which are common in modern imaging and differentiable rendering applications.

Method: Operator-theoretic approach combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise assumptions to establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates.

Result: High-probability parameter error bounds intrinsic to forward operators, independent of specific reconstruction algorithms; verification that Gaussian Splatting rendering operator satisfies assumptions with explicit constants; demonstration of fundamental stability-resolution tradeoff where estimation error is constrained by image resolution to model complexity ratio.

Conclusion: The analysis characterizes operator-level limits for broad class of high-dimensional nonlinear inverse problems, providing theoretical foundations for stability and statistical concentration in modern imaging and differentiable rendering applications.

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [22] [Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification](https://arxiv.org/abs/2602.09425)
*Yiqiao Li,Bo Shang,Jie Wei*

Main category: cs.CV

TL;DR: A framework that adapts Vision-Language Models (VLMs) for fine-grained truck classification from sparse LiDAR point clouds without parameter fine-tuning, achieving competitive accuracy with minimal examples.


<details>
  <summary>Details</summary>
Motivation: Current LiDAR-based truck classification methods are limited by scalability issues due to reliance on supervised deep learning and labor-intensive manual annotation. VLMs offer few-shot generalization but face a modality gap between sparse 3D point clouds and dense 2D imagery.

Method: A depth-aware image generation pipeline transforms sparse LiDAR scans into depth-encoded 2D visual proxies using noise removal, spatial/temporal registration, orientation rectification, morphological operations, and anisotropic smoothing. Off-the-shelf VLMs are adapted without parameter fine-tuning.

Result: Achieves competitive classification accuracy with only 16-30 examples per class on 20 vehicle classes. Shows a "Semantic Anchor" effect where text guidance helps in ultra-low-shot regimes but degrades in more-shot settings. Successfully bootstraps lightweight supervised models and achieves over 75% accuracy for specific drayage categories without training/fine-tuning.

Conclusion: The framework provides a scalable alternative to data-intensive supervised methods for ITS applications, significantly reducing manual labeling requirements while maintaining practical accuracy for fine-grained truck classification.

Abstract: Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>


### [23] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: SceneReVis: Vision-grounded self-reflection framework for 3D scene synthesis that uses iterative "diagnose-and-act" loops to resolve spatial conflicts, trained with a two-stage approach on a new large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Current one-pass 3D scene synthesis methods suffer from spatial hallucinations (collisions) due to lack of deliberative reasoning, creating a need for more robust spatial planning.

Method: Introduces SceneReVis framework with iterative "diagnose-and-act" loop using multi-modal feedback to intercept/resolve spatial conflicts. Creates SceneChain-12k dataset via reverse engineering pipeline. Uses two-stage training: Supervised Fine-Tuning  Agentic Reinforcement Learning to evolve model into active spatial planner.

Result: Achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

Conclusion: SceneReVis bridges the gap in spatial reasoning for 3D scene synthesis through vision-grounded self-reflection and iterative refinement, demonstrating superior performance and generalization.

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [24] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: Fine-T2I is a large-scale, high-quality open dataset for text-to-image fine-tuning with 6M+ text-image pairs, combining synthetic and real images, rigorously filtered for quality.


<details>
  <summary>Details</summary>
Motivation: Existing public fine-tuning datasets have low resolution, poor text-image alignment, and limited diversity, creating a performance gap between open research models and enterprise-grade models.

Method: Created a dataset spanning 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, combining synthetic images from strong modern models with curated real images from professional photographers, with rigorous filtering removing over 95% of initial candidates.

Result: Final dataset contains over 6 million text-image pairs (around 2 TB), approaching pretraining dataset scale while maintaining fine-tuning-level quality. Fine-tuning on Fine-T2I consistently improves generation quality and instruction adherence across diverse pretrained models.

Conclusion: Fine-T2I helps close the data gap in T2I fine-tuning and is released under an open license to benefit the open research community.

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [25] [A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index](https://arxiv.org/abs/2602.09446)
*Mohammad Masudur Rahman,Md. Rashedur Rahman,Ashraful Islam,Saadia B Alam,M Ashraful Amin*

Main category: cs.CV

TL;DR: Scoping review of deep learning approaches for urban visual pollution detection, revealing fragmented research with limited datasets and applications, proposing a comprehensive monitoring framework.


<details>
  <summary>Details</summary>
Motivation: Urban Visual Pollution (UVP) is a critical concern, but research on automatic detection and application remains fragmented, lacking unified approaches for effective management.

Method: Conducted systematic scoping review following PRISMA-ScR guidelines across 7 academic databases, analyzing 26 articles on deep learning-based UVP detection approaches.

Result: Most research focuses on specific pollutant categories using YOLO, Faster R-CNN, and EfficientDet architectures; datasets are limited and lack standardized taxonomies; few studies integrate detection into real-time systems.

Conclusion: Proposes a comprehensive UVP monitoring framework with visual pollution index, highlighting need for unified management system with standardized taxonomy, cross-city benchmark dataset, generalized models, and assessment metrics.

Abstract: Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>


### [26] [Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing](https://arxiv.org/abs/2602.09449)
*Yan Luo,Henry Huang,Todd Y. Zhou,Mengyu Wang*

Main category: cs.CV

TL;DR: The paper proposes two training-free latent-trajectory adjustment methods (Look-Ahead and Look-Back) that refine diffusion model generation paths directly in latent space, outperforming SOTA models without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing training-free flow matching approaches adjust velocity fields, which introduces errors that propagate through the full generation path. Adjustments to latent trajectories are naturally corrected by pretrained velocity networks, reducing error accumulation.

Method: Two complementary training-free latent-trajectory adjustment approaches: 1) Look-Ahead - averages current and next-step latents using curvature-gated weight, 2) Look-Back - smoothes latents using exponential moving average with decay. Both refine generative paths directly in latent space using future and past velocity/latent information.

Result: The proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K, as demonstrated through extensive experiments and comprehensive evaluation metrics.

Conclusion: Latent-trajectory adjustment is more effective than velocity field adjustment for training-free flow matching, as it reduces error accumulation and improves image generation quality across diverse datasets.

Abstract: Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.

</details>


### [27] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: ArtifactLens: A few-shot VLM-based system that detects image generation artifacts using only hundreds of labeled examples, achieving SOTA across multiple benchmarks with orders of magnitude less data.


<details>
  <summary>Details</summary>
Motivation: Current artifact detectors require expensive fine-tuning on tens of thousands of labeled images, making them impractical to update as generators evolve or new artifact types emerge. There's a need for more efficient detection methods that can benchmark generators and train reward models.

Method: ArtifactLens uses pretrained VLMs with a multi-component scaffolding architecture featuring in-context learning and text instruction optimization. It unlocks artifact detection capabilities using only a few hundred labeled examples per category through novel improvements to these components.

Result: Achieves state-of-the-art performance on five human artifact benchmarks (first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The method generalizes to other artifact types (object morphology, animal anatomy, entity interactions) and AIGC detection tasks.

Conclusion: Pretrained VLMs already encode artifact detection knowledge that can be efficiently unlocked with proper scaffolding, enabling practical, scalable artifact detection with minimal labeled data as image generators continue to evolve.

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [28] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: FD-DB is a frequency-decoupled dual-branch model for synthetic-to-real translation that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation to balance photorealism and structural stability.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is cheap and accurately annotated but suffers from domain shift due to appearance and imaging differences. Existing unpaired translation methods face a trade-off between photorealism (which can introduce deformation) and structural stability (which limits adaptation to real-domain statistics).

Method: FD-DB uses two branches: 1) interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, grain) for stable low-frequency appearance, 2) free branch generates high-frequency residuals for fine details. A gated fusion mechanism combines branches under explicit frequency constraints, with two-stage training that first stabilizes editing branch then releases residual branch.

Result: Experiments on YCB-V dataset show FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

Conclusion: FD-DB effectively addresses the photorealism-structural stability trade-off in synthetic-to-real translation through frequency-decoupled design, achieving better domain adaptation for geometry-sensitive vision tasks.

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [29] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: WeakSupCon: A weakly supervised contrastive learning framework that improves patch feature representation for multiple instance learning in digital histopathology by using only slide-level labels without instance-level pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Digital histopathology analysis faces challenges due to limited training labels, as manual annotation of gigapixel WSIs is time-consuming. While weakly supervised MIL uses slide-level labels, most methods use frozen patch features and focus on aggregation, neglecting feature representation learning for encoder pretraining in MIL settings.

Method: Proposes WeakSupCon, a weakly supervised contrastive learning framework that incorporates bag-level label information during training without relying on instance-level pseudo-labeling. The method effectively separates patches with different labels in feature space through contrastive learning using only slide-level supervision.

Result: Experimental results show that image features generated by WeakSupCon lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches across three datasets.

Conclusion: WeakSupCon provides an effective feature representation learning framework for MIL in digital histopathology that leverages bag-level labels to learn discriminative patch features, addressing the neglected area of encoder pretraining in MIL settings and improving downstream performance.

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [30] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: Align-TI is a novel knowledge distillation framework for compressing multimodal large language models by focusing on dynamic token interactions rather than static next-token alignment.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods for MLLMs rely on static next-token alignment, which neglects dynamic token interactions that embed essential capabilities for multimodal understanding and generation.

Method: Align-TI introduces two components: IVA (Instruction-Visual Alignment) to imitate teacher's visual information extraction by aligning on salient visual regions, and TPA (Token-Transition Probability Alignment) to capture teacher's dynamic generative logic by aligning sequential token-to-token transition probabilities.

Result: Achieves 2.6% relative improvement over Vanilla KD, and the distilled Align-TI-2B model outperforms LLaVA-1.5-7B (a much larger MLLM) by 7.0%, establishing new state-of-the-art distillation framework for parameter-efficient MLLMs.

Conclusion: Align-TI provides an effective knowledge distillation framework that addresses the limitations of static alignment methods by focusing on dynamic token interactions, enabling superior compression of multimodal large language models while maintaining strong performance.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [31] [OSI: One-step Inversion Excels in Extracting Diffusion Watermarks](https://arxiv.org/abs/2602.09494)
*Yuwei Chen,Zhenliang He,Jia Tang,Meina Kan,Shiguang Shan*

Main category: cs.CV

TL;DR: OSI: One-step Inversion method for fast watermark extraction from diffusion-generated images, replacing slow multi-step diffusion inversion with learnable sign classification.


<details>
  <summary>Details</summary>
Motivation: Existing training-free watermarking methods like Gaussian Shading require computationally expensive multi-step diffusion inversion for extraction, which is slow and time-consuming.

Method: Reformulate watermark extraction as learnable sign classification problem, initialize OSI model from diffusion backbone, finetune on synthesized noise-image pairs with sign classification objective.

Result: OSI is 20x faster than multi-step diffusion inversion, achieves higher extraction accuracy, doubles watermark payload capacity, and shows consistent improvements across diverse schedulers, backbones, and cryptographic schemes.

Conclusion: OSI provides a general framework for efficient watermark extraction from diffusion-generated images, significantly improving speed, accuracy, and capacity compared to existing methods.

Abstract: Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.

</details>


### [32] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: ECL (Equilibrium Contrastive Learning) is a supervised contrastive learning framework that addresses limitations in existing methods for imbalanced datasets by promoting geometric equilibrium between class features, means, and classifiers.


<details>
  <summary>Details</summary>
Motivation: Existing supervised contrastive learning methods for imbalanced datasets have two key limitations: 1) they don't align class means/prototypes with classifiers, leading to poor generalization, and 2) prototype-based methods treat prototypes as only one additional sample per class, causing unbalanced contributions across classes that depend on batch composition.

Method: ECL uses two main components: 1) representation geometric equilibrium that promotes regular simplex geometry with collapsed class samples and uniformly distributed class means while balancing contributions of class-average features and prototypes, and 2) classifier-class center geometric equilibrium that aligns classifier weights with class prototypes.

Result: ECL outperforms existing state-of-the-art supervised contrastive learning methods on three long-tailed datasets (CIFAR-10/100-LT, ImageNet-LT) and two imbalanced medical datasets (ISIC 2019 and LCCT dataset).

Conclusion: The proposed ECL framework effectively addresses geometric imbalances in supervised contrastive learning for imbalanced classification by establishing equilibrium between representations and classifiers, leading to improved performance across various imbalanced datasets.

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [33] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS is a diffusion-based depth super-resolution framework that robustly recovers high-resolution depth from arbitrarily degraded inputs by adaptively selecting reverse diffusion starting points based on refinement uncertainty.


<details>
  <summary>Details</summary>
Motivation: Conventional depth super-resolution methods directly regress depth values and often fail under severe or unknown degradation patterns, producing artifacts. There's a need for a more robust approach that can handle diverse degradation scenarios.

Method: AdaDS leverages the contraction property of Gaussian smoothing where distributional discrepancies diminish as noise accumulates. It adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, then injects tailored noise to position samples within the high-probability region of the target posterior distribution.

Result: Extensive experiments on real-world and synthetic benchmarks show AdaDS achieves superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

Conclusion: AdaDS provides a robust, generalizable framework for depth super-resolution that effectively handles arbitrary degradation by leveraging diffusion model priors and adaptive noise injection strategies.

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [34] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg Challenge benchmarks federated learning for surgical video classification using Appendix300 dataset, evaluating generalization to unseen centers and local adaptation, revealing limitations in generalization but improvements with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To benchmark state-of-the-art federated learning methods for surgical video classification, assessing generalization to unseen clinical centers and adaptation through local fine-tuning while enabling collaborative model development without sharing patient data.

Method: Participants developed strategies to classify inflammation stages in appendicitis using Appendix300 dataset. Evaluated two tasks: generalization to unseen center and center-specific adaptation after fine-tuning. Approaches included foundation models with linear probing, metric learning with triplet loss, and FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance assessed using F1-score and Expected Cost with bootstrapping and statistical testing.

Result: Generalization task showed limited performance across centers. Adaptation task saw all teams improve after fine-tuning, though ranking stability was low. ViViT-based submission achieved strongest overall performance. Highlighted limitations in generalization, sensitivity to class imbalance, and hyperparameter tuning difficulties in decentralized training.

Conclusion: FedSurg Challenge establishes first benchmark for evaluating FL strategies in surgical video classification. Findings highlight trade-off between local personalization and global robustness, and underscore importance of architecture choice, preprocessing, and loss design. Provides reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [35] [Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems](https://arxiv.org/abs/2602.09515)
*Mas Nurul Achmadiah,Afaroj Ahamad,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: AI-powered IoT system uses frame difference method for fast object detection, achieving 28% higher accuracy, 3.6x better efficiency, and 39% lower latency than end-to-end methods on edge devices.


<details>
  <summary>Details</summary>
Motivation: IoT systems need energy-efficient fast object detection, but end-to-end methods struggle with fast-moving objects and are computationally expensive for edge devices.

Method: Frame difference method with AI classifiers (MobileNet, YOLOX) implemented on AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator for detecting birds, cars, trains, and airplanes.

Result: MobileNet performed best with high accuracy, low latency, and high energy efficiency; YOLOX performed worst. Frame difference method improved accuracy by 28.3%, efficiency by 3.6x, and reduced latency by 39.3% vs end-to-end methods.

Conclusion: Frame difference method is superior for fast object detection in IoT systems, offering lightweight, energy-efficient solution with better performance than end-to-end approaches.

Abstract: This paper presents an Internet of Things (IoT) application that utilizes an AI classifier for fast-object detection using the frame difference method. This method, with its shorter duration, is the most efficient and suitable for fast-object detection in IoT systems, which require energy-efficient applications compared to end-to-end methods. We have implemented this technique on three edge devices: AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator, and four models with artificial neural networks and transformer models. We examined various classes, including birds, cars, trains, and airplanes. Using the frame difference method, the MobileNet model consistently has high accuracy, low latency, and is highly energy-efficient. YOLOX consistently shows the lowest accuracy, lowest latency, and lowest efficiency. The experimental results show that the proposed algorithm has improved the average accuracy gain by 28.314%, the average efficiency gain by 3.6 times, and the average latency reduction by 39.305% compared to the end-to-end method. Of all these classes, the faster objects are trains and airplanes. Experiments show that the accuracy percentage for trains and airplanes is lower than other categories. So, in tasks that require fast detection and accurate results, end-to-end methods can be a disaster because they cannot handle fast object detection. To improve computational efficiency, we designed our proposed method as a lightweight detection algorithm. It is well suited for applications in IoT systems, especially those that require fast-moving object detection and higher accuracy.

</details>


### [36] [A Universal Action Space for General Behavior Analysis](https://arxiv.org/abs/2602.09518)
*Hung-Shuo Chang,Yue-Cheng Yang,Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,James C. Liao,Chien-Chang Chen,Hen-Hsen Huang,Hong-Yuan Mark Liao*

Main category: cs.CV

TL;DR: The paper proposes building a Universal Action Space (UAS) using existing human-action datasets to analyze and categorize mammalian and chimpanzee behavior, leveraging deep learning advances from ImageNet.


<details>
  <summary>Details</summary>
Motivation: Traditional behavior analysis methods from 1970s-1990s relied on hand-crafted features and sparse tracking, which were ill-posed and lacked robustness. The ImageNet revolution enabled large-scale visual recognition through deep learning, creating an opportunity to apply similar approaches to behavior analysis.

Method: Build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets, then use this UAS as a foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets.

Result: The approach enables more robust behavior analysis by leveraging learned high-level representations rather than traditional hand-crafted features. The source code is publicly released on GitHub.

Conclusion: Following the ImageNet paradigm for building comprehensive visual dictionaries can be successfully applied to behavior analysis, creating a Universal Action Space that improves analysis of animal and human behavior across species.

Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>


### [37] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: Training-free attentional intervention algorithm reduces hallucinations in Large Vision-Language Models by reweighting attention to task-relevant visual tokens based on visual-textual similarities.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs suffer from insufficient visual attention leading to hallucinations. Current methods that boost attention for all visual tokens increase attention to irrelevant tokens, creating a need for more targeted attention enhancement.

Method: Proposes a training-free algorithm that extracts vision-text cross-attention submatrices to construct reweighting matrices for reallocating attention to task-relevant tokens (identified by high visual-textual similarities). Also injects visual attention values into beam search decoding to enhance visual token contribution.

Result: Extensive experiments show the method significantly reduces hallucinations across mainstream LVLMs while preserving accuracy and coherence of generated content.

Conclusion: The proposed training-free attentional intervention effectively addresses hallucination problems in LVLMs by selectively enhancing attention to task-relevant visual tokens without compromising content quality.

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [38] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: Singpath-VL is a vision-language model for cervical cytology that uses synthetic data generation and multi-stage fine-tuning to achieve superior performance in cell morphology analysis and diagnostic classification.


<details>
  <summary>Details</summary>
Motivation: There's a lack of AI assistants for cervical cytology due to scarcity of large-scale, high-quality annotated datasets in cytopathology, despite advances in multi-modal large language models for computational pathology.

Method: 1) Developed three-stage pipeline to synthesize million-scale image-description dataset using MLLMs as weak annotators with consensus fusion and expert knowledge injection. 2) Fine-tuned Qwen3-VL-4B model via multi-stage strategy to create specialized cytopathology MLLM.

Result: Singpath-VL demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification compared to existing approaches.

Conclusion: The approach successfully addresses data scarcity in cervical cytology through synthetic data generation and specialized model fine-tuning, with plans to open-source synthetic dataset and benchmark to advance the field.

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [39] [HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.09524)
*Han Zhou,Yuxuan Gao,Yinchao Du,Xuezhe Zheng*

Main category: cs.CV

TL;DR: HLGFA is a novel unsupervised anomaly detection framework that uses high-low resolution feature alignment instead of pixel reconstruction, achieving state-of-the-art performance on industrial inspection benchmarks.


<details>
  <summary>Details</summary>
Motivation: Industrial anomaly detection faces challenges due to scarce defect samples and the need for reliable detection. Traditional methods relying on pixel-level reconstruction have limitations, and there's a need for more effective approaches that can handle industrial environments with nuisance factors.

Method: HLGFA learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations. It uses dual-resolution inputs processed by a shared frozen backbone, decomposes high-resolution features into structure and detail priors, and refines low-resolution features through conditional modulation and gated residual correction. Includes noise-aware data augmentation to suppress nuisance responses.

Result: Achieves 97.9% pixel-level AUROC and 97.5% image-level AUROC on MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

Conclusion: HLGFA provides an effective framework for unsupervised industrial anomaly detection that avoids pixel-level reconstruction limitations and demonstrates superior performance through cross-resolution feature alignment and noise-aware augmentation.

Abstract: Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>


### [40] [SchrMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrdinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: SchrMind reduces hallucinations in multimodal LLMs by solving the Schrdinger bridge problem to map hallucinatory to truthful activations with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: MLLMs have limited use in high-stakes healthcare due to persistent hallucinations where generated text contradicts visual input, despite their ability to comprehend images.

Method: Proposes SchrMind framework that reduces hallucinations by solving the Schrdinger bridge problem, establishing token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training.

Result: Extensive experiments on POPE and MME benchmarks demonstrate state-of-the-art performance with minimal computational overhead.

Conclusion: SchrMind effectively addresses the hallucination problem in MLLMs through optimal transport theory, enabling more reliable deployment in high-stakes domains like healthcare.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchrMind-a novel framework reducing hallucinations via solving the Schrdinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrdinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [41] [SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection](https://arxiv.org/abs/2602.09529)
*Emad Gholibeigi,Abbas Koochari,Azadeh ZamaniFar*

Main category: cs.CV

TL;DR: SCA-Net improves building/road change detection with multi-scale analysis, attention mechanisms, and efficient training, achieving better accuracy and faster training than Change-Agent.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for remote sensing change detection struggle with low sensitivity to small objects and high computational costs, limiting practical applications in urban management, environmental monitoring, and disaster assessment.

Method: Enhanced Change-Agent framework with Difference Pyramid Block for multi-scale change analysis, Adaptive Multi-scale Processing module (shape-aware + high-resolution blocks), multi-level attention mechanisms (PPM and CSAGate), dynamic composite loss function, and four-phase training strategy.

Result: Superior performance on LEVIR-CD and LEVIR-MCI datasets: 2.64% mIoU improvement on LEVIR-MCI, 57.9% IoU increase for small buildings, 61% training time reduction compared to Change-Agent.

Conclusion: SCA-Net provides an efficient, accurate, and robust solution for practical change detection applications, addressing key limitations of existing deep learning approaches.

Abstract: Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>


### [42] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: DR.Experts is a novel BIQA framework that incorporates distortion priors through a degradation-aware vision-language model and dynamic weighting to better align with human perception.


<details>
  <summary>Details</summary>
Motivation: Existing BIQA models fail to capture subtle distortion cues and misalign with human judgments due to lack of reliable distortion priors, learning only shallow relationships between image features and quality scores.

Method: Uses degradation-aware vision-language model to obtain distortion-specific priors, refines them with Distortion-Saliency Differential Module, then fuses with semantics via Dynamic Distortion Weighting Module that weights each distortion feature by perceptual impact.

Result: Extensive experiments on five challenging BIQA benchmarks demonstrate superiority over current methods with excellent generalization and data efficiency.

Conclusion: DR.Experts effectively addresses the distortion prior limitation in BIQA by explicitly incorporating and weighting distortion cues, achieving better alignment with human perception of visual quality.

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [43] [RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes](https://arxiv.org/abs/2602.09532)
*Michael Baltaxe,Dan Levi,Sagie Benaim*

Main category: cs.CV

TL;DR: RAD is a retrieval-augmented framework for monocular metric depth estimation that uses retrieved RGB-D neighbors as geometric proxies to improve accuracy for underrepresented classes in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Accurate depth estimation for underrepresented classes in complex scenes remains challenging for monocular metric depth estimation systems, which are essential for physically intelligent systems.

Method: Uses uncertainty-aware retrieval to identify low-confidence regions and retrieve RGB-D context samples with similar content. Processes input and retrieved context via dual-stream network and fuses them using matched cross-attention module that transfers geometric information only at reliable point correspondences.

Result: Significantly outperforms state-of-the-art baselines on underrepresented classes: reduces relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.

Conclusion: RAD effectively addresses the challenge of accurate depth estimation for underrepresented classes by leveraging retrieved neighbors as structural geometric proxies, demonstrating substantial improvements across multiple datasets.

Abstract: Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.

</details>


### [44] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: AUHead: A two-stage method for controllable talking-head video generation using Action Units (AUs) for fine-grained emotion control, achieving better emotional realism than existing techniques.


<details>
  <summary>Details</summary>
Motivation: Current talking-head video generation methods lack fine-grained emotion control, struggling with nuanced emotional expressions needed for virtual avatars, film production, and interactive systems.

Method: Two-stage approach: 1) Uses large audio-language models with spatial-temporal AU tokenization and "emotion-then-AU" chain-of-thought to extract AUs from speech; 2) AU-driven controllable diffusion model that maps AU sequences to 2D facial representations and models AU-vision interactions with cross-attention, plus AU disentanglement guidance for quality trade-off control.

Result: Achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence on benchmark datasets, significantly surpassing existing techniques.

Conclusion: AUHead successfully disentangles fine-grained emotion control (Action Units) from audio for realistic talking-head video generation, offering improved emotional expressiveness and identity consistency.

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [45] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: Scalpel reduces hallucinations in large vision-language models by refining attention activations toward credible regions using Gaussian mixture models and entropic optimal transport, requiring no additional computation.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models often generate outputs inconsistent with visual content (hallucinations) due to strong language model priors and misaligned attention across modalities.

Method: Predicts trusted attention directions for each Transformer head during inference, uses Gaussian mixture models to capture multi-peak distributions in trust/hallucination manifolds, and employs entropic optimal transport to map Gaussian components precisely. Dynamically adjusts intervention strength based on component membership and mapping relationships.

Result: Extensive experiments across multiple datasets and benchmarks show Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance.

Conclusion: Scalpel is a model- and data-agnostic method that reduces hallucinations in LVLMs without additional computation, requiring only a single decoding step.

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrdinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [46] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: Neural Tangent Kernel Spectral Clustering leverages vision-language models for multi-modal spectral clustering, using cross-modal alignment and prompt-based affinity matrices to achieve state-of-the-art performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional spectral clustering methods are single-modal, missing rich multi-modal information. Vision-language pre-training models offer cross-modal alignment capabilities that could enhance spectral clustering by combining visual and semantic information.

Method: Proposes Neural Tangent Kernel Spectral Clustering that anchors neural tangent kernels with positive nouns semantically close to images. Formulates image affinity as coupling of visual proximity and semantic overlap, with regularized affinity diffusion to ensemble different prompt-induced affinity matrices.

Result: Extensive experiments on 16 benchmarks (classical, large-scale, fine-grained, domain-shifted datasets) show the method consistently outperforms state-of-the-art by a large margin.

Conclusion: The paper successfully extends spectral clustering from single-modal to multi-modal regime using vision-language models, demonstrating that cross-modal alignment and semantic anchoring significantly improve clustering performance across diverse scenarios.

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [47] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: MieDB-100k is a large-scale, high-quality medical image editing dataset addressing data scarcity through diverse task categorization and rigorous curation, enabling superior model performance.


<details>
  <summary>Details</summary>
Motivation: Existing medical image editing datasets suffer from limited diversity, neglect of medical image understanding, and inability to balance quality with scalability, creating a bottleneck for adapting multimodal generative models to medical applications.

Method: Constructed MieDB-100k via a data curation pipeline using modality-specific expert models and rule-based synthetic methods, with rigorous manual inspection for clinical fidelity. Tasks are categorized into Perception, Modification and Transformation perspectives.

Result: Models trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability.

Conclusion: MieDB-100k addresses critical gaps in medical image editing data and serves as a cornerstone for future advancements in specialized medical image editing.

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [48] [Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures](https://arxiv.org/abs/2602.09600)
*Yuxi Wang,Wenqi Ouyang,Tianyi Wei,Yi Dong,Zhiqi Shen,Xingang Pan*

Main category: cs.CV

TL;DR: Hand2World: A unified autoregressive framework for egocentric interactive video generation from single scene images using free-space hand gestures, addressing challenges like distribution shift, motion ambiguity, and long-term stability.


<details>
  <summary>Details</summary>
Motivation: Egocentric interactive world models are crucial for AR and embodied AI, requiring low-latency, geometrically consistent, and stable visual generation that responds to user input. The paper addresses the challenge of generating photorealistic videos from single scene images where hands enter, interact with objects, and induce world dynamics under head motion.

Method: Hand2World uses occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility/occlusion inference from scene context. It injects explicit camera geometry via per-pixel Plcker-ray embeddings to disentangle camera and hand motion. The framework includes a fully automated monocular annotation pipeline and distills a bidirectional diffusion model into a causal generator for arbitrary-length synthesis.

Result: Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

Conclusion: Hand2World effectively addresses key challenges in egocentric interaction generation including distribution shift, motion ambiguity, and the need for arbitrary-length video generation, providing a unified solution for photorealistic, geometrically consistent interactive world modeling.

Abstract: Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plcker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>


### [49] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: AGMark is a novel watermarking framework for Large Vision-Language Models that dynamically identifies semantic-critical tokens using attention weights and context-aware coherence, enabling adaptive vocabulary partitioning to preserve visual fidelity while maintaining detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LVLM watermarking methods have limitations: vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding, while vision-specific watermarks use static weight estimation and ignore weight distribution density, failing to account for dynamic visual dependence changes during generation and potentially introducing low-quality tokens.

Method: AGMark uses attention-guided dynamic watermarking that at each decoding step: 1) dynamically identifies semantic-critical evidence based on attention weights for visual relevance and context-aware coherence cues, creating adaptive evidence-weight distribution; 2) determines proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), enabling adaptive vocabulary partitioning to avoid irrelevant tokens.

Result: AGMark outperforms conventional methods, improving generation quality with strong gains in visual semantic fidelity, especially in later generation stages. It maintains highly competitive detection accuracy (99.36% AUC) and robust attack resilience (88.61% AUC) without sacrificing inference efficiency.

Conclusion: AGMark effectively establishes a new standard for reliability-preserving multi-modal watermarking by embedding detectable signals while strictly preserving visual fidelity through dynamic, attention-guided adaptation to visual dependence changes during generation.

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [50] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: Tele-Omni is a unified multimodal framework for video generation and editing that handles text, images, and reference videos within a single model, using MLLMs for instruction parsing and diffusion models for synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based video generation methods are task-specific, rely mainly on text instructions, and lack ability to handle multimodal inputs and diverse scenarios in a unified framework. Current video editing methods use engineered pipelines for individual operations, limiting scalability and composability.

Method: Tele-Omni uses pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation/editing intents, while diffusion-based generators perform video synthesis. A task-aware data processing pipeline unifies multimodal inputs into structured instruction format while preserving task-specific constraints.

Result: Tele-Omni achieves competitive performance across multiple video-centric tasks including text-to-video, image-to-video, first-last-frame video generation, in-context video generation, and in-context video editing, while maintaining temporal coherence and visual consistency.

Conclusion: By decoupling instruction parsing from video synthesis with task-aware data design, Tele-Omni enables flexible multimodal control in a unified framework, addressing limitations of task-specific approaches and engineered pipelines.

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [51] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: GenSeg-R1 uses a decoupled reason-then-segment pipeline where a VLM generates spatial prompts (bbox + keypoints) and SAM 2 converts them to masks, achieving SOTA on referring image segmentation benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve fine-grained referring image segmentation by decoupling reasoning and segmentation, enabling better spatial understanding without supervised reasoning-chain annotations.

Method: Decoupled pipeline with VLM (Qwen3-VL) generating structured spatial prompts (bounding box + 2 keypoints per instance), then frozen SAM 2 converting prompts to masks. Trained with GRPO without supervised reasoning annotations.

Result: GenSeg-R1-8B achieves 0.7127 cIoU and 0.7382 mIoU on RefCOCOg (+15.3/+21.9 points over baselines). GenSeg-R1-G variant achieves 76.69% target mIoU on GRefCOCO with 82.40% no-target accuracy. GenSeg-R1-4B reaches 68.40% mIoU on ReasonSeg (+7.0 over Seg-Zero-7B).

Conclusion: The decoupled reason-then-segment approach with structured spatial prompts and GRPO training enables superior referring image segmentation performance without requiring supervised reasoning annotations.

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [52] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: LELA is a training-free LLM-based framework for hate video localization that outperforms existing training-free methods by large margins on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing hate video detection solutions either require extensive human annotations or lack fine-grained temporal precision, creating scalability and accuracy limitations.

Method: Decomposes videos into five modalities (image, speech, OCR, music, video context), uses multi-stage prompting to compute frame-level hate scores, and employs composition matching for cross-modal reasoning.

Result: Outperforms all existing training-free baselines by large margins on HateMM and MultiHateClip benchmarks, with extensive ablations and qualitative visualizations.

Conclusion: LELA establishes a strong foundation for scalable and interpretable hate video localization without requiring training or large-scale annotations.

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [53] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir,Tawsif Tashwar Dipto,Mehedi Ahamed,Sabbir Ahmed,Md Hasanul Kabir*

Main category: cs.CV

TL;DR: First systematic benchmark of lightweight SNNs converted from compact CNNs shows up to 15.7x higher energy efficiency than CNNs with competitive accuracy, with SqueezeNet variant performing best and further improved via structured pruning.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient alternatives to CNNs for edge intelligence, but prior work focused on large-scale models, leaving lightweight CNN-to-SNN pipelines underexplored.

Method: Convert compact CNN architectures (ShuffleNet, SqueezeNet, MnasNet, MixNet) to spiking networks using LIF neurons and surrogate gradient descent. Evaluate on CIFAR-10/100/TinyImageNet, then apply structured pruning to remove redundant modules.

Result: SNNs achieve up to 15.7x higher energy efficiency than CNNs with competitive accuracy. SNN-SqueezeNet performs best. Pruned SNN-SqueezeNet-P improves CIFAR-10 accuracy by 6%, reduces parameters by 19%, achieves nearly same accuracy as CNN-SqueezeNet (only 1% lower) with 88.1% energy reduction.

Conclusion: Lightweight SNNs are practical, low-power alternatives for edge deployment, offering a viable path toward high-performance, low-power intelligence on the edge.

Abstract: Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

</details>


### [54] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: VIDA dataset enables video-based 3D affordance grounding; VideoAfford model uses multimodal LLMs with latent action encoding for superior performance.


<details>
  <summary>Details</summary>
Motivation: Previous 3D affordance grounding methods rely on static cues (language/images) that lack dynamic interaction context, temporal cues, and causal relationships needed for robotic manipulation.

Method: Created VIDA dataset (38K human-object-interaction videos, 16 affordance types, 38 object categories, 22K point clouds). Developed VideoAfford model that: 1) Activates multimodal LLMs with affordance segmentation capabilities, 2) Uses latent action encoder to extract dynamic interaction priors from HOI videos, 3) Implements spatial-aware loss for comprehensive 3D spatial knowledge.

Result: Model significantly outperforms established methods and exhibits strong open-world generalization with affordance reasoning abilities.

Conclusion: Video-based approach with dynamic interaction context improves 3D affordance grounding; VIDA dataset and VideoAfford model advance robotic manipulation research.

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [55] [LiDAR-based 3D Change Detection at City Scale](https://arxiv.org/abs/2510.21112)
*Hezam Albagami,Haitian Wang,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Alqamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.CV

TL;DR: Uncertainty-aware object-centric LiDAR change detection method for 3D city maps using registration, segmentation, and instance-level analysis with improved accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for city-scale change detection (DSM/image differencing, point clouds) have limitations: sensitive to vertical bias/viewpoint mismatch, require large memory, assume perfect alignment, and degrade thin structures.

Method: Multi-resolution NDT + point-to-plane ICP for alignment, elevation normalization, per-point detection level from registration covariance/surface roughness. Geometry associations refined by semantic/instance segmentation with class-constrained bipartite assignment for split-merge cases. Tiled processing for memory efficiency and narrow ground changes.

Result: Achieved 95.3% accuracy, 90.8% mF1, and 82.9% mIoU on Subiaco dataset (2023 vs 2025), improving over Triplet KPConv baseline by 0.3, 0.6, and 1.1 points respectively.

Conclusion: Proposed uncertainty-aware object-centric method effectively handles city-scale LiDAR change detection with improved performance, addressing limitations of conventional approaches while providing robust change analysis for urban planning and monitoring.

Abstract: High-definition 3D city maps enable city planning and change detection, which is essential for municipal compliance, map maintenance, and asset monitoring, including both built structures and urban greenery. Conventional Digital Surface Model (DSM) and image differencing are sensitive to vertical bias and viewpoint mismatch, while original point cloud or voxel models require large memory, assume perfect alignment, and degrade thin structures. We propose an uncertainty-aware, object-centric method for city-scale LiDAR-based change detection. Our method aligns data from different time periods using multi-resolution Normal Distributions Transform (NDT) and a point-to-plane Iterative Closest Point (ICP) method, normalizes elevation, and computes a per-point level of detection from registration covariance and surface roughness to calibrate change decisions. Geometry-based associations are refined by semantic and instance segmentation and optimized using class-constrained bipartite assignment with augmented dummies to handle split-merge cases. Tiled processing bounds memory and preserves narrow ground changes, while instance-level decisions integrate overlap, displacement, and volumetric differences under local detection gating. We perform experiments on a Subiaco (Western Australia) dataset captured in 2023 and again in 2025. Our method achieves 95.3% accuracy, 90.8% mF1, and 82.9% mIoU, improving over the strongest baseline, Triplet KPConv, by 0.3, 0.6, and 1.1 points, respectively. The datasets are available on IEEE DataPort (2023: https://ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset and 2025: https://ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset). The source code is available at https://github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.

</details>


### [56] [Code2World: A GUI World Model via Renderable Code Generation](https://arxiv.org/abs/2602.09856)
*Yuhao Zheng,Li'an Zhong,Yi Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu,Linyuan Lv,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: Code2World is a vision-language coder that predicts next GUI states via renderable HTML code generation, addressing visual fidelity and structural controllability issues in GUI world modeling.


<details>
  <summary>Details</summary>
Motivation: Existing text- and pixel-based approaches for GUI world modeling struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability in predicting next visual states for autonomous GUI agents.

Method: 1) Construct AndroidCode dataset by translating GUI trajectories into HTML and refining via visual-feedback revision (80K+ screen-action pairs); 2) Adapt VLMs via SFT for format layout following, then apply Render-Aware Reinforcement Learning using rendered outcome as reward signal for visual semantic fidelity and action consistency.

Result: Code2World-8B achieves top-performing next UI prediction, rivaling GPT-5 and Gemini-3-Pro-Image. Significantly enhances downstream navigation success rates, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation.

Conclusion: Code2World effectively addresses data scarcity and visual-structural tradeoffs in GUI world modeling through code-based simulation, demonstrating superior prediction performance and practical utility in enhancing autonomous GUI agent navigation.

Abstract: Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>


### [57] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: Time2General is a domain generalized video semantic segmentation framework that uses spatio-temporal memory and masked consistency loss to achieve stable, flicker-free predictions across unseen domains without test-time adaptation.


<details>
  <summary>Details</summary>
Motivation: Current DGVSS methods suffer from domain shift and temporal-sampling shift that break correspondence-based propagation and fixed-stride aggregation, causing severe frame-to-frame flicker even in label-stable regions.

Method: Proposes Stability Queries with Spatio-Temporal Memory Decoder that aggregates multi-frame context into clip-level memory and decodes consistent masks without explicit correspondence. Uses Masked Temporal Consistency Loss to regularize prediction discrepancies across different strides and randomizes training strides for robustness.

Result: Achieves substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS on multiple driving benchmarks.

Conclusion: Time2General effectively addresses domain shift and temporal instability in video semantic segmentation without requiring target labels or test-time adaptation, making it practical for real-world deployment.

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [58] [Perception with Guarantees: Certified Pose Estimation via Reachability Analysis](https://arxiv.org/abs/2602.10032)
*Tobias Ladner,Yasser Shoukry,Matthias Althoff*

Main category: cs.CV

TL;DR: Certified 3D pose estimation from camera images using formal verification to guarantee safety in worst-case scenarios


<details>
  <summary>Details</summary>
Motivation: Safety-critical agents in cyber-physical systems need precise pose estimation for formal safety guarantees, but existing methods (lidar, cameras, GPS) provide insufficient accuracy or untrustworthy external services

Method: Formal bounding of pose using reachability analysis and formal neural network verification, leveraging camera images and known target geometry

Result: Efficient and accurate agent localization demonstrated in both synthetic and real-world experiments

Conclusion: Certified pose estimation from camera images provides formal safety guarantees for safety-critical agents without relying on external services

Abstract: Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.

</details>


### [59] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: TreeCUA scales GUI automation using tree-structured trajectories with multi-agent collaboration and adaptive exploration, plus TreeCUA-DPO for improved planning via branch information.


<details>
  <summary>Details</summary>
Motivation: Existing GUI automation work focuses on GUI grounding rather than the more crucial GUI planning, which requires sophisticated data collection. Real-world CUA exploration follows tree structures, suggesting tree organization can reduce data costs and streamline scaling.

Method: Multi-agent collaborative framework for exploration, verification, summarization, and evaluation; tree-based topology to store/replay duplicate nodes; adaptive exploration algorithm balancing depth/difficulty and breadth/diversity; world knowledge guidance and global memory backtracking; TreeCUA-DPO method leveraging branch information from adjacent trajectories.

Result: Experimental results show significant improvements, with out-of-domain studies demonstrating strong generalization capabilities.

Conclusion: TreeCUA efficiently scales GUI automation through tree-structured verifiable evolution, and TreeCUA-DPO further enhances GUI planning by utilizing branch information from tree-structured trajectories.

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [60] [SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116)
*Hongchi Xia,Xuan Li,Zhaoshuo Li,Qianli Ma,Jiashu Xu,Ming-Yu Liu,Yin Cui,Tsung-Yi Lin,Wei-Chiu Ma,Shenlong Wang,Shuran Song,Fangyin Wei*

Main category: cs.CV

TL;DR: SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI tasks through iterative reasoning and self-refinement.


<details>
  <summary>Details</summary>
Motivation: Real-world data collection for embodied agents is costly and unsafe, while existing scene-generation systems produce artifacts and physically invalid scenes, creating a need for scalable, realistic, and simulator-ready 3D environments.

Method: Agentic framework that couples multiple generators for layout and object composition with critics evaluating semantic plausibility, visual realism, and physical stability. Uses iterative reasoning and adaptive tool selection to self-refine scenes until meeting user intent and physical validity.

Result: Generates realistic, diverse environments directly deployable in modern simulators. Policies trained on this data show clear scaling trends and generalize to unseen objects and layouts.

Conclusion: SAGE demonstrates the promise of simulation-driven scaling for embodied AI, providing a scalable solution for generating physically valid, realistic environments for training embodied agents.

Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>


### [61] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: SARS: A shape and appearance-aware 3D reconstruction system that extracts body and face information from a single image to rebuild full 3D human models, addressing limitations of previous 3DMMs that ignored semantic facial features.


<details>
  <summary>Details</summary>
Motivation: Previous 3D Morphable Models (3DMMs) focused only on global face structure/geometry while ignoring important semantic facial features like age, gender, and facial landmarks that characterize facial boundaries, curves, dips, and wrinkles. There was a need to accommodate changes in these high-level facial characteristics for more complete human reconstruction.

Method: SARS is a modular pipeline that extracts both body and face information from a single image. It builds upon 3DMMs (which combine identity/expression blendshapes with basic face mesh) but adds shape and appearance awareness to properly rebuild 3D models of the full human body.

Result: The system can reconstruct complete 3D human models from single images, capturing not just global structure but also semantic facial features that previous methods ignored.

Conclusion: SARS represents an advancement in 3D human reconstruction by addressing the limitation of previous 3DMMs through a shape and appearance-aware approach that captures both body information and detailed facial semantics from single images.

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [62] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: Multi-task deep learning framework for liver segmentation and fibrosis staging using multiparametric MRI, addressing limited annotations and domain shifts through semi-supervised learning and patch-based classification.


<details>
  <summary>Details</summary>
Motivation: Liver fibrosis is clinically challenging, requiring precise segmentation and accurate staging. Limited annotated images, multi-parametric MRI complexity, domain shifts, and modality variations create significant obstacles for automated analysis.

Method: Two-phase approach: 1) LiSeg phase uses semi-supervised learning integrating image segmentation and registration to handle limited labels and domain shifts; 2) LiFS phase employs patch-based classification for fibrosis staging with visualization capabilities.

Result: Method tested on independent test set with in-distribution and out-of-distribution cases using both three-channel (T1, T2, DWI) and seven-channel (T1, T2, DWI, GED1-GED4) MRIs. Code publicly available on GitHub.

Conclusion: Proposed framework effectively addresses multimodality imaging challenges, limited labels, and domain shifts for liver segmentation and fibrosis staging, providing a practical solution for clinical liver fibrosis assessment.

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [63] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: RoSE reformulates monocular normal estimation as shading sequence prediction using image-to-video models, achieving better 3D alignment than direct normal prediction methods.


<details>
  <summary>Details</summary>
Motivation: Existing monocular normal estimation methods suffer from 3D misalignment - estimated normal maps look correct but reconstructed surfaces don't align with geometric details. This happens because models struggle to distinguish varying geometry from subtle color variations in normal maps.

Method: Proposes new paradigm: reformulate normal estimation as shading sequence estimation. RoSE uses image-to-video generative models to predict shading sequences under varying lighting, then converts to normal maps via ordinary least-squares. Trained on MultiShade synthetic dataset with diverse shapes, materials, and lighting.

Result: RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation, demonstrating better 3D alignment than previous methods.

Conclusion: Shading sequences are more sensitive to geometric information than normal maps, making them a better intermediate representation for monocular normal estimation. The proposed paradigm shift addresses 3D misalignment issues in existing approaches.

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [64] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: UOT-based registration-aware matcher for lesion correspondence in longitudinal CT scans handles appearance/disappearance, merging/splitting without retraining or heuristics.


<details>
  <summary>Details</summary>
Motivation: Evaluating lesion evolution in cancer patients' longitudinal CT scans is crucial for treatment assessment, but establishing reliable lesion correspondence across time is challenging. Standard geometric matchers fail when lesions appear, disappear, merge, or split.

Method: Proposes a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts to patient-level tumor-load changes. Transport cost combines size-normalized geometry, local registration trust from deformation-field Jacobian, and optional patch-level appearance consistency. The transport plan is sparsified by relative pruning.

Result: On longitudinal CT data, achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores compared to distance-only baselines.

Conclusion: The UOT-based registration-aware matcher effectively handles complex lesion evolution patterns (appearance, disappearance, merging, splitting) without retraining or heuristic rules, outperforming standard geometric approaches for longitudinal lesion tracking.

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [65] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Stroke3D is a novel framework that generates rigged 3D meshes from 2D drawn strokes and text prompts, enabling intuitive creation of animatable 3D content.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generation methods struggle to produce animatable geometry, while rigging techniques lack fine-grained control over skeleton creation. There's a need for more intuitive workflows to create ready-to-animate 3D content.

Method: Two-stage pipeline: 1) Controllable Skeleton Generation using Skeletal Graph VAE (Sk-VAE) and Skeletal Graph DiT (Sk-DiT) conditioned on text and 2D strokes; 2) Enhanced Mesh Synthesis via TextuRig dataset and SKA-DPO preference optimization strategy guided by skeleton-mesh alignment.

Result: Stroke3D produces plausible skeletons and high-quality meshes, enabling direct generation of rigged 3D assets from user inputs. It's the first method to generate rigged 3D meshes conditioned on user-drawn 2D strokes.

Conclusion: Stroke3D provides an intuitive workflow for creating ready-to-animate 3D content by combining structural control through 2D strokes with semantic guidance via text prompts, addressing limitations in existing 3D generation and rigging techniques.

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [66] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krau,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: HAC architecture combines Transformers for global vessel topology and CNNs for thin-vessel details, with physics-aware pretraining, achieving superior bladder vessel segmentation for cancer surveillance navigation.


<details>
  <summary>Details</summary>
Motivation: Bladder cancer surveillance needs reliable tumor tracking across interventions, but the deformable bladder lacks stable landmarks. Visible blood vessels could serve as patient-specific "vascular fingerprints" for navigation, but automated segmentation faces challenges from imperfect endoscopic data, artifacts, deformations, and mucosal folds that mimic vessels.

Method: Hybrid Attention-Convolution (HAC) architecture combining Transformers (for global vessel topology) with CNNs (for residual refinement of thin-vessel details). Transformer trained on optimized ground truth excluding short/terminal branches to prioritize connectivity. Physics-aware self-supervised pretraining using clinically grounded augmentations on unlabeled data to address data scarcity.

Result: Achieves high accuracy (0.94), superior precision (0.61), and clDice (0.66) on BlaVeS endoscopic video dataset, outperforming state-of-the-art medical segmentation models. Successfully suppresses false positives from mucosal folds that dynamically appear/disappear during bladder filling/emptying.

Conclusion: HAC provides reliable structural stability for clinical navigation in bladder cancer surveillance by effectively segmenting patient-specific vascular fingerprints despite challenging endoscopic conditions.

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [67] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: Hybrid approach combining deep generative prior for crack-free paintings with variational functional for crack detection in artworks


<details>
  <summary>Details</summary>
Motivation: Automated craquelure detection is crucial for artwork conservation but challenging due to visual similarity between cracks and artistic features like brush strokes

Method: Models crack detection as inverse problem: decomposes image into crack-free painting (using deep generative prior) and crack component (using Mumford-Shah-type variational functional with crack prior)

Result: Joint optimization yields pixel-level map of crack localizations in paintings

Conclusion: Proposed hybrid approach enables detailed analysis of artworks for documentation and conservation by accurately detecting craquelure despite complex scenery

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [68] [Coupled Inference in Diffusion Models for Semantic Decomposition](https://arxiv.org/abs/2602.09983)
*Calvin Yeung,Ali Zakeri,Zhuowen Zou,Mohsen Imani*

Main category: cs.CV

TL;DR: A new framework for semantic decomposition using coupled diffusion models that outperforms resonator networks on synthetic tasks.


<details>
  <summary>Details</summary>
Motivation: Many visual scenes are compositions of latent factors, requiring effective decomposition for recognition, reasoning, and editing. While resonator networks (coupled Hopfield networks) were proposed for this, recent similarities between Hopfield networks and diffusion models motivate exploring diffusion-based approaches.

Method: Frames semantic decomposition as an inverse problem using coupled inference in diffusion models. Uses reconstruction-driven guidance to encourage factor composition to match the bound vector, plus a novel iterative sampling scheme. Shows resonator networks are a special case of this framework.

Result: Empirically demonstrates that the coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

Conclusion: The proposed diffusion-based framework provides an effective approach for semantic decomposition that generalizes beyond resonator networks while achieving superior performance.

Abstract: Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>


### [69] [Toward Fine-Grained Facial Control in 3D Talking Head Generation](https://arxiv.org/abs/2602.09736)
*Shaoyang Xie,Xiaofeng Cong,Baosheng Yu,Zhipeng Gui,Jie Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: FG-3DGS improves audio-driven talking head generation by using frequency-aware disentanglement for better lip sync and reduced facial jitter in 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting methods for talking head generation struggle with precise control over fine-grained facial movements, particularly lip-synchronization inaccuracies and facial jitter that contribute to the uncanny valley effect.

Method: Proposes frequency-aware disentanglement strategy: low-frequency regions (cheeks, nose, forehead) modeled with standard MLP, high-frequency regions (eyes, mouth) captured separately with dedicated network using facial area masks. Uses Gaussian deltas for motion dynamics and high-frequency-refined post-rendering alignment from pretrained audio-video models.

Result: Outperforms recent state-of-the-art approaches on widely used datasets, producing high-fidelity, lip-synced talking head videos with better temporal consistency.

Conclusion: FG-3DGS successfully addresses lip-sync and jitter challenges in 3D Gaussian Splatting for talking heads through frequency-aware modeling and post-rendering alignment, advancing digital avatar technology.

Abstract: Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>


### [70] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: Analysis of vision system vulnerabilities in autonomous vehicles, identifying attack surfaces and evaluating security implications for confidentiality, integrity, and availability.


<details>
  <summary>Details</summary>
Motivation: Vision system robustness is critical for Level-5 autonomous driving capabilities, as safe CAV navigation depends on accurate detection of objects, lanes, and traffic signs.

Method: Analyzed key sensors and vision components to derive a reference architecture for CAV vision systems, then identified potential attack surfaces and elaborated on attack vectors targeting each surface.

Result: Developed comprehensive understanding of attack vector dynamics in vision systems, providing basis for identifying vulnerabilities and evaluating their implications for confidentiality, integrity, and availability.

Conclusion: The study provides crucial insights for formulating robust security measures that can uphold the CIA triad principles in autonomous vehicle vision systems.

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [71] [Self-Supervised Learning as Discrete Communication](https://arxiv.org/abs/2602.09764)
*Kawtar Zaher,Ilyass Moummad,Olivier Buisson,Alexis Joly*

Main category: cs.CV

TL;DR: SSL framed as discrete communication between teacher/student networks using binary codes, outperforming continuous alignment methods across multiple vision tasks.


<details>
  <summary>Details</summary>
Motivation: Most SSL methods learn continuous representations with limited control over information structure; need more structured, compact representations with better semantic organization.

Method: Frame SSL as discrete communication with binary channel; student predicts teacher's multi-label binary messages using binary cross-entropy loss with coding-rate regularization; periodic projection head reinitialization strengthens representations.

Result: Consistent improvements over continuous baselines on image classification, retrieval, dense prediction, and domain adaptation; learned binary codes form compact, informative discrete language capturing reusable semantic factors.

Conclusion: Discrete communication approach with binary codes provides better structured representations than continuous alignment, creating reusable semantic language across vision tasks.

Abstract: Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>


### [72] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: Fake-HR1 is a hybrid-reasoning model that adaptively decides when to use Chain-of-Thought reasoning for synthetic image detection, improving efficiency by avoiding unnecessary reasoning on obvious forgeries.


<details>
  <summary>Details</summary>
Motivation: Current CoT-based synthetic image detection models use reasoning for all queries, causing resource overhead (tokens, latency) that's redundant for obviously generated forgeries. Need adaptive reasoning to balance accuracy and efficiency.

Method: Two-stage training: 1) Hybrid Fine-Tuning (HFT) for cold-start initialization, 2) online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select appropriate reasoning mode (reasoning vs. direct answer).

Result: Fake-HR1 adaptively performs reasoning across different query types, surpasses existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

Conclusion: The proposed adaptive hybrid-reasoning approach effectively balances detection accuracy with computational efficiency, making synthetic image detection more practical for real-world applications.

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [73] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: Text-to-image models show severe geographic bias in training data, heavily favoring wealthy Western countries while under-representing South America and Africa, with GDP strongly correlating with representation.


<details>
  <summary>Details</summary>
Motivation: To investigate geographic representativeness in text-to-image models by determining which parts of the world the training examples come from, addressing concerns about biased training data.

Method: Geographically profile multimodal datasets by mapping image-caption pairs to countries using LLMs to extract location information from captions. Study English captions from three datasets (Re-LAION, DataComp1B, Conceptual Captions) across 20 common entities, and analyze non-English subsets from Re-LAION.

Result: US, UK, and Canada account for 48% of samples; South America and Africa severely under-represented (1.8% and 3.8%). Strong correlation between GDP and representation (=0.82). Non-English subsets skew toward countries where those languages are spoken. Higher representation doesn't guarantee greater diversity. Stable Diffusion generations appear realistic but have limited coverage compared to real-world images.

Conclusion: Text-to-image models suffer from severe geographic bias in training data, favoring wealthy Western nations, which limits their ability to generate globally representative images and raises concerns about fairness and inclusivity in AI systems.

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [74] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: SCD decouples temporal reasoning from iterative denoising in causal diffusion models, using a causal transformer encoder for once-per-frame reasoning and lightweight diffusion decoder for rendering, improving efficiency while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers at every denoising step, leading to redundant computation and inefficiency.

Method: Separable Causal Diffusion (SCD) architecture with two components: 1) causal transformer encoder for once-per-frame temporal reasoning, and 2) lightweight diffusion decoder for multi-step frame-wise rendering.

Result: SCD significantly improves throughput and per-frame latency while matching or surpassing generation quality of strong causal diffusion baselines across synthetic and real benchmarks.

Conclusion: Temporal reasoning in causal diffusion models is separable from iterative denoising, enabling more efficient architectures that maintain generation quality while improving computational efficiency.

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [75] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: SciFlow-Bench is a new benchmark for evaluating scientific diagram generation that focuses on structural correctness rather than just visual similarity, using a round-trip protocol that inverse-parses generated images back into graphs for comparison.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models often produce scientifically incorrect diagrams that look plausible but have wrong structures. Existing benchmarks either use image-centric metrics or evaluate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored.

Method: Built from real scientific PDFs, SciFlow-Bench pairs source figures with ground-truth graphs and uses a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This is enabled by a hierarchical multi-agent system coordinating planning, perception, and structural reasoning.

Result: Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology. The benchmark reveals that current models struggle with maintaining accurate structure in generated scientific diagrams.

Conclusion: Structure-aware evaluation is essential for scientific diagram generation, as visual plausibility alone is insufficient. The SciFlow-Bench approach highlights the need for models that can preserve structural correctness in complex scientific visualizations.

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [76] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Learns transferable latent actions from unlabeled video by aligning control effects across contexts, enabling better zero-shot transfer and adaptation to new control interfaces.


<details>
  <summary>Details</summary>
Motivation: Scaling action-controllable world models is limited by scarce action labels. Latent action learning from unlabeled video often fails to transfer across contexts due to entanglement of scene-specific cues and lack of shared coordinate system.

Method: Introduces Seq-REPA, a sequence-level control-effect alignment objective that anchors integrated latent actions to temporal feature differences from a frozen self-supervised video encoder. Presents Olaf-World pipeline for pretraining action-conditioned video world models from large-scale passive video.

Result: Learns more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

Conclusion: By aligning latent actions to observable semantic effects rather than just within-clip patterns, the method enables learning transferable control interfaces from passive video, overcoming limitations of standard latent action learning approaches.

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [77] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: CompSplat is a compression-aware training framework for novel view synthesis from real-world videos that addresses challenges from long sequences, irregular camera trajectories, and lossy compression by modeling frame-wise compression characteristics.


<details>
  <summary>Details</summary>
Motivation: Real-world videos for novel view synthesis face challenges from long sequences with irregular camera trajectories, unknown poses (leading to pose drift and geometric distortion), and lossy compression that introduces inconsistencies degrading geometry and rendering quality. Current approaches don't adequately address diverse compression patterns in long videos.

Method: CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to explicitly model frame-wise compression characteristics, mitigating inter-frame inconsistency and accumulated geometric errors while enhancing robustness and geometric consistency under heavy compression.

Result: Extensive experiments on challenging benchmarks (Tanks and Temples, Free, and Hike) show CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent NVS approaches under severe compression conditions.

Conclusion: CompSplat effectively addresses the combined challenges of long video sequences, irregular camera trajectories, and lossy compression in novel view synthesis, providing a robust solution for real-world applications like cultural heritage preservation and digital twins.

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [78] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: SAKED is a training-free method that mitigates hallucinations in LVLMs by quantifying knowledge stability across layers and using the most reliable internal knowledge for faithful token generation.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in Large Vision-Language Models pose significant security and reliability risks. The paper is inspired by the observation that humans are more error-prone when uncertain, and investigates how instability in a model's internal knowledge contributes to LVLM hallucinations.

Method: Proposes Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation.

Result: Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

Conclusion: SAKED is a training-free method that can be seamlessly integrated into different architectures and effectively mitigates hallucinations in LVLMs by leveraging knowledge stability analysis.

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [79] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: ARK is a multimodal retrieval benchmark focusing on professional knowledge and complex reasoning across diverse domains and visual data types, with targeted hard negatives to prevent shortcut matching.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal retrieval benchmarks focus on daily-life images and lack diagnostics for professional knowledge and complex reasoning, creating a gap in evaluating sophisticated multimodal retrieval systems.

Method: ARK analyzes multimodal retrieval from two perspectives: knowledge domains (5 domains with 17 subtypes) and reasoning skills (6 categories). It evaluates retrieval with unimodal and multimodal queries/candidates across 16 visual data types, using targeted hard negatives to prevent shortcut matching.

Result: Evaluation of 23 representative retrievers shows a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning as persistent bottlenecks. Simple enhancements like re-ranking and rewriting provide consistent improvements but substantial headroom remains.

Conclusion: ARK addresses limitations of existing benchmarks by focusing on professional knowledge and complex reasoning, revealing significant challenges in multimodal retrieval that current methods struggle to overcome, indicating need for more sophisticated approaches.

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [80] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: Kelix is a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations for multimodal learning.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely on hybrid interfaces (discrete text tokens + continuous ViT features), which creates biases toward understanding over generation and prevents full leverage of large-scale self-supervised learning on non-text data. Existing discrete visual tokenization approaches lose information due to limited code capacity, resulting in weaker understanding than continuous-feature models.

Method: Kelix uses a fully discrete autoregressive unified modeling approach that extends the LLM paradigm to multimodal data through shared discrete representations across modalities, enabling both comprehension and generation under self-supervision.

Result: Kelix closes the understanding gap between discrete and continuous visual representations, showing that fully discrete autoregressive models can achieve comparable understanding to continuous-feature VLMs while maintaining unified generation capabilities.

Conclusion: The paper demonstrates that fully discrete autoregressive unified modeling can overcome the limitations of existing discrete visual tokenization approaches, enabling both strong understanding and generation capabilities in multimodal learning while leveraging the benefits of self-supervised learning across modalities.

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [81] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: Reason-IAD: A knowledge-guided dynamic latent reasoning framework for industrial anomaly detection that uses category-specific knowledge retrieval and entropy-driven iterative reasoning in latent space to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models (MLLMs) struggle with industrial anomaly detection because they're pretrained on general-domain data and fail to capture category-specific anomalies, limiting both detection accuracy and interpretability.

Method: Two core components: 1) Retrieval-augmented knowledge module that incorporates category-specific textual descriptions for context-aware reasoning; 2) Entropy-driven latent reasoning mechanism using optimizable latent think tokens for iterative exploration in compact latent space, guided by entropy-based rewards; plus dynamic visual injection strategy that selectively incorporates informative image patches.

Result: Extensive experiments show Reason-IAD consistently outperforms state-of-the-art methods in industrial anomaly detection.

Conclusion: Reason-IAD provides an effective framework for explainable industrial anomaly detection by combining domain-specific knowledge guidance with dynamic latent reasoning, achieving superior performance over existing approaches.

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [82] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: Free-GVC is a training-free generative video compression framework that uses video diffusion priors to compress latent trajectories, achieving superior perceptual quality and temporal coherence at ultra-low bitrates.


<details>
  <summary>Details</summary>
Motivation: Existing generative video compression methods have limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates, which motivates the need for better temporal modeling.

Method: Reformulates video coding as latent trajectory compression guided by video diffusion prior. Uses GOP-level encoding with Adaptive Quality Control module to predict optimal diffusion steps per GOP, and Inter-GOP Alignment module for latent fusion between adjacent groups to enhance temporal coherence.

Result: Achieves 93.29% BD-Rate reduction in DISTS over DCVC-RT, with user study confirming superior perceptual quality and temporal coherence at ultra-low bitrates.

Conclusion: Free-GVC demonstrates that training-free generative video compression with diffusion priors and proper temporal modeling can achieve excellent perceptual quality and temporal coherence at ultra-low bitrates.

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [83] [BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.09872)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: BabyMamba-HAR introduces two lightweight Mamba-inspired architectures for resource-constrained human activity recognition, achieving competitive accuracy with significantly reduced computational costs compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Human activity recognition on wearable/mobile devices faces memory and computational constraints while needing to maintain accuracy across heterogeneous sensor configurations. Selective state space models offer linear-time processing but their design for TinyML applications remains unexplored.

Method: Two novel architectures: (1) CI-BabyMamba-HAR with channel-independent stem processing each sensor channel through shared-weight transformations, and (2) Crossover-BiDir-BabyMamba-HAR with early fusion stem achieving channel-count independent complexity. Both use weight-tied bidirectional scanning and lightweight temporal attention pooling.

Result: Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with ~27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high-channel datasets. Bidirectional scanning provides up to 8.42% F1 improvement, and gated temporal attention gives up to 8.94% gain over mean pooling.

Conclusion: The framework establishes practical design principles for deploying selective state space models as efficient TinyML backbones for human activity recognition, demonstrating that lightweight Mamba architectures can achieve competitive performance with dramatically reduced computational requirements.

Abstract: Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.

</details>


### [84] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: Proposes an embodied 4D world model for robotic manipulation that generates geometrically consistent multi-view RGBD predictions from single-view input, with test-time action optimization for accurate manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing world-model approaches for robotic manipulation are limited to either purely image-based forecasting or reasoning over partial 3D geometry, lacking the ability to predict complete 4D scene dynamics needed for effective manipulation.

Method: 1) Embodied 4D world model that generates arbitrary-view RGBD predictions from single-view RGBD input, using cross-view and cross-modality feature fusion for consistency; 2) Test-time action optimization that backpropagates through the generative model to infer trajectory-level latent; 3) Residual inverse dynamics model to convert trajectory prior into executable actions.

Result: Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation tasks, with ablations providing insights into key design choices.

Conclusion: The proposed embodied 4D world model with test-time action optimization enables geometrically consistent scene prediction and accurate manipulation, advancing the imagine-then-act paradigm for robotics.

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [85] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: AdaTSQ is a post-training quantization framework for Diffusion Transformers that uses temporal sensitivity analysis to optimize bit-width allocation across timesteps, achieving better efficiency-quality trade-offs than existing methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) have high computational costs that limit deployment on edge devices. Existing post-training quantization methods fail for DiTs because they ignore the unique temporal dynamics of diffusion processes.

Method: Two key innovations: 1) Pareto-aware timestep-dynamic bit-width allocation using beam search guided by reconstruction error, and 2) Fisher-guided temporal calibration that prioritizes data from sensitive timesteps and integrates with Hessian-based weight optimization.

Result: Extensive experiments on four advanced DiTs (Flux-Dev, Flux-Schnell, Z-Image, Wan2.1) show AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q.

Conclusion: AdaTSQ effectively addresses the temporal dynamics challenge in DiT quantization, pushing the Pareto frontier of efficiency and quality for deployment on resource-constrained devices.

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [86] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jrgen Weitz,Marius Distler,Sren Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: LASANA dataset provides 1270 stereo laparoscopic training videos with skill ratings and error labels to enable deep learning research for surgical skill assessment.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for laparoscopic skill assessment are limited by small annotated datasets, hindering development and evaluation of automated assessment systems.

Method: Created LASANA dataset with 1270 stereo video recordings of four basic laparoscopic training tasks, annotated with structured skill ratings from three raters and binary error labels, with predefined data splits for benchmarking.

Result: Dataset captures natural skill variation from laparoscopic training courses and provides baseline deep learning results for future comparison.

Conclusion: LASANA dataset addresses the data scarcity problem in surgical skill assessment research and enables benchmarking of video-based assessment approaches.

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [87] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: GeoFormer: Open-source Swin Transformer framework using Sentinel-1/2 imagery and DEM to jointly estimate building height and footprint on 100m grid across 54 cities with improved accuracy over CNN baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D urban data is critical for climate modeling, disaster risk assessment, and urban planning, but remains scarce due to reliance on proprietary sensors or poor cross-city generalization.

Method: Proposes GeoFormer, an open-source Swin Transformer framework that jointly estimates building height and footprint using only Sentinel-1/2 imagery and open DEM data on 100m grid. Uses geo-blocked splitting strategy for strict spatial independence between training and test sets.

Result: Achieves BH RMSE of 3.19m and BF RMSE of 0.05 across 54 diverse cities, improving 7.5% and 15.3% over strongest CNN baseline. Maintains under 3.5m BH RMSE in cross-continent transfer. Ablation studies show DEM is indispensable for height estimation and optical reflectance dominates over SAR, though multi-source fusion yields best accuracy.

Conclusion: GeoFormer provides accurate, generalizable 3D urban data using open-source satellite imagery and DEM, with all code, weights, and global products publicly released to address scarcity of urban 3D data for climate and planning applications.

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [88] [VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization](https://arxiv.org/abs/2602.09934)
*Yikun Liu,Yuan Liu,Shangzhe Di,Haicheng Wang,Zhongyin Zhao,Le Tian,Xiao Zhou,Jie Zhou,Jiangchao Yao,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: VersaViT transforms MLLM vision encoders into versatile backbones for both high-level semantic understanding and pixel-level dense prediction tasks through collaborative multi-task post-training.


<details>
  <summary>Details</summary>
Motivation: While MLLMs excel at visual-language understanding, their vision encoders show deficiencies in dense feature representations and perform poorly on classic vision-centric tasks like semantic segmentation and depth estimation. The paper aims to create a truly versatile vision backbone.

Method: Proposes VersaViT, a novel multi-task framework for collaborative post-training that optimizes vision backbones via lightweight task heads with multi-granularity supervision, addressing dense prediction weaknesses while maintaining semantic alignment.

Result: Extensive experiments show VersaViT effectively creates a versatile vision backbone suitable for both language-mediated reasoning and pixel-level understanding across various downstream tasks.

Conclusion: The work successfully transforms MLLM vision encoders into well-rounded vision transformers that can reliably perform both high-level semantic tasks and classic dense prediction tasks, bridging the gap between language-aligned and vision-centric capabilities.

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>


### [89] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: Automated leftover baked goods monitoring using object detection with limited supervision, achieving 0.91 mAP with image-level labels and 19.3% improvement via pseudo-label fine-tuning.


<details>
  <summary>Details</summary>
Motivation: German bakeries need to monitor leftover baked goods for production optimization, but manual monitoring is labor-intensive and error-prone. The diversity of baked goods makes fully supervised training expensive, and existing open-vocabulary detectors are insufficient for this specialized task.

Method: Two training workflows: 1) Weakly supervised training combining OWLv2 and Grounding DINO localization with image-level supervision, 2) Fine-tuning on video frames using Segment Anything 2 for pseudo-label propagation to improve viewpoint robustness. YOLOv11 is used for detection due to speed-accuracy tradeoff.

Result: Model achieves 0.91 mAP with only image-level supervision. Pseudo-label fine-tuning improves performance by 19.3% under non-ideal deployment conditions. Combined approach surpasses fully-supervised baseline under non-ideal conditions despite using only image-level supervision.

Conclusion: The proposed limited-supervision workflows effectively address specialized computer vision tasks in industries with scarce annotated data, demonstrating practical applicability for bakery leftover monitoring while solving broader industrial deployment challenges.

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [90] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schffler*

Main category: cs.CV

TL;DR: Automated classification of 16 histopathology stains using two approaches: Multi-Instance Learning (MIL) and lightweight thumbnail-based method, with thumbnail approach offering best generalization and 100x faster throughput for quality control.


<details>
  <summary>Details</summary>
Motivation: Accurate stain metadata is critical for quality control in clinical archives and computational pathology datasets, as pathologists use various special stains beyond standard H&E for diagnosis.

Method: Two approaches compared: 1) Multi-Instance Learning (MIL) pipeline using whole slide images, and 2) proposed lightweight thumbnail-based approach for classifying 14 most common special stains plus standard and frozen-section H&E.

Result: MIL achieved highest performance on internal data (macro F1: 0.941 for 16 classes), while thumbnail approach was competitive (0.897). On external TCGA data, thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL) and was 100x faster (5.635 vs. 0.018 slides/s).

Conclusion: Thumbnail-based classification provides a scalable and robust solution for routine visual quality control in digital pathology workflows, offering good generalization and dramatically increased throughput.

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [91] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: Faster-GS consolidates and optimizes 3D Gaussian Splatting techniques, achieving 5x faster training while maintaining quality, plus extending to 4D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS research is fragmented with entangled implementation improvements and algorithmic modifications, making fair comparisons difficult. Many methods trade performance for fidelity.

Method: Consolidates effective strategies from prior 3DGS research, adds novel optimizations, and investigates underexplored aspects like numerical stability, Gaussian truncation, and gradient approximation.

Result: Achieves up to 5x faster training while maintaining visual quality across comprehensive benchmarks. Also successfully applies optimizations to 4D Gaussian reconstruction for efficient non-rigid scene optimization.

Conclusion: Faster-GS establishes a new cost-effective and resource-efficient baseline for 3DGS optimization, with demonstrated applicability to both 3D and 4D reconstruction tasks.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [92] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: Skull-stripped brain MRIs can be linked across databases using simple image similarity methods, posing privacy risks even after de-identification.


<details>
  <summary>Details</summary>
Motivation: Current regulatory frameworks for sharing brain MRI data require removing identifiers, but even skull-stripped brain images contain unique signatures that could enable participant re-identification across databases if other data features are available.

Method: Used standard preprocessing followed by image similarity computation to match individuals' skull-stripped T1-weighted MRIs across different time intervals, scanner types, spatial resolutions, and acquisition protocols.

Result: Achieved nearly perfect linkage accuracy in matching data samples across various conditions, demonstrating that participant re-identification is possible even with skull-stripped images.

Conclusion: Simple image similarity methods can successfully link individuals' brain MRIs across databases, highlighting significant privacy risks that should inform more thoughtful medical data sharing policies.

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [93] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: Conformal prediction algorithm for instance segmentation that generates adaptive confidence sets with provable coverage guarantees for pixel queries.


<details>
  <summary>Details</summary>
Motivation: Current instance segmentation models lack principled uncertainty quantification - outputs are not calibrated, and there's no guarantee that predicted masks are close to ground truth.

Method: Introduces a conformal prediction algorithm that generates confidence sets for instance segmentation. Given an image and pixel coordinate query, produces confidence set of instance predictions with provable probability guarantee that at least one prediction has high IoU with true mask.

Result: Applied to agricultural field delineation, cell segmentation, and vehicle detection. Prediction sets vary in size based on query difficulty, attain target coverage, and outperform baselines (Learn Then Test, Conformal Risk Control, morphological dilation).

Conclusion: Provides both asymptotic and finite sample guarantee versions of the algorithm for reliable uncertainty quantification in instance segmentation.

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [94] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: Proposes Spatio-Temporal Attention (STA) mechanism for video semantic segmentation that extends transformer attention to leverage temporal consistency across frames, improving both accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based semantic segmentation models process video frames independently, failing to leverage temporal consistency that could significantly improve accuracy and stability in dynamic scenes.

Method: Proposes Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, modifying standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal architectural changes.

Result: Substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines on Cityscapes and BDD100k datasets.

Conclusion: STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models, serving as an effective architectural enhancement for video-based semantic segmentation applications.

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [95] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim is an attention-based state-space framework for image forgery detection that jointly localizes manipulated (target) and source regions, achieving SOTA performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional forgery detection methods focus only on manipulated areas using artifact cues, but this can be misleading in real-world scenarios like protest imagery where understanding the full context (both source and target regions) is crucial for proper interpretation.

Method: Forensim uses a visual state-space model with normalized attention maps to identify internal similarities, combined with a region-based block attention module to distinguish manipulated regions. It outputs three-class masks (pristine, source, target) and supports both splicing and copy-move forgeries in a unified architecture.

Result: Forensim achieves state-of-the-art performance on standard benchmarks. The authors also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

Conclusion: Forensim provides a comprehensive solution for image forgery detection by jointly localizing source and target regions, enabling better contextual understanding of manipulations and supporting multiple forgery types within a unified framework.

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [96] [4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094)
*Yihang Luo,Shangchen Zhou,Yushi Lan,Xingang Pan,Chen Change Loy*

Main category: cs.CV

TL;DR: 4RC is a unified feed-forward framework for 4D reconstruction from monocular videos that jointly captures dense geometry and motion dynamics through an encode-once, query-anywhere paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing approaches typically decouple motion from geometry or produce limited 4D attributes like sparse trajectories or two-view scene flow, lacking a holistic 4D representation that jointly captures dense scene geometry and motion dynamics.

Method: Introduces an encode-once, query-anywhere paradigm where a transformer backbone encodes the entire video into a compact spatio-temporal latent space, and a conditional decoder queries 3D geometry and motion for any frame at any timestamp. Represents 4D attributes in minimally factorized form as base geometry plus time-dependent relative motion.

Result: Extensive experiments show 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

Conclusion: 4RC provides a unified framework for holistic 4D reconstruction that jointly captures geometry and motion dynamics, addressing limitations of existing decoupled approaches through its novel encode-once, query-anywhere architecture.

Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>


### [97] [VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102)
*Zhongwei Ren,Yunchao Wei,Xiao Yu,Guixun Luo,Yao Zhao,Bingyi Kang,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: VideoWorld 2 extends VideoWorld to learn transferable knowledge directly from raw real-world videos using a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving significant improvements on real-world tasks and robotics manipulation.


<details>
  <summary>Details</summary>
Motivation: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. Prior video generation and latent-dynamics models struggle with real-world tasks.

Method: Introduces dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: pretrained video diffusion model handles visual appearance, while dLDM learns latent codes focusing on compact task-related dynamics. These latent codes are modeled autoregressively to learn task policies and support long-horizon reasoning.

Result: Achieves up to 70% improvement in task success rate on real-world handcraft making tasks, produces coherent long execution videos. In robotics, acquires effective manipulation knowledge from Open-X dataset, substantially improving task performance on CALVIN benchmark.

Conclusion: Reveals the potential of learning transferable world knowledge directly from raw videos. All code, data, and models will be open-sourced for further research.

Abstract: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>


### [98] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: ConsID-Gen: A view-assisted image-to-video framework that uses auxiliary views and dual-stream encoding to preserve object identity and geometric consistency in video generation.


<details>
  <summary>Details</summary>
Motivation: Existing I2V models suffer from appearance drift and geometric distortion due to sparse single-view 2D observations and weak cross-modal alignment, making it challenging to preserve object identity under changing viewpoints.

Method: 1) Created ConsIDVid dataset with scalable pipeline for high-quality aligned videos; 2) Proposed ConsID-Gen framework with view-assisted approach using unposed auxiliary views, dual-stream visual-geometric encoder, and text-visual connector for unified conditioning of Diffusion Transformer backbone.

Result: ConsID-Gen outperforms leading models like Wan2.1 and HunyuanVideo on ConsIDVid-Bench metrics, achieving superior identity fidelity and temporal coherence in challenging real-world scenarios.

Conclusion: The proposed data curation and view-assisted generation framework effectively addresses identity preservation challenges in I2V, demonstrating significant improvements in multi-view consistency and object fidelity.

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [99] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Mller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: IQARS is the first quantum annealing algorithm for multiple rotation averaging that achieves ~12% higher accuracy than classical methods by better preserving rotation manifold geometry and leveraging quantum tunneling.


<details>
  <summary>Details</summary>
Motivation: Classical MRA methods like L1-IRLS and Shonan have limitations including susceptibility to local minima, reliance on convex relaxations that don't preserve exact manifold geometry, and reduced accuracy in high-noise scenarios.

Method: IQARS reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, removing convex relaxation dependence and better preserving non-Euclidean rotation manifold geometry.

Result: IQARS on D-Wave annealers achieves approximately 12% higher accuracy than Shonan (the best-performing classical method evaluated), despite current quantum annealers being in nascent phase with limited scale support.

Conclusion: IQARS demonstrates quantum annealing's potential for MRA problems by achieving superior accuracy through better manifold geometry preservation and quantum tunneling advantages, though current hardware limitations restrict problem scale.

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [100] [Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm](https://arxiv.org/abs/2602.09046)
*Mohammad Jabari,Carmen Visconte,Giuseppe Quaglia,Med Amine Laribi*

Main category: cs.RO

TL;DR: Optimal tendon force design for a two-segment tendon-driven continuum robot to maximize its feasible static workspace under external loads using genetic algorithm optimization.


<details>
  <summary>Details</summary>
Motivation: To develop an optimal design method for tendon-driven continuum robots that can maximize their feasible static workspace while accounting for external forces and torques, which is crucial for practical applications where robots operate under load conditions.

Method: Uses a genetic algorithm optimization approach to maximize the Euclidean norm of the robot's tip position over the workspace, treating tendon forces as design variables and the feasible static workspace as the optimization objective, with simulations including external loads (torques and forces).

Result: The proposed method effectively identifies optimal tendon forces to maximize the feasible static workspace of the two-segment, eight-tendon robot, even when subjected to external forces and torques.

Conclusion: The genetic algorithm-based optimization approach successfully enables optimal tendon force design for maximizing the feasible static workspace of tendon-driven continuum robots under external loading conditions.

Abstract: This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.

</details>


### [101] [Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception](https://arxiv.org/abs/2602.09076)
*Nhat Le,Daeun Song,Xuesu Xiao*

Main category: cs.RO

TL;DR: Using human skeletal features, especially lower-body 3D keypoints, improves trajectory prediction by 13% ADE reduction, with biomechanical cues adding further 1-4% improvement.


<details>
  <summary>Details</summary>
Motivation: Most existing human trajectory prediction approaches treat humans as point masses, ignoring valuable skeletal information that could improve forecast accuracy for social robot navigation.

Method: Systematically evaluate predictive utility of 2D/3D skeletal keypoints and derived biomechanical cues as additional inputs, testing on JRDB dataset and new 360-degree panoramic video dataset for social navigation.

Result: Lower-body 3D keypoints yield 13% reduction in Average Displacement Error; augmenting 3D keypoints with biomechanical cues provides additional 1-4% improvement. Performance gain persists with 2D keypoints from equirectangular panoramic images.

Conclusion: Robots can forecast human movement efficiently by watching legs, providing actionable insights for designing sensing capabilities for social robot navigation, with monocular surround vision capturing informative motion cues.

Abstract: Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>


### [102] [Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality](https://arxiv.org/abs/2602.09123)
*Jackson Habala,Gabriel B. Margolis,Tianyu Wang,Pratyush Bhatt,Juntao He,Naheel Naeem,Zhaochen Xu,Pulkit Agrawal,Daniel I. Goldman,Di Luo,Baxi Chong*

Main category: cs.RO

TL;DR: A geometric mechanics framework discovers asymmetric hexapod gaits that improve speed by 50% over conventional methods by exploiting symmetry breaking in multi-legged systems.


<details>
  <summary>Details</summary>
Motivation: Current legged robot research focuses on bipeds/quadrupeds despite potential advantages of more legs, lacking principled control frameworks to coordinate many contacts and exploit new symmetries in higher-dimensional systems.

Method: Uses geometric mechanics to reduce contact-rich locomotion planning to graph optimization, and applies spin model duality from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization.

Result: Identified asymmetric hexapod gait achieving 0.61 body lengths per cycle (50% improvement over conventional gaits), with asymmetry at both control (oscillating body orientation) and hardware (two legs unactuated) levels.

Conclusion: The framework enables discovery of novel locomotion behaviors through symmetry reforming in high-dimensional systems, validated by simulations and robophysical experiments.

Abstract: Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>


### [103] [SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes](https://arxiv.org/abs/2602.09153)
*Nicholas Pfaff,Thomas Cohn,Sergey Zakharov,Rick Cory,Russ Tedrake*

Main category: cs.RO

TL;DR: SceneSmith is a hierarchical agentic framework that generates physically realistic, cluttered indoor environments from text prompts for robotic simulation, outperforming prior methods in object density, physical stability, and realism.


<details>
  <summary>Details</summary>
Motivation: Existing simulation environments for home robots lack the dense clutter, articulated furniture, and physical complexity of real indoor spaces, limiting their usefulness for training and evaluating robotic manipulation tasks.

Method: Hierarchical agentic framework with VLM agents (designer, critic, orchestrator) that constructs scenes through successive stages: architectural layout  furniture placement  small object population, integrating text-to-3D synthesis, dataset retrieval, and physical property estimation.

Result: Generates 3-6x more objects than prior methods with <2% inter-object collisions and 96% physical stability; achieves 92% realism and 91% prompt faithfulness win rates in user study with 205 participants; enables end-to-end robot policy evaluation.

Conclusion: SceneSmith addresses the gap in realistic indoor simulation environments for robotics by generating physically complex, cluttered scenes from natural language prompts, demonstrating superior performance over existing methods and practical utility for robotic training and evaluation.

Abstract: Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\unicode{x2013}$from architectural layout to furniture placement to small object population$\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.

</details>


### [104] [Elements of Robot Morphology: Supporting Designers in Robot Form Exploration](https://arxiv.org/abs/2602.09203)
*Amy Koike,Ge,Guo,Xinning He,Callie Y. Kim,Dakota Sullivan,Bilge Mutlu*

Main category: cs.RO

TL;DR: A framework called Elements of Robot Morphology identifies 5 fundamental elements for systematic robot form design, supported by tangible Morphology Exploration Blocks for hands-on experimentation.


<details>
  <summary>Details</summary>
Motivation: Robot morphology is crucial for HRI but lacks systematic design frameworks for exploring robot forms, shapes, and structures.

Method: Developed Elements of Robot Morphology framework with 5 elements (perception, articulation, end effectors, locomotion, structure) based on analysis of existing robots, then created Morphology Exploration Blocks as tangible toolkit for hands-on experimentation.

Result: The framework and toolkit were evaluated through case studies and design workshops, demonstrating support for analysis, ideation, reflection, and collaborative robot design.

Conclusion: The Elements of Robot Morphology framework and Morphology Exploration Blocks provide systematic approach for exploring robot forms in HRI design.

Abstract: Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>


### [105] [Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications](https://arxiv.org/abs/2602.09204)
*Ozan Kaya,Emir Cem Gezer,Roger Skjetne,Ingrid Bouwer Utne*

Main category: cs.RO

TL;DR: Hybrid risk-aware navigation system for autonomous surface vessels combining probabilistic risk mapping with RRT* path planning and B-spline trajectory smoothing for safe navigation in dynamic marine environments.


<details>
  <summary>Details</summary>
Motivation: Need for robust autonomous navigation in changing marine environments that can handle uncertainty, perceive dynamic obstacles, and ensure operational safety beyond conventional sensor-only approaches.

Method: Hybrid architecture with probabilistic risk maps capturing obstacle proximity and dynamic behavior, risk-biased RRT* planner with three rewiring modes (minimize path length, minimize risk, optimize both), and B-spline trajectory smoothing for continuity.

Result: System successfully navigates static and dynamic obstacles, maintains smooth trajectories, dynamically adapts to environmental risks, and shows improved safety and autonomy compared to conventional LIDAR/vision-only approaches.

Conclusion: The hybrid risk-aware navigation framework is a promising solution for autonomous vehicle missions in uncertain, dynamic environments, offering enhanced safety and adaptability over traditional methods.

Abstract: Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>


### [106] [From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers](https://arxiv.org/abs/2602.09227)
*Ananya Yammanuru,Maria Lusardi,Nancy M. Amato,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: The paper introduces MMLO-LMP, a motion planning problem for generating trajectories that are legible to friendly observers but illegible to adversarial ones, considering limited visibility regions.


<details>
  <summary>Details</summary>
Motivation: In mixed-motive environments with multiple observers having different motives (cooperative vs adversarial) and limited visibility, robots need to communicate intentions selectively - legible to allies but illegible to enemies.

Method: Proposes the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem and presents DUBIOUS, a trajectory optimizer that solves it by balancing legibility for positive observers with illegibility for negative observers while considering visibility constraints.

Result: DUBIOUS successfully generates trajectories that balance legibility with observer motives and limited visibility regions, demonstrating multiple strategies an agent can use to achieve the objective.

Conclusion: The MMLO-LMP framework addresses selective intention communication in mixed-motive environments, with future work including moving observers and observer teaming scenarios.

Abstract: In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>


### [107] [STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory](https://arxiv.org/abs/2602.09255)
*Mingfeng Yuan,Hao Zhang,Mahan Mohammadi,Runhao Li,Jinjun Shan,Steven L. Waslander*

Main category: cs.RO

TL;DR: STaR is an agentic reasoning framework for mobile robots that builds scalable long-term memory and uses information bottleneck-based retrieval for precise navigation reasoning in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Mobile robots need scalable long-horizon memory for planning and reasoning in diverse open, dynamic scenes (warehouses, manufacturing, agriculture, roadways) to handle open-ended instructions at variable granularity with precise actionable navigation answers.

Method: STaR framework has two components: (1) task-agnostic multimodal long-term memory preserving fine-grained environmental semantics (object attributes, spatial relations, dynamic events), and (2) Scalable Task-Conditioned Retrieval algorithm based on Information Bottleneck principle to extract compact, non-redundant, information-rich candidate memories for contextual reasoning.

Result: Outperforms strong baselines on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA (warehouse benchmark with visually similar objects), achieving higher success rates and markedly lower spatial error. Successfully deployed on real Husky wheeled robot in indoor/outdoor environments.

Conclusion: STaR demonstrates robust long-horizon reasoning, scalability, and practical utility for mobile robot navigation in complex dynamic environments through its agentic reasoning framework combining scalable memory and efficient retrieval.

Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>


### [108] [Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation](https://arxiv.org/abs/2602.09259)
*Yizhou Li,Shuyuan Yang,Jiaji Su,Zonghe Chua*

Main category: cs.RO

TL;DR: This paper analyzes how expertise level and perceptual modality affect surgical gaze models, finding that passive gaze can substitute for active operative gaze with predictable degradation, and novice passive labels can approximate intermediate expertise.


<details>
  <summary>Details</summary>
Motivation: In robot-assisted surgery, reduced haptic feedback increases reliance on expert visual perception, but collecting operative expert gaze is costly. It's unclear how expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing) affect what attention models learn.

Method: Created a paired active-passive, multi-task surgical gaze dataset on da Vinci SimNow simulator across four drills. Active gaze recorded during task execution using VR headset with eye tracking, then same videos reused to collect passive gaze from observers. Used fixation density overlap analyses and single-frame saliency modeling to evaluate gaze substitutability.

Result: MSI-Net produced stable, interpretable predictions while SalGAN was unstable. Models trained on passive gaze recovered substantial portion of intermediate active attention with predictable degradation. Transfer was asymmetric between active and passive targets. Novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations.

Conclusion: Passive gaze can substitute for operative supervision with predictable degradation, and novice passive labels can approximate intermediate expertise, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

Abstract: In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>


### [109] [Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction](https://arxiv.org/abs/2602.09287)
*Minja Axelsson,Henry Shevlin*

Main category: cs.RO

TL;DR: Paper distinguishes anthropomorphism (users perceiving human-like qualities in robots) from anthropomimesis (developers designing human-like features into robots), clarifying responsibility for human-like qualities in HRI.


<details>
  <summary>Details</summary>
Motivation: To disambiguate theoretical concepts of anthropomorphism and anthropomimesis in Human-Robot Interaction and social robotics, providing conceptual clarity for future research.

Method: Conceptual analysis and theoretical disambiguation, defining anthropomorphism as user perception of human-like qualities and anthropomimesis as designer implementation of human-like features.

Result: Clear distinction between anthropomorphism (perceiver's role) and anthropomimesis (designer's role), establishing responsibility framework for human-like qualities in robots.

Conclusion: Provides foundational conceptual clarification for HRI scholarship, enabling researchers to build on disambiguated concepts for future robot design and evaluation.

Abstract: In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.

</details>


### [110] [CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments](https://arxiv.org/abs/2602.09367)
*Jinghan Yang,Jingyi Hou,Xinbo Yu,Wei He,Yifan Wu*

Main category: cs.RO

TL;DR: CAPER is a framework for robotic scientific experiments that separates reasoning, grounding, and control to ensure procedural correctness and robustness in low-data settings.


<details>
  <summary>Details</summary>
Motivation: End-to-end vision-language-action models struggle with procedurally correct long-horizon manipulation in scientific labs due to recoverable error assumptions and data-driven policy limitations in protocol-sensitive experiments.

Method: CAPER enforces responsibility separation: task-level reasoning generates procedurally valid action sequences under constraints, mid-level multimodal grounding handles subtasks without LLM spatial decisions, and low-level control adapts via RL with minimal demonstrations.

Result: Experiments on scientific workflow benchmarks and long-horizon manipulation datasets show consistent improvements in success rate and procedural correctness, especially in low-data and long-horizon settings.

Conclusion: By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations, improving controllability, robustness, and data efficiency for robotic scientific experiments.

Abstract: Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>


### [111] [Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes](https://arxiv.org/abs/2602.09368)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: Certifiable gradient-based policy synthesis for contact-rich manipulation using smoothed differentiable physics with formal guarantees on true hybrid dynamics.


<details>
  <summary>Details</summary>
Motivation: Gradient-based optimization with differentiable simulators struggles with contact-rich manipulation due to discontinuous/vanishing gradients from hybrid contact dynamics. Smoothing helps but causes model mismatch that can lead to controller failures on real systems.

Method: Plan with smoothed dynamics while explicitly quantifying and compensating for induced errors. Use novel differentiable simulator based on convex optimization to smooth contact dynamics and geometry, characterize discrepancy as set-valued deviation, and constrain time-varying affine feedback policies through analytical bounds on reachable sets.

Result: Achieves guaranteed constraint satisfaction with lower safety violation and goal error than baselines on contact-rich tasks including planar pushing, object rotation, and in-hand dexterous manipulation.

Conclusion: First certifiable gradient-based policy synthesis method for contact-rich manipulation by bridging differentiable physics with set-valued robust control, providing formal guarantees of constraint satisfaction and goal reachability on true hybrid dynamics.

Abstract: Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.

</details>


### [112] [Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation](https://arxiv.org/abs/2602.09370)
*Minsung Yoon,Jeil Jeong,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: PAPL is a reinforcement learning framework that uses phase-conditioned modulation layers to enable quadruped robots to learn unified skateboarding policies across different phases.


<details>
  <summary>Details</summary>
Motivation: Skateboards are efficient personal mobility devices, but controlling them with legged robots is challenging due to perception-driven interactions and multi-modal control objectives across different skateboarding phases.

Method: Phase-Aware Policy Learning (PAPL) integrates phase-conditioned Feature-wise Linear Modulation (FiLM) layers into actor and critic networks, leveraging the cyclic nature of skateboarding to create a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases.

Result: Evaluations in simulation validated command-tracking accuracy, conducted ablation studies quantifying each component's contribution, compared locomotion efficiency against leg and wheel-leg baselines, and demonstrated real-world transferability.

Conclusion: PAPL successfully addresses the challenges of skateboarding with quadruped robots by creating a phase-aware reinforcement learning framework that enables effective skateboard control through phase-conditioned modulation and knowledge sharing across skateboarding phases.

Abstract: Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.

</details>


### [113] [Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments](https://arxiv.org/abs/2602.09430)
*Yiwen Pang,Bo Zhou,Changjin Li,Xuanhao Wang,Shengxiang Xu,Deng-Bao Wang,Min-Ling Zhang,Shimin Di*

Main category: cs.RO

TL;DR: Agentic VLA plugin enables long-horizon scientific experiments by adding LLM-based transition inference between atomic tasks without retraining.


<details>
  <summary>Details</summary>
Motivation: VLA models fail at composite scientific tasks due to distributional mismatch between training-time atomic tasks and inference-time composite tasks, lacking transitional operations between atomic actions.

Method: Proposes an LLM-based agentic inference plugin that performs explicit transition inference and generates transitional robotic action code to guide VLA models through missing steps between atomic tasks.

Result: Increases average success rate per atomic task by 42% in simulation, and method transfers easily from simulation to real scientific laboratories.

Conclusion: The inference-only plugin enables reliable execution of composite scientific workflows without additional training, making it computationally efficient and suitable for open-ended robotic laboratory tasks.

Abstract: Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

</details>


### [114] [LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration](https://arxiv.org/abs/2602.09472)
*Shuyuan Hu,Tao Lin,Kai Ye,Yang Yang,Tianwei Zhang*

Main category: cs.RO

TL;DR: Neuro-symbolic framework combines LLM reasoning with hierarchical LTL specifications to solve multi-robot task allocation and planning problems with real-time adaptation.


<details>
  <summary>Details</summary>
Motivation: LLM-generated plans for multi-robot tasks lack kinematic feasibility and efficiency, especially in long-horizon scenarios, while formal methods like LTL are static and computationally limited.

Method: Proposes a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves Simultaneous Task Allocation and Planning (STAP) using receding horizon planning with real-time perception.

Result: Extensive real-world experiments show the approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

Conclusion: The framework successfully bridges the gap between LLM flexibility and LTL formal guarantees, enabling dynamic, real-time multi-robot task planning with robustness to environmental changes.

Abstract: While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

</details>


### [115] [Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization](https://arxiv.org/abs/2602.09563)
*Lucas Palazzolo,Mickal Binois,Latitia Giraldi*

Main category: cs.RO

TL;DR: Bayesian optimization with B-spline parametrization solves optimal control for microswimmer trajectory tracking, handling complex fluid dynamics without gradient computations.


<details>
  <summary>Details</summary>
Motivation: Trajectory tracking for microswimmers is challenging due to low-Reynolds-number dynamics, requiring control methods that can handle complex fluid-structure interactions without computationally expensive gradient calculations.

Method: Formulate trajectory tracking as optimal control problem, solve using B-spline parametrization combined with Bayesian optimization to avoid complex gradient computations and handle high computational costs.

Result: Successfully reproduces target trajectories for flagellated magnetic swimmer (including biologically inspired paths) and adapts to wall-induced hydrodynamic effects in three-sphere swimmer model; works consistently across different fidelity models from ODE to PDE simulations.

Conclusion: Bayesian optimization is a versatile tool for optimal control in microscale locomotion, robust across different model fidelities and capable of handling complex fluid-structure interactions.

Abstract: Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>


### [116] [Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows](https://arxiv.org/abs/2602.09580)
*Chenyu Yang,Denis Tarasov,Davide Liconti,Hehui Zheng,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: SOFT-FLOW is a sample-efficient off-policy fine-tuning framework using normalizing flow policies for multimodal action chunks and chunk-level critics, enabling stable adaptation on real robotic dexterous manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world fine-tuning of dexterous manipulation policies is challenging due to limited interaction budgets and multimodal action distributions. Diffusion policies lack tractable likelihoods for conservative updates, while Gaussian policies collapse under multimodality, especially with chunked actions. Standard per-step critics also fail with chunked execution.

Method: SOFT-FLOW uses normalizing flow policies that provide exact likelihoods for multimodal action chunks, enabling conservative policy updates through likelihood regularization. It employs an action-chunked critic that evaluates entire action sequences, aligning value estimation with the policy's temporal structure for better credit assignment.

Result: SOFT-FLOW achieves stable, sample-efficient adaptation on two challenging real-world dexterous manipulation tasks: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp. It outperforms standard methods that struggle with these tasks requiring precise, dexterous control over long horizons.

Conclusion: SOFT-FLOW is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware, successfully addressing challenges of sample-efficient fine-tuning for dexterous manipulation with multimodal action distributions and long-horizon control.

Abstract: Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.

</details>


### [117] [Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation](https://arxiv.org/abs/2602.09583)
*Marco Moletta,Michael C. Welle,Danica Kragic*

Main category: cs.RO

TL;DR: RKO combines RPO and KTO frameworks to adapt pretrained visuomotor diffusion policies for personal preferences in cloth folding using limited demonstrations, achieving better performance than standard fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Humans have subtle, personal preferences for how manipulation tasks should be performed, especially with deformable objects like garments. Robots need to account for these preferences for personalization and user satisfaction, but this remains underexplored in robotic manipulation.

Method: RKO (novel preference-alignment method) combines benefits of RPO and KTO frameworks to adapt pretrained visuomotor diffusion policies using limited demonstrations of preferred behaviors. Evaluated on real-world cloth-folding tasks with multiple garments and preference settings.

Result: Preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. RKO outperforms common preference learning frameworks including RPO, KTO, and vanilla diffusion policy baselines.

Conclusion: The results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

Abstract: Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>


### [118] [AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception](https://arxiv.org/abs/2602.09617)
*Ruoxuan Feng,Yuxuan Zhou,Siyu Mei,Dongzhan Zhou,Pengwei Wang,Shaowei Cui,Bin Fang,Guocai Yao,Di Hu*

Main category: cs.RO

TL;DR: ToucHD is a large-scale hierarchical tactile dataset with AnyTouch 2 framework for dynamic tactile perception, enabling force-aware manipulation across optical tactile sensors.


<details>
  <summary>Details</summary>
Motivation: Real-world contact-rich manipulation requires temporal tactile feedback, surface deformation sensing, and force dynamics reasoning, but existing tactile datasets/models are limited to object-level attributes and lack fine-grained dynamic interaction data.

Method: Created ToucHD dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data; developed AnyTouch 2 framework that captures pixel-level deformations, action-specific dynamics, and explicitly models physical force dynamics for multi-level perception.

Result: Demonstrated consistent strong performance across sensors and tasks covering static object properties, dynamic physical attributes, and real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities.

Conclusion: The hierarchical dataset ecosystem and unified representation learning framework advance dynamic tactile perception, enabling robots to achieve force-aware dexterous manipulation through comprehensive tactile dynamic understanding.

Abstract: Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.

</details>


### [119] [TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior](https://arxiv.org/abs/2602.09628)
*Jie Li,Bing Tang,Feng Wu,Rongyun Cao*

Main category: cs.RO

TL;DR: TeleGate: A unified whole-body teleoperation framework for humanoid robots using expert policies with a gating network and motion prior module for high-precision real-time tracking without performance degradation.


<details>
  <summary>Details</summary>
Motivation: Existing unified controllers for humanoid robot teleoperation suffer from performance degradation when distilling multiple expert policies into one general policy, especially for dynamic motions. There's a need for a robust framework that maintains expert capabilities while supporting diverse human motions.

Method: TeleGate preserves domain-specific expert policies and trains a lightweight gating network that dynamically activates experts based on proprioceptive states and reference trajectories. It also includes a VAE-based motion prior module to extract implicit future motion intent from historical observations for anticipatory control.

Result: Using only 2.5 hours of motion capture data, TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (running, fall recovery, jumping), significantly outperforming baseline methods in both tracking accuracy and success rate. Successfully deployed on Unitree G1 humanoid robot.

Conclusion: TeleGate provides a unified teleoperation framework that avoids performance degradation from knowledge distillation by dynamically activating expert policies, enabling robust real-time whole-body control for humanoid robots performing complex dynamic motions.

Abstract: Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.

</details>


### [120] [AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild](https://arxiv.org/abs/2602.09657)
*Xiaolou Sun,Wufei Si,Wenhui Ni,Yuntian Li,Dongming Wu,Fei Xie,Runwei Guan,He-Yang Xu,Henghui Ding,Yuan Wu,Yutao Yue,Yongming Huang,Hui Xiong*

Main category: cs.RO

TL;DR: AutoFly: An end-to-end Vision-Language-Action model for autonomous UAV navigation using pseudo-depth encoding and progressive training, with a new dataset shifting from instruction-following to autonomous behavior modeling.


<details>
  <summary>Details</summary>
Motivation: Current VLN research for UAVs relies on detailed pre-specified instructions and predetermined routes, but real-world outdoor exploration occurs in unknown environments where only coarse-grained positional/directional guidance is available, requiring autonomous navigation through continuous planning and obstacle avoidance.

Method: AutoFly incorporates a pseudo-depth encoder to derive depth-aware features from RGB inputs for enhanced spatial reasoning, coupled with a progressive two-stage training strategy that aligns visual, depth, and linguistic representations with action policies. Also constructs a novel autonomous navigation dataset emphasizing continuous obstacle avoidance and autonomous planning.

Result: AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

Conclusion: AutoFly bridges the gap between instruction-following VLN and real-world autonomous UAV navigation by combining pseudo-depth encoding, progressive training, and a new dataset paradigm focused on autonomous behavior modeling rather than explicit instruction-following.

Abstract: Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>


### [121] [RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination](https://arxiv.org/abs/2602.09661)
*Ameer Alhashemi,Layan Abdulhadi,Karam Abuodeh,Tala Baghdadi,Suryanarayana Datla*

Main category: cs.RO

TL;DR: RANT is an ant-inspired multi-robot exploration framework that uses particle filtering, gradient-driven hotspot exploitation, and virtual pheromone coordination to map noisy environments.


<details>
  <summary>Details</summary>
Motivation: To develop an effective multi-robot exploration system for noisy, uncertain environments that can efficiently discover hotspots while minimizing redundant coverage and interference between robots.

Method: Combines particle-filter localization, behavior-based controllers with gradient-driven hotspot exploitation, and lightweight no-revisit coordination using virtual pheromone blocking for a team of differential-drive robots exploring 10x10m terrain.

Result: Particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

Conclusion: RANT successfully demonstrates how ant-inspired coordination mechanisms combined with probabilistic localization can enable effective multi-robot exploration in noisy environments, balancing coverage efficiency with hotspot discovery.

Abstract: This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>


### [122] [Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments](https://arxiv.org/abs/2602.09714)
*Alejandro Gonzalez-Garcia,Sebastiaan Wyns,Sonia De Santis,Jan Swevers,Wilm Decr*

Main category: cs.RO

TL;DR: Fast motion planning framework for non-holonomic robots using deterministic free-space decomposition into overlapping rectangular corridors to reduce search space while maintaining path quality.


<details>
  <summary>Details</summary>
Motivation: Conventional grid-based planners struggle with scalability in complex structured environments, while kinematically-feasible planners impose heavy computational burdens due to search space complexity.

Method: Introduces deterministic free-space decomposition creating a compact graph of overlapping rectangular corridors, then performs online motion planning by finding rectangle sequences and generating near-time-optimal trajectories using an analytical planner.

Result: Highly efficient solution for large-scale navigation validated through extensive simulations and physical robot experiments, with publicly available open-source implementation.

Conclusion: The framework enables fast motion planning for non-holonomic robots in complex structured environments by significantly reducing search space without sacrificing path resolution, providing an efficient solution for large-scale navigation.

Abstract: We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>


### [123] [Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization](https://arxiv.org/abs/2602.09722)
*Ye Wang,Sipeng Zheng,Hao Luo,Wanpeng Zhang,Haoqi Yuan,Chaoyi Xu,Haiweng Xu,Yicheng Feng,Mingyang Yu,Zhiyu Kang,Zongqing Lu,Qin Jin*

Main category: cs.RO

TL;DR: Systematic study challenges assumptions about scaling Vision-Language-Action models for robotics, finding that naive data pooling causes negative transfer, unified action representation is critical, and common regularization methods don't consistently help at scale.


<details>
  <summary>Details</summary>
Motivation: It's unclear whether standard "scale data" approaches work for robotics where training data is inherently heterogeneous across embodiments, sensors, and action spaces. Need systematic understanding of VLA scaling in robotics context.

Method: Controlled study using representative VLA framework with vision-language backbone and flow-matching. Ablated key design decisions under matched conditions with simulation and real-robot experiments. Introduced Grouped Blind Ensemble protocol to reduce experimenter bias.

Result: 1) Unified end-effector-relative action representation critical for cross-embodiment transfer. 2) Naive pooling of heterogeneous robot datasets often causes negative transfer rather than gains. 3) Intuitive strategies like sensory dropout and multi-stage fine-tuning don't consistently improve performance at scale.

Conclusion: Challenges common assumptions about embodied scaling and provides practical guidance for training large-scale VLA policies from diverse robotic data, emphasizing careful design choices over indiscriminate data scaling.

Abstract: While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>


### [124] [NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/abs/2602.09765)
*Xijie Huang,Weiqi Gai,Tianyue Wu,Congyu Wang,Zhiyang Liu,Xin Zhou,Yuze Wu,Fei Gao*

Main category: cs.RO

TL;DR: NavDreamer uses generative video models as a universal interface between language instructions and navigation trajectories, achieving zero-shot generalization through video-based planning and sampling optimization.


<details>
  <summary>Details</summary>
Motivation: Previous Vision-Language-Action models face limitations: scarce and labor-intensive data collection, and static representations that fail to capture temporal dynamics and physical laws in navigation tasks.

Method: Proposes NavDreamer framework using generative video models to connect language instructions with navigation trajectories. Introduces sampling-based optimization with VLM for trajectory scoring/selection, and inverse dynamics model to decode executable waypoints from generated video plans.

Result: Extensive experiments demonstrate robust generalization across novel objects and unseen environments. Navigation's high-level decision-making nature is particularly suited for video-based planning.

Conclusion: Video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation through the proposed NavDreamer framework.

Abstract: Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>


### [125] [Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning](https://arxiv.org/abs/2602.09767)
*Ruopeng Cui,Yifei Bi,Haojie Luo,Wei Li*

Main category: cs.RO

TL;DR: OMoE architecture with multi-discriminator framework enables efficient unsupervised skill discovery for quadruped locomotion, preventing representation collapse and reward hacking while expanding state-space coverage.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning requires expert reward shaping, imitation learning needs costly task-specific data, and existing unsupervised skill discovery methods suffer from low efficiency due to single-policy limitations and reward hacking where rewards increase without actual skill diversity.

Method: Orthogonal Mixture-of-Experts (OMoE) architecture prevents behavior collapse into overlapping representations, plus multi-discriminator framework with different discriminators operating on distinct observation spaces to mitigate reward hacking.

Result: Method demonstrated on 12-DOF Unitree A1 quadruped robot showing diverse locomotion skills, 18.3% expansion in state-space coverage compared to baseline, and improved training efficiency.

Conclusion: The proposed framework addresses key limitations in unsupervised skill discovery by enabling efficient learning of diverse skills while preventing representation collapse and reward hacking, making it promising for complex locomotion tasks.

Abstract: Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>


### [126] [Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics](https://arxiv.org/abs/2602.09772)
*Jonathan Styrud,Matteo Iovino,Rebecca Stower,Mart Kartaev,Mikael Norrlf,Mrten Bjrkman,Christian Smith*

Main category: cs.RO

TL;DR: BETR-GUI combines AI-assisted BT programming with drag-and-drop GUI, enabling faster robot programming by non-experts through multiple AI methods integration.


<details>
  <summary>Details</summary>
Motivation: Need for faster reactive robot programming without requiring extensively trained programmers, and lack of exploration in combining automated BT generation methods with user-friendly GUIs for human validation and editing.

Method: Developed BETR-GUI that combines large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor to create Behavior Trees with AI assistance.

Result: User study with 60 participants shows users perform better at solving robot programming tasks with combined assistive methods, and humans using full BETR-GUI outperform AI assistant alone.

Conclusion: Combining multiple AI-assisted methods with intuitive GUI enables more effective robot programming by humans, demonstrating human-AI collaboration superiority over AI-only approaches.

Abstract: The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.

</details>


### [127] [BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation](https://arxiv.org/abs/2602.09849)
*Yucheng Hu,Jianke Zhang,Yuanfei Luo,Yanjiang Guo,Xiaoyu Chen,Xinshu Sun,Kun Feng,Qingzhou Lu,Sheng Chen,Yangang Zhang,Wei Li,Jianyu Chen*

Main category: cs.RO

TL;DR: BagelVLA is a unified VLA model that integrates linguistic planning, visual forecasting, and action generation in one framework, using Residual Flow Guidance to efficiently couple modalities and improve long-horizon manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models focus on either linguistic planning or visual forecasting in isolation, leading to suboptimal performance in complex, long-horizon manipulation tasks that require simultaneous reasoning about tasks, foreseeing physical outcomes, and generating precise actions.

Method: BagelVLA integrates linguistic planning, visual forecasting, and action generation within a single framework initialized from a pretrained unified model. It uses Residual Flow Guidance (RFG) which initializes from current observation and leverages single-step denoising to extract predictive visual features for guiding action generation with minimal latency.

Result: BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly excelling in tasks requiring multi-stage reasoning.

Conclusion: The unified integration of linguistic planning, visual forecasting, and action generation within a single framework, enabled by efficient modality coupling through RFG, significantly improves performance in complex manipulation tasks requiring reasoning and prediction.

Abstract: Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>


### [128] [TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback](https://arxiv.org/abs/2602.09888)
*Zihao Li,Yanan Zhou,Ranpeng Qiu,Hangyu Wu,Guoqiang Ren,Weiming Zhi*

Main category: cs.RO

TL;DR: TriPilot-FF is a whole-body teleoperation system for bimanual mobile manipulators that uses foot-operated pedals with lidar-driven haptic feedback and bimanual leader-follower control for upper body.


<details>
  <summary>Details</summary>
Motivation: Whole-body teleoperation of mobile manipulators is challenging because operators must coordinate wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are hand-centric and don't effectively use foot-operated channels for continuous base control.

Method: Developed TriPilot-FF system with: 1) foot-operated pedal with lidar-driven haptic feedback that provides resistive cues from proximity-to-obstacle signals, 2) upper-body bimanual leader-follower teleoperation, 3) arm-side force reflection for contact awareness, and 4) real-time force and visual guidance of bimanual manipulability to prompt base repositioning.

Result: System effectively "co-pilots" human operators over long time-horizons and tasks requiring precise mobile base movement and coordination. Also demonstrated improved performance when teleoperation feedback signals are incorporated into an Action Chunking with Transformers (ACT) policy.

Conclusion: TriPilot-FF provides an effective whole-body teleoperation solution for mobile manipulators using innovative foot-operated control with haptic feedback, enabling better coordination and collision-averse behavior without explicit collision-avoidance controllers.

Abstract: Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>


### [129] [TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data](https://arxiv.org/abs/2602.09893)
*Zhengxue Cheng,Yan Zhao,Keyu Wang,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: TaCo is the first comprehensive benchmark for tactile data compression, evaluating 30 methods across 5 datasets and 4 tasks, with novel data-driven codecs TaCo-LL and TaCo-L showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Tactile sensing is crucial for embodied intelligence but efficient tactile data compression remains underexplored despite being essential for real-time robotic applications under bandwidth constraints. The heterogeneity and spatiotemporal complexity of tactile data further complicate compression challenges.

Method: Introduced TaCo benchmark evaluating 30 compression methods (off-the-shelf algorithms and neural codecs) across 5 diverse datasets from various sensor types. Systematically assessed lossless and lossy compression on four tasks: lossless storage, human visualization, material/object classification, and dexterous robotic grasping. Developed novel data-driven codecs TaCo-LL (lossless) and TaCo-L (lossy) explicitly trained on tactile data.

Result: Results validated superior performance of TaCo-LL and TaCo-L codecs. The benchmark provides foundational framework for understanding trade-offs between compression efficiency and task performance.

Conclusion: TaCo establishes the first comprehensive benchmark for tactile data compression, paving the way for future advances in tactile perception by providing systematic evaluation framework and demonstrating effectiveness of data-driven tactile codecs.

Abstract: Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>


### [130] [Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation](https://arxiv.org/abs/2602.09940)
*Archit Sharma,Dharmendra Sharma,John Rebeiro,Peeyush Thakur,Narendra Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: Lightweight on-device pipeline converts natural language instructions to robot manipulation actions using Instruct2Act for parsing and RAN with DATRN for trajectory generation, achieving 90% success on real-robot tasks.


<details>
  <summary>Details</summary>
Motivation: Robots struggle with free-form human instructions due to computational/sensing limitations; need lightweight on-device solution for real-world manipulation without cloud services.

Method: Two-stage pipeline: (1) Instruct2Act (BiLSTM with multi-head-attention autoencoder) parses instructions into atomic action sequences; (2) Robot Action Network (RAN) uses DATRN with YOLOv8 vision analyzer to generate precise control trajectories.

Result: 91.5% sub-action prediction accuracy on custom dataset; 90% success rate on real-robot tasks (pick-place, pick-pour, wipe, pick-give); sub-action inference <3.8s; end-to-end execution 30-60s.

Conclusion: Fine-grained instruction parsing with DATRN-based trajectory generation and vision grounding enables practical deterministic real-time manipulation in resource-constrained single-camera settings.

Abstract: Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>


### [131] [Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning](https://arxiv.org/abs/2602.09972)
*Zixuan Wang,Huang Fang,Shaoan Wang,Yuanfei Luo,Heng Dong,Wei Li,Yiming Gan*

Main category: cs.RO

TL;DR: Hydra-Nav introduces a dual-system VLM architecture for object navigation that adaptively switches between slow deliberative reasoning and fast reactive execution, achieving state-of-the-art performance with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Current VLM-based object navigation methods suffer from low success rates and inefficient localization due to weak temporal-spatial reasoning, while reasoning-enhanced approaches incur substantial computational overhead.

Method: Hydra-Nav uses a unified VLM architecture with adaptive switching between deliberative slow system (for analyzing history and planning) and reactive fast system (for execution). Training involves three-stage curriculum: spatial-action alignment, memory-reasoning integration, and iterative rejection fine-tuning.

Result: Achieves SOTA on HM3D, MP3D, and OVON benchmarks, outperforming second-best methods by 11.1%, 17.4%, and 21.2% respectively. Introduces SOT metric showing adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

Conclusion: Hydra-Nav effectively addresses both effectiveness and efficiency challenges in object navigation through adaptive reasoning, demonstrating superior performance and introducing a new efficiency metric for VLM-based navigation systems.

Abstract: While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>


### [132] [RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation](https://arxiv.org/abs/2602.09973)
*Hao Li,Ziqin Wang,Zi-han Ding,Shuai Yang,Yilun Chen,Yang Tian,Xiaolin Hu,Tai Wang,Dahua Lin,Feng Zhao,Si Liu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: RoboInter is a comprehensive suite (data, benchmarks, models) for robotic manipulation with dense intermediate representation annotations, enabling better vision-language-action models through a plan-then-execute paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing manipulation datasets are costly, embodiment-specific, and lack diversity, hindering VLA model generalization. Current plan-then-execute approaches need intermediate supervision that's missing from existing datasets.

Method: Created RoboInter-Tool (GUI for semi-automatic annotation) and RoboInter-Data (230k+ episodes across 571 scenes with dense per-frame annotations for 10+ intermediate representation categories). Introduced RoboInter-VQA for embodied reasoning benchmarking and RoboInter-VLA for plan-then-execute frameworks.

Result: Large-scale dataset with 230k+ episodes and dense annotations substantially exceeding prior work in scale and quality. Provides systematic benchmarks for embodied reasoning and integrated VLA frameworks with intermediate supervision.

Conclusion: RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning through fine-grained, diverse intermediate representations, bridging the gap between high-level planning and low-level execution.

Abstract: Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>


### [133] [Acoustic Drone Package Delivery Detection](https://arxiv.org/abs/2602.09991)
*Franois Marcoux,Franois Grondin*

Main category: cs.RO

TL;DR: First acoustic-based algorithm detects drone package deliveries in restricted areas using microphone arrays, achieving 96% detection accuracy with 8% false positive rate.


<details>
  <summary>Details</summary>
Motivation: Illicit UAV deliveries in restricted areas like prisons pose security challenges, but existing research focuses on drone detection/localization rather than identifying actual delivery events.

Method: Uses ground-based microphone array to analyze acoustic features. Deep neural network detects drone presence and estimates propeller speed (blade passing frequency) from mel spectrograms. Algorithm analyzes BPF changes to identify delivery moments based on sudden frequency shifts.

Result: BPF estimator has 16 Hz mean absolute error within 150m range. Drone presence detection: 97% accuracy. Delivery detection: 96% correct identification with 8% false positive rate. Effective range: up to 100 meters.

Conclusion: Acoustic signals can effectively identify drone delivery events in restricted areas, providing a novel solution to security challenges posed by illicit UAV operations.

Abstract: In recent years, the illicit use of unmanned aerial vehicles (UAVs) for deliveries in restricted area such as prisons became a significant security challenge. While numerous studies have focused on UAV detection or localization, little attention has been given to delivery events identification. This study presents the first acoustic package delivery detection algorithm using a ground-based microphone array. The proposed method estimates both the drone's propeller speed and the delivery event using solely acoustic features. A deep neural network detects the presence of a drone and estimates the propeller's rotation speed or blade passing frequency (BPF) from a mel spectrogram. The algorithm analyzes the BPFs to identify probable delivery moments based on sudden changes before and after a specific time. Results demonstrate a mean absolute error of the blade passing frequency estimator of 16 Hz when the drone is less than 150 meters away from the microphone array. The drone presence detection estimator has a accuracy of 97%. The delivery detection algorithm correctly identifies 96% of events with a false positive rate of 8%. This study shows that deliveries can be identified using acoustic signals up to a range of 100 meters.

</details>


### [134] [A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging](https://arxiv.org/abs/2602.10007)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.RO

TL;DR: Proposes MARL-MASS, a lane change controller combining Multi-Agent Reinforcement Learning with a Safety Shield to balance safety and efficiency in dense traffic.


<details>
  <summary>Details</summary>
Motivation: Existing lane change controllers for CAVs either ensure safety OR improve traffic efficiency, but don't handle these conflicting objectives together in dense traffic scenarios.

Method: Develops Multi-Agent Safety Shield (MASS) using Control Barrier Functions to enable safe collaborative lane changes, then integrates it with MARL controller using custom reward function to prioritize efficiency.

Result: MARL-MASS enables collaborative lane changes with safety guarantees while improving traffic efficiency in congested on-ramp merging simulations. Custom reward function stabilizes MARL policies with safety shield.

Conclusion: MARL-MASS effectively balances safety-efficiency trade-off by encouraging exploration of collaborative policies while respecting safety constraints in congested traffic.

Abstract: Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

</details>


### [135] [Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper](https://arxiv.org/abs/2602.10013)
*Xuhui Kang,Tongxuan Tian,Sung-Wook Lee,Binghao Huang,Yunzhu Li,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: TF-Gripper: low-cost force-controlled gripper with tactile sensing for precise force regulation in manipulation tasks, paired with RETAF framework for reactive force adaptation.


<details>
  <summary>Details</summary>
Motivation: Robots need precise force regulation for manipulating delicate everyday objects (like potato chips) to prevent damage, but commercial grippers are either too expensive or have high minimum force, making them unsuitable for studying force-controlled policy learning.

Method: 1) Developed TF-Gripper: low-cost (~$150) force-controlled parallel-jaw gripper with tactile sensing feedback (0.45-45N range). 2) Created teleoperation device to record human grasping forces. 3) Proposed RETAF framework: decouples force control from arm pose prediction, using high-frequency wrist images and tactile feedback for force regulation while base policy handles pose and gripper actions.

Result: Direct force control significantly improves grasp stability and task performance over position control. Tactile feedback is essential for force regulation. RETAF consistently outperforms baselines across five real-world tasks requiring precise force regulation and integrates well with various base policies.

Conclusion: The TF-Gripper and RETAF framework enable robots to achieve precise force regulation like humans, opening a path for scaling force-controlled policy learning in robotic manipulation of delicate everyday objects.

Abstract: Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .

</details>


### [136] [RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments](https://arxiv.org/abs/2602.10015)
*Dharmendra Sharma,Archit Sharma,John Reberio,Vaibhav Kesharwani,Peeyush Thakur,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: RoboSubtaskNet: A multi-stage framework for fine-grained sub-task segmentation in collaborative manipulation videos, using attention-enhanced I3D features with modified MS-TCN and Fibonacci dilation, trained with composite loss, achieving state-of-the-art performance and validated on physical robot.


<details>
  <summary>Details</summary>
Motivation: Need for temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos for safe human-robot collaboration, requiring robot-executable sub-task labels rather than generic activity recognition.

Method: Multi-stage framework coupling attention-enhanced I3D features (RGB + optical flow) with modified MS-TCN using Fibonacci dilation schedule to capture short-horizon transitions; trained with composite objective (cross-entropy + temporal regularizers including truncated MSE and transition-aware term).

Result: Outperforms MS-TCN and MS-TCN++ on GTEA and RoboSubtask benchmarks: F1@50=79.5%, Edit=88.6%, Acc=78.9% on GTEA; F1@50=94.2%, Edit=95.6%, Acc=92.2% on RoboSubtask; competitive on Breakfast benchmark. Physical validation on 7-DoF Kinova Gen3 achieved 91.25% task success rate.

Conclusion: Demonstrates practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings, closing gap between vision benchmarks and robot control through deterministic mapping to manipulator primitives.

Abstract: Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>


### [137] [A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation](https://arxiv.org/abs/2602.10035)
*Marc-Philip Ecker,Christoph Frhlich,Johannes Huemer,David Gruber,Bernhard Bischof,Tobias Glck,Wolfgang Kemmetmller*

Main category: cs.RO

TL;DR: First unified MPC for forestry cranes that simultaneously handles collision avoidance and payload sway damping using real-time LiDAR mapping and Euclidean distance fields.


<details>
  <summary>Details</summary>
Motivation: Forestry cranes operate in dynamic outdoor environments where both collision avoidance and payload sway control are critical for safety, but existing approaches address these challenges separately rather than in an integrated framework.

Method: Model predictive controller (MPC) that integrates LiDAR-based environment mapping using online Euclidean distance fields (EDF), enabling real-time environmental adaptation while simultaneously enforcing collision constraints and damping payload sway.

Result: Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance, with capabilities for replanning under environmental changes, maintaining collision-free operation under disturbances, and safe stopping when no bypass exists.

Conclusion: The proposed unified MPC framework successfully addresses both collision avoidance and sway damping in forestry crane operations, representing the first integrated solution for these critical safety challenges in unstructured outdoor environments.

Abstract: Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>


### [138] [Humanoid Factors: Design Principles for AI Humanoids in Human Worlds](https://arxiv.org/abs/2602.10069)
*Xinyuan Liu,Eren Sadikoglu,Ransalu Senanayake,Lixiao Huang*

Main category: cs.RO

TL;DR: The paper introduces "humanoid factors" as a framework for designing humanoid robots to coexist with humans, addressing physical, cognitive, social, and ethical dimensions beyond traditional human factors.


<details>
  <summary>Details</summary>
Motivation: As humanoid robots enter human environments, traditional human factors research is insufficient because it only considers human performance. Humanoids introduce new challenges with human-like behavior expectations, communication needs, and social presence that affect usability, trust, and safety.

Method: The authors propose a "humanoid factors" framework structured around four pillars: physical, cognitive, social, and ethical dimensions. They apply this framework to evaluate a real-world humanoid control algorithm to demonstrate its practical utility.

Result: The framework reveals that conventional robotics metrics overlook key human cognitive and interaction principles. It provides a structured way to characterize the overlap and divergence between human capabilities and AI-powered humanoids.

Conclusion: Humanoid factors serves as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence, addressing the unique challenges posed by humanoids in shared environments.

Abstract: Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.

</details>


### [139] [UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking](https://arxiv.org/abs/2602.10093)
*Baijun Chen,Weijie Wan,Tianxing Chen,Xianda Guo,Congsheng Xu,Yuanyang Qi,Haojie Zhang,Longyan Wu,Tianling Xu,Zixuan Li,Yizhe Wu,Rui Li,Xiaokang Yang,Ping Luo,Wei Sui,Yao Mu*

Main category: cs.RO

TL;DR: UniVTAC is a simulation platform for generating large-scale visuo-tactile data, with an encoder trained on synthetic data and a benchmark for evaluating tactile-driven manipulation policies.


<details>
  <summary>Details</summary>
Motivation: Vision-only policies struggle with contact-rich manipulation tasks like insertion, while real-world tactile data collection is expensive and lacks unified evaluation platforms.

Method: Developed UniVTAC simulation platform for scalable visuo-tactile data synthesis, trained UniVTAC Encoder on synthetic data with designed supervisory signals, and created UniVTAC Benchmark with eight manipulation tasks.

Result: UniVTAC Encoder improves average success rates by 17.1% on the benchmark, with real-world experiments showing 25% improvement in task success.

Conclusion: UniVTAC provides an effective simulation-based solution for visuo-tactile perception in manipulation, enabling scalable data generation and improved policy performance for contact-rich tasks.

Abstract: Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>


### [140] [VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model](https://arxiv.org/abs/2602.10098)
*Jingwen Sun,Wenyao Zhang,Zekun Qi,Shaojie Ren,Zezhi Liu,Hanxin Zhu,Guangzhong Sun,Xin Jin,Zhibo Chen*

Main category: cs.RO

TL;DR: VLA-JEPA is a new pretraining framework for vision-language-action policies that uses leakage-free state prediction to learn robust dynamics abstractions, avoiding appearance bias and information leakage common in current methods.


<details>
  <summary>Details</summary>
Motivation: Current VLA pretraining methods suffer from appearance bias, nuisance motion, and information leakage because they anchor to pixel variation rather than action-relevant state transitions.

Method: Uses JEPA-style pretraining with leakage-free state prediction: target encoder produces latent representations from future frames, student sees only current observation, predicting in latent space rather than pixel space.

Result: Achieves consistent gains in generalization and robustness over existing methods on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks.

Conclusion: VLA-JEPA provides a simple two-stage recipe (JEPA pretraining + action-head fine-tuning) that learns robust dynamics abstractions without multi-stage complexity, improving generalization and robustness in VLA policies.

Abstract: Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>


### [141] [Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2602.10101)
*Sizhe Yang,Linning Xu,Hao Li,Juncheng Mu,Jia Zeng,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Robo3R is a real-time feed-forward 3D reconstruction model that predicts accurate, metric-scale scene geometry from RGB images and robot states for robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D perception methods for robotics have limitations: depth sensors are noisy and material-sensitive, while current reconstruction models lack the precision and metric consistency needed for physical interaction in manipulation tasks.

Method: Robo3R jointly infers scale-invariant local geometry and relative camera poses, unified via learned global similarity transformation. It uses a masked point head for fine-grained point clouds and keypoint-based PnP formulation for camera extrinsics refinement. Trained on Robo3R-4M, a 4M-frame synthetic dataset.

Result: Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. It shows performance gains across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning.

Conclusion: Robo3R demonstrates promise as an alternative 3D sensing module for robotic manipulation, providing accurate, metric-scale geometry that enables better performance in various manipulation tasks compared to existing methods.

Abstract: 3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>


### [142] [DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos](https://arxiv.org/abs/2602.10105)
*Juncheng Mu,Sizhe Yang,Yiming Bao,Hojin Bae,Tianming Wei,Linning Xu,Boyi Li,Huazhe Xu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: DexImit is an automated framework that converts monocular human manipulation videos into physically plausible robot data to address data scarcity in bimanual dexterous manipulation.


<details>
  <summary>Details</summary>
Motivation: Data scarcity limits generalization of bimanual dexterous manipulation due to expensive real-world data collection. Human manipulation videos offer potential for scaling robot learning but face embodiment gap challenges between human and robotic hands.

Method: Four-stage pipeline: 1) reconstruct hand-object interactions from arbitrary viewpoints with near-metric scale, 2) subtask decomposition and bimanual scheduling, 3) synthesize robot trajectories consistent with demonstrated interactions, 4) comprehensive data augmentation for zero-shot real-world deployment.

Result: DexImit can generate large-scale robot data from human videos (Internet or generated), handling diverse manipulation tasks including tool use, long-horizon tasks, and fine-grained manipulations.

Conclusion: DexImit bridges the embodiment gap between human and robotic hands, enabling scalable robot learning from human manipulation videos without additional information requirements.

Abstract: Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>


### [143] [EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration](https://arxiv.org/abs/2602.10106)
*Modi Shi,Shijia Peng,Jin Chen,Haoran Jiang,Yinghui Li,Di Huang,Ping Luo,Hongyang Li,Li Chen*

Main category: cs.RO

TL;DR: EgoHumanoid is the first framework that uses egocentric human demonstrations combined with limited robot data to train humanoid loco-manipulation policies, achieving 51% better performance than robot-only baselines.


<details>
  <summary>Details</summary>
Motivation: Human demonstrations offer rich environmental diversity and scale naturally, but their potential for challenging humanoid loco-manipulation remains unexplored. Current approaches rely heavily on robot teleoperation which is limited in scale and diversity.

Method: Developed a systematic alignment pipeline with view alignment (reducing visual domain discrepancies) and action alignment (mapping human motions to kinematically feasible humanoid actions). Created portable system for scalable human data collection with practical transfer protocols.

Result: Incorporating robot-free egocentric human data significantly outperforms robot-only baselines by 51%, particularly in unseen environments. The framework enables humanoids to perform loco-manipulation across diverse real-world environments.

Conclusion: EgoHumanoid demonstrates that co-training with abundant human demonstrations and limited robot data enables effective humanoid loco-manipulation, with analysis revealing which behaviors transfer effectively and the potential for scaling human data.

Abstract: Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>


### [144] [ST4VLA: Spatially Guided Training for Vision-Language-Action Models](https://arxiv.org/abs/2602.10109)
*Jinhui Ye,Fangjing Wang,Ning Gao,Junqiu Yu,Yangkun Zhu,Bin Wang,Jinyu Zhang,Weiyang Jin,Yanwei Fu,Feng Zheng,Yilun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: ST4VLA is a dual-system Vision-Language-Action framework that uses Spatial Guided Training to align action learning with spatial priors in VLMs, achieving significant performance improvements on robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models excel at multimodal understanding but struggle with embodied tasks requiring transformation of instructions into low-level motor actions. There's a gap between high-level understanding and actionable robotic control.

Method: Two-stage approach: (1) spatial grounding pre-training with scalable point, box, and trajectory prediction from web-scale and robot-specific data, (2) spatially guided action post-training using spatial prompting to produce richer spatial priors that guide action generation.

Result: Substantial improvements over vanilla VLA: 66.1  84.6 on Google Robot, 54.7  73.2 on WidowX Robot, establishing new SOTA on SimplerEnv. Shows stronger generalization to unseen objects/paraphrased instructions and robustness to long-horizon perturbations.

Conclusion: Scalable spatially guided training is a promising direction for robust, generalizable robot learning, effectively bridging the gap between vision-language understanding and embodied action generation.

Abstract: Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>


### [145] [Learning Agile Quadrotor Flight in the Real World](https://arxiv.org/abs/2602.10111)
*Yunfan Ren,Zhiyuan Zhu,Jiaxu Xing,Davide Scaramuzza*

Main category: cs.RO

TL;DR: Self-adaptive framework enables quadrotors to safely explore physical limits and improve agility through online adaptation without precise system identification.


<details>
  <summary>Details</summary>
Motivation: Learning-based quadrotor controllers rely on massive simulation training and accurate system identification, but fixed policies are vulnerable to out-of-distribution scenarios and must operate conservatively, limiting agility.

Method: Proposes Adaptive Temporal Scaling (ATS) to actively explore platform limits, online residual learning to augment a simple nominal model, and Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) for efficient in-flight policy updates.

Result: Quadrotor reliably executes agile maneuvers near actuator saturation limits, evolving from 1.9 m/s to 7.3 m/s peak speed within ~100 seconds of flight time.

Conclusion: Real-world adaptation serves not just to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

Abstract: Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

</details>


### [146] [Decoupled MPPI-Based Multi-Arm Motion Planning](https://arxiv.org/abs/2602.10114)
*Dan Evron,Elias Goldsztejn,Ronen I. Brafman*

Main category: cs.RO

TL;DR: MR-STORM extends STORM algorithm for distributed multi-robot motion planning with dynamic obstacles and priority schemes


<details>
  <summary>Details</summary>
Motivation: Current GPU-accelerated sampling-based motion planning algorithms for high DOF arms scale poorly when controlling multiple robots jointly, requiring a distributed approach

Method: Extends STORM (sampling-based MPC) to handle multiple robots: 1) adds dynamic obstacle handling, 2) each robot computes and shares its motion plan prefix as dynamic obstacle for others, 3) implements dynamic priority scheme

Result: MR-STORM demonstrates clear empirical advantages over state-of-the-art algorithms when operating with both static and dynamic obstacles

Conclusion: The distributed MR-STORM algorithm effectively addresses scalability issues in multi-robot motion planning while maintaining performance advantages over existing approaches

Abstract: Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [147] [A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation](https://arxiv.org/abs/2602.09112)
*Russ Webb,Jason Ramapuram*

Main category: cs.AI

TL;DR: Cadmus is a low-cost system for studying program synthesis using small transformer models trained on true programs, enabling transparent research into program completion, OOD reasoning, and inductive reasoning without LLM confounding factors.


<details>
  <summary>Details</summary>
Motivation: LLM-based program synthesis research faces issues with distribution understanding, fine-tuning effects, tokenization impacts, and high computational costs. Researchers need affordable, transparent systems where they can control training distributions and inspect models.

Method: Cadmus system includes: 1) integer virtual machine, 2) dataset of diverse true programs, 3) autoregressive transformer model trained for under $200. Enables fine-grained control over training distribution and model instrumentation.

Result: Cadmus models outperform GPT-5 (100% vs 95% accuracy) on integer arithmetic program completion in domain-specific language. System provides transparency into dataset-task relationship, unlike GPT-5 which brings unknown priors confounding research.

Conclusion: Small models like Cadmus enable affordable, transparent program synthesis research with full control over training distributions, avoiding LLM confounding factors while maintaining competitive performance on complex reasoning tasks.

Abstract: What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\% accuracy while GPT-5 has 95\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.

</details>


### [148] [Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization](https://arxiv.org/abs/2602.09121)
*Rmi Grzeczkowicz,Eric Soriano,Ali Janati,Miyu Zhang,Gerard Comas-Quiles,Victor Carballo Araruna,Aneesh Jonelagadda*

Main category: cs.AI

TL;DR: Lightweight, privacy-preserving multimodal emotion recognition framework for edge devices using speech, text, and facial imagery with uncertainty-aware fusion based on Dempster-Shafer theory.


<details>
  <summary>Details</summary>
Motivation: Need for efficient, privacy-preserving emotion recognition systems deployable on edge devices that can handle uncertainty across modalities and ambiguous/missing inputs for real-world applications.

Method: Modular framework with dedicated backbones (Emotion2Vec for speech, ResNet for facial, DistilRoBERTa for text) and model-agnostic fusion using Dempster-Shafer theory and Dirichlet evidence operating on model logits without additional training.

Result: Competitive accuracy on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS, CREMA-D) while maintaining computational efficiency and robustness to ambiguous/missing inputs.

Conclusion: Proposed framework enables uncertainty-aware multimodal systems for healthcare, human-computer interaction, and emotion-informed applications with emphasis on modularity, scalability, and real-world feasibility.

Abstract: In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks. Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence. Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs. Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.

</details>


### [149] [PABU: Progress-Aware Belief Update for Efficient LLM Agents](https://arxiv.org/abs/2602.09138)
*Haitao Jiang,Lin Ge,Hengrui Cai,Rui Song*

Main category: cs.AI

TL;DR: PABU is a belief-state framework for LLM agents that models task progress and selectively retains relevant actions/observations, improving efficiency and performance over full-history approaches.


<details>
  <summary>Details</summary>
Motivation: LLM agents typically condition actions on full action-observation histories, which introduces task-irrelevant information leading to redundant actions and higher inference costs.

Method: Progress-Aware Belief Update (PABU) compactly represents agent state by explicitly modeling task progress and selectively retaining past actions/observations. At each step, the agent predicts relative progress and decides whether new interactions should be stored, conditioning future decisions only on retained subsets.

Result: Across eight environments in AgentGym benchmark, PABU achieves 81.0% task completion rate (outperforming previous SoTA by 23.9%) and reduces average interaction steps to 9.5 (26.9% reduction). Ablation studies confirm both progress prediction and selective retention are necessary.

Conclusion: PABU demonstrates that explicit progress modeling and selective retention of relevant information significantly improve LLM agent efficiency and performance compared to full-history belief approaches.

Abstract: Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.

</details>


### [150] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa is a decentralized LLM-agent framework for oncology decision support that uses game-theoretic objectives and deterministic embedding projections for contribution-aware credit assignment, improving accuracy and stability over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent frameworks for oncology decision support often rely on stochastic narrative-based reasoning, which lacks explicit evidence attribution and stability. There's a need for more interpretable, mathematically grounded decision pathways that can handle dynamic, heterogeneous patient data in oncology.

Method: CoMMa uses a decentralized LLM-agent framework where specialists operate on partitioned evidence and coordinate through game-theoretic objectives. It employs deterministic embedding projections to approximate contribution-aware credit assignment, estimating each agent's marginal utility for explicit evidence attribution.

Result: Evaluated on diverse oncology benchmarks including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agent baselines.

Conclusion: CoMMa provides a robust framework for oncology decision support that offers interpretable, mathematically grounded decision pathways with improved stability and accuracy through contribution-aware credit assignment and game-theoretic coordination.

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [151] [FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases](https://arxiv.org/abs/2602.09163)
*Xingjian Zhang,Sophia Moylan,Ziyang Xiong,Qiaozhu Mei,Yichen Luo,Jiaqi W. Ma*

Main category: cs.AI

TL;DR: FlyBench is a benchmark for evaluating AI agents on end-to-end scientific knowledge base curation from literature, requiring agents to search 16,898 papers and produce structured annotations for 100 genes based on FlyBase expert annotations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on isolated subtasks like NER or relation extraction, but don't capture the complete workflow of scientific knowledge base curation where experts search papers, reconcile evidence across documents, and produce ontology-grounded annotations.

Method: Created FlyBench with 7,397 expert-curated annotations across 100 genes from FlyBase. Agents must search a corpus of 16,898 full-text papers given only a gene symbol and produce structured annotations including Gene Ontology terms, expression patterns, and historical synonyms. Evaluated four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent.

Result: Architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives. Scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Agents primarily use retrieval to confirm parametric knowledge rather than discover new information.

Conclusion: FlyBench provides a comprehensive benchmark for evaluating AI agents on end-to-end scientific knowledge base curation, revealing important insights about agent design and retrieval behavior that can guide future development in retrieval-augmented scientific reasoning.

Abstract: Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.

</details>


### [152] [Human Control Is the Anchor, Not the Answer: Early Divergence of Oversight in Agentic AI Communities](https://arxiv.org/abs/2602.09286)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: Two Reddit communities (r/OpenClaw for deployment/operations and r/Moltbook for social interaction) show different interpretations of "human control" for AI oversight during early adoption phase.


<details>
  <summary>Details</summary>
Motivation: To understand how different socio-technical roles shape oversight expectations for agentic AI, moving beyond the single concept of "human control" to recognize role-specific interpretations.

Method: Comparative analysis of two Reddit communities using topic modeling, oversight-theme abstraction, engagement-weighted salience, and divergence tests (JSD, cosine similarity, permutation tests) during Jan-Feb 2026.

Result: Communities are strongly separable (JSD=0.418, cosine=0.372, p=0.0005). r/OpenClaw emphasizes execution guardrails and recovery (action-risk), while r/Moltbook focuses on identity, legitimacy, and accountability (meaning-risk).

Conclusion: Oversight mechanisms should be designed to match agent roles rather than using one-size-fits-all control policies, recognizing the distinction between action-risk and meaning-risk approaches.

Abstract: Oversight for agentic AI is often discussed as a single goal ("human control"), yet early adoption may produce role-specific expectations. We present a comparative analysis of two newly active Reddit communities in Jan--Feb 2026 that reflect different socio-technical roles: r/OpenClaw (deployment and operations) and r/Moltbook (agent-centered social interaction). We conceptualize this period as an early-stage crystallization phase, where oversight expectations form before norms reach equilibrium.
  Using topic modeling in a shared comparison space, a coarse-grained oversight-theme abstraction, engagement-weighted salience, and divergence tests, we show the communities are strongly separable (JSD =0.418, cosine =0.372, permutation $p=0.0005$). Across both communities, "human control" is an anchor term, but its operational meaning diverges: r/OpenClaw} emphasizes execution guardrails and recovery (action-risk), while r/Moltbook} emphasizes identity, legitimacy, and accountability in public interaction (meaning-risk). The resulting distinction offers a portable lens for designing and evaluating oversight mechanisms that match agent role, rather than applying one-size-fits-all control policies.

</details>


### [153] [Measuring Dataset Diversity from a Geometric Perspective](https://arxiv.org/abs/2602.09340)
*Yang Ba,Mohammad Sadeq Abolhasani,Michelle V Mancenido,Rong Pan*

Main category: cs.AI

TL;DR: PLDiv: A topological data analysis framework using persistence landscapes to measure dataset diversity by capturing geometric structure beyond traditional entropy-based metrics.


<details>
  <summary>Details</summary>
Motivation: Existing diversity metrics focus primarily on statistical variation and entropy but largely ignore the geometric structure of datasets, creating a gap in capturing meaningful diversity.

Method: Introduces a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data, providing theoretically grounded diversity measurement.

Result: PLDiv is shown to be powerful, reliable, and interpretable across diverse modalities, directly linking data diversity to underlying geometry.

Conclusion: The approach offers a foundational tool for dataset construction, augmentation, and evaluation by capturing rich geometric and structural properties beyond entropy-based diversity metrics.

Abstract: Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.

</details>


### [154] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: AgentAuditor replaces majority voting in multi-agent systems with path search over Reasoning Trees to resolve conflicts at divergence points, plus ACPO training to reward evidence-based minority selections over popular errors.


<details>
  <summary>Details</summary>
Motivation: Majority voting in multi-agent systems discards evidential structure of reasoning traces and is brittle under confabulation consensus where agents share correlated biases and converge on incorrect rationales.

Method: AgentAuditor uses path search over a Reasoning Tree that explicitly represents agreements/divergences among agent traces, resolving conflicts by comparing reasoning branches at critical divergence points. Plus Anti-Consensus Preference Optimization (ACPO) trains adjudicator on majority-failure cases to reward evidence-based minority selections.

Result: Across 5 popular settings, AgentAuditor yields up to 5% absolute accuracy improvement over majority vote, and up to 3% over using LLM-as-Judge.

Conclusion: AgentAuditor provides an effective alternative to majority voting in multi-agent systems by leveraging reasoning structure and training against consensus errors, with significant accuracy improvements across diverse settings.

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [155] [Not-in-Perspective: Towards Shielding Google's Perspective API Against Adversarial Negation Attacks](https://arxiv.org/abs/2602.09343)
*Michail S. Alexiou,J. Sukarno Mertoguno*

Main category: cs.AI

TL;DR: Formal reasoning wrapper improves ML toxicity detection systems against negation-based adversarial attacks


<details>
  <summary>Details</summary>
Motivation: Cyberbullying and toxic comments on social media require better moderation. Existing ML-based toxicity detection systems are vulnerable to adversarial attacks using logical modifications like negation.

Method: Develop formal reasoning-based methodologies that wrap around existing ML toxicity detection systems as pre-processing and post-processing steps to handle negation attacks.

Result: The formal reasoning wrapper significantly improves accuracy and efficacy of toxicity scoring against negation adversarial datasets across multiple ML models.

Conclusion: Hybrid approaches combining formal reasoning with machine learning outperform purely statistical solutions for toxicity detection, especially against logical adversarial attacks.

Abstract: The rise of cyberbullying in social media platforms involving toxic comments has escalated the need for effective ways to monitor and moderate online interactions. Existing solutions of automated toxicity detection systems, are based on a machine or deep learning algorithms. However, statistics-based solutions are generally prone to adversarial attacks that contain logic based modifications such as negation in phrases and sentences. In that regard, we present a set of formal reasoning-based methodologies that wrap around existing machine learning toxicity detection systems. Acting as both pre-processing and post-processing steps, our formal reasoning wrapper helps alleviating the negation attack problems and significantly improves the accuracy and efficacy of toxicity scoring. We evaluate different variations of our wrapper on multiple machine learning models against a negation adversarial dataset. Experimental results highlight the improvement of hybrid (formal reasoning and machine-learning) methods against various purely statistical solutions.

</details>


### [156] [Image Quality in the Era of Artificial Intelligence](https://arxiv.org/abs/2602.09347)
*Jana G. Delfino,Jason L. Granstedt,Frank W. Samuelson,Robert Ochs,Krishna Juluru*

Main category: cs.AI

TL;DR: AI in radiology improves image quality and speed but introduces new risks and disconnects between perceived quality and actual information content, requiring awareness of limitations for safe use.


<details>
  <summary>Details</summary>
Motivation: While AI has proven excellent for reconstructing and enhancing radiological images (making them sharper, smoother, more detailed, faster to acquire, and quicker to review), it also introduces new failure modes and exacerbates the disconnect between perceived image quality and actual information content. Understanding these limitations is critical for safe and effective use of AI in radiology.

Method: This is a communication/awareness paper that aims to bring attention to the limitations of AI-enabled image reconstruction and enhancement in radiology. The approach is educational and cautionary rather than presenting a specific technical method.

Result: The paper identifies that AI introduces new failure modes in radiological imaging and creates a problematic disconnect between how good an image appears (perceived quality) and what diagnostic information it actually contains (information content).

Conclusion: Users must understand the limitations of AI in radiological image reconstruction and enhancement to reap the technology's benefits while minimizing risks. Awareness of these limitations is essential for safe and effective deployment of AI in clinical radiology practice.

Abstract: Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.

</details>


### [157] [P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads](https://arxiv.org/abs/2602.09443)
*Yun Luo,Futing Wang,Qianjia Cheng,Fangchen Yu,Haodi Lei,Jianhao Yan,Chenxi Li,Jiacheng Chen,Yufeng Zhao,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Wenxuan Zeng,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.AI

TL;DR: P1-VL is an open-source vision-language model family that achieves state-of-the-art performance on physics Olympiad problems by combining curriculum reinforcement learning with agentic augmentation, becoming the first open-source VLM to win 12 gold medals on the HiPhO benchmark.


<details>
  <summary>Details</summary>
Motivation: Physics requires models to maintain physical consistency with universal laws, which fundamentally requires multimodal perception to ground abstract logic in reality. At Olympiad levels, diagrams contain essential constraints (boundary conditions, spatial symmetries) absent from text, creating a visual-logical gap that needs bridging.

Method: P1-VL combines Curriculum Reinforcement Learning (progressive difficulty expansion to stabilize post-training) with Agentic Augmentation (iterative self-verification at inference). This approach harmonizes training stability with reasoning refinement.

Result: P1-VL-235B-A22B becomes the first open-source VLM to secure 12 gold medals on HiPhO benchmark (13 exams from 2024-2025), achieving state-of-the-art performance among open-source models and No.2 overall rank globally (trailing only Gemini-3-Pro). Shows remarkable scientific reasoning capacity and generalizability across STEM benchmarks.

Conclusion: P1-VL represents a foundational step toward general-purpose physical intelligence, better aligning visual perceptions with abstract physical laws for machine scientific discovery. The open-source release enables broader research in multimodal scientific reasoning.

Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>


### [158] [SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning](https://arxiv.org/abs/2602.09463)
*Furong Jia,Ling Dai,Wenjin Deng,Fan Zhang,Chen Hu,Daxin Jiang,Yu Liu*

Main category: cs.AI

TL;DR: SpotAgent: An agentic reasoning framework for geo-localization that combines visual interpretation with tool-assisted verification to address sparse, ambiguous visual cues and provide verifiable results.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with real-world geo-localization where visual cues are sparse, long-tailed, and ambiguous, often producing confident but ungrounded predictions due to limitations in internal knowledge and lack of verification mechanisms.

Method: Proposes SpotAgent framework with 3-stage post-training: 1) SFT for basic alignment, 2) Agentic Cold Start using multi-agent synthesized trajectories to instill tool-calling expertise, 3) RL refinement with Spatially-Aware Dynamic Filtering to prioritize learnable samples based on spatial difficulty.

Result: Achieves state-of-the-art performance on standard benchmarks, effectively mitigates hallucinations, and delivers precise, verifiable geo-localization results.

Conclusion: SpotAgent successfully addresses limitations of previous approaches by formalizing geo-localization as an agentic reasoning process that synergizes visual interpretation with tool-assisted verification, providing reliable and verifiable predictions in challenging real-world scenarios.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>


### [159] [Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models](https://arxiv.org/abs/2602.09485)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: XMCC is an explainable multimodal chain-of-thought compressor that uses reinforcement learning to shorten reasoning trajectories while preserving answer correctness and providing explanations for compression decisions.


<details>
  <summary>Details</summary>
Motivation: Long chains of thought in multimodal reasoning are often excessively lengthy and contain redundant steps, hindering inference efficiency. Existing compression approaches compromise visual-textual reasoning integrity and lack explainability.

Method: XMCC formulates compression as a sequential decision-making process optimized via reinforcement learning. It preserves key reasoning steps while shortening trajectories and generates natural-language explanations for compression decisions.

Result: Extensive experiments on multimodal reasoning benchmarks show XMCC reduces reasoning length while maintaining answer correctness and provides explainable explanations, validating its effectiveness.

Conclusion: XMCC successfully addresses the challenges of multimodal CoT compression by providing an explainable approach that preserves reasoning integrity while improving efficiency.

Abstract: Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>


### [160] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: TabPFN foundation models enable fast, accurate Shapley value computation for explainable AI by leveraging in-context learning to approximate conditional expectations without retraining.


<details>
  <summary>Details</summary>
Motivation: Shapley values are computationally expensive to compute, especially with dependent features, requiring many conditional expectation approximations. Traditional methods (Monte Carlo or regression) are slow, and deep learning approaches require retraining for each conditional expectation.

Method: Use TabPFN tabular foundation models with in-context learning to approximate conditional expectations without retraining. Compare multiple TabPFN variants with state-of-the-art methods on simulated and real datasets for Shapley value computation.

Result: TabPFN yields best performance in most cases; where it doesn't, it's only marginally worse than the best method but at a fraction of the runtime.

Conclusion: Tabular foundation models like TabPFN provide an efficient solution for Shapley value computation, with potential for further improvements by adapting them specifically for conditional Shapley value estimation.

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [161] [Autoregressive Direct Preference Optimization](https://arxiv.org/abs/2602.09533)
*Masanari Oi,Mahiro Ukai,Masahiro Kaneko,Naoaki Okazaki,Nakamasa Inoue*

Main category: cs.AI

TL;DR: ADPO reformulates DPO by explicitly integrating autoregressive modeling before applying the Bradley-Terry model, shifting summation outside log-sigmoid and distinguishing token vs feedback length measures.


<details>
  <summary>Details</summary>
Motivation: Standard DPO relies on response-level Bradley-Terry model but assumes autoregressive nature only after deriving objective, limiting its potential. Need to explicitly incorporate autoregressive modeling earlier in the theoretical foundation.

Method: Revisit DPO theoretical foundations, introduce autoregressive assumption prior to applying BT model, derive Autoregressive DPO (ADPO) with loss that shifts summation outside log-sigmoid function.

Result: ADPO provides elegant reformulation, identifies two distinct length measures (token length  and feedback length ') that must be considered in DPO-based algorithms, offering theoretical insights for preference optimization.

Conclusion: ADPO extends DPO framework with explicit autoregressive modeling, provides theoretical clarification of length measures, and offers improved foundation for preference optimization in LLMs.

Abstract: Direct preference optimization (DPO) has emerged as a promising approach for aligning large language models (LLMs) with human preferences. However, the widespread reliance on the response-level Bradley-Terry (BT) model may limit its full potential, as the reference and learnable models are assumed to be autoregressive only after deriving the objective function. Motivated by this limitation, we revisit the theoretical foundations of DPO and propose a novel formulation that explicitly introduces the autoregressive assumption prior to applying the BT model. By reformulating and extending DPO, we derive a novel variant, termed Autoregressive DPO (ADPO), that explicitly integrates autoregressive modeling into the preference optimization framework. Without violating the theoretical foundations, the derived loss takes an elegant form: it shifts the summation operation in the DPO objective outside the log-sigmoid function. Furthermore, through theoretical analysis of ADPO, we show that there exist two length measures to be considered when designing DPO-based algorithms: the token length $$ and the feedback length $$'. To the best of our knowledge, we are the first to explicitly distinguish these two measures and analyze their implications for preference optimization in LLMs.

</details>


### [162] [Detecting radar targets swarms in range profiles with a partially complex-valued neural network](https://arxiv.org/abs/2602.09597)
*Martin Bauw*

Main category: cs.AI

TL;DR: Paper proposes partially complex-valued neural networks for radar target detection to handle multiple proximate targets and distorted echoes, outperforming traditional pulse compression methods.


<details>
  <summary>Details</summary>
Motivation: Radar target detection faces challenges from clutter, waveform distortion, and especially target proximity where multiple targets can be perceived as one or interfere with each other's detection thresholds. Traditional methods struggle with these issues.

Method: Uses partially complex-valued neural networks as adaptive range profile processing. The generative architecture processes entire received signals at once to generate complete detection profiles, unlike pulse compression which processes one pulse length at a time.

Result: Experiments using simulated datasets show the neural network approach outperforms common pulse compression methods, particularly in handling multiple targets with varying proximity and distorted echoes.

Conclusion: Partially complex-valued neural networks offer an effective solution for radar target detection in challenging scenarios with multiple proximate targets and distorted signals, providing superior performance over traditional pulse compression approaches.

Abstract: Correctly detecting radar targets is usually challenged by clutter and waveform distortion. An additional difficulty stems from the relative proximity of several targets, the latter being perceived as a single target in the worst case, or influencing each other's detection thresholds. The negative impact of targets proximity notably depends on the range resolution defined by the radar parameters and the adaptive threshold adopted. This paper addresses the matter of targets detection in radar range profiles containing multiple targets with varying proximity and distorted echoes. Inspired by recent contributions in the radar and signal processing literature, this work proposes partially complex-valued neural networks as an adaptive range profile processing. Simulated datasets are generated and experiments are conducted to compare a common pulse compression approach with a simple neural network partially defined by complex-valued parameters. Whereas the pulse compression processes one pulse length at a time, the neural network put forward is a generative architecture going through the entire received signal in one go to generate a complete detection profile.

</details>


### [163] [FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints](https://arxiv.org/abs/2602.09620)
*Jorge Fandinno,Pedro Cabalar,Philipp Wanko,Torsten Schaub*

Main category: cs.AI

TL;DR: FLINGO is a new language that extends Constraint Answer Set Programming (CASP) to preserve ASP-style expressiveness for numerical attributes within constraints, bridging the gap between ASP predicates and constraint-based representations.


<details>
  <summary>Details</summary>
Motivation: Current CASP solvers lose important ASP features when switching to constraint-based representations - features like default values, undefined attributes, non-deterministic assignments with choice rules, and aggregated values that are natural in ASP but missing in CASP constraint specifications.

Method: Developed the FLINGO language and tool that incorporates ASP-style expressiveness inside numerical constraints, with a translation from FLINGO syntax to regular CASP programs following the CLINGCON input format, building on established semantic foundations.

Result: Created a working FLINGO system that allows numerical attributes to maintain ASP features like default values, undefined states, choice rules, and aggregates while being represented as constraints, demonstrated with several examples.

Conclusion: FLINGO successfully bridges the expressiveness gap between ASP and CASP, allowing constraint-based numerical attributes to retain the rich features available in standard ASP specifications while maintaining constraint processing capabilities.

Abstract: Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.

</details>


### [164] [ClinAlign: Scaling Healthcare Alignment from Clinician Preference](https://arxiv.org/abs/2602.09653)
*Shiwei Lyu,Xidong Wang,Lei Liu,Hao Zhu,Chaohe Zhang,Jian Wang,Jinjie Gu,Benyou Wang,Yue Shen*

Main category: cs.AI

TL;DR: Two-stage framework for aligning LLMs with clinician preferences: HealthRubrics dataset of physician-verified examples, distilled into HealthPrinciples for scalable supervision, achieving state-of-the-art performance on medical benchmarks with efficient models.


<details>
  <summary>Details</summary>
Motivation: LLMs show expert medical knowledge but struggle to align open-ended outputs with fine-grained clinician preferences. Existing methods use coarse objectives or unreliable automated judges not grounded in professional guidelines.

Method: Two-stage framework: 1) HealthRubrics dataset (7,034 physician-verified preference examples where clinicians refine LLM-drafted rubrics), 2) Distilled into HealthPrinciples (119 reusable, clinically grounded principles organized by clinical dimensions). Used for offline alignment (synthesizing rubrics for unlabeled queries) and inference-time guided self-revision.

Result: 30B parameter model activating only 3B parameters at inference achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing resource-efficient baseline for clinical alignment.

Conclusion: The framework successfully addresses the gap in aligning LLMs with clinician preferences through physician-verified rubrics and distilled principles, enabling scalable supervision and establishing efficient clinical alignment baselines.

Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>


### [165] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: GHS-TDA improves Chain-of-Thought reasoning by using global hypothesis graphs for error correction and topological data analysis for stable reasoning extraction.


<details>
  <summary>Details</summary>
Motivation: Current CoT methods suffer from error propagation due to lack of global coordination and revision mechanisms, and lack structured analysis for filtering redundancy and extracting key reasoning features, leading to unstable reasoning and limited interpretability.

Method: GHS-TDA constructs a semantically enriched global hypothesis graph to aggregate and coordinate multiple candidate reasoning paths, then applies topological data analysis based on persistent homology to capture stable multi-scale structures and extract reliable reasoning skeletons.

Result: GHS-TDA consistently outperforms strong baselines in both accuracy and robustness across multiple reasoning benchmarks, achieving self-adaptive convergence and producing high-confidence, interpretable reasoning paths.

Conclusion: By jointly leveraging reasoning diversity and topological stability, GHS-TDA addresses fundamental limitations of existing CoT methods, providing a more reliable and interpretable reasoning framework for LLMs.

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [166] [Symbolic Pattern Temporal Numeric Planning with Intermediate Conditions and Effects](https://arxiv.org/abs/2602.09798)
*Matteo Cardellini,Enrico Giunchiglia*

Main category: cs.AI

TL;DR: Extends Symbolic Pattern Planning (SPP) to temporal planning with Intermediate Conditions and Effects (ICEs), creating planner Patty that outperforms existing planners in temporal domains and shows strong performance in ICE domains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to extend the SPP approach to handle more complex temporal planning problems with durative actions and intermediate conditions/effects (ICEs), which are important for real-world applications where actions can overlap and conditions/effects can occur at specific times during execution.

Method: Extends the Symbolic Pattern Planning (SPP) approach to temporal planning with ICEs. Patterns suggest causal orders between actions, which are encoded in SMT formulas. If patterns are inaccurate, they are extended until they contain the causal order of a valid plan, maintaining completeness. The approach handles durative actions that can overlap and conditions/effects that can be checked/applied at any time during execution.

Result: The SPP planner Patty: (1) outperforms all other planners in most temporal domains without ICEs, (2) obtains comparable results with state-of-the-art search planners in literature domains with ICEs, and (3) outperforms the same planner in a novel domain based on a real-world application.

Conclusion: The extension of SPP to temporal planning with ICEs is successful, with Patty demonstrating superior performance in temporal domains and competitive/strong performance in ICE domains, showing the approach's effectiveness for real-world temporal planning problems.

Abstract: Recently, a Symbolic Pattern Planning (SPP) approach was proposed for numeric planning where a pattern (i.e., a finite sequence of actions) suggests a causal order between actions. The pattern is then encoded in a SMT formula whose models correspond to valid plans. If the suggestion by the pattern is inaccurate and no valid plan can be found, the pattern is extended until it contains the causal order of actions in a valid plan, making the approach complete. In this paper, we extend the SPP approach to the temporal planning with Intermediate Conditions and Effects (ICEs) fragment, where $(i)$ actions are durative (and thus can overlap over time) and have conditions/effects which can be checked/applied at any time during an action's execution, and $(ii)$ one can specify plan's conditions/effects that must be checked/applied at specific times during the plan execution. Experimental results show that our SPP planner Patty $(i)$ outperforms all other planners in the literature in the majority of temporal domains without ICEs, $(ii)$ obtains comparable results with the SoTA search planner for ICS in literature domains with ICEs, and $(iii)$ outperforms the same planner in a novel domain based on a real-world application.

</details>


### [167] [Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices](https://arxiv.org/abs/2602.09802)
*Manon Reusens,Sofie Goethals,Toon Calders,David Martens*

Main category: cs.AI

TL;DR: LLMs can generate meaningful willingness-to-pay estimates for subjective travel decisions but show systematic deviations from human benchmarks, particularly overestimating values for expensive options and business personas.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly deployed for subjective decision support (travel assistance, purchasing) where no objectively correct answers exist, there's a need to understand how their decision-making compares to human preferences and valuations.

Method: Presented LLMs with travel choice dilemmas, analyzed responses using multinomial logit models to derive implied willingness-to-pay (WTP) estimates, compared to human benchmarks from economics literature. Examined baseline settings plus realistic conditions: providing information about users' past choices and persona-based prompting.

Result: Larger LLMs can produce meaningful WTP values but show systematic attribute-level deviations. They tend to overestimate human WTP overall, especially for expensive options or business-oriented personas. Conditioning on prior preferences for cheaper options yields valuations closer to human benchmarks.

Conclusion: LLMs show both potential and limitations for subjective decision support. Careful model selection, prompt design, and user representation are crucial for practical deployment, as LLM valuations can systematically differ from human preferences.

Abstract: As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>


### [168] [Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning](https://arxiv.org/abs/2602.09813)
*Dexun Li,Sidney Tio,Pradeep Varakantham*

Main category: cs.AI

TL;DR: Hierarchical MDP framework for environment design with teacher using student policy representations and generative data augmentation to reduce teacher-student interactions in resource-constrained UED.


<details>
  <summary>Details</summary>
Motivation: Current UED methods assume infinite environment generation through stochastic processes, which is impractical in resource-constrained scenarios with limited teacher-student interaction opportunities.

Method: Introduces hierarchical MDP framework with teacher agent leveraging student policy representations from discovered evaluation environments, plus generative model to augment training data and reduce interaction needs.

Result: Outperforms baseline approaches across several domains while requiring fewer teacher-student interactions per episode.

Conclusion: The approach is applicable in settings where training opportunities are limited, offering efficient curriculum generation for general-purpose agent development.

Abstract: Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>


### [169] [Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?](https://arxiv.org/abs/2602.09937)
*Taeyoon Kim,Woohyeok Park,Hoyeong Yun,Kyungyong Lee*

Main category: cs.AI

TL;DR: LLM-based RCA agents for cloud systems have systematic failures across all models, with hallucinated data interpretation and incomplete exploration being most common. Prompt engineering alone can't fix these, but better communication protocols help.


<details>
  <summary>Details</summary>
Motivation: Automated Root Cause Analysis (RCA) is crucial for cloud system stability, but current LLM-based agents have low accuracy. Existing evaluations only check final answers without understanding why reasoning fails, leaving root causes of agent failures unknown.

Method: Conducted process-level failure analysis by executing the full OpenRCA benchmark across five LLM models (1,675 agent runs). Classified failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Performed controlled mitigation experiments with prompt engineering and communication protocol improvements.

Result: Most prevalent pitfalls (hallucinated data interpretation and incomplete exploration) persist across all models regardless of capability tier, indicating failures originate from shared agent architecture rather than individual model limitations. Prompt engineering alone cannot resolve dominant pitfalls, but enriching inter-agent communication protocol reduces communication-related failures by up to 15 percentage points.

Conclusion: The pitfall taxonomy and diagnostic methodology provide a foundation for designing more reliable autonomous agents for cloud RCA. Failures are systematic architectural issues, not just model limitations, requiring structural improvements to agent design.

Abstract: Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.

</details>


### [170] [Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning](https://arxiv.org/abs/2602.09945)
*Jinsong Liu,Yuhang Jiang,Ramayya Krishnan,Rema Padman,Yiye Zhang,Jiang Bian*

Main category: cs.AI

TL;DR: DRL framework improves clinical AI by learning from reasoning discrepancies between reference rationales and agent's chain-of-thought, using graph edit distance analysis and retrieval-augmented instruction patching.


<details>
  <summary>Details</summary>
Motivation: Clinical decision support requires not just correct answers but clinically valid reasoning. Current approaches lack mechanisms to systematically improve reasoning fidelity by learning from discrepancies between reference clinical knowledge and agent reasoning.

Method: Differential Reasoning Learning (DRL) extracts reasoning graphs from reference rationales and agent's chain-of-thought, performs clinically weighted graph edit distance analysis, uses LLM-as-judge to align nodes and diagnose discrepancies, stores findings in Differential Reasoning Knowledge Base (DR-KB), and retrieves top-k instructions via RAG to augment agent prompts at inference.

Result: Evaluation on medical QA benchmarks and Return Visit Admissions prediction shows gains over baselines in both answer accuracy and reasoning fidelity. Ablation studies confirm benefits from reference rationales and top-k retrieval. Clinician review provides assurance of approach effectiveness.

Conclusion: DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers practical deployment mechanism under limited token budgets by systematically learning from reasoning discrepancies.

Abstract: Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>


### [171] [ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference](https://arxiv.org/abs/2602.10004)
*Junda Wang,Zhichao Yang,Dongxu Zhang,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: ESTAR introduces early stopping for large reasoning models to reduce redundant computation after correct answers are reached, achieving 3.7x shorter reasoning chains while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models waste computation on redundant reasoning after already reaching correct answers, creating inefficiency in chain-of-thought reasoning.

Method: Combines trajectory-based classifier for safe stopping detection, supervised fine-tuning to teach models to generate <stop> signals, and <stop>-aware reinforcement learning with compute-aware rewards.

Result: Reduces reasoning length by 3.7x (from 4,799 to 1,290 tokens) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization across four reasoning datasets.

Conclusion: Early stopping is a simple yet powerful mechanism for improving reasoning efficiency in large reasoning models without sacrificing accuracy.

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>


### [172] [Discovering High Level Patterns from Simulation Traces](https://arxiv.org/abs/2602.10009)
*Sean Memery,Kartic Subr*

Main category: cs.AI

TL;DR: Proposes using natural language to discover coarse-grained patterns from physics simulation logs, enabling better LM reasoning about physical systems for reward program generation.


<details>
  <summary>Details</summary>
Motivation: LMs struggle with physics reasoning as they learn from observational data rather than grounded simulation. Simulation traces as context don't scale well due to large volumes of fine-grained data.

Method: Use natural language guidance to discover coarse-grained patterns (like 'rigid-body collision', 'stable support') from detailed simulation logs. Synthesize programs that map simulation logs to high-level activated patterns.

Result: The annotated representation of simulation logs is more amenable to natural language reasoning about physical systems. Enables LMs to generate effective reward programs from natural language goals.

Conclusion: This approach bridges the gap between LMs and physics reasoning by creating scalable, interpretable representations from simulation data for planning and supervised learning applications.

Abstract: Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>


### [173] [Chain of Mindset: Reasoning with Adaptive Cognitive Modes](https://arxiv.org/abs/2602.10063)
*Tianyi Jiang,Arctanx An,Hengyi Feng,Naixin Zhai,Haodong Li,Xiaomin Yu,Jiahui Liu,Hanwen Du,Shuo Zhang,Zhi Yang,Jie Huang,Yuhua Li,Yongxin Ni,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: Chain of Mindset (CoM) is a training-free agentic framework that enables step-level adaptive mindset orchestration for LLM reasoning, achieving state-of-the-art performance across multiple challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM reasoning methods apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence.

Method: CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow.

Result: Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96% and 4.72% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency.

Conclusion: CoM addresses the limitation of fixed-mindset reasoning in LLMs by enabling adaptive mindset orchestration, representing a significant advancement toward more human-like problem-solving capabilities in AI systems.

Abstract: Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

</details>


### [174] [CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085)
*Richard Bornemann,Pierluigi Vito Amadori,Antoine Cully*

Main category: cs.AI

TL;DR: CODE-SHARP is a framework that uses Foundation Models to open-endedly discover and evolve hierarchical skills as executable reward programs, enabling agents to solve complex long-horizon tasks without pre-defined rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning relies on hand-designed reward functions, which is infeasible for open-ended skill discovery where meaningful skills aren't known beforehand. Existing methods only refine rewards for pre-defined tasks, limiting true open-ended discovery.

Method: Uses Foundation Models to open-endedly expand and refine a hierarchical skill archive structured as a directed graph of executable reward functions in code (SHARP skills). Combines goal-conditioned agents trained on discovered rewards with high-level FM-based planning.

Result: Goal-conditioned agents trained on SHARP-generated rewards learn to solve increasingly long-horizon goals in Craftax environment. When composed by FM-based planner, the agent outperforms pretrained agents and task-specific expert policies by over 134% on average for complex long-horizon tasks.

Conclusion: CODE-SHARP successfully enables open-ended skill discovery through hierarchical reward programs, demonstrating that Foundation Models can autonomously discover meaningful skills that enable agents to solve complex tasks beyond pre-defined capabilities.

Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

</details>


### [175] [Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090)
*Zhaoyang Wang,Canwen Xu,Boyi Liu,Yite Wang,Siwei Han,Zhewei Yao,Huaxiu Yao,Yuxiong He*

Main category: cs.AI

TL;DR: AWM is a synthetic environment generation pipeline that creates 1,000 code-driven environments for training multi-turn tool-use agents, enabling scalable RL training with reliable state transitions and strong out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered agent training is limited by lack of diverse, reliable environments. Realistic environments are scarce, while LLM-simulated environments lack consistency. Need scalable, high-quality synthetic environments for training multi-turn tool-use agents.

Method: Propose Agent World Model (AWM) - a fully synthetic environment generation pipeline that creates 1,000 code-driven environments backed by databases. Each environment has rich toolsets (avg 35 tools) and provides high-quality observations. Environments are fully executable with accessible database states.

Result: Created 1,000 diverse environments covering everyday scenarios. Enables large-scale RL for multi-turn tool-use agents with reliable reward functions. Training exclusively in synthetic environments yields strong out-of-distribution generalization on three benchmarks.

Conclusion: AWM provides scalable, reliable synthetic environments that overcome limitations of existing approaches. Code-driven, database-backed environments enable efficient agent training with consistent state transitions and strong generalization capabilities.

Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

</details>
