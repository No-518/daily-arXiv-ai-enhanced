<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 296]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.AI](#cs.AI) [Total: 82]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: This paper presents an AI system that analyzes handwritten exam scripts using OCR and sentiment analysis to quantify student stress levels through a numerical Stress Index.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional grading systems and gain deeper insights into students' cognitive and emotional states during examinations by quantifying psychological stress.

Method: Uses high-resolution image processing, TrOCR for optical character recognition, and sentiment entropy fusion with RoBERTa-based models. Implements a five-model voting mechanism and unsupervised anomaly detection for robustness.

Result: Developed an innovative framework in academic forensics that generates a numerical Stress Index from handwritten examination scripts.

Conclusion: The system successfully integrates graphology and AI to provide a data-driven approach for assessing student psychological stress, offering new possibilities in educational assessment and mental health monitoring.

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [2] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: This paper presents a real-time pothole detection system using SVM classifier with 98.1% accuracy on sensor data collected from vehicles.


<details>
  <summary>Details</summary>
Motivation: Road conditions significantly impact daily commute and traffic flow. Small cracks can develop into large potholes due to temperature changes and vehicle forces, requiring frequent monitoring for better road management.

Method: Used SVM classifier to detect potholes in real-time using onboard vehicle sensors. Data was collected from a 2km local road with 26 potholes.

Result: Achieved 98.1% accuracy in pothole detection based on the collected sensor data.

Conclusion: The proposed system enables effective real-time pothole detection using vehicle sensors, which can support large-scale road condition analysis and management.

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [3] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: This study addresses the lack of standardized habitat classification for cultivated land ecosystems by developing a comprehensive dataset and proposing DWFF-Net, a dynamic-weighted feature fusion network that improves segmentation accuracy for multi-scale habitats.


<details>
  <summary>Details</summary>
Motivation: Current systems lack standardized habitat classification for cultivated land, have incomplete coverage of habitat types, and existing models fail to effectively integrate semantic and texture features, leading to poor segmentation accuracy and blurred boundaries for multi-scale habitats.

Method: Developed an annotated ultra-high-resolution remote sensing dataset with 15 habitat categories. Proposed DWFF-Net with frozen-parameter DINOv3 encoder for foundational features, data-level adaptive dynamic weighting for feature fusion, dynamic weight computation network in decoder for multi-layer feature integration, and hybrid loss function for training optimization.

Result: The model achieved mIoU of 0.6979 and F1-score of 0.8049 on the constructed dataset, outperforming baseline by 0.021 and 0.0161 respectively. Ablation studies confirmed multi-layer feature fusion improves IoU for micro-habitats like field ridges.

Conclusion: Established a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at low cost and providing technical support for fine-grained habitat monitoring in cultivated landscapes.

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [4] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet is a few-shot medical image segmentation framework that uses edge-aware geodesic distance learning to improve boundary delineation with limited training data, achieving better performance than existing methods while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation requires large annotated datasets, creating a bottleneck for clinical applications. Existing few-shot methods show suboptimal performance in precise boundary delineation, especially when anatomically similar regions lack sufficient spatial context.

Method: The framework combines: (1) edge-aware geodesic distance learning with iterative Fast Marching refinement, (2) adaptive prototype extraction with spatially-weighted aggregation, and (3) adaptive parameter learning that adjusts to different organ characteristics.

Result: Extensive experiments show improvements over state-of-the-art methods, with reduced boundary errors while maintaining computational efficiency.

Conclusion: AGENet is highly suitable for clinical applications requiring precise segmentation with limited annotated data, leveraging predictable geometric patterns in medical structures to guide prototype extraction.

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [5] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: EPSegFZ is a pre-training-free network for few- and zero-shot 3D point cloud semantic segmentation that uses prototype-enhanced attention, dual positional encoding, and language-guided embedding to improve performance without requiring pre-training.


<details>
  <summary>Details</summary>
Motivation: Current few-shot 3D point cloud segmentation methods rely heavily on pre-training, limiting flexibility, and fail to fully utilize textual annotations from support sets, which restricts zero-shot capabilities.

Method: The method incorporates three components: Prototype-Enhanced Registers Attention (ProERA) module, Dual Relative Positional Encoding (DRPE)-based cross-attention, and Language-Guided Prototype Embedding (LGPE) module to leverage textual information.

Result: The method achieves state-of-the-art performance, outperforming previous methods by 5.68% on S3DIS and 3.82% on ScanNet benchmarks.

Conclusion: EPSegFZ demonstrates that pre-training-free approaches can achieve superior performance in few- and zero-shot 3D point cloud segmentation by better utilizing support set information including textual annotations.

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [6] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: TASA is a geometry-optimized framework for 3D scene-level affordance segmentation that combines 2D semantic cues with 3D geometric reasoning in a coarse-to-fine approach to efficiently detect manipulable regions from language instructions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene-level affordance understanding are limited - they either focus on object-level affordances or simply lift 2D predictions to 3D, missing rich geometric structure and being computationally expensive.

Method: TASA uses a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding task-relevant view selection. Then a 3D affordance refinement module integrates 2D semantic priors with local 3D geometry for accurate 3D affordance masks.

Result: Experiments on SceneFun3D show TASA significantly outperforms baselines in both accuracy and efficiency for scene-level affordance segmentation.

Conclusion: The proposed TASA framework effectively addresses limitations of existing methods by leveraging both 2D semantic cues and 3D geometric reasoning, achieving superior performance in 3D scene-level affordance understanding.

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [7] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet is a lightweight, enhanced version of Capsule Network that achieves 4x faster inference, 76.73% accuracy on CIFAR-10, and 94.3% accuracy on AffNIST with only 3.8M parameters.


<details>
  <summary>Details</summary>
Motivation: CapsNet has advantages like better detection of overlapping categories and transformed images, but suffers from slow speed, high resource consumption, and lower accuracy compared to CNNs.

Method: Proposed LE-CapsNet as a light, enhanced variant of CapsNet with optimized structure to reduce parameters and improve efficiency.

Result: LE-CapsNet achieves 76.73% accuracy on CIFAR-10 (4x faster inference than CapsNet) and 94.3% accuracy on AffNIST (vs CapsNet's 90.52%), using only 3.8M weights.

Conclusion: LE-CapsNet successfully addresses CapsNet's limitations by providing a more efficient, faster, and accurate alternative while maintaining robustness to affine transformations.

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [8] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: TBSD addresses the trade-off in SDS variants between texture quality and shape distortion by formulating 3D generation as multi-objective optimization with adaptive negative prompt utilization.


<details>
  <summary>Details</summary>
Motivation: Vanilla SDS suffers from over-saturation and smoothing, while recent variants using negative prompts face a critical trade-off: limited texture optimization or texture gains with shape distortion.

Method: Target-Balanced Score Distillation (TBSD) formulates generation as multi-objective optimization with an adaptive strategy that balances target negative prompts to resolve the texture-shape trade-off.

Result: TBSD significantly outperforms state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

Conclusion: The proposed TBSD method effectively resolves the fundamental trade-off in SDS variants by adaptively balancing target negative prompts, achieving superior 3D asset generation quality.

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [9] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS is a neural architecture search framework that globally optimizes rank selection for tensor decomposition to compress CNNs for deployment on microcontrollers and lightweight NPUs, achieving significant compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Deep CNNs are becoming too large and computationally demanding for deployment on resource-constrained devices like microcontrollers and lightweight NPUs, requiring effective compression methods that maintain accuracy.

Method: Uses CompressNAS framework inspired by MicroNAS, treating rank selection as a global search problem with fast accuracy estimators to evaluate candidate tensor decompositions under memory and accuracy constraints.

Result: Achieved 8x compression of ResNet-18 on ImageNet with <4% accuracy drop; 2x compression of YOLOv5s on COCO with no accuracy loss; 2x compression of YOLOv5n with 2.5% accuracy drop; introduced STResNet family with competitive performance.

Conclusion: CompressNAS enables efficient global rank selection for tensor decomposition, providing significant model compression for edge devices while maintaining competitive accuracy, making deep CNNs more deployable on resource-constrained hardware.

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [10] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly is a prompt-guided test-time adaptation framework for UAV semantic segmentation that enables resource-efficient adaptation without weight updates, using token-prompt retrieval for limited-resource UAVs and gradient-free visual prompt optimization for resource-massive UAVs.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models for UAV networks deteriorate under weather, lighting, and viewpoint changes. Resource-limited UAVs cannot run gradient-based adaptation, while resource-massive UAVs adapt independently, wasting shared experience.

Method: Two complementary adaptation modes: lightweight token-prompt retrieval from shared global memory for resource-limited UAVs, and gradient-free sparse visual prompt optimization using Covariance Matrix Adaptation Evolution Strategy for resource-massive UAVs. Includes activation-statistic detector and cross-UAV knowledge pool.

Result: Extensive experiments on UAVid and VDD benchmarks and real-world UAV deployments show significant improvements in segmentation accuracy and robustness over static models and state-of-the-art TTA baselines under diverse weather conditions.

Conclusion: AdaptFly provides a practical path to resilient, communication-efficient perception for the low-altitude economy by enabling fleet-wide collaboration with negligible bandwidth overhead.

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [11] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: A biologically plausible masked autoencoder with human blind spot-inspired masking strategy learns visual representations from child's egocentric data, enabling effective word-referent mapping learning.


<details>
  <summary>Details</summary>
Motivation: To develop a biologically plausible visual learning strategy that mimics how human brains process visual information, particularly addressing the challenge of learning word-referent mappings from limited data in early childhood development.

Method: Uses masked autoencoder with novel masking strategy inspired by human eye's blind spot, pretrains visual encoder, then applies contrastive learning for video-text modeling to learn word-referent mappings from longitudinal egocentric child data.

Result: The biologically plausible masking strategy performs at least as effectively as standard random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

Conclusion: Biologically inspired masking strategies can be as effective as conventional approaches while providing more plausible models of human visual learning, offering insights into early language acquisition processes.

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [12] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER is a novel framework that adaptively integrates spatial multi-omics data with histopathological images using graph convolutional networks, contrastive learning, and dynamic expert routing to overcome modality heterogeneity and resolution mismatches.


<details>
  <summary>Details</summary>
Motivation: Current spatial omics data (transcriptomics, proteomics, epigenomics) lack pathological morphological context, and integrating them with histopathological images is essential for comprehensive tissue analysis. However, substantial heterogeneity across modalities, resolution mismatches, and biological perturbations during sample preparation pose significant integration challenges.

Method: GROVER uses a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture nonlinear dependencies between modalities and spatial structure. It employs spot-feature-pair contrastive learning to align representations across modalities and a dynamic expert routing mechanism to adaptively select informative modalities while suppressing noisy inputs.

Result: Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing robust and reliable multimodal integration.

Conclusion: GROVER offers an effective solution for integrating heterogeneous spatial multi-omics data with histopathological images, addressing key challenges in multimodal spatial data analysis for comprehensive tissue understanding.

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [13] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect uses hyperspectral imaging reconstruction from RGB inputs to improve Deepfake detection by amplifying manipulation artifacts in spectral bands.


<details>
  <summary>Details</summary>
Motivation: Current detection methods operate in RGB space with only three spectral channels, limiting their ability to detect subtle manipulation artifacts that may be more visible in denser spectral bands.

Method: A two-stage pipeline that first reconstructs a 31-channel hyperspectral image from standard RGB input, then performs detection in the hyperspectral domain where manipulation artifacts are amplified.

Result: HSI-Detect shows consistent improvements over RGB-only baselines across the FaceForensics++ dataset, demonstrating better detection performance.

Conclusion: Spectral-domain mapping shows promise for Deepfake detection by leveraging hyperspectral imaging to reveal manipulation artifacts that are weak or invisible in the RGB domain.

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [14] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: The paper examines whether invariant neural networks for point clouds preserve symmetry-aware distances through bi-Lipschitz equivalence, showing standard networks fail with Procrustes Matching metric but can be modified to achieve bi-Lipschitz guarantees.


<details>
  <summary>Details</summary>
Motivation: Recent work in equivariant learning highlights advantages of bi-Lipschitz models, motivating examination of whether invariant point cloud networks preserve natural symmetry-aware distances.

Method: Analyze two symmetry-aware metrics (Procrustes Matching and Hard Gromov Wasserstein), show they're not bi-Lipschitz equivalent, modify standard invariant networks to achieve bi-Lipschitz guarantees.

Result: Standard invariant networks are not bi-Lipschitz with respect to PM metric, but modified networks can obtain bi-Lipschitz guarantees and show advantages in correspondence tasks.

Conclusion: Modified bi-Lipschitz invariant networks outperform standard models for finding correspondences between 3D point clouds, demonstrating the importance of preserving symmetry-aware distances.

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [15] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: Concept-RuleNet is a neurosymbolic system that combines visual concept generation with symbolic reasoning to improve interpretability and reduce hallucinations in vision-language models.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack interpretability and often hallucinate facts, especially with out-of-distribution data. Neurosymbolic approaches exist but extract symbols only from task labels, lacking proper visual grounding.

Method: A multi-agent system with: 1) multimodal concept generator that mines visual concepts from training images, 2) symbol discovery conditioned on visual concepts, 3) LLM reasoner that composes symbols into first-order rules, and 4) vision verifier that quantifies symbol presence during inference.

Result: Achieves 5% average improvement over state-of-the-art neurosymbolic baselines across five benchmarks (including medical imaging and underrepresented datasets), and reduces hallucinated symbols in rules by up to 50%.

Conclusion: The system successfully reinstates visual grounding while maintaining transparent reasoning, providing explicit reasoning pathways and reducing hallucinations in vision-language models.

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [16] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: A novel Batch Transformer architecture uses implicit sparse attention to focus on important dimensions, reducing bottleneck size in encoder-decoder networks for improved face recognition with makeup/occlusion data.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional Transformers by focusing attention on key dimensions rather than entire entities, enabling more efficient encoder-decoder architectures for data-limited scenarios.

Method: Proposed Batch Transformers with implicit sparse attention that selects important dimensions (primary components) instead of attending to full dimensionality, reducing bottleneck size in encoder-decoder architectures.

Result: The architecture was successfully tested on synthetic image generation for face recognition with makeup and occlusion datasets, demonstrating increased variability from limited original data.

Conclusion: Batch Transformers with dimension-selective attention provide an efficient alternative to traditional Transformers, enabling better performance with reduced computational complexity in data augmentation tasks.

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [17] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER is a reinforcement learning framework that orchestrates multiple text-to-image and image-to-image models to handle long, compositional prompts through dynamic task decomposition and structured feedback from a vision-language model critic.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation models struggle with long, compositional prompts typical of creative workflows, as no single system reliably executes them end-to-end.

Method: Uses reflective reinforcement learning to orchestrate a registry of pretrained experts, handles prompts through dynamic task decomposition, and supervises alignment via structured feedback from a vision-language model critic, casting image synthesis as a Markov Decision Process.

Result: Outperforms baselines including frontier models across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations.

Conclusion: Reinforcement learning can enable AI systems to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [18] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer is a constant-memory temporal transformer that unifies object detection, tracking, and short-term trajectory prediction in a single end-to-end framework, achieving high performance on challenging tracking scenarios.


<details>
  <summary>Details</summary>
Motivation: Address challenges in single-object tracking and motion forecasting under occlusion, scale variation, and temporal drift that disrupt temporal coherence for real-time perception.

Method: Uses a minimal constant-memory temporal transformer with ground-truth-primed memory and burn-in anchor loss for stable identity propagation, plus a single lightweight temporal-attention layer for cross-frame embedding refinement.

Result: Achieves 76.3 AUC and 53.7 FPS on Mini-LaSOT benchmark with 4.3 GB VRAM, outperforming transformer baselines like TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

Conclusion: SOTFormer provides an efficient unified framework for real-time object tracking and forecasting with stable performance under challenging conditions.

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [19] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: MP-GFormer is a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations to predict machining operation sequences, achieving significant improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic graph learning approaches in machining process planning fail to incorporate 3D geometric information of parts, lacking domain awareness in predicting machining operation sequences despite capturing spatio-temporal dependencies.

Method: Proposes MP-GFormer, a 3D-geometry-aware dynamic graph transformer that uses StereoLithography surface meshes representing 3D geometry after each machining operation, with boundary representation for initial designs, integrated through attention mechanisms.

Result: Evaluation on synthesized dataset shows improvements of 24% and 36% in accuracy for main and sub-operation predictions respectively compared to state-of-the-art approaches.

Conclusion: Integrating evolving 3D geometric representations into dynamic graph learning significantly enhances machining operation sequence prediction accuracy by providing better domain awareness.

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [20] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard is a dual-stage weight protection framework that prevents unauthorized model merging by disrupting merging compatibility while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: To protect intellectual property rights and ensure model ownership/accountability against unauthorized model merging practices that combine fine-tuned models without authorization.

Method: Two-stage approach: 1) Redistribute task-relevant information across layers via L2-regularized optimization, 2) Inject structured perturbations to misalign task subspaces and break curvature compatibility in loss landscape.

Result: Reduces merged model accuracy by up to 90% with less than 1.5% performance loss on protected models. Tested on vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models.

Conclusion: MergeGuard effectively protects models from unauthorized merging by reshaping parameter geometry to cause destructive interference in merged models while maintaining functionality of protected models.

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [21] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: FocusSDF is a novel boundary-aware loss function for medical image segmentation that uses signed distance functions to assign higher weights to boundary pixels, improving segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Most medical image segmentation models don't explicitly encode boundary information, making boundary preservation a persistent challenge that affects diagnostic precision and therapeutic interventions.

Method: Proposed FocusSDF loss function based on signed distance functions (SDFs) that adaptively assigns higher weights to pixels closer to lesion or organ boundaries, making the network boundary-aware.

Result: Extensive evaluations across diverse datasets (cerebral aneurysm, stroke, liver, breast tumor) and multiple imaging modalities showed FocusSDF consistently outperforms existing distance transform based loss functions.

Conclusion: FocusSDF effectively addresses boundary preservation challenges in medical image segmentation and demonstrates superior performance over state-of-the-art methods including foundation model MedSAM.

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [22] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Jolle Taillon,Jrme Thau*

Main category: cs.CV

TL;DR: This study investigates using synthetic imagery to improve muskox detection in object detection models when real training data is scarce, showing improved performance in zero-shot and few-shot settings.


<details>
  <summary>Details</summary>
Motivation: Traditional wildlife survey methods are resource-intensive and limited by logistical challenges. Deep learning models face difficulties with sparse species like muskoxen due to small datasets.

Method: Compared baseline model trained on real imagery with 5 zero-shot and 5 few-shot models incorporating progressively more synthetic imagery. Tested performance in precision, recall, and F1 score.

Result: Zero-shot models improved with synthetic imagery, with performance plateauing when synthetic data exceeded 100% of baseline. Few-shot models showed better recall and slightly higher accuracy but improvements weren't statistically significant.

Conclusion: Synthetic imagery shows potential for training accurate object detection models when real data is scarce, enabling monitoring of rare or inaccessible species and increasing monitoring frequency.

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [23] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: This paper introduces Harpia, a CUDA-based processing library for Annotat3D that enables scalable, interactive segmentation of large 3D datasets in HPC environments with strict memory control and GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: High-resolution volumetric imaging techniques generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration.

Method: Developed Harpia - a CUDA-based processing library with strict memory control, native chunked execution, and GPU-accelerated filtering, annotation, and quantification tools that can handle datasets exceeding single-GPU memory capacity.

Result: Experimental results show significant improvements in processing speed, memory efficiency, and scalability compared to NVIDIA cuCIM and scikit-image frameworks.

Conclusion: The system's interactive, human-in-the-loop interface with efficient GPU resource management makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [24] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: Automated prompt optimization using DSPy framework significantly improves medical vision-language model performance, achieving 53% median relative improvement over zero-shot baselines without requiring model finetuning or manual prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Vision-language models underperform on medical benchmarks, and existing solutions like finetuning require large datasets/compute while manual prompt engineering is hard to generalize and inaccessible to medical institutions.

Method: Adapted DSPy framework for automated prompt optimization, implementing prompting pipelines for 5 medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with 4 prompt optimization techniques.

Result: Optimized pipelines achieved 53% median relative improvement over zero-shot baselines, with largest gains of 300% to 3,400% on tasks where zero-shot performance was low, while preserving data privacy and scalability.

Conclusion: Automated prompt optimization has substantial potential for medical AI systems, enabling significant performance gains without dependence on manual prompt design, allowing clinicians to focus on patient care while supporting reproducible research.

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [25] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM is a dual-path architecture for medical data imputation that dynamically routes samples to either statistical (MICE) or neural (GAIN) imputation based on missingness complexity, with cross-path attention fusion and joint optimization.


<details>
  <summary>Details</summary>
Motivation: Medical imaging and multi-modal clinical settings often face missing modality challenges, and existing imputation methods either lack representational capacity or are computationally expensive.

Method: Dual-path architecture with intelligent routing (low missingness to MICE, complex patterns to GAIN), cross-path attention fusion using missingness-aware embeddings, and end-to-end joint optimization of imputation and downstream tasks.

Result: State-of-the-art performance on MIMIC-III and multimodal benchmarks: RMSE of 0.108 (vs. baselines' 0.119-0.152) and AUROC of 0.812 for mortality prediction.

Conclusion: PI-NAIM provides a unified solution for real-world scenarios with incomplete data, enabling seamless integration into vision pipelines handling missing modalities or corrupted inputs.

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [26] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus is a lightweight visual token selection module that dynamically selects important video tokens based on text queries, reducing vision tokens by 89% and latency by 28% while maintaining near-parity accuracy on long video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Long video understanding in multimodal LLMs faces challenges due to linearly growing vision tokens causing attention cost explosion, memory issues, and latency problems.

Method: QTSplus uses cross-attention scoring, predicts instance-specific retention budgets based on query complexity, and selects Top-n tokens with differentiable straight-through estimator during training and hard gate at inference. A small re-encoder preserves temporal order using absolute time information.

Result: Compresses vision stream by 89%, reduces end-to-end latency by 28% on long videos. Achieves near-parity accuracy overall, outperforms original model by +20.5 and +5.6 points on TempCompass direction and order accuracies.

Conclusion: QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [27] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: First use of event cameras for image dehazing, leveraging their high dynamic range (120dB vs 60dB) and microsecond latency to overcome limitations of RGB-only methods in hazy conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-based dehazing methods suffer from limited dynamic range, making dehazing ill-posed and causing loss of structure and illumination details. Event cameras offer superior HDR and latency advantages for hazy scenes.

Method: Proposes an event-guided diffusion model that transfers HDR information from events to frames. Uses an event-guided module to map sparse HDR event features (edges, corners) into diffusion latent space for precise structural guidance during generation.

Result: Achieves state-of-the-art results on two benchmarks and a newly collected drone dataset in heavy haze (AQI=341) with synchronized RGB and event sensors.

Conclusion: Event cameras combined with diffusion models provide effective solution for dehazing by leveraging HDR cues from events, improving visual realism and reducing semantic drift in hazy conditions.

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [28] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Lus Gustavo,Dimmy Magalhes,Lucciani Vieira,Mauro Arajo*

Main category: cs.CV

TL;DR: Comparative analysis of three U-Net variants for semantic segmentation of Brazilian rock art petroglyphs, with Attention-Residual BEGL-UNet achieving best performance (Dice Score 0.710) using attention mechanisms and residual blocks.


<details>
  <summary>Details</summary>
Motivation: To develop effective deep learning methods for digital preservation of archaeological heritage, specifically for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites.

Method: Three U-Net-based architectures were compared: (1) BEGL-UNet with Border-Enhanced Gaussian Loss, (2) Attention-Residual BEGL-UNet with residual blocks and gated attention, and (3) Spatial Channel Attention BEGL-UNet with spatial-channel attention modules. All used BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments used 5-fold cross-validation on images from Poo da Bebidinha Archaeological Complex.

Result: Attention-Residual BEGL-UNet achieved best performance with Dice Score 0.710, validation loss 0.067, and recall 0.854. Spatial Channel Attention BEGL-UNet had DSC 0.707 and recall 0.857. Baseline BEGL-UNet registered DSC 0.690. Attention mechanisms improved Dice Score by 2.5-2.9% over baseline.

Conclusion: Attention mechanisms significantly enhance semantic segmentation performance for archaeological heritage preservation, with Attention-Residual BEGL-UNet demonstrating the most effective architecture for rock art petroglyph segmentation.

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poo da Bebidinha Archaeological Complex, Piau, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [29] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: This paper evaluates vision-language models for fine-grained glomerular subtyping in kidney biopsy interpretation under few-shot learning conditions, finding that pathology-specialized models with vanilla fine-tuning perform best even with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Fine-grained glomerular subtyping is clinically important but faces data scarcity challenges, and it's unclear how vision-language models should be adapted for this task under realistic clinical data constraints.

Method: Systematic evaluation of both pathology-specialized and general-purpose vision-language models for few-shot glomerular subtyping, analyzing classification performance, feature alignment, and representation separability across different shot counts, architectures, and adaptation strategies.

Result: Pathology-specialized vision-language backbones with vanilla fine-tuning are most effective, achieving substantial gains in discrimination and calibration with only 4-8 labeled examples per subtype, though additional supervision provides incremental improvements.

Conclusion: Supervision level and adaptation strategy jointly influence both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment in clinical settings.

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [30] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: This paper presents an IPPG method that breaks the constraint of facial close-ups by using a Dual-Line Inference pipeline with identity-semantic separation, Identity Adaptive Fusion strategy, and Identity Aggregation Prepending module to achieve synergistic optimization of identity fidelity and scene semantic creation.


<details>
  <summary>Details</summary>
Motivation: Existing IPPG approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups with weak visual narrativity and poor semantic consistency under complex text prompts, due to identity feature embeddings undermining semantic expressiveness.

Method: The method includes: 1) Dual-Line Inference pipeline with identity-semantic separation, 2) Identity Adaptive Fusion strategy that defers ID-semantic fusion to noise prediction stage with adaptive attention fusion and noise decision masking, 3) Identity Aggregation Prepending module to aggregate ID information and replace random initializations.

Result: Experimental results validate stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. The method works as a plug-and-play component for existing IPPG frameworks.

Conclusion: The proposed method addresses the over-reliance on facial close-ups in IPPG, facilitates film-level character-scene creation, and provides richer personalized generation capabilities for related domains.

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [31] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: The paper addresses limitations in transformation-based adversarial attacks by proposing a Dynamic Parameter Optimization (DPO) method that improves transferability while reducing computational complexity from O(m^n) to O(nlogm).


<details>
  <summary>Details</summary>
Motivation: Existing transformation-based attacks have blind spots in parameter optimization: they only consider low-iteration settings, use uniform parameters across different scenarios, and rely on inefficient grid search with high computational overhead.

Method: The authors conducted empirical studies revealing dynamic patterns of transferability, proposed a Concentric Decay Model (CDM) to explain these patterns, and developed DPO based on the rise-then-fall pattern for efficient parameter optimization.

Result: Comprehensive experiments across different surrogate models, iterations, and tasks demonstrate that DPO significantly improves transferability of transformation-based attacks.

Conclusion: The proposed DPO method effectively addresses the limitations of existing transformation-based attacks by providing an efficient parameter optimization approach that enhances transferability while reducing computational complexity.

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [32] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: LithoSeg is a coarse-to-fine network for lithography SEM image segmentation that uses SAM with human-in-the-loop bootstrapping for robustness and recasts 2D segmentation as 1D regression for precise groove contour delineation.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of lithography SEM images is crucial for semiconductor process control and yield optimization, but existing methods lack the precision and robustness needed for diverse pattern geometries and process windows.

Method: Two-stage approach: 1) Coarse stage uses Human-in-the-Loop Bootstrapping with SAM for robust segmentation with minimal supervision; 2) Fine stage recasts 2D segmentation as 1D regression by sampling groove-normal profiles and refining with lightweight MLP.

Result: LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision.

Conclusion: LithoSeg offers promising prospects for real-world semiconductor manufacturing applications by providing precise and robust lithography segmentation with reduced supervision requirements.

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [33] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: SIT-ADDA-Auto adapts only early convolutional layers for microscopy image domain adaptation, using adversarial alignment and uncertainty to automatically select adaptation depth without target labels, outperforming full-network adaptation.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for microscopy often fail when applied to images from new instruments or settings, and conventional adversarial domain adaptation disrupts learned semantic representations by retraining entire networks.

Method: SIT-ADDA-Auto adapts only the earliest convolutional layers while freezing deeper layers, integrating shallow-layer adversarial alignment with predictive uncertainty for automatic depth selection without requiring target labels.

Result: Across exposure/illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced semantic feature drift.

Conclusion: The method provides a design rule for label-free adaptation in microscopy and a practical recipe for field settings, demonstrating that adapting only shallow layers yields reliable transfer while preserving deeper semantic representations.

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [34] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: A multi-camera computer vision framework for real-time traffic safety assessment at intersections using Post-Encroachment Time (PET) computation, achieving sub-second precision and real-time processing on edge devices.


<details>
  <summary>Details</summary>
Motivation: Traditional crash-based traffic safety studies suffer from data sparsity and latency issues, creating a need for real-time, continuous safety assessment methods at signalized intersections.

Method: Four synchronized cameras provide continuous coverage, with YOLOv11 segmentation for vehicle detection and homography matrices to transform vehicle polygons into unified bird's-eye maps. A novel pixel-level PET algorithm measures vehicle positions without fixed cells.

Result: The framework achieves fine-grained hazard visualization with 3.3 sq-cm accuracy, processes data at 2.68 FPS for 800x800 pixel heatmaps, and successfully identifies high-risk regions with sub-second precision on edge devices.

Conclusion: The study validates decentralized vision-based PET analysis as a feasible, replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation in intelligent transportation systems.

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [35] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: The paper introduces WGREC, a generalized version of WREC that handles expressions with variable numbers of referents, and proposes LIHE framework with hybrid hyperbolic-Euclidean similarity to address supervisory ambiguity and semantic collapse challenges.


<details>
  <summary>Details</summary>
Motivation: Existing WREC methods are limited by one-to-one mapping assumption, making them unable to handle realistic scenarios where expressions may correspond to zero or multiple targets. The paper aims to bridge this gap with a more practical paradigm.

Method: Proposes LIHE framework with two stages: Referential Decoupling (predicts number of targets and decomposes expressions) and Referent Grounding (localizes sub-expressions using HEMix hybrid similarity module combining Euclidean proximity and hyperbolic geometry).

Result: LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM datasets, while HEMix achieves consistent improvements on standard REC benchmarks with up to 2.5% improvement in IoU@0.5.

Conclusion: The proposed LIHE framework successfully addresses the challenges of WGREC through linguistic instance-splitting and hybrid hyperbolic-Euclidean similarity, demonstrating effectiveness in handling variable numbers of referents while preventing semantic collapse.

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [36] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: NSDD is a single-pass diffusion distillation method for lensless imaging that achieves photorealistic reconstructions without paired supervision by separating range-space enforcement from null-space diffusion updates, outperforming iterative methods in speed while maintaining perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Existing photorealistic reconstructions for lensless cameras rely on paired lensless-lensed supervision which introduces domain bias, while generic diffusion priors fail under the noisy, multiplexed, ill-posed lensless deconvolution setting.

Method: Null-Space Diffusion Distillation (NSDD) - a single-pass student model that distills the null-space component of an iterative DDNM+ solver, conditioned on lensless measurement and range-space anchor, preserving measurement consistency.

Result: NSDD is the second fastest method behind Wiener, achieves near-teacher perceptual quality (second-best LPIPS below DDNM+), outperforms DPS and classical convex baselines on Lensless-FFHQ and PhlatCam datasets.

Conclusion: NSDD provides a practical path toward fast, ground-truth-free, photorealistic lensless imaging by effectively separating range-space and null-space updates in a single-pass distillation framework.

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [37] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT is the first large-scale multimodal dataset for surgical point tracking that combines visual tracking with textual descriptions of point status, enabling context-aware tracking systems that perform better in challenging surgical conditions.


<details>
  <summary>Details</summary>
Motivation: Existing surgical tracking datasets lack semantic context to understand tracking failure mechanisms in complex surgical environments with smoke occlusion, specular reflections, and tissue deformation.

Method: Created VL-SurgPT dataset with 908 in vivo video clips (754 for tissue tracking, 154 for instrument tracking) with detailed annotations, established benchmarks using 8 state-of-the-art trackers, and proposed TG-SurgPT - a text-guided tracking approach that leverages semantic descriptions.

Result: Incorporating point status information significantly improves tracking accuracy and reliability, especially in adverse visual scenarios where conventional vision-only methods struggle.

Conclusion: Bridging visual and linguistic modalities enables development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that maintain performance under challenging intraoperative conditions.

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [38] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent is a novel framework that addresses long-video understanding challenges in MLLMs through a Global-Context-Aware Agent with Schematic and Narrative Episodic Memory, achieving state-of-the-art performance on Video-MME benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with long-video understanding due to token limitations and inability to capture long-term temporal dependencies and global context for deep video reasoning.

Method: Proposes GCAgent framework with Schematic and Narrative Episodic Memory that structurally models events and their relations, operating in a multi-stage Perception-Action-Reflection cycle with Memory Manager for context-aware inference.

Result: Achieves up to 23.5% accuracy improvement on Video-MME Long split over baseline, with 73.4% accuracy on Long split and highest overall average (71.9%) on Video-MME benchmark among 7B-scale MLLMs.

Conclusion: The framework validates agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding, fundamentally resolving long-term dependency problems.

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [39] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: A novel framework for 3D hand-object pose estimation that jointly integrates visual and physical cues through joint visual-physical learning and candidate pose aggregation, achieving state-of-the-art performance in both accuracy and physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely solely on visual cues, often producing physically implausible results with interpenetration or non-contact issues. Current physics-based approaches use post-optimization or non-differentiable engines that compromise visual consistency and end-to-end trainability.

Method: Two key innovations: 1) Joint visual-physical cue learning that extracts both 2D visual and 3D physical cues for comprehensive representation learning; 2) Candidate pose aggregation that refines multiple diffusion-generated poses using both visual and physical predictions.

Result: Extensive experiments show the method significantly outperforms state-of-the-art approaches in both pose accuracy and physical plausibility.

Conclusion: The proposed framework successfully integrates visual and physical reasoning to produce 3D hand-object poses that are both visually consistent and physically plausible, overcoming limitations of existing methods.

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [40] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG enhances masked image generation by incorporating explicit token-level semantic dependency knowledge from training data through three knowledge graphs, improving generation quality without changing existing MIG architectures.


<details>
  <summary>Details</summary>
Motivation: Existing MIG methods struggle to learn semantic dependencies from visual token sequences because individual tokens lack clear semantic meaning and sequences are long, making direct learning challenging.

Method: Proposes KA-MIG framework with three knowledge graphs (co-occurrence, semantic similarity, position-token incompatibility), uses graph-aware encoder to learn token and position-aware representations, and lightweight fusion to integrate with existing MIG methods.

Result: Experimental results show improved performance for class-conditional image generation on ImageNet compared to existing MIG methods.

Conclusion: Incorporating explicit token-level semantic dependency knowledge as priors effectively enhances the model's ability to capture semantic dependencies and improves generation quality in masked image generation.

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [41] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: CalMRL addresses multimodal representation learning with missing modalities by calibrating incomplete alignments through representation-level imputation and bi-step optimization.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal methods require all modalities to be present, making it challenging to utilize datasets with missing modalities. The paper identifies an 'anchor shift' problem where observed modalities align with suboptimal anchors when some modalities are missing.

Method: Proposes CalMRL which leverages modality priors and connections to model missing modality imputation at representation level. Uses bi-step learning with closed-form solution for posterior distribution of shared latents to resolve optimization challenges.

Result: Theoretical validation shows mitigation of anchor shift and convergence. Extensive experiments demonstrate superiority over existing methods, enabling flexible use of data with missing modalities.

Conclusion: CalMRL successfully addresses the missing modality problem in multimodal representation learning through calibrated alignment, providing new flexibility for utilizing incomplete datasets while maintaining performance.

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [42] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat is a feed-forward framework that reconstructs high-resolution 3D scenes from sparse low-resolution images by leveraging external reference images and internal texture cues to compensate for missing high-frequency details.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods from sparse, low-resolution images fail to recover fine texture details due to inherent lack of high-frequency information in LR inputs, limiting their practical applications in autonomous driving and embodied AI.

Method: Uses MLLMs and diffusion models to create scene-specific reference galleries, then employs Reference-Guided Feature Enhancement (RGFE) to fuse features from LR inputs and reference images, and Texture-Aware Density Control (TADC) to adaptively adjust Gaussian density based on texture richness.

Result: Outperforms existing methods on RealEstate10K, ACID, and DTU datasets, demonstrating strong cross-dataset and cross-resolution generalization capabilities.

Conclusion: SRSplat effectively addresses the texture detail recovery problem in 3D reconstruction from sparse LR images through joint external reference guidance and internal texture-aware processing.

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [43] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: FedSDA addresses non-IID histopathological image data in federated learning by aligning stain distributions across clients using diffusion models and stain separation, improving model performance while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Non-IID data poses a major challenge in federated learning, particularly for histopathological images with feature distribution shifts. Current approaches have limited focus on data distribution perspective.

Method: Proposes Federated Stain Distribution Alignment (FedSDA) that uses diffusion models and stain separation to align client stain distributions with a target distribution, while avoiding privacy risks by not training diffusion models on raw data.

Result: Extensive experiments show FedSDA effectively improves baseline methods, outperforms other non-IID data approaches, and provides practical value for computational pathology.

Conclusion: FedSDA successfully mitigates non-IID data issues in federated learning for histopathological images through stain distribution alignment, offering valuable insights for the computational pathology community.

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [44] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: DCMM-Transformer is a novel Vision Transformer architecture for medical images that incorporates anatomical groupings through a differentiable Degree-Corrected Mixed-Membership model as an additive bias in self-attention, overcoming limitations of previous binary masking approaches.


<details>
  <summary>Details</summary>
Motivation: Standard Vision Transformers fail to exploit latent anatomical groupings in medical images (organs, tissues, pathological regions), and existing approaches like SBM-Transformer suffer from non-differentiability, training instability, and inability to model complex community structure.

Method: Introduces DCMM-Transformer that uses a Degree-Corrected Mixed-Membership model as an additive bias in self-attention, providing community structure and degree heterogeneity in a fully differentiable and interpretable manner, unlike prior multiplicative masking and binary sampling approaches.

Result: Comprehensive experiments across diverse medical imaging datasets (brain, chest, breast, ocular) demonstrate superior performance and generalizability. The learned group structure enhances interpretability by producing anatomically meaningful and semantically coherent attention maps.

Conclusion: DCMM-Transformer effectively incorporates anatomical structure into Vision Transformers through differentiable community modeling, achieving better performance, stability, and interpretability compared to previous approaches for medical image analysis.

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [45] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: DeiTFake uses a DeiT-based transformer with two-stage progressive training to detect deepfakes, achieving 99.22% accuracy and 0.9997 AUROC on OpenForensics dataset.


<details>
  <summary>Details</summary>
Motivation: Deepfakes threaten digital media integrity, requiring robust detection methods to identify subtle manipulation artifacts in facial content.

Method: DeiT-based transformer with two-stage progressive training: initial transfer learning with standard augmentations, followed by fine-tuning with advanced affine and deepfake-specific augmentations. Uses knowledge distillation to capture subtle artifacts.

Result: Achieves 98.71% accuracy after stage one and 99.22% accuracy with 0.9997 AUROC after stage two on OpenForensics dataset (190,335 images), outperforming latest OpenForensics baselines.

Conclusion: The two-stage progressive training with increasing augmentation complexity effectively improves deepfake detection performance, providing practical benchmarks for facial deepfake detection.

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [46] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: UniABG is a dual-stage unsupervised cross-view geo-localization framework that uses adversarial view bridging and graph-based calibration to achieve state-of-the-art performance without pairwise annotations.


<details>
  <summary>Details</summary>
Motivation: Supervised CVGL methods require extensive pairwise annotations which limit scalability, while unsupervised alternatives suffer from noisy pseudo-labels due to cross-view domain gaps.

Method: Two-stage approach: 1) View-Aware Adversarial Bridging (VAAB) models view-invariant features to enhance pseudo-label robustness; 2) Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations using dual inter-view structure graphs.

Result: Achieves state-of-the-art unsupervised performance: +10.63% AP on University-1652 and +16.73% AP on SUES-200 for SatelliteDrone localization, even surpassing supervised baselines.

Conclusion: UniABG effectively addresses the limitations of both supervised and unsupervised CVGL methods, providing a scalable solution that achieves competitive performance without annotation costs.

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [47] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT is a pipelining framework that accelerates video generation by enabling parallel computation across multiple GPUs, reducing both inference latency and memory consumption.


<details>
  <summary>Details</summary>
Motivation: Current diffusion transformer (DiT) based video generation models face practical deployment challenges due to slow inference speeds and high memory consumption, limiting their real-world applicability.

Method: Three main innovations: 1) PipeSP - pipelining algorithm for sequence parallelism enabling concurrent computation and communication, 2) DeDiVAE - decoupling diffusion and VAE modules into separate GPU groups with pipelined execution, 3) Aco - attention co-processing to better utilize GPU resources in VAE group.

Result: Integrated into OpenSoraPlan and HunyuanVideo frameworks, PipeDiT achieves 1.06x to 4.02x speedups over baseline systems across various resolution and timestep configurations on 8-GPU systems.

Conclusion: PipeDiT effectively addresses the performance bottlenecks in video generation by leveraging pipelining techniques, making DiT-based models more practical for deployment while maintaining state-of-the-art performance.

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [48] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL is a movement-semantics contrastive learning framework that addresses limitations in trajectory similarity computation by using hierarchical patch-based encoding and curvature-guided augmentation, achieving state-of-the-art performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based methods for trajectory similarity computation have three key limitations: insufficient modeling of trajectory semantics and hierarchy, high computational costs from point-wise encoding, and use of physically implausible augmentations that distort trajectory semantics.

Method: MovSemCL transforms GPS trajectories into movement-semantics features, segments them into patches, uses intra- and inter-patch attentions for hierarchical representation, and employs curvature-guided augmentation that preserves informative segments while masking redundant ones.

Result: Experiments show MovSemCL outperforms state-of-the-art methods, achieving mean ranks close to the ideal value of 1 in similarity search tasks, improving heuristic approximation by up to 20.3%, and reducing inference latency by up to 43.4%.

Conclusion: MovSemCL effectively addresses key limitations in trajectory similarity computation through its hierarchical patch-based encoding and physically plausible augmentation strategy, demonstrating superior performance and efficiency compared to existing methods.

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [49] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DCA-LUT is a deep learning framework for purple fringing removal that introduces a Chromatic-Aware Coordinate Transformation module to isolate fringing into a dedicated dimension and uses a 5D LUT for color correction.


<details>
  <summary>Details</summary>
Motivation: Traditional solutions rely on expensive hardware and handcrafted features, ignoring data-driven approaches. Purple fringing caused by Longitudinal Chromatic Aberration degrades image quality.

Method: Uses Chromatic-Aware Coordinate Transformation to learn an image-adaptive color space that isolates fringing into a dedicated dimension, then applies a 5D Look-Up Table for color correction.

Result: Achieves state-of-the-art performance in purple fringing removal on both synthetic and real-world datasets.

Conclusion: The proposed DCA-LUT framework effectively addresses purple fringing through a data-driven approach with superior performance compared to traditional methods.

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [50] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: VAEmotionLLM is a two-stage framework that enables visual language models to understand audio-visual emotions with limited audio pretraining, achieving SOTA on the new ArtEmoBenchmark.


<details>
  <summary>Details</summary>
Motivation: Current AVLMs require large-scale audio pretraining and overlook emotion understanding in multimodal contexts, especially in artistic content where emotion is intentionally expressed through combined visual and auditory elements.

Method: Two-stage approach: 1) VG-Align distills visual pathway into audio pathway via next-token distribution alignment; 2) EmoAdapter injects emotion-sensitive residuals and applies emotion supervision for cross-modal emotion understanding.

Result: VAEmotionLLM achieves state-of-the-art performance on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablation studies confirm the complementary nature of proposed components.

Conclusion: The framework successfully enables VLMs to understand audio-visual emotions with limited audio data and demonstrates superior emotion understanding across modalities in artistic contexts.

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [51] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: A multimodal prompting-driven quantization framework for point cloud analysis that uses text embeddings as robust prototype priors and refines them through multimodal prompts to create hybrid geometric-semantic representations.


<details>
  <summary>Details</summary>
Motivation: Current vector quantization methods using trainable vectors or clustered centroids lack representativeness and interpretability, despite multimodal alignment showing promise in vision-language models.

Method: Uses text embeddings from pre-trained models as prototype priors, refines them with multimodal prompts, creates dual-constrained quantization space with compactness and separation regularization, and employs Gumbel-Softmax for differentiable discretization.

Result: Extensive experiments on ModelNet40 and ScanObjectNN datasets demonstrate superior effectiveness compared to existing methods.

Conclusion: The proposed multimodal prompting-driven quantization framework effectively addresses limitations of current prototype-based approaches by leveraging text embeddings as robust priors and enabling adaptive refinement through multimodal prompts.

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [52] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: A novel multilabel image classification method using modified ResNet-101 with probabilistic reasoning achieves state-of-the-art performance (0.794 mAP) on COCO-2014 dataset.


<details>
  <summary>Details</summary>
Motivation: Multilabel image categorization has important computer vision applications, but existing methods struggle with label dependencies and uncertainties in multilabel scenarios.

Method: Modified ResNet-101 architecture integrated with probabilistic reasoning to model label dependencies and uncertainties for improved multilabel classification.

Result: Achieved 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785), with strong precision-recall scores.

Conclusion: Integrating probabilistic reasoning into deep learning models effectively addresses multilabel classification challenges and achieves state-of-the-art performance.

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [53] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: SemanticStitch is a deep learning framework that uses semantic priors to preserve foreground object integrity in image stitching, addressing misalignments from varying capture angles and object movements.


<details>
  <summary>Details</summary>
Motivation: Traditional image stitching methods struggle with misalignments from varying capture angles, positional differences, and object movements, and neglect semantic information which disrupts foreground continuity.

Method: Deep learning-based framework incorporating semantic priors of foreground objects with a novel loss function that emphasizes semantic integrity of salient objects.

Result: Experimental results show substantial improvements over traditional techniques, with enhanced stitching quality and visual coherence.

Conclusion: SemanticStitch provides robust support for practical applications by preserving foreground integrity and improving overall stitching quality through semantic-aware processing.

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [54] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Proposes hierarchical layer-grouped prompt tuning for continual learning to reduce catastrophic forgetting by sharing prompts within layer groups and using a root prompt to generate sub-prompts, improving model stability.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based continual learning methods attach independent task-specific prompts to each layer, which provides flexibility but makes layers susceptible to unnecessary updates and increases catastrophic forgetting risk when prompts are aggregated across tasks.

Method: Hierarchical layer-grouped prompt tuning with: (i) layers in same group sharing similar prompts adjusted by position encoding to preserve pre-trained model's feature relationships, (ii) single task-specific root prompt generating sub-prompts for each layer group to enhance synergy and reduce independence.

Result: Extensive experiments across four benchmarks demonstrate favorable performance compared with several state-of-the-art continual learning methods.

Conclusion: The proposed hierarchical layer-grouped prompt tuning method effectively improves model stability in continual learning by preserving intrinsic feature relationships and enhancing prompt synergy, achieving competitive performance while mitigating catastrophic forgetting.

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [55] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE is a dataset distillation framework for spiking neural networks (SNNs) and event-based vision that compresses large training datasets into compact synthetic ones, enabling fast SNN training with 50x speedup and 6000x storage reduction while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient for event-based vision but remain costly to train due to temporal coding, limiting practical deployment. PACE addresses this by reducing training costs through dataset distillation.

Method: PACE uses two core modules: ST-DSM (densifies spike-based features and performs spatiotemporal matching) and PEQ-N (plug-and-play probabilistic integer quantizer compatible with event-frame pipelines).

Result: On N-MNIST, PACE achieves 84.4% accuracy (85% of full training set performance) while reducing training time by 50x and storage cost by 6000x. It outperforms existing baselines on DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets.

Conclusion: PACE enables minute-scale SNN training and efficient edge deployment by providing compact dataset surrogates that maintain performance while dramatically reducing computational and storage requirements.

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [56] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vt Zeman,Martin Mikk,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: Unified framework for object detection and 6D pose estimation with flexible onboarding from CAD models or neural reconstruction, using CNOS detector and OPFormer transformer architecture with geometric priors.


<details>
  <summary>Details</summary>
Motivation: To create a seamless end-to-end system that handles both object detection and pose estimation while supporting flexible object representation onboarding from either CAD models or neural reconstructions when models are unavailable.

Method: Pipeline with onboarding stage generating object representations from CAD models or NeRF reconstruction from multi-view images. Uses CNOS detector for object localization and OPFormer transformer-based pose estimator that encodes multiple template views with NOCS geometric priors to establish 2D-3D correspondences.

Result: Demonstrates strong balance between accuracy and efficiency on BOP benchmarks, showing practical applicability in both model-based and model-free scenarios.

Conclusion: The integrated system provides a versatile solution for 6D pose estimation that works effectively with both traditional CAD models and neural representations, achieving robust performance across different object representation scenarios.

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [57] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM is the first semi-structured N:M pruning framework for SNNs that learns sparse networks from scratch with at most N non-zeros per M-weight block, using efficient parameterization and neuroscience-inspired distillation to maintain accuracy while enabling hardware-friendly sparsity.


<details>
  <summary>Details</summary>
Motivation: Deeper SNN architectures increase parameters and computational costs, hindering edge deployment. Existing pruning methods are either unstructured (hard to accelerate) or structured (inflexible and accuracy-degrading), creating a need for a balanced approach.

Method: SpikeNM uses M-way basis-logit parameterization with differentiable top-k sampler to linearize per-block complexity from exponential to O(M). It also employs eligibility-inspired distillation (EID) that converts temporal credits into block-wise soft targets to align mask probabilities with spiking dynamics.

Result: At 2:4 sparsity, SpikeNM maintains and even improves accuracy across mainstream datasets while producing hardware-amenable patterns that complement intrinsic spike sparsity.

Conclusion: SpikeNM successfully bridges the gap between unstructured and structured pruning for SNNs, offering a semi-structured approach that maintains accuracy while enabling efficient hardware deployment through its novel parameterization and distillation techniques.

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [58] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot approach for temporal interaction localization in egocentric videos that identifies hand-object contact and separation moments without needing object masks or verb-noun taxonomies.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on 'how to interact' but struggle with capturing the critical moments of 'when to interact' - the precise contact and separation timestamps between hand and object, which is crucial for VR/AR applications and robotic policy transfer.

Method: EgoLoc uses hand-dynamics-guided sampling to generate visual prompts, leverages vision-language models to identify contact/separation attributes and localize timestamps, and employs closed-loop feedback for refinement without requiring object masks or category annotations.

Result: Comprehensive experiments show EgoLoc achieves plausible temporal interaction localization on public datasets and novel benchmarks, and effectively facilitates downstream applications in egocentric vision and robotic manipulation.

Conclusion: EgoLoc provides a generalizable zero-shot solution for temporal interaction localization that eliminates dependency on object masks and verb-noun taxonomies, enabling accurate detection of hand-object contact/separation moments in egocentric videos.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [59] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: Proposed DGCF framework combines frozen DINOv3 Transformer with trainable CNN encoder-decoder for synthetic CT generation from CBCT/MRI, achieving SOTA performance on SynthRAD2023 dataset.


<details>
  <summary>Details</summary>
Motivation: Existing CNN models lack global semantic understanding, while Transformers overfit small medical datasets. Need balanced approach for medical image translation.

Method: DINOv3-Guided Cross Fusion (DGCF) integrates frozen self-supervised DINOv3 Transformer with trainable CNN encoder-decoder, using cross fusion module and Multi-Level DINOv3 Perceptual (MLDP) loss.

Result: Achieved state-of-the-art performance on SynthRAD2023 pelvic dataset for both MRICT and CBCTCT translation tasks in terms of MS-SSIM, PSNR and segmentation-based metrics.

Conclusion: First work to employ DINOv3 representations for medical image translation, demonstrating potential of self-supervised Transformer guidance for semantic-aware CT synthesis.

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [60] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: DiffPixelFormer is a Transformer-based model for RGB-D indoor semantic segmentation that improves feature alignment and discriminative representation through intra-modal self-attention and inter-modal differential-shared interactions.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D fusion methods rely on computationally intensive cross-attention and inadequately model intra- and inter-modal feature relationships, leading to imprecise feature alignment and limited discriminative capabilities.

Method: Proposes DiffPixelFormer with Intra-Inter Modal Interaction Block (IIMIB) that uses self-attention for intra-modal dependencies and Differential-Shared Inter-Modal (DSIM) module for disentangling modality-specific/shared cues. Includes dynamic fusion strategy for balancing modality contributions.

Result: Achieves mIoU scores of 54.28% on SUN RGB-D and 59.95% on NYUDv2, outperforming DFormer-L by 1.78% and 2.75% respectively.

Conclusion: DiffPixelFormer effectively enhances RGB-D indoor semantic segmentation through improved intra-modal representations and fine-grained inter-modal interactions, demonstrating superior performance on standard benchmarks.

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [61] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: Proposes Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive video diffusion models to improve long video generation by maintaining global consistency and enhancing local dynamics.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive video diffusion models suffer from denoising latency, error accumulation, fragile consistency, and poor motion dynamics when generating long videos.

Method: Introduces ada-BOV tokens that adaptively absorb denoised preceding frames via adaptive-layer-norm-like modulation, refinement strategy for stream denoising, and disturbance-augmented training noise schedule.

Result: Achieves compelling qualitative and quantitative results across multiple metrics, demonstrating improved global consistency and local dynamics in long video generation.

Conclusion: The proposed ada-BOV framework effectively addresses limitations of existing autoregressive video diffusion models, enabling high-quality long video generation with better consistency and motion dynamics.

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [62] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything is the first simulation-ready physical 3D generative framework that creates high-quality 3D assets with explicit geometry, articulation, and physical attributes from single in-the-wild images, enabling direct use in embodied AI applications.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generation methods overlook physical and articulation properties, limiting their utility in embodied AI. There's a need for sim-ready 3D assets that can be directly used in simulation and interaction.

Method: Proposes a VLM-based physical 3D generative model with a new 3D representation that tokenizes geometry efficiently (193x reduction in tokens). Also introduces PhysX-Mobility dataset with 2K+ real-world objects and rich physical annotations.

Result: Extensive experiments show strong generative performance and robust generalization. Simulation-based experiments validate that the generated assets can be directly used for contact-rich robotic policy learning in MuJoCo-style environments.

Conclusion: PhysX-Anything bridges the gap between 3D generation and physical simulation, empowering downstream applications in embodied AI and physics-based simulation with sim-ready 3D assets.

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [63] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: SS-CA is a counterfactual augmentation method that uses attribution-based region masking to address incomplete causal learning in visual models, improving generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Models rely on limited sufficient causes for predictions, making them sensitive to distribution shifts. Attribution methods reveal critical regions, but masking them causes model misclassification while humans still recognize images, showing model dependencies aren't sufficiently causal.

Method: Propose Subset-Selected Counterfactual Augmentation (SS-CA) that integrates counterfactual explanations into training. Uses Counterfactual LIMA to identify minimal spatial regions whose removal alters predictions, then replaces these regions with natural background for data augmentation, training jointly on augmented and original samples.

Result: Extensive experiments on ImageNet variants show SS-CA improves in-distribution generalization and achieves superior performance on out-of-distribution benchmarks (ImageNet-R, ImageNet-S). Models also exhibit enhanced generalization under perturbations including noise.

Conclusion: SS-CA effectively uses interpretability insights to correct model deficiencies, improving both performance and robustness by addressing incomplete causal learning through targeted counterfactual augmentation.

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [64] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: The SenseNova-SI family scales up multimodal foundation models to improve spatial intelligence using 8 million diverse data samples, achieving state-of-the-art performance across multiple spatial benchmarks while maintaining strong general multimodal understanding.


<details>
  <summary>Details</summary>
Motivation: Despite progress in multimodal foundation models, they still exhibit surprising deficiencies in spatial intelligence, which this work aims to address through systematic scaling and data curation.

Method: Built upon established multimodal foundations (Qwen3-VL, InternVL3, Bagel), the authors systematically curated SenseNova-SI-8M - eight million diverse data samples under a rigorous taxonomy of spatial capabilities to cultivate spatial intelligence.

Result: SenseNova-SI achieves unprecedented performance: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, 50.1% on SITE, while maintaining 84.9% on MMBench-En for general multimodal understanding.

Conclusion: The work demonstrates successful cultivation of spatial intelligence through data scaling, shows early signs of emergent generalization, and releases all models publicly to facilitate further research in spatial intelligence for multimodal foundation models.

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [65] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER is a pose-based transformer framework for Bengali Sign Language recognition that achieves 97.92% accuracy with optimized architecture and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient sign language recognition system for Bengali Sign Language (BdSL) that addresses the challenges of limited data and computational constraints in real-world accessibility applications.

Method: Extends SPOTER paradigm with cultural-specific preprocessing, uses a compact four-layer transformer encoder with optimized learnable positional encodings, and employs curriculum learning to enhance generalization on limited data.

Result: Achieves 97.92% Top-1 validation accuracy on BdSLW60 benchmark, representing 22.82% improvement over Bi-LSTM baseline, with reduced parameters, lower FLOPs, and higher FPS.

Conclusion: BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [66] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO is a global dataset providing quarterly building density and height maps from 2018-2025 using deep learning on satellite imagery, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: To enable large-scale monitoring of urban development patterns and climate impacts by creating temporally resolved building data that is more efficient than existing approaches.

Method: Multi-task deep learning model trained on building footprint/height data paired with quarterly PlanetScope satellite images, applied globally at 37.6-meter resolution.

Result: Achieves 85-88% F1 score on validation, 0.96 trend-consistency score over 5 years, and captures quarterly settlement changes with low computational cost.

Conclusion: TEMPO provides an efficient solution for global resilience and adaptation efforts by enabling temporal monitoring of built environments at unprecedented scale and resolution.

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [67] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: Proposes DFF-Adapter, a parameter-efficient fine-tuning method for DINOv2 that uses lightweight multi-head LoRA modules to simultaneously detect deepfake authenticity and classify manipulation types, achieving state-of-the-art performance with only 3.5M trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection approaches treat DINOv2 as generic binary classification, overlooking distinct artifacts from different manipulation methods, which limits detection effectiveness.

Method: Incorporates lightweight multi-head LoRA modules into every transformer block of DINOv2, with a shared branch that propagates fine-grained manipulation cues to the authenticity head for multi-task cooperative optimization.

Result: Achieves detection accuracy comparable to or surpassing current state-of-the-art methods while using only 3.5M trainable parameters, demonstrating parameter efficiency.

Conclusion: The proposed DFF-Adapter effectively enhances deepfake detection by leveraging manipulation-specific knowledge through fine-grained classification, providing an efficient and powerful solution for combating sophisticated deepfakes.

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [68] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: MEMR-Seg introduces multi-round entity-level reasoning for medical image segmentation, addressing limitations of single-round dialogue methods. The approach includes a new dataset (MR-MedSeg) and baseline model (MediRound) with error correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing medical segmentation methods are task-specific and lack interactivity. Text-prompt approaches enable user-driven segmentation but are limited to single-round dialogues without multi-round reasoning capabilities.

Method: Proposed MediRound model for multi-round medical reasoning segmentation, featuring a lightweight Judgment & Correction Mechanism to mitigate error propagation in the chain-like pipeline of multi-round segmentation.

Result: Experimental results show the method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

Conclusion: MEMR-Seg enables more interactive and reasoning-based medical image segmentation through multi-round dialogues with entity-level reasoning, advancing beyond single-round approaches.

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [69] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP is a unified method for 3D scene motion perception using 4D mmWave radar that jointly performs radar target detection and motion estimation, achieving reliable performance across diverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: 4D mmWave radar provides all-weather operational capability for autonomous driving, but sparse and noisy radar points often lead to imprecise motion perception, especially when optical sensors degrade under adverse weather conditions.

Method: RadarMP jointly models radar target detection and motion estimation in a unified architecture using low-level radar echo signals from two consecutive frames. It employs self-supervised loss functions guided by Doppler shifts and echo intensity to supervise spatial and motion consistency without explicit annotations.

Result: Extensive experiments on public datasets show RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines.

Conclusion: RadarMP enhances perception capabilities for full-scenario autonomous driving systems by providing precise 3D scene motion perception that works reliably in all weather conditions.

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [70] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: OAD-Promoter is a novel approach that enhances LLM-based Visual Question Answering by mitigating language biases and improving out-of-distribution generalization through object-concentrated example generation, memory knowledge assistance, and optimized prompting.


<details>
  <summary>Details</summary>
Motivation: LLMs in VQA inherit language biases from massive training data, making predictions unreliable and struggling with out-of-distribution generalization despite strong knowledge reasoning capabilities.

Method: OAD-Promoter consists of three components: Object-concentrated Example Generation (OEG) module for generating global captions and object-focused samples, Memory Knowledge Assistance (MKA) module for retrieving relevant knowledge from stored examples, and OAD Prompt that integrates outputs to optimize LLM inference.

Result: OAD-Promoter significantly improves LLM-based VQA performance in few-shot or zero-shot settings, achieving new state-of-the-art results.

Conclusion: The proposed approach effectively addresses language bias and domain-shift robustness issues in LLM-based VQA, demonstrating superior performance through complementary visual cues and knowledge retrieval mechanisms.

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [71] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: A lightweight C-based runtime for efficient SNN inference on edge devices with optimizations that reduce latency and memory while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: SNNs offer advantages for temporal processing and energy efficiency on edge devices, but face challenges in training and deployment due to their event-driven nature.

Method: Trained SNN models from SNNTorch are translated to compact C representation with static, cache-friendly data layouts and preallocation. Sparse spiking activity is exploited to prune inactive neurons and synapses, reducing computation in convolutional layers.

Result: Achieved functional parity with Python baseline while obtaining ~10x speedups on desktop CPU, additional gains with pruning, and large memory reductions enabling deployment on Arduino Portenta H7 microcontroller.

Conclusion: SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression.

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [72] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS is the first benchmark for evaluating multimodal source attribution systems that handle visual questions, retrieve multimodal evidence, and generate cited long-form answers.


<details>
  <summary>Details</summary>
Motivation: Existing source attribution work has focused on text-only scenarios and overlooked multimodality, creating a need for multimodal source attribution evaluation.

Method: Developed a dataset with 157K visual QA instances with fact-level citations to multimodal documents, created fine-grained automatic metrics for informativeness, groundedness, and fluency, and evaluated various LVLMs with multimodal RAG approaches.

Result: LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but show weaker groundedness for image documents than text documents. There's a trade-off between informativeness and groundedness across prompting methods.

Conclusion: Mitigating contextual bias in interpreting image documents is crucial for future research in multimodal source attribution systems.

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [73] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT is a cross-modal training framework that uses Time-step Mixup to interpolate RGB and DVS inputs at various time steps, enabling smoother knowledge transfer from RGB to event cameras for spiking neural networks.


<details>
  <summary>Details</summary>
Motivation: Event cameras and SNNs offer energy-efficient visual intelligence, but face challenges due to scarce event data and the sparsity of DVS outputs. Traditional RGB-to-DVS knowledge transfer underperforms due to substantial modality distribution gaps.

Method: Proposes TMKT with probabilistic Time-step Mixup (TSM) strategy that interpolates RGB and DVS inputs at various time steps. Includes two modality-aware objectives: Modality Aware Guidance (MAG) for per-frame supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation.

Result: Achieves superior performance in spiking image classification tasks across diverse benchmarks and multiple SNN backbones. Extensive experiments and ablations demonstrate effectiveness.

Conclusion: TMKT enables smoother knowledge transfer, mitigates modality mismatch during training, and provides a stable optimization framework for cross-modal learning between RGB and event camera data.

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [74] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit is an inversion-free image editing framework that uses Frequency-Interactive Attention to achieve high-fidelity, semantically precise edits with better background preservation and spatial consistency than previous flow-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing inversion-free methods for text-guided image editing often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing.

Method: Proposes FIA-Edit with two key components: (1) Frequency Representation Interaction (FRI) module for cross-domain alignment through frequency component exchange, and (2) Feature Injection (FIJ) module that incorporates source-side queries, keys, values, and text embeddings into target branch's cross-attention.

Result: Achieves high-fidelity editing at low computational cost (~6s per 512*512 image on RTX 4090), outperforms existing methods in visual quality, background fidelity, and controllability. Successfully extended to clinical applications for medical data augmentation.

Conclusion: FIA-Edit provides an efficient and effective solution for text-guided image editing with superior performance and opens new opportunities for medical applications through anatomically coherent hemorrhage synthesis.

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [75] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: CRH is an end-to-end deep hashing framework that dynamically reassigns hash centers from a preset codebook while jointly optimizing the hash function, eliminating the need for explicit center optimization phases and improving semantic relationships.


<details>
  <summary>Details</summary>
Motivation: Existing hash center-based methods suffer from random center initialization that ignores inter-class semantic relationships, while two-stage methods introduce complexity, computational overhead, and performance gaps due to stage-wise discrepancies.

Method: Proposes Center-Reassigned Hashing (CRH) with dynamic hash center reassignment from a preset codebook, joint hash function optimization, and a multi-head mechanism to enhance representational capacity of hash centers.

Result: Extensive experiments on three benchmarks show CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

Conclusion: CRH provides an effective end-to-end solution that dynamically adapts hash centers to data distribution while capturing rich semantic structures, achieving superior retrieval performance without the limitations of two-stage approaches.

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [76] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: The paper proposes a new paradigm called Completion-by-Correction for point cloud completion, which shifts from unconstrained synthesis to guided refinement using a topologically complete shape prior, achieving better structural consistency than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional point cloud completion methods following the Completion-by-Inpainting paradigm often result in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints, motivating the need for a more robust approach.

Method: The proposed PGNet framework uses a multi-stage approach with dual-feature encoding to ground generative priors, synthesizes a coarse structurally aligned scaffold, and progressively refines geometric details via hierarchical correction.

Result: Experiments on ShapeNetViPC dataset show PGNet outperforms state-of-the-art baselines with 23.5% improvement in Chamfer Distance and 7.1% improvement in F-score.

Conclusion: The Completion-by-Correction paradigm provides a more effective approach for point cloud completion by shifting from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction.

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [77] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR introduces a novel framework that combines discrete tokens with continuous autoregressive modeling through mixture training paradigms, addressing the limitations of both discrete quantization and continuous space challenges.


<details>
  <summary>Details</summary>
Motivation: Autoregressive approaches in discrete token spaces discard fine-grained information due to quantization and limited codebook size, while continuous latent spaces offer higher quality but pose challenges for efficient modeling due to their vast and unstructured nature.

Method: MixAR uses factorized formulation with discrete tokens as prior guidance for continuous AR prediction, exploring three mixture strategies: DC-SA (self-attention), DC-CA (cross-attention), and DC-Mix (replacing mask tokens with discrete counterparts), plus TI-Mix to bridge training-inference distribution gaps.

Result: Experiments show DC-Mix achieves favorable balance between computational efficiency and generation fidelity, with TI-Mix providing consistent improvements.

Conclusion: MixAR successfully bridges discrete and continuous autoregressive modeling through mixture training, offering improved generation quality while maintaining efficiency.

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [78] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MMRINet is a lightweight brain tumor segmentation model using Mamba state-space models instead of attention for efficient 3D MRI processing, achieving strong performance with only ~2.5M parameters.


<details>
  <summary>Details</summary>
Motivation: Deep 3D networks for brain tumor segmentation are computationally prohibitive in resource-constrained settings, requiring more efficient alternatives.

Method: Replaces quadratic-complexity attention with linear-complexity Mamba state-space models, uses Dual-Path Feature Refinement modules for feature diversity, and Progressive Feature Aggregation for multi-scale fusion.

Result: Achieved average Dice score of 0.752 and HD95 of 12.23 in BraTS-Lighthouse SSA 2025 with only ~2.5M parameters.

Conclusion: MMRINet provides efficient and accurate brain tumor segmentation suitable for low-resource clinical environments.

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [79] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: Proposes a two-phase cross-view, cross-modal unsupervised domain adaptation framework for driver activity recognition that addresses viewpoint variations and domain shifts simultaneously, improving accuracy by up to 50% compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Driver distraction causes thousands of fatalities annually, and existing deep learning methods struggle with real-world deployment due to camera viewpoint variations and domain shifts across different vehicle configurations.

Method: Two-phase framework: (1) Learn view-invariant and action-discriminative features using contrastive learning on multi-view data within single modality, (2) Perform domain adaptation to new modality using information bottleneck loss without requiring labeled data from target domain.

Result: Improves top-1 accuracy on RGB video data by almost 50% compared to supervised contrastive learning-based cross-view methods, and outperforms unsupervised domain adaptation-only methods by up to 5% using the same video transformer backbone.

Conclusion: The joint cross-view, cross-modal framework enables robust and scalable deployment of driver monitoring systems across diverse vehicle configurations by effectively addressing both viewpoint variations and domain shifts.

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [80] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: A Hierarchical Semantic Learning framework for Cross-domain Few-shot Segmentation that addresses segmentation granularity gaps through style randomization and multi-scale semantic mining.


<details>
  <summary>Details</summary>
Motivation: Existing CD-FSS methods focus only on style gaps but ignore segmentation granularity gaps, leading to insufficient semantic discriminability for novel classes in target domains.

Method: Proposes HSL framework with three modules: Dual Style Randomization (DSR) for simulating target domain style variations, Hierarchical Semantic Mining (HSM) using multi-scale superpixels for granularity-aware learning, and Prototype Confidence-modulated Thresholding (PCMT) to handle segmentation ambiguity.

Result: Achieves state-of-the-art performance on four popular target domain datasets through extensive experiments.

Conclusion: The proposed HSL framework effectively addresses both style and granularity gaps in CD-FSS, enhancing semantic discriminability for novel classes in target domains.

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [81] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse is a training-aware fine-grained sparse attention framework for long-video MLLMs that achieves 2.7x speedup and 2.4x memory reduction while matching full attention performance through dynamic token budget allocation across queries, KV, and heads.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods fail to bridge the training-inference gap and lack fine-grained token selection across multiple dimensions (queries, KV, heads), leading to suboptimal performance and limited acceleration gains.

Method: Three adaptive mechanisms: (1) query selection via lazy-active classification, (2) KV selection with head-level dynamic budget allocation, and (3) KV cache slimming based on head-level decoding query patterns.

Result: Matches performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

Conclusion: OmniSparse effectively bridges the training-inference gap and enables fine-grained sparse attention across multiple dimensions for long-video MLLMs.

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [82] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: LSS3D is a high-quality image-to-3D approach that uses learnable spatial shifting to address multi-view inconsistencies and non-frontal input views, achieving superior 3D generation with complete geometric details and clean textures.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view diffusion-based 3D generation methods suffer from shape and texture misalignment across views, leading to incomplete geometric details and textural ghosting. They also lack robustness to oblique perspective inputs.

Method: Assigns learnable spatial shifting parameters to each view and adjusts views toward spatial consistency guided by reconstructed mesh. Includes input view as extra constraint for optimization to enhance robustness to non-frontal angles.

Result: Achieves leading results in geometric and texture evaluation metrics across flexible input viewpoints, with more complete geometric details and clean textures.

Conclusion: LSS3D effectively handles multi-view inconsistencies and non-frontal inputs, providing high-quality 3D generation and contributing a comprehensive quantitative evaluation pipeline to the community.

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [83] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: Proposes Geometry-guided Multi-View Diffusion Model (GeoMVD) that uses depth maps, normal maps, and segmentation masks to ensure cross-view consistency in multi-view image generation while maintaining high resolution and detail.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in existing multi-view generation methods that struggle with cross-view consistency and high-resolution output generation for applications in 3D reconstruction, VR, and AR.

Method: Multi-view geometry extraction module using depth/normal maps and segmentation masks; decoupled geometry-enhanced attention mechanism; adaptive learning strategy; iterative refinement process; dynamic geometry intensity adjustment.

Result: Generates multi-view images with improved cross-view consistency, enhanced detail preservation, and realistic spatial relationships between views.

Conclusion: The proposed GeoMVD model effectively addresses consistency and quality issues in multi-view image generation through geometric guidance and adaptive mechanisms, enabling high-quality applications in 3D reconstruction and immersive technologies.

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [84] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: AI-powered system using YOLOv8 and EasyOCR for automated detection of helmet violations, missing rear-view mirrors on motorcycles, and license plate recognition, achieving high precision and recall metrics.


<details>
  <summary>Details</summary>
Motivation: Manual enforcement of traffic laws like helmet compliance and vehicle safety standards is resource-intensive and inconsistent, necessitating automated solutions for better efficiency and road safety.

Method: Leverages YOLOv8 for object detection and EasyOCR for license plate recognition, trained on custom annotated dataset with data augmentation, and uses Streamlit for real-time monitoring interface with advanced image preprocessing.

Result: Achieved overall precision of 0.9147, recall of 0.886, mAP@50 of 0.843, and mAP@50:95 of 0.503, demonstrating strong detection capabilities under various conditions.

Conclusion: The system provides a practical and effective automated solution for traffic rule enforcement, with strong performance metrics and considerations for real-world deployment.

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [85] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: MiniGPT-Pancreas is a multimodal AI chatbot that integrates CT scans and text to assist clinicians in pancreas cancer diagnosis, achieving good classification accuracy but needing improvement in tumor detection.


<details>
  <summary>Details</summary>
Motivation: Pancreas radiological imaging is challenging due to the organ's small size, blurred boundaries, and variability in shape and position among patients, making automated diagnostic support valuable.

Method: Fine-tuned MiniGPT-v2 model with cascaded training for pancreas detection, tumor classification, and tumor detection using multimodal prompts combining questions and CT scans from NIH, MSD, and AbdomenCT-1k datasets.

Result: Achieved IoU of 0.595/0.550 for pancreas detection, 0.876 accuracy for cancer classification, and multi-organ detection IoUs of 0.8399 (liver), 0.722 (kidney), 0.705 (spleen), 0.497 (pancreas), but only 0.168 IoU for tumor detection.

Conclusion: MiniGPT-Pancreas shows promise for supporting clinicians in pancreas cancer classification, but future work is needed to improve tumor detection performance.

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [86] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Prez,Juan-Manuel~Prez-Ra,Tao Xiang,Wei Liu,Shikun Liu,Jrgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS (Mixture of States) is a novel fusion paradigm for multimodal diffusion models that uses a learnable token-wise router to create denoising timestep- and input-dependent interactions between modalities' hidden states, achieving state-of-the-art results with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and flexible fusion paradigm for multimodal diffusion models that can precisely align token-level features with the diffusion trajectory while minimizing computational costs and parameter requirements.

Method: Uses a learnable token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states. The router sparsely selects top-k hidden states and is trained with an -greedy strategy for efficient feature selection with minimal parameters.

Result: Achieved state-of-the-art results in text-to-image generation (MoS-Image) and editing (MoS-Editing). With only 3B to 5B parameters, models match or surpass counterparts up to 4 larger, demonstrating superior compute efficiency.

Conclusion: MoS establishes a flexible and compute-efficient paradigm for scaling multimodal diffusion models, offering significant performance improvements with minimal computational overhead.

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [87] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: FaNe is a semantic-enhanced vision-language pre-training framework that addresses false negatives from semantically similar texts and improves fine-grained cross-modal alignment through semantic-aware positive pair mining, text-conditioned sparse attention pooling, and hard-negative aware contrastive loss.


<details>
  <summary>Details</summary>
Motivation: Existing medical VLP methods suffer from false negatives caused by semantically similar texts and lack sufficient fine-grained cross-modal alignment, limiting their effectiveness in medical image understanding.

Method: The framework includes: 1) semantic-aware positive pair mining using text-text similarity with adaptive normalization, 2) text-conditioned sparse attention pooling for fine-grained alignment through localized visual representations guided by text, and 3) hard-negative aware contrastive loss that adaptively reweights semantically similar negatives.

Result: Extensive experiments on five medical imaging benchmarks show state-of-the-art performance across image classification, object detection, and semantic segmentation tasks.

Conclusion: FaNe effectively addresses false negatives and enables fine-grained cross-modal alignment, demonstrating superior performance in medical vision-language pre-training.

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [88] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF is a training-free method that reduces hallucinations in vision-language models by filtering out low-rank hallucination modes from feature representations using spectral analysis.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often produce hallucinations due to over-reliance on language priors and imprecise cross-modal grounding, leading to descriptions of non-existent objects, attributes, or relations.

Method: Spectral Representation Filtering (SRF) identifies hallucination modes through eigendecomposition of covariance differences between truthful and hallucinatory caption features, then applies a soft spectral filter to attenuate these modes in deeper VLM layers.

Result: SRF consistently reduces hallucination rates across LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2 models on MSCOCO, POPE-VQA, and other benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

Conclusion: SRF provides an effective post-hoc solution for hallucination suppression that requires no training, incurs zero inference overhead, and works across multiple VLM families while maintaining semantic fidelity.

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [89] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: DHMI is the first diffusion-based model inversion attack framework for deep hashing models, successfully reconstructing high-quality training images from hash codes even in black-box settings where no training data is accessible.


<details>
  <summary>Details</summary>
Motivation: Deep hashing introduces severe privacy risks by enabling reconstruction of original training data from hash codes, potentially leading to biometric forgery and privacy breaches, but model inversion attacks specifically for deep hashing remain unexplored due to challenges like inaccessible training hash codes and discrete Hamming space.

Method: DHMI clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors, then uses surrogate-guided denoising optimization with a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples, guided by a cluster of surrogate models to refine high-fidelity images.

Result: Experiments show DHMI successfully reconstructs high-resolution, high-quality images even in the most challenging black-box setting where no training hash codes are available, outperforming existing state-of-the-art model inversion attacks in black-box scenarios.

Conclusion: DHMI demonstrates both practical efficacy in reconstructing training data and confirms the critical privacy risks inherent in deep hashing systems, highlighting the need for improved security measures in hashing-based retrieval systems.

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [90] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0 is a streamlined video retrieval system that achieves up to 75% faster retrieval times while improving accuracy and user satisfaction through optimized components and a redesigned UI.


<details>
  <summary>Details</summary>
Motivation: To meet the Video Browser Showdown's demand for accurate results under strict time constraints by creating a faster, more efficient video retrieval system.

Method: Re-engineered core modules: ffmpeg for keyframe extraction, Vintern-1B-v3.5 for multilingual OCR, faster-whisper for real-time ASR, and lightweight vision-language models for QA. Also redesigned UI for better responsiveness and workflow efficiency.

Result: Retrieval time reduced by up to 75% while accuracy and user satisfaction both increased.

Conclusion: Fusionista2.0 is confirmed as a competitive and user-friendly system for large-scale video search, balancing speed and accuracy effectively.

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [91] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: A prompt-conditioned framework using MedSigLIP with FiLM and multi-scale pooling achieves state-of-the-art LDCT image quality assessment with limited training data.


<details>
  <summary>Details</summary>
Motivation: To enable data-efficient learning and rapid adaptation for LDCT image quality assessment by conditioning patch-token features on clinical intent through text prompts.

Method: Built on MedSigLIP, injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Combines global, local, and texture-aware pooling through separate regression heads fused by lightweight MLP, trained with pairwise ranking loss.

Result: Achieved PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301 on LDCTIQA2023 dataset with only 1,000 training images, surpassing top-ranked published challenge submissions.

Conclusion: The prompt-guided approach demonstrates effectiveness in LDCT quality assessment, enabling superior performance with limited training data through clinical intent conditioning.

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [92] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: A novel dual-stage disease-aware framework for chest X-ray report generation that addresses limitations in disease-awareness and vision-language alignment through disease-aware semantic tokens and multimodal similarity retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing radiology report generation approaches lack sufficient disease-awareness in visual representations and adequate vision-language alignment, causing them to overlook critical pathological features and struggle with clinical accuracy.

Method: Dual-stage framework: Stage 1 learns Disease-Aware Semantic Tokens (DASTs) via cross-attention and multi-label classification while aligning vision-language representations through contrastive learning. Stage 2 introduces Disease-Visual Attention Fusion (DVAF) module and Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities for exemplar retrieval.

Result: Extensive experiments on CheXpert Plus, IU X-ray, and MIMIC-CXR datasets demonstrate state-of-the-art performance with significant improvements in clinical accuracy and linguistic quality.

Conclusion: The proposed disease-aware framework effectively addresses limitations in existing approaches and achieves superior performance in chest X-ray report generation by enhancing disease-awareness and vision-language alignment.

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [93] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid is the first benchmark for evaluating multimodal large language models' spatial-temporal reasoning across multiple videos, featuring 5,331 videos and 9,015 QA pairs across 10 tasks.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding benchmarks focus on single-video analysis and fail to assess models' ability to reason across multiple videos simultaneously, which is crucial for real-world cross-video reasoning scenarios.

Method: Created CrossVid benchmark with 4 high-level dimensions and 10 specific tasks covering diverse cross-video reasoning scenarios, using 5,331 videos and 9,015 challenging QA pairs in multiple formats (single-choice, multiple-choice, open-ended).

Result: Gemini-2.5-Pro performed best with 50.4% average accuracy, but most MLLMs struggle with cross-video reasoning due to inability to integrate or compare evidence across multiple videos.

Conclusion: CrossVid effectively reveals limitations in current MLLMs' cross-video reasoning capabilities and provides a valuable benchmark to guide future improvements in multimodal video understanding.

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [94] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: ZoomEarth is an adaptive cropping-zooming framework for ultra-high-resolution remote sensing that actively revisits information-rich regions, achieving state-of-the-art performance and demonstrating strong versatility across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for ultra-high-resolution remote sensing processing suffer from redundancy when handling finer visual inputs due to passive perception paradigms, requiring a more active approach to efficiently process rich fine-grained information.

Method: Proposes ZoomEarth framework with Region-Guided reward, trained via supervised fine-tuning and Group Relative Policy Optimization, enabling adaptive cropping-zooming to actively revisit information-rich regions in ultra-high-resolution images.

Result: Achieves state-of-the-art performance on the LRS-GRO benchmark and zero-shot performance on three public UHR remote sensing benchmarks. Can be integrated with downstream models for cloud removal, denoising, segmentation, and image editing.

Conclusion: The active perception paradigm with adaptive cropping-zooming effectively addresses UHR remote sensing challenges, demonstrating superior performance and strong versatility across multiple applications through simple tool interfaces.

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [95] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: TM-UNet is a lightweight medical image segmentation framework that uses token sequence modeling with memory mechanisms to achieve efficient global reasoning with linear complexity, outperforming SOTA methods with reduced computation.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods achieve good results but have high computational costs that hinder clinical deployment, necessitating more efficient segmentation approaches.

Method: Proposes TM-UNet with multi-scale token-memory (MSTM) blocks that transform 2D features into token sequences using spatial scanning, matrix memory cells for selective information retention, exponential gating for token effectiveness, and multi-scale contextual extraction via parallel pooling.

Result: Extensive experiments show TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost.

Conclusion: TM-UNet provides an efficient solution for medical image segmentation that balances performance and computational efficiency, making it suitable for clinical deployment.

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [96] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: DToM is a dynamic token merging method that accelerates Diffusion MLLMs by reducing visual token redundancy during denoising steps, achieving faster inference while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion MLLMs suffer from slow inference due to full bidirectional self-attention over thousands of visual tokens, resulting in cubic decoding complexity that becomes computationally impractical.

Method: Uses decider tokens from previous denoising steps to build importance maps, maintains salient tokens and merges redundant ones through similarity-based aggregation. Features dynamic merge ratios that vary with each denoising step and integrates as a plug-and-play module without altering model parameters.

Result: Extensive experiments show DToM accelerates inference while preserving competitive performance under equivalent computational budgets.

Conclusion: DToM effectively addresses the computational bottleneck in Diffusion MLLMs through dynamic token merging, enabling faster inference without compromising model capabilities.

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [97] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: A novel multi-modal extrinsic calibration framework for simultaneous calibration of event cameras, LiDARs, and RGB cameras using a custom 3D calibration target with features designed for each sensor modality.


<details>
  <summary>Details</summary>
Motivation: To address the challenging calibration of event cameras and enable precise multi-sensor alignment in autonomous driving systems, overcoming limitations of existing pairwise calibration approaches.

Method: Uses a novel 3D calibration target with planes for LiDARs, ChArUco patterns for RGB cameras, and active LED patterns for event cameras, enabling one-shot joint extrinsic calibration across all three modalities.

Result: Validated through extensive experiments on a custom autonomous driving dataset, demonstrating accurate and robust calibration performance.

Conclusion: The proposed framework provides an effective solution for simultaneous multi-modal sensor calibration, particularly addressing the challenging event camera calibration in autonomous driving applications.

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [98] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: FreRec addresses frequency misalignment in generative data augmentation for medical AI by recalibrating frequency distributions through statistical replacement and reconstructive mapping, improving downstream classification performance.


<details>
  <summary>Details</summary>
Motivation: Medical AI suffers from data scarcity and generative data augmentation (GDA) introduces bias through frequency misalignment between real and synthesized images, which can harm downstream tasks.

Method: Proposes Frequency Recalibration (FreRec) with two components: (1) Statistical High-frequency Replacement (SHR) for rough high-frequency alignment, and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details.

Result: Extensive experiments on brain MRIs, chest X-rays, and fundus images show FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples.

Conclusion: FreRec is an effective standalone post-processing method compatible with any generative model that reduces frequency distributional discrepancy and improves reliability of generative data augmentation in medical imaging.

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [99] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++ enhances LiDAR Gaussian Splatting with diffusion priors to address artifacts in extrapolated novel view synthesis, achieving real-time high-fidelity reconstruction on urban roads.


<details>
  <summary>Details</summary>
Motivation: Existing GS-based LiDAR rendering methods suffer from artifacts in extrapolated novel views due to incomplete reconstruction from single traversal scans.

Method: Uses a controllable LiDAR generation model with diffusion priors conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans, plus an effective distillation mechanism for expansive reconstruction.

Result: Achieves state-of-the-art performance on multiple public datasets for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

Conclusion: LiDAR-GS++ effectively extends reconstruction to under-fitted regions, ensuring global geometric consistency for extrapolative novel views while preserving detailed scene surfaces.

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [100] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: A framework that adds temporal reasoning to standard classifiers without architectural changes, using a Support-Exemplar-Query learning paradigm with temporal trajectories and soft-DTW loss for improved performance in both static and temporal tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world visual data evolves gradually over time through variations in pose, lighting, and context, but conventional classifiers assume temporal independence, limiting their ability to capture dynamics.

Method: Support-Exemplar-Query (SEQ) learning paradigm structures training data into temporally coherent trajectories, learns class-specific temporal prototypes, aligns prediction sequences via differentiable soft-DTW loss, and uses multi-term objective for semantic consistency and temporal smoothness.

Result: Effective in both static and temporal tasks: enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection.

Conclusion: The approach bridges static and temporal learning in a modular and data-efficient manner using only simple classifiers on pre-extracted features, introducing temporal inductive bias through loss design alone.

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [101] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: Proposes a training-free framework that models negation as a subspace in VLMs' embedding space, improving negation understanding by ~30% without compromising zero-shot performance on affirmative prompts.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with negation and existing fine-tuning methods compromise zero-shot performance on affirmative prompts.

Method: Divides VLM embedding space into semantically consistent subspaces, models negation as a subspace rather than single point, constructs spherical caps around embeddings of A and N, scores images by direction close to A and far from N.

Result: Improves negation understanding by about 30% on average over prior methods across retrieval, MCQ, and text-to-image tasks, closes gap between affirmative and negated prompts.

Conclusion: Training-free subspace approach effectively handles negation while preserving zero-shot performance that fine-tuned models fail to maintain.

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [102] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: Using 3D ground plane analysis instead of 2D image plane analysis improves accuracy of vehicle turning movement counts at intersections, with multi-camera fusion providing the highest accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate turning movement counts at intersections are crucial for signal control, traffic management and urban planning, but current computer vision systems rely on 2D image plane analysis which may not be optimal.

Method: Back-projecting vehicle detections from infrastructure cameras to the ground plane for analysis in real-world 3D coordinates, including single-camera and multi-camera weak fusion approaches.

Result: Back-projection yields more accurate trajectory classification and turning movement counts than image plane analysis, with multi-camera fusion achieving even higher accuracy.

Conclusion: Traffic analysis should be performed on the ground plane rather than the image plane for better accuracy in turning movement counts.

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [103] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: CLAReSNet is a hybrid CNN-transformer architecture for hyperspectral image classification that addresses high dimensionality, complex spectral-spatial correlations, and limited imbalanced training samples through multi-scale convolutional extraction, spectral attention with adaptive latent bottleneck, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address critical challenges in HSI classification: high spectral dimensionality, complex spectral-spatial correlations, limited training samples with severe class imbalance, and limitations of isolated CNN/transformer approaches due to quadratic complexity and insufficient inductive biases.

Method: Hybrid architecture integrating multi-scale convolutional extraction with transformer-style attention via adaptive latent bottleneck. Uses multi-scale convolutional stem with deep residual blocks and enhanced CBAM for spatial features, spectral encoder with bidirectional RNNs and Multi-Scale Spectral Latent Attention (MSLA) that reduces complexity from O(TD) to O(Tlog(T)D) through adaptive latent token allocation.

Result: State-of-the-art performance on Indian Pines (99.71% OA) and Salinas (99.96% OA) datasets, significantly surpassing HybridSN, SSRN, and SpectralFormer. Learned embeddings show superior inter-class separability and compact intra-class clustering.

Conclusion: CLAReSNet effectively handles limited samples and severe class imbalance through its hybrid design, achieving robust classification performance and validating the effectiveness of integrating multi-scale convolutional features with spectral attention via adaptive latent bottleneck.

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [104] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: XAIGID-RewardBench is the first benchmark to evaluate MLLMs' ability to judge explanation quality for AI-generated image detection, revealing a significant gap between current models (88.76% accuracy) and human performance (98.30%).


<details>
  <summary>Details</summary>
Motivation: Current AI-generated image detection methods lack explainable reasoning, reducing trustworthiness. While MLLMs are used for explanations and evaluation, their self-judgment capabilities remain unstudied.

Method: Created a benchmark with ~3,000 annotated triplets from various image generation models, using MLLMs as policy models (detectors) to assess MLLMs as reward models (judges).

Result: Best reward model scored 88.76% accuracy, significantly below human inter-annotator agreement of 98.30%, showing substantial performance gap.

Conclusion: Current MLLMs have visible limitations in judging explanation quality for AI-generated image detection compared to human-level reasoning, with identified common pitfalls.

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [105] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1 is a reinforcement learning framework that trains LLMs to create digital twin representations of visual inputs, enabling unified visual reasoning across multiple tasks and modalities without task-specific architectures.


<details>
  <summary>Details</summary>
Motivation: Existing visual reasoning approaches require task-specific architectures and training for different tasks like segmentation, grounding, and VQA, preventing unified solutions and limiting cross-task generalization.

Method: DT-R1 uses reinforcement learning (GRPO) with a novel reward function that validates both structural integrity and output accuracy to train LLMs to construct digital twin representations of visual inputs.

Result: DT-R1 achieves consistent improvements over state-of-the-art task-specific models across six visual reasoning benchmarks covering two modalities and four task types.

Conclusion: DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations, enabling unified cross-task and cross-modality reasoning.

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [106] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg is a distillation method that compresses large reasoning segmentation models using digital twin representations and reinforcement fine-tuning, achieving state-of-the-art performance with 7.79 FPS throughput and 2.1GB memory consumption.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning segmentation methods require large multimodal models that exceed computational capabilities of edge devices where embodied AI systems typically operate, creating a need for efficient compression while preserving reasoning capabilities.

Method: Uses digital twin representations to decouple perception from reasoning, followed by supervised fine-tuning on teacher-generated reasoning chains and reinforcement fine-tuning with joint rewards for segmentation accuracy and reasoning quality alignment.

Result: Achieves state-of-the-art performance on four benchmarks (JiTBench, RVTBench, ReasonSeg, LLM-Seg40K), with the 0.6B variant outperforming models 20x larger while achieving 7.79 FPS throughput and 2.1GB memory consumption.

Conclusion: FastReasonSeg enables efficient deployment in resource-constrained environments for real-time reasoning segmentation, making embodied AI systems more practical for edge devices.

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [107] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Snderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: First online Scene Change Detection method that is pose-agnostic, label-free, multi-view consistent, runs at 10+ FPS, and outperforms even offline approaches.


<details>
  <summary>Details</summary>
Motivation: Existing online SCD methods are significantly less accurate than offline approaches, and there's a need for pose-agnostic, label-free methods that ensure multi-view consistency while maintaining real-time performance.

Method: Uses self-supervised fusion loss for multi-cue change inference, PnP-based fast pose estimation against reference scene, and fast change-guided update strategy for 3D Gaussian Splatting scene representation.

Result: Achieves state-of-the-art performance surpassing offline approaches, operates at over 10 FPS, and demonstrates superior performance on complex real-world datasets compared to both online and offline baselines.

Conclusion: The proposed approach successfully addresses the challenges of online SCD by combining efficient pose estimation, self-supervised learning, and optimized scene representation updates, setting new benchmarks for real-time scene change detection.

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [108] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: This paper introduces reasoning text-to-video retrieval, a new paradigm that handles implicit queries requiring reasoning, using digital twin representations and large language models to outperform existing methods by over 50 percentage points.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video retrieval methods fail with implicit queries where identifying relevant videos requires reasoning, as they only handle explicit queries where visual content is described directly.

Method: A two-stage framework that represents video content as digital twins (structured scene representations), performs compositional alignment between sub-queries and digital twins, then applies LLM-based reasoning with just-in-time refinement using specialist models to address information gaps.

Result: Achieves 81.2% R@1 on ReasonT2VBench-135 (outperforming strongest baseline by >50 percentage points), 81.7% R@1 on ReasonT2VBench-1000, and state-of-the-art results on three conventional benchmarks (MSR-VTT, MSVD, VATEX).

Conclusion: The proposed reasoning text-to-video retrieval paradigm successfully handles implicit queries through digital twin representations and LLM reasoning, significantly outperforming existing methods while providing object-level grounding masks.

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [109] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: AGGRNet framework extracts informative and non-informative features to improve fine-grained medical image classification by addressing inter-class similarity and intra-class variability challenges.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis faces challenges with subtle class distinctions due to intricate visual patterns, limited labeled data, and expert interpretation variability. Existing attention-based models struggle to capture inter-class similarity and intra-class variability, leading to incorrect diagnoses.

Method: Proposed AGGRNet framework that extracts both informative and non-informative features to better understand fine-grained visual patterns for medical image classification.

Result: Achieves state-of-the-art performance on various medical imaging datasets, with up to 5% improvement over SOTA models on the Kvasir dataset.

Conclusion: AGGRNet effectively addresses challenges in complex medical image analysis by capturing subtle visual patterns through informative and non-informative feature extraction, leading to improved classification performance.

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [110] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: This paper presents a hybrid quantum-classical framework using CT images to diagnose kidney stones, cysts, and tumors, achieving 99% test accuracy with quantum convolutional neural networks.


<details>
  <summary>Details</summary>
Motivation: To improve medical diagnostic performance for kidney conditions by leveraging quantum computing advantages in combination with classical deep learning methods.

Method: Combines pretrained ResNet50 encoder with Quantum CNN, using denoising and histogram equalization for preprocessing, data augmentation for class imbalance, and angle encoding to transform features into qubits.

Result: Achieved 0.99 test accuracy with both 8-qubit and 12-qubit configurations, with 12-qubit providing better recall and precision - perfect recall for cysts and 0.9956 F1-score for tumors.

Conclusion: Integrating classical preprocessing and deep feature extraction with quantum circuits enhances medical diagnostic performance for kidney condition classification.

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [111] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: Uncertainty-Guided Inference-Time Selection framework disentangles aleatoric and epistemic uncertainty in deep feature space without sampling or ensembling, enabling 60% compute reduction with negligible accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Most estimators collapse all uncertainty modes into single confidence scores, preventing reliable reasoning about when to allocate more compute or adjust inference.

Method: Aleatoric uncertainty estimated using regularized global density model; epistemic uncertainty formed from three orthogonal components: local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. No sampling, ensembling, or additional forward passes required.

Result: 60% compute reduction on MOT17 with negligible accuracy loss; distribution-free conformal calibration yields significantly tighter prediction intervals at matched coverage; orthogonal uncertainty decomposition improves computational savings by 13.6 percentage points over baseline.

Conclusion: The framework enables practical self-regulating visual inference through uncertainty-guided adaptive model selection and decomposed uncertainty calibration.

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [112] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA is a parameter-efficient adapter that reweights feature responses instead of fine-tuning the backbone, working across both CNNs and ViTs with less than 5% of backbone parameters.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank adaptation methods are mostly confined to vision transformers and struggle to generalize across different architectures like CNNs and ViTs.

Method: Combines low-rank linear projection with multi-scale nonlinear transformation to jointly modulate spatial and channel attention, fused through pointwise multiplication and residual connection.

Result: Consistently improves transfer performance on classification, detection, and segmentation tasks with stable optimization and fast convergence.

Conclusion: MSLoRA provides a simple and universal approach for efficient adaptation of frozen vision backbones by reweighting rather than re-tuning.

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [113] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: VLA-R is an open-world end-to-end autonomous driving framework that uses vision-language models for perception and vision-action retrieval for driving behavior, enabling strong generalization in unstructured environments without domain-specific tuning.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving in unstructured outdoor environments often encounters unfamiliar conditions during deployment, requiring strong generalization capabilities beyond training data.

Method: Integrates frozen vision-language models for open-world perception, uses Q-Former bottleneck to bridge perception and action domains, and employs vision-action contrastive learning to align vision-language and action embeddings for action retrieval.

Result: Demonstrates strong generalization and exploratory performance in unstructured, unseen environments on real-world robotic platforms, even with limited training data.

Conclusion: VLA-R provides an effective open-world end-to-end autonomous driving solution that can handle unfamiliar conditions through vision-language-action integration without requiring domain-specific model tuning.

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [114] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: PROBE is a self-supervised framework that uses visual probing of target domains without labels, featuring a Self-supervised Prompt Enhancement Module and Domain-Aware Prompt Alignment to achieve robust zero-shot transfer for pavement defect detection.


<details>
  <summary>Details</summary>
Motivation: Automated pavement defect detection suffers from poor cross-domain generalization, where supervised methods require costly re-annotation and standard self-supervised methods remain vulnerable to domain shift.

Method: PROBE introduces SPEM to derive defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and DAPA objective to align prompt-conditioned source and target representations.

Result: Outperforms supervised, self-supervised, and adaptation baselines on four benchmarks, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation.

Conclusion: Self-supervised prompting is a practical direction for building scalable and adaptive visual inspection systems that can generalize across domains without costly re-annotation.

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [115] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: This paper proposes a rotation-only optimization framework for Structure from Motion that represents translation in terms of rotation, condensing the imaging geometry onto the rotation manifold for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve SfM performance by exploring the critical relationship between scene structures, rotation and translation, building on recent pose-only imaging geometry that decouples 3D coordinates from camera poses.

Method: A rotation-only optimization framework based on reprojection error that expresses translation in terms of rotation, condensing imaging geometry representation onto the rotation manifold for both two-view and multi-view scenarios.

Result: The method demonstrates superior accuracy and robustness over current state-of-the-art rotation estimation methods, achieving performance comparable to multiple bundle adjustment iterations.

Conclusion: This rotation-only approach contributes to more accurate, efficient and reliable 3D visual computing by simplifying the optimization problem while maintaining high performance.

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [116] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MFI-ResNet combines ResNet with flow matching to improve parameter efficiency while maintaining accuracy, reducing parameters by ~46% while slightly improving performance on CIFAR datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge generative modeling and discriminative learning by viewing ResNet as discretized ODEs and leveraging flow matching's mean velocity field concept to improve ResNet's parameter efficiency and performance.

Method: Compression-expansion strategy: compression phase simplifies multi-layer ResNet stages to 1-2 MeanFlow modules; expansion phase selectively incubates first three stages to match baseline ResNet configuration while keeping last stage in MeanFlow form, followed by fine-tuning.

Result: On CIFAR-10 and CIFAR-100, reduces parameters by 46.28% and 45.59% compared to ResNet-50 while improving accuracy by 0.23% and 0.17% respectively.

Conclusion: Generative flow-fields can effectively characterize ResNet's feature transformation, providing new insights into the relationship between generative modeling and discriminative learning.

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [117] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM is a diffusion-based model that simultaneously removes rain artifacts and enhances image details for small object detection, addressing the conflict between weather removal and super-resolution.


<details>
  <summary>Details</summary>
Motivation: Real-world images are often degraded by adverse weather, and existing weather restoration methods sacrifice high-frequency details critical for analyzing small objects. Simply cascading restoration and super-resolution creates conflicts between noise removal and detail enhancement.

Method: Proposes DHGM, a Diffusion-based High-frequency Guided Model that integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details.

Result: Extensive experiments demonstrate that DHGM achieves superior performance over existing methods with lower computational costs.

Conclusion: DHGM effectively bridges the gap between weather removal and super-resolution, generating clean and high-resolution images suitable for small object detection tasks.

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [118] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: This paper develops a real-time driver drowsiness detection system using deep convolutional neural networks (DCNNs) and OpenCV to analyze facial landmarks like eye openings and mouth movements, achieving high accuracy rates of 99.6% and 97% on different datasets.


<details>
  <summary>Details</summary>
Motivation: Long road trips can cause driver drowsiness, which poses serious safety risks. The research aims to create a real-time detection system to prevent accidents caused by drowsy driving.

Method: The system uses live camera footage processed through OpenCV to detect facial landmarks (eye openings and yawn-like mouth movements), then applies DCNNs with pre-trained models to classify drowsiness states.

Result: The proposed model achieved 99.6% accuracy on the NTHU-DDD dataset and 97% accuracy on the Yawn-Eye-Dataset for drowsiness detection classification.

Conclusion: The system provides a non-invasive, inexpensive, and cost-effective solution for real-time drowsiness detection that can be embedded in Smart Car technology, potentially saving lives on roadways.

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [119] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0 is a dynamic modality-balanced multimodal framework for e-commerce product understanding that addresses modality imbalance, underutilized alignment relationships, and data noise through Modality-driven MoE, Dual-level Alignment, and Image-text Co-augmentation with Dynamic Sample Filtering.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in e-commerce multimodal understanding: modality imbalance from mixed training, underutilization of intrinsic visual-textual alignment within products, and limited handling of noise in e-commerce multimodal data.

Method: Proposes MOON2.0 framework with: (1) Modality-driven Mixture-of-Experts for adaptive processing and Multimodal Joint Learning; (2) Dual-level Alignment for semantic alignment; (3) MLLM-based Image-text Co-augmentation with Dynamic Sample Filtering. Also introduces MBE2.0 benchmark.

Result: Achieves state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Attention-based heatmap visualization shows improved multimodal alignment.

Conclusion: MOON2.0 effectively addresses key challenges in e-commerce multimodal understanding through balanced modality processing, enhanced alignment, and noise handling, demonstrating superior performance and improved multimodal alignment.

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [120] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: RedVTP is a response-driven visual token pruning method for Diffusion Vision-Language Models that improves inference efficiency by pruning less important visual tokens after the first inference step, achieving up to 186% throughput improvement and 64.97% latency reduction without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion Vision-Language Models (DVLMs) enable parallel token decoding but suffer from high computational demands due to large numbers of visual tokens. While visual token pruning has been studied for autoregressive VLMs, it remains unexplored for DVLMs, creating an opportunity to improve their inference efficiency.

Method: RedVTP estimates visual token importance using attention from masked response tokens and prunes less important visual tokens after the first inference step, leveraging the observation that importance scores remain consistent across steps.

Result: RedVTP improves token generation throughput by up to 186% for LLaDA-V and 28.05% for LaViDa, and reduces inference latency by up to 64.97% and 21.87% respectively, while maintaining or even improving accuracy.

Conclusion: The proposed RedVTP method effectively addresses the computational inefficiency of DVLMs through response-driven visual token pruning, achieving significant speedup without compromising model performance, making DVLMs more practical for real-world applications.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [121] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: MaskAnyNet treats masked image regions as auxiliary knowledge rather than discarding them, using a relearning mechanism to exploit both visible and masked information for improved feature diversity and fine-grained detail preservation.


<details>
  <summary>Details</summary>
Motivation: Traditional image masking underutilizes discarded pixels and may remove critical features, while MIM shows masked regions can be reconstructed and contain valuable contextual information with semantic diversity.

Method: Proposes MaskAnyNet with a relearning mechanism that combines masking and treats masked content as auxiliary knowledge, adding an additional branch to jointly learn from recomposed masked regions.

Result: Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks, with improved semantic diversity through masked content reuse.

Conclusion: Masked regions should be treated as valuable auxiliary knowledge rather than ignored, as they provide semantic diversity that enriches features and preserves fine-grained details.

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [122] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: UP-Fusion is a unified multi-modality image fusion framework that addresses gradient conflicts and modality differences through semantic-aware channel pruning, geometric affine modulation, and text-guided channel perturbation to enhance fusion quality while maintaining generalization.


<details>
  <summary>Details</summary>
Motivation: Existing unified models for multi-modality image fusion suffer from gradient conflicts due to large modality differences, while methods using modality-specific encoders reduce generalization across different fusion tasks.

Method: Proposes UP-Fusion with three key modules: Semantic-Aware Channel Pruning Module (SCPM) for filtering redundant features using pre-trained model knowledge, Geometric Affine Modulation Module (GAM) for maintaining modal discriminability through affine transformations, and Text-Guided Channel Perturbation Module (TCPM) for reshaping channel distribution during decoding.

Result: Extensive experiments show the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

Conclusion: The UP-Fusion framework successfully addresses limitations of existing unified models by suppressing redundant modal information, maintaining feature encoder discriminability, and reducing dependence on modality-specific channels, achieving superior performance in fusion tasks.

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [123] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: TopoFG is a fine-grained lane topology reasoning framework that uses hierarchical priors, region-focused decoding, and boundary-point topology reasoning to accurately model complex lane structures for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing methods represent lanes with single queries and struggle with complex lane structures, leading to unreliable topology predictions that are critical for autonomous driving navigation and control.

Method: Divides the process into three phases: Hierarchical Prior Extractor (HPE) for spatial and sequential priors, Region-Focused Decoder (RFD) for fine-grained query construction, and Robust Boundary-Point Topology Reasoning (RBTR) with topological denoising.

Result: Achieves state-of-the-art performance on OpenLane-V2 benchmark with OLS of 48.0% on subsetA and 45.4% on subsetB.

Conclusion: By integrating spatial and sequential priors into fine-grained queries and applying denoising to boundary-point topology, TopoFG precisely models complex lane structures and delivers trustworthy topology predictions.

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [124] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: CoTBox-TTT is a test-time training approach that adapts vision-language models at inference by updating soft prompts, using visual chain-of-thought to identify relevant regions and ensure answer consistency across original and localized images.


<details>
  <summary>Details</summary>
Motivation: Address reliability gap in medical VQA where models fail under domain shift and produce weakly grounded answers due to attending to spurious regions, when retraining or additional labels are impractical at deployment.

Method: Evidence-first test-time training approach that adapts vision-language models at inference while keeping all backbones frozen. Updates only small set of continuous soft prompts, identifies question-relevant regions through visual chain-of-thought signal, and encourages answer consistency across original image and localized crop.

Result: Practical for real deployments - adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA. The approach is label-free and plug-and-play with diverse backbones.

Conclusion: CoTBox-TTT effectively addresses domain shift in medical VQA through test-time adaptation with frozen backbones, improving reliability and accuracy without requiring retraining or additional labels.

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [125] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: C3Net is a novel dual-pathway decoder architecture for camouflaged object detection that addresses six fundamental challenges through specialized components: Edge Refinement Pathway for boundary recovery and Contextual Localization Pathway for saliency suppression, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Camouflaged object detection is challenging because objects blend seamlessly with surroundings through similar colors, textures, and patterns. Traditional segmentation methods and modern foundation models fail dramatically on camouflaged objects due to six fundamental challenges that frequently co-occur and compound detection difficulty.

Method: C3Net uses a specialized dual-pathway decoder architecture: 1) Edge Refinement Pathway with gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features, and 2) Contextual Localization Pathway with Image-based Context Guidance mechanism for intrinsic saliency suppression without external models. An Attentive Fusion Module combines both pathways via spatial gating.

Result: State-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K datasets, while maintaining efficient processing.

Conclusion: Complex, multifaceted detection challenges require architectural innovation with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. C3Net demonstrates that addressing all six fundamental challenges through integrated architectural solutions enables superior camouflaged object detection.

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [126] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace is a diffusion transformer framework that uses unified tokenization and multivariate transformer blocks for effective multimodal facial generation from semantic masks and text, with a decoupled attention mechanism that reduces computational overhead by over 94% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Conventional feature fusion approaches fail to enable effective cross-modal interactions between semantic masks and text descriptions, leading to suboptimal facial generation outcomes.

Method: Uses unified tokenization strategy for semantic masks and text inputs, stacked multivariate transformer blocks for synchronous condition processing, and a novel decoupled attention mechanism that separates mask tokens from temporal embeddings into dynamic and static pathways.

Result: Significantly outperforms competing methods in facial fidelity and conditional consistency while reducing computational overhead introduced by mask condition by over 94%.

Conclusion: MDiTFace enables comprehensive multimodal feature interaction through its transformer-based architecture and efficient attention mechanism, achieving superior facial generation quality with reduced computational cost.

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [127] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate is an audio-driven annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets by allowing annotators to narrate observations while synchronously linking spoken phrases to visual regions.


<details>
  <summary>Details</summary>
Motivation: Current multimodal training datasets rely on sparse annotations from internet mining or manual typing, which capture only a fraction of visual content. Traditional text-based annotation pipelines limit expressiveness, slow annotation speed, and underrepresent nuanced visual features, especially in specialized domains like multicultural imagery and 3D assets.

Method: An audio-driven online annotation platform where annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. The platform incorporates speech-to-text transcription and region-of-attention marking.

Result: Created a human-annotated multimodal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects with audio-aligned dense annotations in 20 languages. Models trained on this dataset showed improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.

Conclusion: DenseAnnotate offers a feasible approach for creating high-quality dense annotations that significantly improve model performance across multiple domains, demonstrating its applicability to various vision-language research tasks and diverse data types.

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [128] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: BridgeEQA is a new embodied question answering benchmark for infrastructure inspection using real bridge data, featuring 2,200 questions across 200 bridges with professional inspection reports and NBI ratings. The paper also proposes EMVR, a navigation-based reasoning method that outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Current embodied agents struggle with real-world question answering due to lack of benchmarks that capture practical conditions. Infrastructure inspection is chosen as it requires multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while providing standardized evaluation metrics.

Method: The paper introduces BridgeEQA dataset with 2,200 open-vocabulary QA pairs grounded in professional bridge inspection reports. It also proposes Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph using Markov decision processes.

Result: Evaluations show substantial performance gaps in state-of-the-art vision-language models under episodic memory EQA settings. EMVR demonstrates strong performance over baselines, effectively handling the complex reasoning required for bridge inspection tasks.

Conclusion: BridgeEQA provides a challenging benchmark for embodied question answering in real-world settings, and EMVR offers an effective approach for sequential visual reasoning in infrastructure inspection scenarios. Both dataset and code are publicly released.

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [129] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: A framework combining LLMs and grid-based integer programming for automated interior design that jointly optimizes room layout and furniture placement from text prompts.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing two-stage design pipelines by enabling joint optimization of room layouts and furniture arrangements through structured constraint extraction from natural language prompts.

Method: Uses LLM-driven agent workflow to extract design constraints, encodes them into unified grid-based representation inspired by Modulor, and employs coarse-to-fine optimization strategy with integer programming.

Result: Significantly outperforms existing two-stage design pipelines in solution quality and achieves notable computational efficiency through the coarse-to-fine optimization approach.

Conclusion: The joint optimization framework effectively combines LLM-based constraint extraction with mathematical optimization for automated interior design, delivering superior results compared to sequential approaches.

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [130] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: RSeg is a training-free framework for robust out-of-distribution tumor segmentation that uses LLM-guided anatomical reasoning and statistical testing to reduce false positives without parameter updates.


<details>
  <summary>Details</summary>
Motivation: Foundation models for medical image segmentation struggle with out-of-distribution shifts, producing fragmented false positives on OOD tumors.

Method: Two-stage Reason-and-Reject process: 1) LLM-guided anatomical reasoning planner localizes organ anchors and generates multi-scale ROIs, 2) Statistical testing rejects candidates not significantly different from normal tissue using a frozen foundation model (BiomedParse).

Result: Substantially improves Dice, specificity, and sensitivity on multi-center and multi-modal tumor segmentation benchmarks compared to baselines and original foundation models.

Conclusion: RSeg provides robust OOD tumor segmentation without parameter updates, avoiding catastrophic forgetting and compatible with zero-update test-time augmentation.

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [131] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pl Halvorsen*

Main category: cs.CV

TL;DR: HEDGE is a unified framework for detecting hallucinations in vision-language models that combines visual perturbations, semantic clustering, and uncertainty metrics to evaluate multimodal reliability across different architectures.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are prone to hallucinations despite enabling open-ended visual question answering, creating a need for systematic detection methods to evaluate their reliability.

Method: HEDGE integrates controlled visual perturbations, semantic clustering (entailment- and embedding-based), and robust uncertainty metrics into a reproducible pipeline applicable across multimodal architectures.

Result: Evaluations show hallucination detectability varies by architecture - highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for restricted tokenization models (Med-Gemma). The VASE metric consistently provides the most robust hallucination signal, especially with embedding clustering and moderate sampling.

Conclusion: HEDGE provides a principled foundation for evaluating multimodal reliability by framing hallucination detection as a geometric robustness problem shaped by sampling scale, prompt structure, model architecture, and clustering strategy.

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [132] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: C3DFusion is a novel 3D fusion module that addresses the limitation of current camera-based 3D semantic scene completion methods in reconstructing out-of-frame areas by explicitly aligning 3D-lifted point features from current and historical frames.


<details>
  <summary>Details</summary>
Motivation: Existing temporal fusion methods focus mainly on enhancing in-frame regions but struggle with reconstructing critical out-of-frame areas near ego-vehicle sides, despite previous frames containing valuable contextual information about these unseen regions.

Method: Proposes Current-Centric Contextual 3D Fusion (C3DFusion) module with two complementary techniques: historical context blurring (suppresses noise from inaccurately warped historical point features) and current-centric feature densification (enhances current point features by increasing their volumetric contribution).

Result: Significantly outperforms state-of-the-art methods on SemanticKITTI and SSCBench-KITTI-360 datasets, and demonstrates robust generalization with notable performance gains when applied to other baseline models.

Conclusion: C3DFusion effectively addresses the out-of-frame reconstruction problem in 3D semantic scene completion through enhanced temporal fusion techniques, showing strong effectiveness and generalization capabilities.

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [133] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: Stable Diffusion embeddings can be viewed as point clouds in Wasserstein space rather than matrices, enabling optimal transport-based interpolation that produces smoother image transitions.


<details>
  <summary>Details</summary>
Motivation: The permutation-invariance property of Stable Diffusion with respect to CLIP embedding rows suggests these embeddings are better understood as point clouds in Wasserstein space rather than matrices in Euclidean space.

Method: Reframe interpolation between prompt embeddings as an optimal transport problem, computing geodesics (shortest paths) in Wasserstein space to capture more natural transitions through embedding space.

Result: Optimal transport-based interpolation produces smoother and more coherent intermediate images compared to standard interpolation methods when rendered by Stable Diffusion.

Conclusion: Viewing embeddings as point clouds in Wasserstein space better reflects and leverages the geometry of embedding space, leading to improved interpolation quality.

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [134] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: Proposes a neural network that directly maps images to visible 3D structure points, eliminating the need for image retrieval or search heuristics in camera relocalization.


<details>
  <summary>Details</summary>
Motivation: Existing structure-based relocalization methods rely on complex search heuristics or image retrieval, leading to elaborate pipelines and storage requirements that scale with scene size.

Method: Train a neural network to directly predict visible 3D structure points from a query image, reducing the 2D-3D correspondence search space without requiring image retrieval.

Result: Achieves localization accuracy comparable to state-of-the-art methods while requiring lower computational and storage footprint.

Conclusion: Direct mapping from images to visible scene structure via neural networks provides an efficient alternative to traditional relocalization pipelines.

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [135] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: A lightweight color harmonization method for AR using optimal transport theory, achieving real-time on-device performance with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Color harmonization is essential for seamless AR composites but current methods are too slow for real-time AR pipelines, requiring on-device solutions.

Method: Leverages classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map for efficient color transformation.

Result: Achieves best aggregated score on real composite AR images compared to state-of-the-art methods while supporting on-device inference.

Conclusion: Proposed MKL-Harmonizer enables real-time color harmonization for AR applications and includes a dedicated AR dataset for further research.

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [136] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: A blur-robust AI-generated image detection framework using teacher-student knowledge distillation to maintain detection accuracy under motion blur degradation.


<details>
  <summary>Details</summary>
Motivation: Most AI-generated image detectors struggle with real-world degradations like motion blur, which distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in practical settings.

Method: Uses teacher-student knowledge distillation with a frozen DINOv3 teacher trained on clean images to provide stable representations, then distills these features to a student trained on blurred images to maintain consistent detection under motion degradation.

Result: Achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability compared to existing methods.

Conclusion: The proposed framework effectively addresses the motion blur challenge in AIGI detection through knowledge distillation, enabling robust performance in real-world scenarios while maintaining accuracy on clean images.

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [137] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: A Multi-Scale Recursive Network (MSRNet) for camouflaged object detection that combines multi-scale features from Pyramid Vision Transformer with attention-based integration and recursive refinement to handle challenging scenarios with small and multiple objects.


<details>
  <summary>Details</summary>
Motivation: Current camouflaged object detection methods struggle with complex scenarios involving low-light conditions, partial occlusion, small objects, intricate backgrounds, and multiple objects, indicating need for improved detection precision.

Method: Uses Pyramid Vision Transformer backbone for multi-scale feature extraction, Attention-Based Scale Integration Units for selective feature merging, Multi-Granularity Fusion Units in decoder, and novel recursive-feedback decoding strategy for global context enhancement.

Result: Achieves state-of-the-art results on two benchmark datasets and ranks second on two others, successfully detecting small and multiple camouflaged objects with performance gains.

Conclusion: The proposed MSRNet effectively addresses challenges in camouflaged object detection through joint multi-scale learning and recursive feature optimization, demonstrating superior performance in complex scenarios.

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [138] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: Proposes MdaIF, a degradation-aware infrared and visible image fusion framework using LLM-driven mixture-of-experts to handle multiple weather degradation scenarios like haze, rain, and snow.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to account for visible image degradation under adverse weather conditions and use fixed architectures that limit adaptability to diverse degradation scenarios.

Method: Uses mixture-of-experts system guided by semantic prior from pre-trained VLM, with degradation-aware channel attention module and degradation prototype decomposition for multi-modal feature interaction.

Result: Extensive experiments show superior performance over state-of-the-art methods in complex degradation scenarios.

Conclusion: MdaIF effectively addresses multi-degradation scenarios in infrared-visible image fusion through LLM-driven adaptive framework with semantic prior guidance.

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [139] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA is a comprehensive framework for AI-generated video source attribution that identifies specific generative models across five granular levels, using a novel video transformer and achieving state-of-the-art performance with only 0.5% labeled data per class.


<details>
  <summary>Details</summary>
Motivation: The proliferation of hyper-realistic synthetic videos created by generative AI has escalated misuse risks and rendered traditional binary real/fake detectors insufficient, creating an urgent need for more sophisticated source attribution capabilities.

Method: SAGA uses a novel video transformer architecture that leverages features from a robust vision foundation model to capture spatio-temporal artifacts. It employs a data-efficient pretrain-and-attribute strategy and introduces Temporal Attention Signatures (T-Sigs) for interpretability.

Result: SAGA achieves state-of-the-art attribution performance using only 0.5% of source-labeled data per class, matching fully supervised performance. It demonstrates strong results on public datasets including cross-domain scenarios.

Conclusion: SAGA sets a new benchmark for synthetic video provenance by providing comprehensive, multi-granular source attribution with interpretable insights, offering crucial capabilities for forensic and regulatory applications in the era of generative AI.

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [140] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: D-VPR is a framework that combines knowledge distillation and deformable attention to reduce the computational overhead of DINOv2-based Visual Place Recognition while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the high model complexity and computational overhead of DINOv2-based VPR systems that impede deployment on resource-constrained devices, while retaining strong feature extraction capabilities.

Method: Two-stage training with knowledge distillation and fine-tuning, plus a Distillation Recovery Module for feature space alignment and a Top-Down-attention-based Deformable Aggregator for adaptive ROI adjustment.

Result: Achieves competitive performance with state-of-the-art methods while reducing parameters by ~64.2% and FLOPs by ~62.6% compared to CricaVPR.

Conclusion: The proposed D-VPR framework successfully balances performance and efficiency, making powerful VPR systems more deployable on resource-constrained devices.

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [141] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: Video-finetuned multimodal LLMs implicitly capture temporal reasoning between frames, while image-only models benefit significantly from explicit transitional cues (vCoT) for video understanding.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs extend from images to videos by naively concatenating frame tokens, lacking proper temporal reasoning. The paper investigates what video finetuning actually brings to these models.

Method: Proposed Visual Chain-of-Thought (vCoT) - an explicit reasoning process that generates transitional event descriptions between consecutive frames. Systematically compared image-only LVLMs with video-finetuned counterparts with and without vCoT.

Result: vCoT significantly improves image-only models on long-form video QA but yields only marginal gains for video-finetuned models. Video models transfer temporal reasoning ability to static settings, outperforming image models on relational visual reasoning.

Conclusion: Video-finetuned models already implicitly capture frame-to-frame transitions, demonstrating effective temporal reasoning that transfers to static visual reasoning tasks.

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [142] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: ReaSon is a framework that uses reinforcement learning and causal information bottleneck to select keyframes that are both informative and causally decisive for video understanding with VLMs.


<details>
  <summary>Details</summary>
Motivation: Keyframe selection is crucial for video understanding due to limited input tokens and temporal sparsity of relevant information across frames. Existing methods need to ensure frames are not just informative but also causally necessary.

Method: Proposes Causal Information Bottleneck (CIB) to define keyframes that satisfy predictive sufficiency and causal necessity. Uses a learnable policy network to select keyframes from candidate frames, assesses causal necessity via counterfactual interventions, and guides selection through reinforcement learning with a composite reward.

Result: Extensive experiments on NExT-QA, EgoSchema, and Video-MME show ReaSon consistently outperforms state-of-the-art methods under limited-frame settings.

Conclusion: ReaSon effectively addresses keyframe selection by combining causal reasoning with information bottleneck principles, demonstrating strong generalization ability across multiple video understanding benchmarks.

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [143] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: HiGFA is a hierarchical diffusion-based data augmentation method that uses strong text/contour guidance early and fine-grained classifier guidance late, with dynamic strength modulation based on confidence, to generate faithful synthetic images for fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models using text-based CFG lack specificity for fine-grained tasks, potentially generating misleading examples that degrade classifier performance due to inability to capture subtle category-defining features.

Method: Hierarchically Guided Fine-grained Augmentation (HiGFA) leverages diffusion temporal dynamics: uses strong text/contour guidance in early-mid stages for scene/style/structure, then activates fine-grained classifier guidance in final stages with dynamic strength modulation based on prediction confidence.

Result: Experiments on several FGVC datasets demonstrate HiGFA's effectiveness in generating diverse yet faithful synthetic images for fine-grained visual classification tasks.

Conclusion: HiGFA's hierarchical, confidence-driven orchestration successfully balances global structure formation with precise detail refinement, enabling accurate data augmentation for fine-grained tasks.

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [144] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport is the first end-to-end trained MLLM framework for multi-task, multi-sport video understanding that enables active, iterative reasoning through dynamic frame interrogation and achieves SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current sports video understanding approaches which are either single-sport centric, task-specific, or use training-free paradigms lacking robust reasoning processes.

Method: Proposes an end-to-end framework with active reasoning via frame-extraction tool, data distillation pipeline synthesizing 78k CoT trajectories from 10 sources, and two-stage training (SFT + RL with gated tool-use reward).

Result: Achieves state-of-the-art performance on 6.7k question benchmark, significantly outperforming both proprietary and open-source baseline models.

Conclusion: Establishes a new foundation for domain-specific video reasoning to handle the complexities of diverse sports through learned reasoning processes.

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [145] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse is a large-scale open-source dataset for visual emotion analysis that provides interpretable annotations through Background-Attribute-Subject triplets and dual categorical/dimensional emotion labels, enabling detailed emotional reasoning and attribution explanations.


<details>
  <summary>Details</summary>
Motivation: Current visual emotion analysis is limited by lack of open-source interpretable datasets and oversimplified single emotion labels per image, which don't reveal how visual elements contribute to emotions.

Method: Created EmoVerse dataset with 219k+ images using multi-layered knowledge-graph-inspired annotations (B-A-S triplets), dual CES/DES emotion labeling, and a multi-stage annotation pipeline for reliability with minimal human effort.

Result: Developed an interpretable model that maps visual cues to dimensional emotion space representations and provides detailed attribution explanations, forming a comprehensive foundation for explainable emotion understanding.

Conclusion: EmoVerse dataset, pipeline, and model together advance visual emotion analysis by enabling interpretable, fine-grained emotional reasoning and bridging the affective gap between visual content and human emotions.

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [146] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: PFAvatar is a two-stage method for high-quality 3D avatar reconstruction from OOTD photos using pose-aware diffusion fine-tuning and NeRF distillation, achieving 48x speed-up and superior detail preservation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in 3D avatar reconstruction from real-world OOTD photos with diverse poses, occlusions, and complex backgrounds, avoiding problematic image decomposition approaches used in previous methods.

Method: Two-stage approach: (1) Fine-tune pose-aware diffusion model with ControlNet and CPPL for full-body appearance modeling, (2) Distill 3D avatar using NeRF with canonical SMPL-X sampling and Multi-Resolution 3D-SDS for continuous representation.

Result: Achieves 48x speed-up (5 minutes personalization), outperforms SOTA in reconstruction fidelity, detail preservation, and occlusion robustness. Supports downstream applications like virtual try-on and animation.

Conclusion: PFAvatar advances practical 3D avatar generation from real-world OOTD albums through efficient full-body modeling and continuous radiance field representation, enabling versatile applications.

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [147] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: SEMC is a Structure-Enhanced Mixture-of-Experts Contrastive learning framework for ultrasound standard plane recognition that combines structure-aware feature fusion with expert-guided contrastive learning to address limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ultrasound standard plane recognition methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, resulting in suboptimal recognition performance.

Method: Proposes SEMC framework with two key modules: 1) Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and align shallow/deep features, and 2) Mixture-of-Experts Contrastive Recognition Module (MCRM) for hierarchical contrastive learning and classification using MoE mechanism.

Result: Extensive experiments on in-house liver ultrasound dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

Conclusion: SEMC effectively addresses the limitations of existing methods by combining structure-aware feature fusion with expert-guided contrastive learning, achieving superior performance in ultrasound standard plane recognition.

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [148] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI is a deep learning system for real-time gastrointestinal polyp detection and segmentation during endoscopy, achieving 88.3% mAP for detection and 69% Dice coefficient for segmentation with speeds over 35 FPS.


<details>
  <summary>Details</summary>
Motivation: Precise real-time detection of gastrointestinal polyps is crucial for early colorectal cancer diagnosis and prevention during endoscopic procedures.

Method: Deep learning architecture using Hyper-Kvasir dataset, incorporating clinically relevant metrics and novel thermal-aware procedure for robustness and efficiency.

Result: Achieved 88.3% mean Average Precision for polyp detection, 69% Dice coefficient for segmentation, and real-time inference speeds exceeding 35 FPS on GPU hardware.

Conclusion: The integrated AI solution enables seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [149] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: A novel method combining signal processing and machine learning to reconstruct surface temperatures through forest vegetation, enabling early wildfire detection using autonomous drones by recovering subtle thermal signals from blurred synthetic aperture data.


<details>
  <summary>Details</summary>
Motivation: Enable fully automated aerial wildfire monitoring using autonomous drones for early detection of ground fires before smoke or flames are visible, overcoming occlusion from forest canopy and thermal blur from synthetic aperture sensing.

Method: Train a visual state space model to recover thermal signals from blurred synthetic aperture data, using a latent diffusion model integrated with vector quantization to generate realistic surface temperature simulations from real wildfire recordings, expanded through temperature augmentation and procedural thermal forest simulation.

Result: On simulated data, reduced RMSE by 2-2.5x compared to conventional thermal and uncorrected SA imaging. In field experiments on high-temperature hotspots, achieved 12.8x RMSE gain over conventional thermal and 2.6x gain over uncorrected SA. Successfully reconstructed complete morphology of fire and human signatures.

Conclusion: The method effectively reconstructs surface temperatures through occluding vegetation, significantly outperforming conventional approaches and demonstrating generalization to other thermal signals like human signatures for search and rescue applications.

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [150] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: CalibrateMix improves semi-supervised learning model calibration using targeted mixup of easy and hard samples identified through training dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods suffer from poor calibration with overconfident predictions, and while mixup helps in supervised settings, it's challenging in SSL due to unreliable pseudolabels.

Method: Targeted mixup approach that identifies easy-to-learn and hard-to-learn samples using training dynamics, then mixes easy and hard samples strategically.

Result: Achieves lower expected calibration error (ECE) and superior accuracy across multiple benchmark image datasets compared to existing SSL approaches.

Conclusion: CalibrateMix effectively improves SSL model calibration while maintaining or enhancing classification accuracy through targeted sample mixing.

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [151] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: The paper proposes a typographical attack method that adds deceptive text to images to protect geo-privacy against Large Visual Language Models, achieving effective protection while preserving visual quality.


<details>
  <summary>Details</summary>
Motivation: Large Visual Language Models pose serious privacy threats by inferring users' geolocations from shared images, and existing adversarial perturbations degrade visual quality too much.

Method: A two-stage, semantics-aware typographical attack that generates deceptive text extensions outside the visual content to disrupt geolocation inference.

Result: Extensive experiments show the approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs across three datasets.

Conclusion: The typographical attack establishes a practical and visually-preserving protection strategy against emerging geo-privacy threats from LVLMs.

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [152] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: UNSEEN is a plug-and-play framework that improves dataset pruning by scoring samples based on models not exposed to them during training, addressing limitations of fitting-centric approaches where score concentration reduces sample distinction.


<details>
  <summary>Details</summary>
Motivation: Existing dataset pruning methods rely on fitting-centric approaches where sample scores become dense and less distinguishable as models achieve near-optimal performance on training data, hindering effective selection.

Method: Proposes UNSEEN framework that scores samples from generalization perspective using models not exposed to them during training, with multi-step scaling and incremental selection through models trained on varying coresets.

Result: Significantly outperforms SOTA methods on CIFAR-10, CIFAR-100, and ImageNet-1K, achieving lossless performance on ImageNet-1K while reducing training data by 30%.

Conclusion: Generalization-based scoring and multi-step incremental selection effectively address limitations of conventional fitting-centric dataset pruning methods, enabling more effective coreset construction.

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [153] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster is a novel framework for long video generation that formulates the task as next-frame-rate prediction, generating low-frame-rate clips first and progressively increasing frame rates to refine details and motion.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating long videos with both temporal coherence and visual quality, overcoming limitations in existing methods that struggle with maintaining consistency over extended sequences.

Method: Generate low-frame-rate clips as coarse blueprints, then progressively increase frame rate using bidirectional attention within each level and autoregression across frame rates for efficient parallel synthesis.

Result: Extensive experiments show TempoMaster establishes new state-of-the-art in long video generation, excelling in both visual quality and temporal coherence.

Conclusion: The proposed next-frame-rate prediction approach with progressive refinement enables effective long video generation with superior temporal and visual quality compared to existing methods.

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [154] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: SAGE is a training-free method that mitigates multimodal spurious biases in CLIP models by selecting prompts that maximize semantic separation between classes, improving zero-shot robustness without fine-tuning or external knowledge.


<details>
  <summary>Details</summary>
Motivation: CLIP models develop multimodal spurious biases where they rely on spurious features (like backgrounds) rather than core object features, impairing robustness on out-of-distribution data. Existing methods require fine-tuning or prior bias knowledge, undermining CLIP's out-of-the-box usability.

Method: SAGE (Spuriousness-Aware Guided Exploration) explores prompt template spaces and selects prompts that induce the largest semantic separation between classes, without requiring training, fine-tuning, or external annotations.

Result: Extensive experiments on four benchmark datasets and five backbone models show SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without external knowledge or model updates.

Conclusion: SAGE effectively mitigates multimodal spurious bias in CLIP models through guided prompt selection, enhancing zero-shot robustness while maintaining the model's out-of-the-box usability without requiring training or external annotations.

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [155] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: CountIHC is a novel framework for multi-class cell counting in IHC images that selectively distills knowledge from foundation models using rank-aware teacher selection and reformulates counting as vision-language alignment with semantic anchors.


<details>
  <summary>Details</summary>
Motivation: Accurate cell counting in IHC images is critical for cancer diagnosis but challenging due to chromogen overlap, variable staining, and diverse morphologies. Current regression methods rarely support end-to-end multi-class counting and underutilize foundation models.

Method: Proposes rank-aware agglomeration with RATS strategy for sample-wise teacher selection based on global-to-local patch rankings. For multi-class counting, uses vision-language alignment with discrete semantic anchors from structured text prompts to guide class-specific density map regression.

Result: CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, showing high agreement with pathologists' assessments. Also effective on H&E-stained data, confirming scalability.

Conclusion: The proposed framework effectively addresses IHC heterogeneity through selective knowledge distillation and vision-language alignment, achieving superior multi-class cell counting performance with strong clinical relevance and broad applicability.

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [156] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: Efficient training and sampling scheme for MeanFlow in Representation Autoencoder latent space using Consistency Mid-Training and two-stage distillation, achieving better performance with significantly reduced computation costs.


<details>
  <summary>Details</summary>
Motivation: MeanFlow training is computationally demanding and unstable, SD-VAE decoder dominates generation cost, and complex guidance hyperparameters are needed for class-conditional generation.

Method: Use RAE latent space with pre-trained vision encoder, adopt Consistency Mid-Training for trajectory-aware initialization, and employ two-stage scheme: distillation from pre-trained flow matching teacher followed by optional bootstrapping with one-point velocity estimator.

Result: Achieves 1-step FID of 2.03 (vs vanilla MF's 3.43), reduces sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. Scales to ImageNet 512 with 1-step FID of 3.23 and lowest GFLOPS among baselines.

Conclusion: The proposed method enables efficient and stable MeanFlow training in RAE latent space, removing need for guidance, simplifying training configurations, and significantly reducing computation in both training and sampling.

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [157] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: SpectralAdapt is a semi-supervised domain adaptation framework that bridges the domain gap between general and human-centered HSI datasets for hyperspectral image reconstruction, addressing data scarcity in medical applications.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging has great potential for healthcare but faces challenges due to costly data acquisition and scarcity of human HSI data, limiting progress in medical applications.

Method: Proposes SpectralAdapt with two key components: Spectral Density Masking (SDM) that adaptively masks RGB channels based on spectral complexity, and Spectral Endmember Representation Alignment (SERA) that uses physically interpretable endmembers as domain-invariant anchors.

Result: Experiments show consistent improvements in spectral fidelity, cross-domain generalization, and training stability on benchmark datasets.

Conclusion: SSDA presents an efficient solution for hyperspectral imaging in healthcare by effectively mitigating domain shift, spectral degradation, and data scarcity.

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [158] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR is a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem, replacing discriminative learning with latent learning and achieving state-of-the-art performance across various segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive modeling (VAR) strategies have shown promise in image generation but their potential for segmentation tasks requiring precise spatial perception remains unexplored. The authors aim to bridge this gap by applying autoregressive reasoning to segmentation.

Method: Seg-VAR incorporates three core components: (1) image encoder generating latent priors, (2) spatial-aware seglat encoder mapping masks into discrete latent tokens using location-sensitive color mapping, and (3) decoder reconstructing masks. Uses multi-stage training: learning seglat representations, refining latent transformations, and aligning image-encoder latents with seglat distributions.

Result: Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks, demonstrating superior performance.

Conclusion: By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems, showing the potential of autoregressive modeling for precise spatial perception tasks.

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [159] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: The paper introduces the RFxG taxonomy to organize saliency explanations along Reference-Frame (pointwise vs contrastive) and Granularity (class-level vs group-level) axes, revealing limitations in existing evaluation metrics and proposing four novel faithfulness metrics for comprehensive assessment.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental lack of consensus in saliency map purposes and their alignment with diverse user queries, which hinders effective evaluation and practical utility of explanation methods.

Method: Proposed Reference-Frame  Granularity (RFxG) taxonomy framework, developed four novel faithfulness metrics, and conducted comprehensive evaluation of ten saliency methods across four model architectures and three datasets.

Result: Demonstrated critical limitations in existing evaluation metrics that prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. The RFxG framework provides systematic assessment across both dimensions.

Conclusion: Advocates a shift toward user-intent-driven evaluation, providing both conceptual foundation and practical tools to develop visual explanations that are faithful to model behavior and aligned with human understanding complexity.

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [160] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: A novel teacher-student framework using CNN teacher and ViT student with LoRA fine-tuning for efficient single-image morphing attack detection, achieving superior performance and computational efficiency compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Face Recognition Systems are vulnerable to morphing attacks where synthetic images blend biometric features from multiple individuals, creating security risks that need effective detection methods.

Method: Proposes a teacher-student framework with CNN-based teacher model refining ViT-based student model, integrated with Low-Rank Adaptation (LoRA) for efficient fine-tuning to reduce computational costs while maintaining accuracy.

Result: Extensive experiments on morphing dataset from three public face datasets with ten morphing algorithms show superior detection performance and computational efficiency compared to six state-of-the-art S-MAD techniques.

Conclusion: The proposed approach effectively detects morphing attacks with high accuracy and computational efficiency, addressing critical security vulnerabilities in Face Recognition Systems.

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [161] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: ILA is a black-box adversarial attack framework that manipulates indoor lighting to disrupt Vision-and-Language Navigation agents, revealing their vulnerability to realistic lighting variations.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial evaluations use unrealistic perturbations like unusual textures, while indoor lighting is an intrinsic scene attribute that strongly influences navigation but has been largely overlooked.

Method: Proposes Indoor Lighting-based Adversarial Attack (ILA) with two modes: Static (SILA) with constant lighting intensity, and Dynamic (DILA) with lights switched on/off at critical moments to create abrupt illumination changes.

Result: ILA significantly increases failure rates while reducing trajectory efficiency across two state-of-the-art VLN models and three navigation tasks, revealing previously unrecognized vulnerabilities.

Conclusion: VLN agents are vulnerable to realistic indoor lighting variations, highlighting the need for more robust navigation systems that can handle common environmental changes like lighting.

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [162] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: SoccerNet-GAR is a multimodal dataset for group activity recognition that enables direct comparison between video and tracking modalities, showing tracking-based methods with role-aware graph networks outperform video approaches while being more efficient.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks for comparing video and tracking modalities in group activity recognition, and understand which modality leads to better performance.

Method: Created SoccerNet-GAR dataset with synchronized broadcast video and player tracking data from 64 World Cup matches (94,285 activities). Developed role-aware graph neural network for tracking data that encodes tactical structure through positional edges and temporal attention.

Result: Tracking model achieved 67.2% balanced accuracy vs 58.1% for best video baseline, while training 4.25x faster with 438x fewer parameters (197K vs 86.3M).

Conclusion: Tracking-based approaches with role-aware modeling are more effective and efficient than video-based methods for group activity recognition, highlighting the importance of modality choice.

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [163] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: This paper explores using computer vision for road distress segmentation, evaluating GAN-generated synthetic data and comparing CNN vs transformer-based models, with MaskFormer showing superior performance.


<details>
  <summary>Details</summary>
Motivation: America's infrastructure is graded poorly (C for overall, D for roads), and current road inspection methods are inefficient, costly manual/laser-based approaches. With autonomous vehicles providing real-time visual data, there's an opportunity to use computer vision for better road monitoring.

Method: Evaluated synthetic data generated with GANs for model training, applied Convolutional Neural Networks (CNNs) for road distress segmentation, and examined transformer-based MaskFormer model.

Result: GAN-generated data improved model performance, and MaskFormer outperformed CNN models in mAP50 and IoU metrics.

Conclusion: Computer vision methods, particularly transformer-based models like MaskFormer combined with synthetic data augmentation, offer promising solutions for efficient road distress monitoring and infrastructure management.

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [164] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: Proposes a Hierarchical Ladder Network with Attention Affine Network for test-time adaptation to handle out-of-distribution samples and domain shifts, improving classification performance.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods suffer performance drops when encountering OOD samples, which can be misclassified as in-distribution classes, degrading accuracy and adaptation process.

Method: Uses Hierarchical Ladder Network to extract OOD features from Transformer layers, combines predictions via weighted probability fusion, employs Attention Affine Network to refine self-attention for domain adaptation, and uses weighted entropy to suppress low-confidence samples.

Result: Experimental results on benchmark datasets show significant performance improvement on widely used classification datasets.

Conclusion: The proposed method effectively enhances TTA performance by better handling OOD samples and domain shifts through hierarchical feature extraction and adaptive attention mechanisms.

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [165] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: SOMA is a dense registration framework that integrates structural gradient priors into deep features and uses a hybrid matching strategy to improve SAR-Optical image registration, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: SAR and optical images have fundamentally different imaging mechanisms and visual characteristics, making pixel-level registration challenging. Deep learning methods have underperformed in this task, and traditional gradient-based information hasn't been effectively leveraged in deep learning frameworks for SAR-Optical matching.

Method: Proposes SOMA framework with two key components: Feature Gradient Enhancer (FGE) that embeds multi-scale, multi-directional gradient filters into feature space using attention and reconstruction mechanisms, and Global-Local Affine-Flow Matcher (GLAM) that combines affine transformation and flow-based refinement in a coarse-to-fine architecture.

Result: Significantly improves registration precision, increasing CMR@1px by 12.29% on SEN1-2 dataset and 18.50% on GFGE_SO dataset. Exhibits strong robustness and generalizes well across diverse scenes and resolutions.

Conclusion: SOMA effectively addresses the SAR-Optical registration challenge by integrating structural gradient priors into deep features and using a hybrid matching strategy, demonstrating superior performance and generalization capabilities.

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [166] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: GeoX-Bench is a comprehensive benchmark for evaluating large multimodal models on cross-view geo-localization and pose estimation, containing 10,859 panoramic-satellite image pairs and 755,976 QA pairs across 128 cities in 49 countries.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the capabilities of large multimodal models in cross-view geo-localization and pose estimation domains, which remain unexplored despite potential benefits for navigation, autonomous driving, and outdoor robotics.

Method: Created GeoX-Bench with 10,859 panoramic-satellite image pairs and 755,976 QA pairs, then evaluated 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and explored instruction-tuning capabilities.

Result: Current LMMs achieve impressive performance in geo-localization tasks but their effectiveness declines significantly on more complex pose estimation tasks. Instruction-tuning LMMs on GeoX-Bench training data can significantly improve cross-view geo-sense abilities.

Conclusion: GeoX-Bench reveals that while LMMs perform well on geo-localization, pose estimation remains a critical area needing improvement, and instruction-tuning can enhance cross-view geo-sense capabilities.

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [167] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: A computer vision framework for group activity recognition using Mask R-CNN for actor localization, multiple backbone networks for feature extraction, and Actor Relation Graphs with Graph Convolutional Networks to model interactions and predict individual and group activities.


<details>
  <summary>Details</summary>
Motivation: Group activity detection is challenging due to complex human interactions, occlusions, and appearance variations over time in multi-person scenes.

Method: Uses Mask R-CNN for actor localization with bounding boxes and masks, multiple backbones (Inception V3, MobileNet, VGG16) for feature extraction with RoIAlign, fuses mask information with features, constructs Actor Relation Graphs using appearance similarity and positional relations, and applies Graph Convolutional Networks for relationship reasoning.

Result: Experiments on Collective Activity dataset show improved recognition performance across crowded and non-crowded scenarios through mask-based feature refinement, robust similarity search, and graph neural network reasoning.

Conclusion: The approach demonstrates the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [168] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: The paper introduces Denoising-VAE, a ViT-based autoencoder that addresses the optimization dilemma in high-dimensional latent spaces by using spectral self-regularization to suppress redundant high-frequency noise, improving both reconstruction quality and generative performance.


<details>
  <summary>Details</summary>
Motivation: VAEs face a trade-off where higher-dimensional latent spaces improve reconstruction but degrade generative performance. Existing methods use external vision foundation models for regularization, but it's unclear how high-dimensional latents affect generative model optimization.

Method: Proposed spectral self-regularization strategy to suppress redundant high-frequency noise in high-dimensional latent spaces while preserving reconstruction quality. Also introduced spectral alignment strategy for optimizing Denoising-VAE-based generative models.

Result: Denoising-VAE achieves state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on ImageNet 256256 benchmark. Enables diffusion models to converge 2 faster than with SD-VAE.

Conclusion: The analysis reveals that redundant high-frequency components in high-dimensional latent spaces hinder diffusion model convergence. The proposed spectral self-regularization effectively addresses this issue, producing cleaner latents that improve both reconstruction and generation quality without relying on external vision foundation models.

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [169] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefana Mata,Francisco Filizzola,Kevin Wignall,Luca Franco Troilo,Mara de los Angeles Cenoz,Melissa Thompson,Mercedes Legua,Ignacio Larrabide,Jos Ignacio Orlando*

Main category: cs.CV

TL;DR: A hybrid semi-supervised learning approach for retinal image quality assessment that combines manual labels for overall quality with pseudo-labels of quality details in a multi-task framework, improving interpretability without extensive manual labeling.


<details>
  <summary>Details</summary>
Motivation: Most retinal image quality assessment tools only classify overall image quality without indicating specific acquisition defects to guide recapture, mainly due to the high cost of detailed annotations.

Method: Uses a hybrid semi-supervised approach combining manual labels for overall quality with pseudo-labels of quality details in a multi-task framework. Pseudo-labels are generated by a Teacher model trained on a small dataset and used to fine-tune a pre-trained model with ResNet-18 backbone.

Result: Outperformed single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. Multi-task model achieved performance statistically comparable to Teacher for most detail prediction tasks (p > 0.05). Performed similarly to experts on newly annotated EyeQ subset.

Conclusion: The semi-supervised approach improves overall quality assessment while providing interpretable feedback on capture conditions (illumination, clarity, contrast), enhancing interpretability at no extra manual labeling cost and offering clinically actionable outputs to guide image recapture.

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [170] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP is a prompt tuning method that incorporates Large Language Models (LLMs) to transfer medical knowledge into Vision-Language Models (VLMs), enabling more precise disease-specific feature extraction and instance-adaptive prompts for medical image classification.


<details>
  <summary>Details</summary>
Motivation: Existing prompt tuning methods cannot effectively distinguish different medical concepts and miss disease-specific features across various medical imaging modalities. LLMs possess specialized medical knowledge that can enhance VLM performance in medical tasks.

Method: CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and uses them to create disease-specific prompts. It incorporates a conditional mechanism to generate instance-adaptive prompts based on individual medical images.

Result: Extensive experiments across diverse medical image datasets show that CILMP consistently outperforms state-of-the-art prompt tuning methods.

Conclusion: CILMP effectively bridges LLMs and VLMs to transfer medical knowledge, demonstrating superior performance in medical image classification tasks compared to existing prompt tuning approaches.

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [171] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: The paper proposes gDDCM, a generalized version of DDCM that extends image compression capabilities to various diffusion models including DDPM, Score-Based Models, Consistency Models, and Rectified Flow, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: DDCM was limited to DDPM only and couldn't be applied to other diffusion models, creating a need for a more generalized approach to enable image compression across different diffusion frameworks.

Method: Proposed generalized Denoising Diffusion Compression Model (gDDCM) that extends DDCM's concept of replacing random noise with specific noise sets to mainstream diffusion models and their variants.

Result: Successfully generalized DDCM to multiple diffusion models and achieved improved performance on CIFAR-10 and LSUN Bedroom datasets.

Conclusion: gDDCM effectively extends image compression capabilities beyond DDPM to various diffusion models, demonstrating broader applicability and enhanced performance.

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [172] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++ is a hierarchical quantization optimization framework that combines heterogeneous precision design and GPU-native kernel fusion to significantly improve computational efficiency of deep visual SLAM systems while maintaining trajectory accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based Visual SLAM systems have excellent geometric reasoning but suffer from prohibitive computational overhead that limits deployment on resource-constrained autonomous platforms.

Method: Hierarchical quantization optimization framework with: 1) learnable scale parameterization, 2) heterogeneous precision design (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and 3) GPU-native kernel fusion for fake quantization using custom CUDA kernels.

Result: On TartanAir: 52.1% FPS increase, 29.1% latency reduction, 64.9% GPU memory reduction. On EuRoC: 30.1% FPS increase, 23.1% latency reduction, 37.7% GPU memory reduction - all while maintaining comparable trajectory accuracy (ATE) to original DPVO model.

Conclusion: DPVO-QAT++ effectively bridges the gap between high-precision deep visual odometry and practical deployment efficiency requirements, offering a viable engineering paradigm for real-world embedded platforms.

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [173] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: DTPQA is a Visual Question Answering benchmark designed to evaluate Vision-Language Models' perception capabilities in traffic scenarios, with distance annotations to analyze performance degradation at long ranges.


<details>
  <summary>Details</summary>
Motivation: To ensure VLMs can be trusted in safety-critical automated driving applications by testing their robust perception capabilities, especially at long distances where critical objects appear in traffic scenes.

Method: Created a VQA benchmark with two parts: DTP-Synthetic using a simulator and DTP-Real using existing real traffic images, with each sample containing an image, question, ground truth answer, and object distance annotation.

Result: The dataset enables systematic evaluation of how VLM perception performance degrades with increasing object distance from the camera in traffic scenarios.

Conclusion: DTPQA provides a specialized benchmark to isolate and evaluate VLM perception capabilities in driving contexts, with distance-based performance analysis and tools for generating additional data.

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [174] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: FSTS is a Fourier Series-based Tampering Synthesis framework that generates realistic tampered text images by modeling real-world tampering behaviors, improving generalization in Text Image Forgery Localization.


<details>
  <summary>Details</summary>
Motivation: Existing T-IFL methods suffer from poor generalization due to limited real-world datasets and synthetic data that doesn't capture real tampering complexity.

Method: Collects 16,750 real tampering instances, analyzes editing traces, and models parameters using a hierarchical Fourier series-inspired framework with basis functions and learned weights.

Result: Models trained with FSTS data achieve significantly improved generalization across four evaluation protocols on real-world datasets.

Conclusion: FSTS provides an interpretable framework for synthesizing realistic training data that better reflects real-world forgery traces, addressing the generalization gap in text image forgery localization.

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [175] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS is a novel Scene Text Editing framework that achieves triple feature disentanglement (text style, content, and background) using SCB Groups and contrastive regularization, enabling flexible text editing with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous STE methods struggled with incomplete attribute disentanglement, typically addressing only one aspect like text content editing, which limited controllability and visual consistency.

Method: Proposes TripleFDS framework with SCB Synthesis dataset using SCB Groups for triple feature disentanglement, employing inter-group contrastive regularization and intra-sample multi-feature orthogonality, with feature remapping during synthesis.

Result: Achieves state-of-the-art image fidelity (SSIM 44.54) and text accuracy (ACC 93.58%) on mainstream STE benchmarks, trained on 125,000 SCB Groups.

Conclusion: TripleFDS enables more flexible editing operations like style replacement and background transfer while maintaining superior performance compared to previous methods.

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [176] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: A real-time conversational digital human system combining realistic 3D avatars, expressive speech synthesis, and knowledge-grounded dialogue with asynchronous pipeline for low-latency interaction.


<details>
  <summary>Details</summary>
Motivation: Achieving both visual realism and real-time responsiveness for digital humans in interactive applications remains challenging despite increasing use in communication, education, and entertainment.

Method: Asynchronous execution pipeline coordinating multi-modal components, retrieval-augmented methods with history augmentation and intent-based routing, wake word detection, emotionally expressive prosody, and context-aware response generation.

Result: Integrated system enabling responsive and believable digital humans with minimal latency, supporting natural conversational flow and efficient knowledge access.

Conclusion: The system successfully combines visual realism with real-time performance, making digital humans suitable for immersive applications through coordinated multi-modal components and advanced interaction features.

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [177] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee is a training-free MLLM-based pipeline for image forgery analysis that achieves superior localization accuracy and rich textual explanations without additional training, outperforming existing methods across various tampering types.


<details>
  <summary>Details</summary>
Motivation: Existing image forgery detection methods struggle with generalization and interpretability, while current MLLM-based approaches require expensive training and fail to leverage vanilla MLLMs' inherent generalization potential.

Method: Proposes Foresee pipeline with type-prior-driven strategy and Flexible Feature Detector (FFD) module specifically for copy-move manipulations, enabling training-free inference while unleashing vanilla MLLMs' forensic capabilities.

Result: Extensive experiments show superior localization accuracy, comprehensive textual explanations, and stronger generalization across copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing tampering types.

Conclusion: Foresee demonstrates that vanilla MLLMs have strong inherent potential for image forensics without additional training, achieving state-of-the-art performance while being computationally efficient and highly interpretable.

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [178] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: Proposes a real-time optical flow and disparity estimation model using non-causal selective state space fusion for dense perception tasks, achieving fast inference with high accuracy and low GPU usage.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and real-time solution for optical flow and disparity estimation that can handle the constraints of real-time applications while maintaining efficiency.

Method: Uses a non-causal Mamba block-based model that fuses pairwise input images in a non-causal selective state space for dense perception tasks.

Result: The model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation, validated in real-life scenarios.

Conclusion: The proposed model is suitable for unified real-time and accurate 3D dense perception estimation tasks.

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [179] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer is a framework that converts raster slide images into editable SVG formats using Vision-Language Models, preserving semantic structure and enabling iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Multimedia documents are often distributed as static raster images, limiting editability. Existing geometric raster-vectorization methods fail to preserve high-level structure and semantic distinctions between elements like text and images.

Method: Uses Vision-Language Models to detect and extract attributes from individual elements in raster inputs, organizes them into coherent SVG format, and iteratively refines predictions during inference.

Result: Achieves reconstruction LPIPS of 0.069 and is preferred by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline. Also introduces Slide2SVG dataset for future research.

Conclusion: SliDer effectively addresses semantic document derendering by producing compact, editable SVG representations that faithfully reconstruct original raster documents while preserving semantic structure.

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [180] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: Proposes a task-aware evaluation framework for Novel View Synthesis (NVS) using features from Zero123 foundation model, introducing two metrics (D_PRSIM and MMD_PRSIM) that better assess synthesis quality and align with human preferences compared to existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics for NVS struggle to assess whether generated images are both realistic and faithful to source view and viewpoint transformation, often mis-ranking incorrect results due to inability to capture nuanced relationships between source, viewpoint change, and output.

Method: Leverages features from Zero123 NVS foundation model with lightweight tuning, introducing two complementary metrics: reference-based D_PRSIM and reference-free MMD_PRSIM that use these features to evaluate synthesis quality.

Result: The proposed metrics reliably identify incorrect generations and rank models in agreement with human preference studies, with MMD_PRSIM producing clear and stable rankings across three benchmarks (Toys4K, GSO, OmniObject3D) where lower scores indicate stronger models.

Conclusion: The framework provides a principled and practical approach to assessing NVS synthesis quality, addressing a fundamental gap in evaluation and enabling more reliable progress in novel view synthesis.

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [181] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: This paper introduces a novel defense strategy against backdoor attacks in multimodal CLIP models by identifying triggers, victim samples, and affected labels using an image segmentation oracle, then curating a compact dataset for efficient model rectification.


<details>
  <summary>Details</summary>
Motivation: Multimodal models like CLIP are vulnerable to backdoor attacks, and existing defenses require extensive retraining without precisely identifying affected components. There's a need for efficient, targeted defense methods.

Method: Uses an image segmentation oracle as supervisor to identify backdoor triggers by comparing CLIP and oracle outputs. Develops algorithms to pinpoint affected labels/victim samples and creates a compact fine-tuning dataset for model rectification.

Result: Extensive experiments on visual recognition benchmarks demonstrate the strategy's effectiveness in defending against CLIP-based backdoor attacks.

Conclusion: The proposed approach successfully enhances CLIP model robustness against backdoor attacks by efficiently identifying and rectifying poisoned components without requiring full retraining.

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [182] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: Proposes Hierarchical Prompt Learning (HPL), a unified framework that jointly optimizes image-to-image (I2I) and text-to-image (T2I) person re-identification tasks using task-aware prompt modeling and cross-modal regularization.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat I2I and T2I person re-identification tasks separately, leading to representation entanglement and suboptimal performance. A unified approach is needed to address both discriminative identity learning (I2I) and cross-modal semantic alignment (T2I).

Method: Introduces Task-Routed Transformer with dual classification tokens for routing features to I2I and T2I branches. Develops hierarchical prompt generation combining identity-level learnable tokens with instance-level pseudo-text tokens derived from modality-specific inversion networks. Uses Cross-Modal Prompt Regularization for semantic alignment.

Result: Achieves state-of-the-art performance on multiple ReID benchmarks for both I2I and T2I tasks, demonstrating the effectiveness of the unified framework.

Conclusion: The proposed HPL framework successfully unifies I2I and T2I person re-identification through hierarchical prompt learning and cross-modal regularization, overcoming the limitations of separate task treatment and achieving superior performance.

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [183] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: VVS is a novel speculative decoding framework that accelerates visual autoregressive models by partially skipping verification steps, reducing target model forward passes by 2.8 while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive models suffer from high inference latency due to next-token-prediction paradigm. While speculative decoding helps, its "draft one step, verify one step" approach limits acceleration potential by not reducing forward passes directly.

Method: Proposes VVS framework with three modules: (1) verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Leverages visual token interchangeability to skip verification steps.

Result: Reduces target model forward passes by 2.8 compared to vanilla AR decoding while maintaining competitive generation quality. Offers superior speed-quality trade-off over conventional SD frameworks.

Conclusion: VVS demonstrates strong potential to reshape the speculative decoding paradigm for visual autoregressive generation by enabling partial verification skipping while preserving quality.

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [184] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: A controllability-based interpretability framework for State Space Models (SSMs) that quantifies how input tokens influence internal state dynamics through Jacobian-based and Gramian-based methods, revealing hierarchical feature refinement patterns.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparent interpretability mechanisms in State Space Models (SSMs) like Mamba, which lack attention-like mechanisms despite their competitive performance and linear computational complexity.

Method: Two complementary formulations: Jacobian-based method for any SSM architecture measuring influence through state propagation chains, and Gramian-based approach for diagonal SSMs with closed-form analytical solutions. Both operate in single forward pass with linear complexity.

Result: SSMs naturally implement hierarchical feature refinement from diffuse low-level textures to focused clinically meaningful patterns, revealing domain-specific controllability signatures, progressive spatial selectivity, and influence of scanning strategies on attention patterns.

Conclusion: The framework establishes controllability analysis as a unified interpretability paradigm for SSMs across domains including computer vision, NLP, and cross-domain tasks, with applications validated on three medical imaging modalities.

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [185] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: The paper introduces two novel margin-based -divergence losses (Q-Margin and A3M) for face and speaker verification, addressing the challenge of integrating angular margin into -divergence frameworks and achieving significant performance gains on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current face and speaker verification systems rely heavily on margin-based softmax losses like CosFace and ArcFace. While -divergence losses offer advantages like sparse solutions, integrating angular margin - crucial for verification tasks - is not straightforward.

Method: Proposed two distinct approaches to integrate angular margin into -divergence: Q-Margin (margin in reference measure) and A3M (margin in logits). Addressed A3M training instability with prototype re-initialization strategy.

Result: Achieved significant performance gains on IJB-B and IJB-C face verification benchmarks and strong performance on VoxCeleb speaker verification. Models significantly outperform baselines at low false acceptance rates (FAR).

Conclusion: The proposed margin-based -divergence losses provide effective alternatives to traditional softmax losses, particularly valuable for high-security applications requiring minimal false authentications.

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [186] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC is an amodal counting framework that addresses object counting under occlusion by reconstructing occluded object features through hierarchical multimodal guidance and visual equivalence objectives.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art object counting methods fail under occlusion because backbone networks encode occluding surfaces instead of target objects, corrupting feature representations needed for accurate counting.

Method: CountOCC explicitly reconstructs occluded object features by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. It also introduces a visual equivalence objective that enforces consistency in attention space between occluded and unoccluded views.

Result: CountOCC achieves state-of-the-art performance with 26.72% and 20.80% MAE reduction on FSC 147 validation and test sets under occlusion, 49.89% MAE reduction on CARPK, and 28.79% MAE reduction on CAPTUREReal, demonstrating robust performance across diverse visual domains.

Conclusion: CountOCC's hierarchical multimodal guidance and visual equivalence mechanisms effectively preserve discriminative properties essential for accurate counting under occlusion, setting new state-of-the-art results across multiple datasets and validating robust amodal counting capabilities.

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [187] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM is a few-shot framework for driver attention modeling that jointly predicts attention and generates captions using only ~100 annotated examples, achieving competitive performance with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Existing driver attention models require large-scale gaze datasets that are labor-intensive to collect. There's a need for efficient models that can work with limited data while maintaining explainability.

Method: Dual-pathway architecture with separate modules for spatial attention prediction and caption generation, using cross-modal alignment to maintain semantic consistency between attention maps and explanations.

Result: Achieves competitive attention prediction performance, generates coherent context-aware explanations, and demonstrates robust zero-shot generalization across multiple driving benchmarks with minimal supervision.

Conclusion: Effective attention-conditioned generation is achievable with limited supervision, enabling practical deployment of explainable driver attention systems in data-constrained scenarios.

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [188] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2 enables segment anything at any granularity without human annotations by discovering mask-granularity pairs and using granularity control embedding, achieving significant improvements across multiple benchmarks with minimal data and parameters.


<details>
  <summary>Details</summary>
Motivation: SAM family models lack granularity control, requiring manual refinement through additional prompts or mask selection, which is ambiguous and expensive for dense annotations across all granularities.

Method: Extends UnSAM's divide-and-conquer strategy by discovering abundant mask-granularity pairs and introducing granularity control embedding for precise, continuous control over segmentation scale using only 6K unlabeled images and 0.02% additional parameters.

Result: Substantially enhances SAM-2 across interactive, whole-image, and video segmentation tasks. Improves NoC90 (5.694.75), 1-IoU (58.073.1), and AR1000 (49.668.3) on over 11 benchmarks.

Conclusion: Small amounts of unlabeled data with granularity-aware self-supervised learning can unlock the potential of vision foundation models for segment anything at any granularity.

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [189] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: First study of backdoor attacks on open-vocabulary object detectors (OVODs) using prompt tuning, proposing TrAP - a multi-modal backdoor injection strategy that optimizes prompt parameters with visual triggers without retraining model weights.


<details>
  <summary>Details</summary>
Motivation: As OVODs gain traction in high-stakes applications like robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. The study reveals a new attack surface introduced by prompt tuning.

Method: Proposes TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. Uses curriculum-based training to progressively shrink trigger size.

Result: Experiments show TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while improving clean image performance on downstream datasets compared to zero-shot setting.

Conclusion: TrAP enables effective backdoor attacks on OVODs through lightweight prompt tuning, preserving model generalization while embedding hidden malicious behavior that can be activated with small trigger patches.

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [190] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: VLMs often fail to properly attend to relevant visual tokens when answering visual questions. The authors propose a KL attention loss that directly supervises visual attention using ground truth attention maps, improving performance on geometric tasks, pointing, and referring expression comprehension.


<details>
  <summary>Details</summary>
Motivation: Standard next-token prediction loss provides insufficient signal for directing attention to visual tokens in VLMs, leading to wrong answers on visual tasks. The authors hypothesize that direct supervision of visual token attention would improve performance.

Method: Propose a KL attention loss that aligns the attention distribution of visual tokens to ground truth attention maps obtained from task geometry or grounding annotations. This loss is combined with standard next-token prediction loss to encourage VLMs to attend to relevant visual tokens.

Result: The method shows notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data. The authors also introduce a new dataset for evaluating line tracing abilities, where even commercial VLMs perform poorly.

Conclusion: Directly supervising visual token attention through KL divergence loss significantly improves VLM performance on visual tasks, addressing the limitation of standard next-token prediction loss in properly directing attention to relevant visual information.

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [191] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: This study proposes a multi-target regression approach using KPConv with cost-sensitive learning to infer low-level voxel content (target occupancy percentages) from high-level voxelized LiDAR data of forests, showing that voxel size choice significantly impacts accuracy.


<details>
  <summary>Details</summary>
Motivation: Voxelization reduces LiDAR computational costs but loses fine-scale structural information. The research aims to determine if target occupancy percentages (bark, leaf, soil, miscellaneous) can be inferred from voxelized data to recover some lost information.

Method: Multi-target regression using Kernel Point Convolutions (KPConv) with cost-sensitive learning (density-based relevance) to handle class imbalance. Uses weighted MSE, Focal Regression, and regularization. Conducts sensitivity analysis on voxel sizes from 0.25-2 meters.

Result: Larger voxel sizes (2m) yield lower errors due to reduced variability, while smaller sizes (0.25-0.5m) show higher errors, especially in canopy areas. Bark and leaf targets had significantly higher errors at fine resolutions, indicating difficulty in accurate within-canopy estimation.

Conclusion: Voxel size choice is application-dependent. The work addresses gaps in deep imbalance learning for multi-target regression and provides simulated datasets for 3D LiDAR point clouds of forests.

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [192] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: SAGE (Saliency-Guided Contrastive Embeddings) is a novel loss function that integrates human saliency priors into neural network training by using contrastive embeddings in the latent space rather than image space, improving classification performance across various scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing saliency-guided training methods rely on internal model mechanisms that may be unreliable, and placing guidance solely in image space creates challenges. The insight is to move guidance to the latent space using embeddings.

Method: Uses salient-preserving and saliency-degrading augmentations on inputs, captures embedding and logit changes, applies contrastive triplet loss to guide model toward salient features and away from non-salient features, and performs sanity checks on logit distributions.

Result: Demonstrates improved classification performance across both open- and closed-set scenarios compared to state-of-the-art saliency-based methods, with effectiveness across various backbones and wide generalization across tasks.

Conclusion: SAGE provides an effective approach to integrate human perceptual priors into neural network training by leveraging latent space embeddings and contrastive learning, overcoming limitations of image-space guidance methods.

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [193] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Ctlin-Alexandru Rpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: Introduces RoCoISLR, the first large-scale Romanian Isolated Sign Language Recognition dataset with 9,000+ videos across 6,000 glosses, and benchmarks 7 state-of-the-art models showing transformer architectures outperform CNNs.


<details>
  <summary>Details</summary>
Motivation: Address the lack of standardized datasets for Romanian Sign Language recognition, which limits research progress compared to American Sign Language resources.

Method: Created RoCoISLR dataset with over 9,000 video samples across nearly 6,000 standardized glosses. Evaluated 7 video recognition models (I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, PoseConv3D) under consistent experimental setups.

Result: Transformer-based architectures outperformed convolutional baselines. Swin Transformer achieved the highest Top-1 accuracy of 34.1%. Results highlight challenges with long-tail class distributions in low-resource sign languages.

Conclusion: RoCoISLR provides the foundational dataset for systematic Romanian Isolated Sign Language Recognition research, establishing benchmarks that reveal transformer models' superiority and the difficulties of long-tail distributions in low-resource languages.

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [194] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: An uncertainty-aware brain tumor segmentation framework that combines tumor localization with healthy brain structure segmentation and provides voxel-wise uncertainty estimates to aid clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods lack uncertainty estimates for errors and fail to provide segmentation of healthy brain structures around tumors, limiting clinical utility for surgical planning.

Method: Augments nnUNet with a channel for voxel-wise uncertainty estimation and combines normal and cancer datasets in a unified model for whole-brain context segmentation.

Result: Achieved 0.750 correlation and 0.047 RMSD for uncertainty estimation without compromising tumor accuracy, with DSC of 0.81 for brain structures and 0.86 for tumor segmentation.

Conclusion: The framework provides the first model that outputs tumor segmentation in natural surroundings with overlaid uncertainty maps, offering key insights for evaluating predictions and supporting informed surgical decisions.

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [195] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: ViCoKD is a knowledge distillation framework that transfers knowledge from a multi-modal teacher to a modality-limited student for multi-view action recognition in partially overlapping sensor setups, addressing view misalignment through cross-modal attention and view-aware consistency.


<details>
  <summary>Details</summary>
Motivation: Multi-view action recognition in partially overlapping settings where actions are only visible in subset of views is underexplored, especially in real-world scenarios with limited input modalities and sequence-level annotations instead of dense frame-level labels.

Method: Proposes View-aware Cross-modal Knowledge Distillation (ViCoKD) with cross-modal adapter using cross-modal attention, and View-aware Consistency module that enforces prediction alignment for co-visible actions using human-detection masks and confidence-weighted Jensen-Shannon divergence.

Result: Experiments on MultiSensor-Home dataset show ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

Conclusion: ViCoKD effectively addresses challenges in partially overlapping multi-view action recognition by enabling knowledge transfer from multi-modal teachers to modality-limited students while handling view misalignment through cross-modal attention and view-aware consistency mechanisms.

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [196] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: Proposes a data-driven framework for automatic and interpretable creativity assessment from drawings by analyzing both content (what is drawn) and style (how it's drawn) through multi-modal, multi-task learning.


<details>
  <summary>Details</summary>
Motivation: Current creativity assessment relies on subjective expert scoring which is labor-intensive and inconsistent. Need for automated, objective evaluation methods.

Method: Multi-modal, multi-task learning framework that predicts creativity scores, categorizes content types, and extracts stylistic features. Uses conditional learning to adapt visual feature extraction based on creativity-relevant signals from style and semantic cues.

Result: Achieves state-of-the-art performance compared to existing regression-based approaches and provides interpretable visualizations that align with human judgments.

Conclusion: The proposed framework successfully automates creativity assessment from drawings while maintaining interpretability, bridging the gap between computational methods and human cognitive understanding of creativity.

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [197] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR introduces dynamic activation with dual sparsity across weights and tokens to reduce computational costs in Visual Autoregressive models while maintaining performance through expert sub-networks and token selection.


<details>
  <summary>Details</summary>
Motivation: Address escalating computational costs in VAR models as sequence length grows, overcoming limitations of static pruning methods that degrade performance by permanently removing weights or tokens.

Method: Decompose FFNs into lightweight expert sub-networks with learnable router for dynamic token-specific expert selection, plus gated token selector for high-update-potential tokens while reconstructing unselected tokens. Uses two-stage knowledge distillation supervised by original VAR model.

Result: Achieves up to 21.2% FLOPs reduction on ImageNet 256256 benchmark with minimal performance degradation.

Conclusion: ActVAR effectively enhances VAR model efficiency through dynamic activation framework without sacrificing capacity, providing significant computational savings while preserving model performance.

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [198] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: NH-3DGS is the first method for 3D scene reconstruction that directly models native HDR observations using a novel luminance-chromaticity decomposition, enabling professional-grade reconstruction from single-exposure HDR captures.


<details>
  <summary>Details</summary>
Motivation: Professional digital media workflows require HDR imaging, but existing 3D reconstruction methods primarily focus on LDR data, limiting applicability. Current HDR reconstruction approaches rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision.

Method: Proposes Native High dynamic range 3D Gaussian Splatting (NH-3DGS) with a novel luminance-chromaticity decomposition of color representation that preserves full dynamic range throughout the reconstruction pipeline, enabling direct optimization from native HDR camera data.

Result: NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation on both synthetic and real multi-view HDR datasets.

Conclusion: The method enables professional-grade 3D reconstruction directly from native HDR captures, overcoming limitations of previous approaches and supporting professional digital media workflows.

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [199] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: FDP is a frequency-decomposition preprocessing framework that enhances unsupervised anomaly detection in brain MRI by leveraging unique frequency patterns of anomalies while preserving normal anatomical structures.


<details>
  <summary>Details</summary>
Motivation: Current UAD methods use artificial noise perturbations that lack biophysical fidelity and morphological complexity of real clinical lesions. Frequency-domain analysis revealed anomalies have unique frequency patterns distinguishable from normal anatomy, and low-frequency signals are consistent across healthy scans.

Method: Frequency-Decomposition Preprocessing (FDP) framework leverages frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. It can integrate seamlessly with existing anomaly simulation techniques.

Result: FDP consistently improves anomaly detection performance across diverse architectures. Notably achieves 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines.

Conclusion: FDP is the first UAD method to leverage frequency-domain reconstruction and demonstrates consistent performance improvements when integrated with existing anomaly detection methods.

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [200] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: This paper proposes CASL, a curvature-augmented self-supervised learning framework for 3D anomaly detection that outperforms both specialized anomaly detection models and classical self-supervised approaches while maintaining generalizability to other 3D tasks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D anomaly detection methods are too task-specific and lack generalizability, while classical self-supervised models perform suboptimally for anomaly detection under unified fine-tuning. The authors discovered that simple curvature-based anomaly scoring already outperforms many existing methods.

Method: CASL framework based on reconstruction paradigm using U-Net architecture with multi-scale curvature prompts to guide decoder in predicting point spatial coordinates. Uses straightforward anomaly classification fine-tuning without dedicated anomaly detection mechanisms.

Result: Achieves leading detection performance in 3D anomaly detection. The learned representations also generalize well to standard 3D understanding tasks like point cloud classification.

Conclusion: Curvature plays a critical role in 3D anomaly detection. CASL demonstrates that a general-purpose 3D model can effectively detect anomalies without task-specific designs while maintaining strong performance on other 3D tasks.

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [201] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: Proposes MuNG, a novel fine-tuning method for MLLMs that injects beneficial random noise to improve cross-modal alignment, outperforming full fine-tuning with only 1-2% additional parameters.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods ignore cross-modal heterogeneity, limiting MLLMs' full potential in multimodal intelligence.

Method: Multimodal Noise Generator (MuNG) dynamically analyzes cross-modal relationships to generate task-adaptive beneficial noise, which is injected into frozen MLLMs to suppress irrelevant semantic components.

Result: Experiments on QwenVL and LLaVA show MuNG surpasses full-parameter fine-tuning and other methods while requiring only 1-2% additional parameters.

Conclusion: Injecting beneficial noise through MuNG enables efficient modality fine-tuning, significantly improving cross-modal representation alignment and downstream task performance.

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [202] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR is an autoregressive framework for 6D pose estimation of novel objects using only one reference view, addressing limitations of existing methods through discrete tokenization and probabilistic prediction.


<details>
  <summary>Details</summary>
Motivation: To overcome the dependency on complete 3D models for object pose estimation and address limitations of real-valued coordinate regression methods that suffer from global consistency issues and poor performance in symmetric/occluded scenarios.

Method: Formulates 3D-3D correspondences as discrete tokens using coordinate map tokenization, modality-decoupled encoding (separating RGB and coordinate features), and autoregressive transformer decoder conditioned on query features and generated token sequences.

Result: Significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other real-world challenges.

Conclusion: CoordAR provides an effective autoregressive approach for one-reference 6D pose estimation that overcomes key limitations of previous methods through probabilistic modeling and discrete token representation.

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [203] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl is the first video cinematic editing framework that provides fine control over professional camera parameters like bokeh and shutter speed, using a decoupled cross-attention mechanism and comprehensive data generation strategy.


<details>
  <summary>Details</summary>
Motivation: Current generative video models lack control over photographic elements like depth of field and exposure, which are crucial for cinematic storytelling, as existing methods are mostly limited to camera motion control.

Method: Proposes a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, and develops a comprehensive data generation strategy using simulated effects and real-world collection to build a large-scale training dataset.

Result: Extensive experiments show the model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

Conclusion: CineCtrl successfully enables fine-grained control over professional camera parameters in video generation, overcoming previous limitations in cinematic editing capabilities.

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [204] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: A unified text-driven framework for traffic scene image generation and editing that uses controllable masks, multi-view data, and a two-stage training approach with mask-region-weighted loss to improve semantic richness, viewpoint diversity, and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Address challenges in text-driven traffic scene generation including insufficient semantic richness, limited camera viewpoints, low visual fidelity, and poor text-image alignment for intelligent transportation systems.

Method: Proposes a unified framework with controllable mask mechanism, incorporates vehicle-side and roadside multi-view data, uses two-stage training (conceptual learning then fine-tuning), and introduces mask-region-weighted loss to emphasize small traffic elements.

Result: Extensive experiments show the method achieves leading performance in text-based image generation and editing within traffic scenes, with enhanced geometric diversity and generation fidelity.

Conclusion: The proposed framework effectively addresses key challenges in traffic scene generation and editing, providing a robust solution for applications in traffic monitoring and autonomous driving.

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [205] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: ProtoAnomalyNCD is a prototype-learning framework that discovers and classifies multiple unseen anomaly types in industrial settings by leveraging object localization priors and anomaly-map-guided attention to enhance feature discrimination.


<details>
  <summary>Details</summary>
Motivation: Existing industrial anomaly detection methods only detect presence of anomalies but don't classify multiple anomaly types. Industrial anomalies are semantically subtle and current methods don't sufficiently exploit image priors, making direct clustering ineffective.

Method: Uses Grounded SAM with text prompts to localize object regions as priors, introduces Anomaly-Map-Guided Attention with Region Guidance Factor to distinguish background/object/anomalous regions, and employs prototype learning to discover unseen anomaly classes while enabling multi-type classification.

Result: Outperforms state-of-the-art approaches on MVTec AD, MTD, and Real-IAD datasets, achieving task-level unification for detecting unseen outliers.

Conclusion: ProtoAnomalyNCD effectively discovers and classifies multiple unseen anomaly types by leveraging object localization and anomaly-map guidance, demonstrating superior performance over existing methods while unifying anomaly detection and classification tasks.

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [206] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: A semi-supervised method for HDR image reconstruction that uses a teacher-student framework with uncertainty-based masking to reduce reliance on HDR ground truths, achieving comparable performance to fully-supervised methods with only 6.7% of HDR ground truths.


<details>
  <summary>Details</summary>
Motivation: Learning-based HDR reconstruction methods require LDR-HDR image pairs which are hard to obtain, creating a need for annotation-efficient approaches that can achieve good performance with limited HDR ground truth data.

Method: Uses a teacher-student framework where the teacher generates pseudo HDR ground truths for unlabeled LDR samples, and implements uncertainty-based masking at pixel and patch levels to filter out unreliable parts of pseudo ground truths before student training.

Result: Outperforms previous annotation-efficient algorithms and achieves comparable performance to state-of-the-art fully-supervised methods while using only 6.7% of HDR ground truths.

Conclusion: The proposed uncertainty-based masking approach effectively addresses confirmation bias in semi-supervised HDR reconstruction, enabling high-quality results with significantly reduced annotation requirements.

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [207] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: The paper proposes RAD, a recurrent autoregressive diffusion framework that integrates LSTM with attention into diffusion transformers to address memory limitations in long video generation, improving historical information retention and reducing spatiotemporal inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models with local full attention lack effective memory compression and retrieval for long-term generation beyond window size, causing forgetting and spatiotemporal inconsistencies.

Method: Introduces a recurrent neural network (RNN) into diffusion transformer framework, specifically using LSTM with attention, and proposes RAD framework for frame-wise autoregression with consistent memory update across training and inference.

Result: Experiments on Memory Maze and Minecraft datasets demonstrate superiority for long video generation, showing LSTM's efficiency in sequence modeling with comparable performance to state-of-the-art RNN blocks like TTT and Mamba2.

Conclusion: The RAD framework effectively enhances memory retention in long video generation, addressing training-inference gaps and window overlap issues through recurrent autoregressive diffusion with LSTM integration.

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [208] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign is a novel text-to-image based adversarial attack framework that generates physically robust, highly effective, transferable, and stealthy appearance attacks against Traffic Sign Recognition systems in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial appearance attacks on TSR systems have limitations - pixel-level perturbation methods lack stealthiness and transferability, while T2I diffusion model approaches show limited effectiveness and poor generalization to out-of-distribution sign types.

Method: Proposes a carefully designed attack pipeline integrating CLIP-based loss and masked prompts for improved attack focus and controllability, plus two novel style customization methods to guide visual appearance and enhance out-of-domain generalization and stealthiness.

Result: Achieves an average physical-world attack success rate of 83.3% under varied real-world conditions including different distances, angles, light conditions, and sign categories, demonstrating high effectiveness in attack transferability.

Conclusion: DiffSign successfully overcomes limitations of prior approaches by generating physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems.

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [209] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: GrOCE is a training-free framework for precise concept erasure in text-to-image diffusion models using graph-based semantic reasoning, achieving SOTA performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods require costly fine-tuning or use coarse semantic separation, which degrades unrelated concepts and lacks adaptability to evolving concept sets.

Method: Uses dynamic semantic graph modeling with three components: Dynamic Topological Graph Construction, Adaptive Cluster Identification with similarity-decay scoring, and Selective Edge Severing for targeted removal while preserving global semantics.

Result: Achieves state-of-the-art performance on Concept Similarity (CS) and Frchet Inception Distance (FID) metrics, demonstrating efficient, accurate, and stable concept erasure.

Conclusion: GrOCE provides an effective training-free solution for precise concept removal in diffusion models through principled graph-based reasoning over semantic dependencies.

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Frchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [210] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion is a deep learning framework that predicts spatial transcriptomics gene expression from H&E-stained whole-slide images using hierarchical intra-spot modeling and context-aware cross-scale fusion, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing computational methods for predicting gene expression from histology images fail to capture biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual tissue information, limiting clinical adoption of spatial transcriptomics.

Method: HiFusion integrates two complementary components: 1) Hierarchical Intra-Spot Modeling that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition with feature alignment loss, and 2) Context-aware Cross-scale Fusion that uses cross-attention to selectively incorporate biologically relevant regional context.

Result: Extensive experiments on two benchmark ST datasets show HiFusion achieves state-of-the-art performance in both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios.

Conclusion: HiFusion demonstrates potential as a robust, accurate, and scalable solution for spatial transcriptomics inference from routine histopathology, overcoming limitations of existing approaches.

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [211] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: MCAQ-YOLO introduces morphological complexity-aware quantization for object detection, using five metrics to guide spatially adaptive bit allocation and achieving superior accuracy with 4.2 average bits and 7.6x compression.


<details>
  <summary>Details</summary>
Motivation: Most neural network quantization methods use uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data, which limits efficiency and accuracy.

Method: Uses five morphological metrics (fractal dimension, texture entropy, gradient variance, edge density, contour complexity) to characterize local visual morphology and guide spatially adaptive bit allocation, combined with curriculum-based quantization-aware training to stabilize optimization.

Result: Achieves 85.6% mAP@0.5 with average 4.2 bits and 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization with only 1.8 ms additional runtime overhead. Cross-dataset validation on COCO and Pascal VOC confirms consistent gains.

Conclusion: Morphology-driven spatial quantization enhances efficiency and robustness for computationally constrained, safety-critical visual recognition tasks, demonstrating strong correlation between morphological complexity and quantization sensitivity.

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [212] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld automatically converts rigid 3D objects into articulated URDF models using scene descriptions and LLM knowledge, outperforming existing methods across simulated and real-world scenes.


<details>
  <summary>Details</summary>
Motivation: Manual conversion of rigid 3D assets to articulated objects for robot-learning environments is labor-intensive and costly, creating a need for automated solutions.

Method: Uses Arti4URDF pipeline with 3D point clouds, LLM prior knowledge, and URDF-oriented prompts to identify and convert rigid objects into interactive articulated models while preserving geometry.

Result: Outperforms existing approaches across 3D simulated objects, full scenes, and real-world scans, achieving state-of-the-art performance with preserved geometry and correct interactivity.

Conclusion: Provides a practical solution for building interactive, robot-ready simulation environments directly from existing 3D assets, enabling scalable robot-learning.

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [213] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: CCI is a new interpretability method that uses CLIP's patch embeddings to identify semantically coherent clusters, mask them, and evaluate prediction changes. It achieves state-of-the-art faithfulness and enables automatic categorization of predictions as foreground- or background-driven. The paper also introduces COVAR benchmark to systematically evaluate background correlations and other error sources.


<details>
  <summary>Details</summary>
Motivation: Contrastive VLMs like CLIP are vulnerable to spurious correlations, particularly background over-reliance, which limits their robustness and interpretability.

Method: Cluster-based Concept Importance (CCI) groups spatial patches into semantically coherent clusters using CLIP's patch embeddings, masks them, and evaluates relative changes in predictions. Combined with GroundedSAM for automatic categorization of predictions.

Result: CCI achieves state-of-the-art performance on faithfulness benchmarks with more than twofold improvement on deletion-AUC metric for MS COCO retrieval. Comprehensive evaluation of 18 CLIP variants using COVAR benchmark.

Conclusion: CCI provides methodological advances for interpretability and robustness in VLMs, with empirical evidence showing the need to disentangle background correlations from other error sources like viewpoint variation and scale shifts.

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [214] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: WSAE-Net introduces weighted semantic maps and auto-adaptive candidate editing to improve visual counterfactual explanations by ensuring semantic relevance of replacement regions and optimizing computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional visual counterfactual explanation methods often replace image sections without considering semantic relevance to the target object, which reduces interpretability and hinders editing workflows.

Method: Proposes WSAE-Net with two key components: 1) Weighted semantic maps to reduce non-semantic feature computations, and 2) Auto-adaptive candidate editing sequences to determine optimal computational order while maintaining semantic relevance.

Result: The methodology demonstrates superior performance in comprehensive experiments, enabling more efficient generation of counterfactuals while preserving semantic relevance.

Conclusion: WSAE-Net contributes to clearer and deeper understanding of visual counterfactual explanations by addressing semantic relevance and computational efficiency challenges in traditional methods.

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [215] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: PerTouch is a diffusion-based image retouching framework that supports semantic-level editing while maintaining global aesthetics, using parameter maps for fine-grained control and VLM-driven agents to handle user instructions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing controllability and subjectivity in image retouching, enabling personalized aesthetic preferences while maintaining semantic-level control.

Method: Uses parameter maps containing attribute values in semantic regions as input, constructs explicit parameter-to-image mapping, introduces semantic replacement and parameter perturbation for boundary perception, and employs VLM-driven agents with feedback-driven rethinking and scene-aware memory.

Result: Extensive experiments demonstrate each component's effectiveness and superior performance in personalized image retouching.

Conclusion: PerTouch provides an effective unified framework for personalized image retouching that better aligns with user intent and captures long-term preferences through its semantic-aware approach and intelligent agent system.

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [216] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S is a medical segmentation foundation model that supports native-resolution spatial and textual prompts in an end-to-end framework, achieving superior multi-class segmentation across CT, MRI, PET, ultrasound, and microscopy with 90% faster inference than sequential methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of text-only methods lacking spatial awareness and resolution mismatches in medical segmentation, while enabling efficient multi-class segmentation across diverse medical imaging modalities.

Method: Uses channel-wise alignment between volumetric prompts and text embeddings, lightweight 3D convolutional module for voxel-space refinement, dynamic resampling for data augmentation, and supports both text-only and hybrid prompting modes with optimized preprocessing and two-stage inference.

Result: Outperforms SAT with DSC 75.44 vs 69.83, NSD 77.34 vs 71.06, F1 38.24 vs 24.88, and DSC TP 65.46 vs 46.97 on five-modality average, with 90% reduction in inference time for 24-class segmentation.

Conclusion: Medal S successfully harmonizes spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches.

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [217] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story is a training-free framework for consistent text-to-image generation that addresses identity and style inconsistency in multi-prompt storytelling through three techniques: Identity Prompt Replacement and a unified attention guidance mechanism, achieving state-of-the-art performance with 6x faster inference than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address key challenges in consistent text-to-image generation for multi-prompt storytelling scenarios, specifically identity inconsistency and style inconsistency, without requiring fine-tuning or suffering from slow inference like prior diffusion-based approaches.

Method: Built on a scale-wise autoregressive model with three complementary techniques: Identity Prompt Replacement to mitigate context bias in text encoders, and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation to enforce global style and identity consistency while preserving prompt fidelity.

Result: Achieves state-of-the-art generation performance with high identity and style consistency across diverse prompts, offering over 6x faster inference (1.72 seconds per image) than existing fastest consistent T2I models.

Conclusion: Infinite-Story is an effective and practical training-free framework for real-world visual storytelling that delivers superior consistency and significantly faster inference compared to existing methods.

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [218] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: DTGS is a unified framework that combines Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for robust novel view synthesis under extreme low-light conditions, achieving illumination-invariant reconstruction.


<details>
  <summary>Details</summary>
Motivation: Standard 3D Gaussian Splatting fails in extreme low-light due to illumination inconsistencies and geometric distortion from independent view enhancement. There's a need for joint optimization that maintains radiometric stability across views.

Method: DTGS integrates Retinex-based illumination decomposition within the 3DGS pipeline, using thermal supervision and cyclic enhancement-reconstruction. It dynamically balances enhancement, structural, and thermal losses through a thermal supervisory branch.

Result: DTGS significantly outperforms existing low-light enhancement and 3D reconstruction methods, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination conditions.

Conclusion: The proposed DTGS framework successfully addresses low-light NVS challenges through tight coupling of illumination decomposition and thermal guidance, providing physically interpretable and illumination-invariant 3D reconstruction.

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [219] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: BP-FPN is a novel backpropagation-driven feature pyramid network that addresses the fundamental bottleneck in infrared small target detection by improving per-frame feature representations through gradient-isolated low-level shortcuts and directional gradient regularization.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for infrared small target detection focus on spatio-temporal feature aggregation but have limited gains, revealing that the core bottleneck is ambiguous per-frame feature representations rather than spatio-temporal modeling itself.

Method: Proposes BP-FPN with two key components: Gradient-Isolated Low-Level Shortcut (GILS) to incorporate fine-grained target details without shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation.

Result: Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance in infrared small target detection.

Conclusion: BP-FPN is the first feature pyramid network designed specifically for infrared small target detection from a backpropagation perspective, offering theoretically grounded design with negligible computational overhead that can be seamlessly integrated into existing frameworks.

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [220] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: GeoUniPS is a universal photometric stereo network that integrates synthetic supervision with geometric priors from 3D reconstruction models to handle complex in-the-wild scenes where traditional methods fail due to unreliable multi-illumination cues.


<details>
  <summary>Details</summary>
Motivation: Universal photometric stereo struggles with unreliable multi-illumination cues in complex real-world scenarios like biased lighting, shadows, and self-occluded regions, limiting its practical applicability.

Method: Proposes a Light-Geometry Dual-Branch Encoder that extracts multi-illumination cues and geometric priors from frozen 3D reconstruction models, and introduces PS-Perp dataset with realistic perspective projection to overcome orthographic projection limitations.

Result: Extensive experiments show GeoUniPS achieves state-of-the-art performance across multiple datasets, particularly excelling in complex in-the-wild scenes both quantitatively and qualitatively.

Conclusion: Integrating geometric priors from 3D foundation models with photometric stereo enables robust normal estimation in challenging real-world conditions, overcoming limitations of traditional universal photometric stereo methods.

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [221] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: REVISOR is a novel multimodal reflection framework that enhances long-form video understanding by enabling cross-modal collaborative reflection between text and visual information, addressing limitations of purely text-based reflection mechanisms.


<details>
  <summary>Details</summary>
Motivation: Purely text-based reflection mechanisms have limitations in long-form video understanding due to insufficient visual information rethinking and lack of cross-modal interaction capabilities.

Method: Proposed REVISOR framework with Dual Attribution Decoupled Reward (DADR) mechanism integrated into GRPO training strategy, enabling collaborative introspective reflection across textual and visual modalities.

Result: Significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on VideoMME, LongVideoBench, MLVU, and LVBench benchmarks.

Conclusion: REVISOR effectively addresses the limitations of text-only reflection by enabling cross-modal collaborative reflection, significantly improving reasoning capability for long-form video understanding tasks.

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [222] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean is an object-centric framework for 3D Semantic Scene Completion that decomposes scenes into object instances using MobileSAM segmentation and employs attention mechanisms and diffusion processes to improve semantic occupancy prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing ego-centric approaches for 3D Semantic Scene Completion often overlook fine-grained object-level details, leading to semantic and geometric ambiguities in complex environments like autonomous driving scenarios.

Method: The framework uses MobileSAM for instance mask extraction, 3D Semantic Group Attention for object-centric feature aggregation, Global Similarity-Guided Attention to handle segmentation errors, and Instance-aware Local Diffusion for feature refinement in BEV space.

Result: Ocean achieves state-of-the-art performance with mIoU scores of 17.40 on SemanticKITTI and 20.28 on SSCBench-KITTI-360 benchmarks.

Conclusion: Object-centric decomposition and attention mechanisms significantly improve 3D semantic scene completion accuracy by better capturing fine-grained object details compared to traditional ego-centric approaches.

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [223] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Uni-Inter is a unified framework for human motion generation that handles multiple interaction types (human-human, human-object, human-scene) using a single architecture with Unified Interactive Volume representation.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on task-specific designs with limited generalization, requiring a unified approach for scalable motion synthesis across diverse interaction scenarios.

Method: Introduces Unified Interactive Volume (UIV) - a volumetric representation that encodes heterogeneous interactive entities into shared spatial field, enabling joint-wise probabilistic prediction for motion generation.

Result: Achieves competitive performance across three interaction tasks and demonstrates strong generalization to novel entity combinations.

Conclusion: Unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [224] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: A lightweight framework for multilingual vision-language alignment that uses English as semantic anchor, requiring no image-text pairs and only training a small projection module, achieving significant improvements in underrepresented languages.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual vision-language models perform poorly in low-resource languages due to scarce multilingual image-text data, particularly for Czech, Finnish, Croatian, Hungarian, and Romanian.

Method: Freeze pretrained image and multilingual text encoders, train only a compact 1.7M-parameter projection module using contrastive loss over English representations as semantic anchors, requiring no image-text or text-text pairs.

Result: Significant gains in retrieval performance for five underrepresented languages on XM3600 benchmark, demonstrating robust multilingual alignment even with limited supervision.

Conclusion: The pivot-based, parameter-efficient alignment strategy enables inclusive multimodal learning for low-resource languages without requiring extensive multilingual training data.

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [225] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: MGCA-Net is a multi-grained category-aware network for open-vocabulary temporal action localization that improves recognition accuracy through coarse-to-fine classification for novel actions and conventional classification for base actions.


<details>
  <summary>Details</summary>
Motivation: Existing methods recognize action categories at a single granularity, which degrades recognition accuracy for both base and novel action categories in open-vocabulary temporal action localization.

Method: Proposed MGCA-Net with four components: localizer for category-agnostic action proposals, action presence predictor, conventional classifier for base actions at snippet granularity, and coarse-to-fine classifier for novel actions that first identifies action presence at video granularity then assigns categories at proposal granularity.

Result: Achieves state-of-the-art performance on THUMOS'14 and ActivityNet-1.3 benchmarks, and state-of-the-art results under Zero-Shot Temporal Action Localization setting.

Conclusion: Multi-grained category awareness through coarse-to-fine classification for novel actions and conventional classification for base actions effectively enhances localization performance in open-vocabulary temporal action localization.

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [226] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: The paper introduces Pretext-GRPO and ViSS-R1 frameworks to enhance visual-centric video reasoning in MLLMs, addressing limitations of text-centric approaches that underutilize visual information.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs for video reasoning often prioritize text-centric approaches derived from text/image models, leading to underutilization of rich visual information, shortcut learning, and increased hallucination.

Method: Proposes self-supervised reinforcement learning (Pretext-GRPO) that rewards solving pretext tasks on transformed visual inputs, and ViSS-R1 framework that integrates pretext-task learning into MLLM's R1 post-training, requiring models to reason about transformations and reconstruct original videos.

Result: Comprehensive evaluations on six video reasoning benchmarks demonstrate effectiveness and superiority of the proposed methods for complex video reasoning tasks.

Conclusion: The proposed Pretext-GRPO and ViSS-R1 frameworks successfully address visual underutilization in video reasoning, enabling more robust visual-centric understanding in MLLMs.

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [227] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc is a BEV-free monocular 3D lane detector that explicitly models aleatoric uncertainty using local lane structures, representing lane segments as 3D Gaussians and outperforming previous SoTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing monocular 3D lane detection methods fail to capture structural variations and aleatoric uncertainty from inherent observation noise, relying on simplified geometric assumptions like independent point predictions or global planar modeling.

Method: Projects 3D lanes to front-view space as parametric curves, generates curve-point query embeddings for 3D predictions, models each lane segment as a 3D Gaussian parameterized by local structure and uncertainty, and uses a novel 3D Gaussian matching loss.

Result: Outperforms previous state-of-the-art methods across all benchmarks on ONCE-3DLanes and OpenLane datasets under stricter evaluation criteria, with proposed comprehensive metrics using average and maximum bidirectional Chamfer distances.

Conclusion: MonoUnc effectively addresses aleatoric uncertainty in monocular 3D lane detection through local structural modeling and explicit uncertainty estimation, achieving superior performance while being BEV-free.

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [228] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: Proposes a framework that transfers knowledge from Segment Anything 2 (SAM2) pre-trained on natural images to electron microscopy neuron segmentation, using feature-guided attention and dual-affinity decoding to bridge domain gaps.


<details>
  <summary>Details</summary>
Motivation: Neural structure segmentation in EM images faces challenges from complex morphologies, low signal-to-noise ratios, and limited annotations, requiring better generalization and accuracy.

Method: Extracts SAM2 features, uses Feature-Guided Attention module to guide lightweight encoder on challenging regions, and employs dual-affinity decoder for coarse and refined affinity maps.

Result: Achieves comparable performance to SOTA with frozen SAM2 weights, and significantly outperforms SOTA methods after fine-tuning on EM data.

Conclusion: Natural image pre-trained representations combined with domain-adaptive guidance can effectively address neuron segmentation challenges in EM images.

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [229] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: RobustGait is a framework for evaluating gait recognition robustness across perturbations, silhouette methods, model architectures, and deployment scenarios, revealing key insights about noise propagation, extractor biases, and effective robustness strategies.


<details>
  <summary>Details</summary>
Motivation: Systematic evaluation of appearance-based gait recognition robustness to real-world corruptions and silhouette variability is lacking, despite strong performance on controlled datasets.

Method: Developed RobustGait framework with 4D evaluation (perturbation types, silhouette extraction methods, model architectures, deployment scenarios) across multiple datasets with 15 corruption types at 5 severity levels, evaluating six state-of-the-art gait systems.

Result: Key findings: RGB-level noise better reflects real degradation; gait accuracy is highly sensitive to silhouette extractor biases; robustness depends on perturbation type and architectural design; noise-aware training and knowledge distillation improve performance.

Conclusion: RobustGait provides comprehensive robustness evaluation revealing critical vulnerabilities and effective enhancement strategies, moving toward deployment-ready gait recognition systems.

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [230] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: AdaptiveAD is an autonomous driving architecture that addresses over-reliance on ego status by using dual-branch processing with scene-driven and ego-driven reasoning, followed by adaptive fusion for final planning.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving architectures suffer from over-reliance on ego status, which limits generalization and robust scene understanding by allowing ego status to dominate planning decisions.

Method: Proposes AdaptiveAD with dual-branch structure: one branch for scene-driven reasoning without ego status in BEV encoder, another for ego-driven planning. Uses scene-aware fusion module, path attention mechanism, BEV unidirectional distillation, and autoregressive online mapping.

Result: Achieves state-of-the-art open-loop planning performance on nuScenes dataset, significantly mitigates ego status over-reliance, and shows impressive generalization across diverse scenarios.

Conclusion: AdaptiveAD effectively addresses the ego status dependency problem through architectural decoupling and adaptive fusion, enabling more robust and generalizable autonomous driving planning.

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [231] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Ma K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: MergeSlide treats lifelong learning on WSIs as a model merging problem using a vision-language pathology foundation model, with orthogonal continual merging and task-to-class prompt-aligned inference to handle class-incremental learning.


<details>
  <summary>Details</summary>
Motivation: To reduce resources and effort for data transfer and processing in lifelong learning on gigabyte-scale Whole Slide Images by developing an efficient framework that avoids catastrophic forgetting.

Method: Uses class-aware prompts for new tasks, fine-tunes with MLP-free backbone for few epochs, applies orthogonal continual merging strategy, and introduces Task-to-Class Prompt-aligned inference for class-incremental learning.

Result: Outperforms both rehearsal-based continual learning and vision-language zero-shot baselines on six TCGA datasets.

Conclusion: MergeSlide provides an effective framework for lifelong learning on WSIs through model merging and prompt-aligned inference, demonstrating superior performance over existing approaches.

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [232] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: CapeNext addresses limitations in Category-Agnostic Pose Estimation by replacing static joint embeddings with dynamic hierarchical cross-modal interaction and dual-stream feature refinement, achieving state-of-the-art performance on MP-100 dataset.


<details>
  <summary>Details</summary>
Motivation: Fixed textual keypoint descriptions in CAPE suffer from polysemy-induced cross-category ambiguity (e.g., 'leg' meaning different things across categories) and insufficient discriminability for fine-grained intra-category variations (e.g., posture differences between cat poses).

Method: Proposes a framework integrating hierarchical cross-modal interaction with dual-stream feature refinement, enhancing joint embeddings with both class-level and instance-specific cues from textual descriptions and specific images.

Result: Experiments on MP-100 dataset show CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin, regardless of the network backbone used.

Conclusion: Dynamic hierarchical cross-modal interaction and dual-stream feature refinement effectively overcome limitations of static joint embeddings in CAPE, providing more robust and discriminative pose estimation across diverse categories.

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [233] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack is a novel multi-object tracking framework that adaptively fuses Kalman filters and data-driven motion predictors through multi-perceptive motion understanding, achieving state-of-the-art performance without modifying existing predictors.


<details>
  <summary>Details</summary>
Motivation: Kalman filters are computationally efficient but fail on non-linear motion, while data-driven predictors capture complex dynamics but suffer from limited generalization and computational overhead. Analysis shows both approaches are complementary in real-world tracking scenarios.

Method: Proposes PlugTrack framework that uses multi-perceptive motion analysis to generate adaptive blending factors for fusing Kalman filter and data-driven motion predictors, leveraging their complementary strengths.

Result: Achieves significant performance gains on MOT17/MOT20 datasets and state-of-the-art performance on DanceTrack dataset without modifying existing motion predictors.

Conclusion: PlugTrack successfully bridges classical and modern motion prediction paradigms through adaptive fusion, demonstrating the value of combining both approaches for robust multi-object tracking.

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [234] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: Proposes the first dataset distillation method for medical image enhancement that addresses the many-to-many mapping challenge in low-level tasks by using shared anatomical priors and patient-specific personalization while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods mainly target high-level tasks with many-to-one mappings, but low-level medical image enhancement requires pixel-level fidelity and faces many-to-many mapping challenges, making it an underdetermined problem that cannot be fully constrained by small distilled datasets.

Method: Leverages anatomical similarities to construct shared anatomical priors from representative patients, then personalizes for each patient using Structure-Preserving Personalized Generation (SPG) module. Uses gradient alignment between networks trained on distilled data and raw patient data to inject patient-specific knowledge while maintaining privacy.

Result: Enables creation of distilled datasets that capture patient-specific anatomical information while preserving pixel-level fidelity for medical image enhancement tasks, without requiring access to raw patient data.

Conclusion: The proposed method successfully addresses the challenges of low-level dataset distillation for medical image enhancement by combining shared anatomical priors with patient-specific personalization, enabling privacy-preserving deployment while maintaining task performance.

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [235] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: DGS-Net is a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components for AI-generated image detection, addressing catastrophic forgetting in fine-tuning multimodal models like CLIP.


<details>
  <summary>Details</summary>
Motivation: The rapid proliferation of AI-generated images raises concerns about misinformation and trust erosion. Fine-tuning multimodal models for detection causes catastrophic forgetting, degrading pre-trained priors and limiting cross-domain generalization.

Method: Proposes Distillation-guided Gradient Surgery Network (DGS-Net) with gradient-space decomposition that separates harmful and beneficial descent directions. Projects task gradients onto orthogonal complement of harmful directions and aligns with beneficial ones distilled from frozen CLIP encoder.

Result: Extensive experiments on 50 generative models show DGS-Net outperforms state-of-the-art approaches by average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

Conclusion: DGS-Net effectively addresses catastrophic forgetting in fine-tuning multimodal models for AI-generated image detection, enabling unified optimization of prior preservation and irrelevant suppression for improved cross-domain generalization.

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [236] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: Proposes an unsupervised image dehazing method using implicit neural degradation representation that combines channel-independent and channel-dependent mechanisms for better nonlinear dependency learning, achieving competitive performance on complex scenes.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing methods struggle to balance fine-grained feature representation of inhomogeneous haze distribution with global consistency modeling in complex scenes.

Method: Uses implicit neural representation to model haze degradation as continuous function, combines channel-independent and channel-dependent mechanisms inspired by Kolmogorov-Arnold theorem, and includes dense residual enhancement module to eliminate redundant information.

Result: Achieves competitive dehaze performance on various public and real-world datasets with good visual perception in complex scenes.

Conclusion: The proposed unsupervised method effectively handles complex haze distributions through implicit neural degradation representation and achieves high-quality image restoration without explicit feature extraction or physical models.

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [237] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: MPHM network integrates CLIP and DINOv2 priors with hierarchical Mamba modules for superior image deraining, achieving state-of-the-art performance with 0.57 dB PSNR gain on Rain200H.


<details>
  <summary>Details</summary>
Motivation: Rain degrades computer vision systems, and existing deraining methods struggle with semantic and spatial detail fidelity, requiring better integration of semantic and structural priors.

Method: Proposes Multi-Prior Hierarchical Mamba network that integrates CLIP for semantic guidance and DINOv2 for structural information, with progressive Priors Fusion Injection and Fourier-enhanced hierarchical Mamba modules.

Result: Achieves state-of-the-art performance with 0.57 dB PSNR gain on Rain200H dataset and superior generalization on real-world rainy scenarios.

Conclusion: MPHM effectively addresses semantic and structural detail preservation in image deraining through synergistic integration of heterogeneous priors and hierarchical feature modeling.

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [238] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: Proposes RIF framework for 3D anomaly detection using rotationally invariant features to handle orientation/position changes in point clouds, achieving significant performance improvements on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing 3D anomaly detection methods struggle with point clouds that have orientation and position changes, as these variations cause significant feature inconsistencies that degrade detection performance.

Method: Uses Point Coordinate Mapping (PCM) to create rotationally invariant space, lightweight CTF-Net for feature extraction, transfer learning with 3D data augmentation, and memory bank approach for anomaly detection.

Result: Achieves 17.7% average P-AUROC improvement on Anomaly-ShapeNet and 1.6% improvement on Real3D-AD dataset, with strong generalization when combined with traditional methods.

Conclusion: RIF framework effectively handles orientation/position variations in 3D point clouds, demonstrating superior performance and strong generalization for industrial anomaly detection applications.

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [239] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot is a diffusion-based framework for close-up novel view synthesis from sparse inputs that addresses limitations in existing methods by using hierarchical warping, occlusion-aware noise suppression, and global structure guidance from point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for 3D scene reconstruction and novel view synthesis struggle with fine-grained details in close-up scenarios due to severe input sparsity and background leakage in sparse-view settings.

Method: Uses point-conditioned video diffusion with hierarchical warping and occlusion-aware noise suppression to enhance conditioning quality, plus global structure guidance from dense fused point clouds to provide consistent geometric context.

Result: Extensive experiments show the method outperforms existing approaches, especially in close-up novel view synthesis, validating the effectiveness of the proposed design.

Conclusion: The proposed CloseUpShot framework successfully addresses the challenges of close-up novel view synthesis from sparse inputs through improved conditioning techniques and global geometric constraints.

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [240] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: RePo is a novel trajectory similarity method that jointly encodes region-wise and point-wise features to capture both spatial context and fine-grained movement patterns, achieving 22.2% average accuracy improvement over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Current learning-based methods for trajectory similarity computation fail to leverage the comprehensive spectrum of trajectory information, particularly the combination of spatial context and fine-grained moving patterns.

Method: RePo jointly encodes region-wise features (grid sequences with structural and semantic context from visual features) and point-wise features (local, correlation, and continuous movement patterns via expert networks), then fuses them using cross-attention after adaptive fusion by a router network.

Result: RePo achieves an average accuracy improvement of 22.2% over state-of-the-art baselines across all evaluation metrics.

Conclusion: The proposed joint encoding of region-wise and point-wise features effectively captures comprehensive trajectory information for similarity modeling, significantly outperforming existing methods.

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [241] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: VEIL is a jailbreak framework that uses benign-looking prompts with implicit cues to bypass safety guardrails in text-to-video models, achieving 23% higher attack success rates than prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior jailbreak attacks on T2V models use obvious adversarial perturbations that are easy to detect. The motivation is to develop more stealthy attacks that exploit models' cross-modal associations through implicit cues.

Method: VEIL uses modular prompt design with three components: neutral scene anchors (surface description), latent auditory triggers (innocuous audio descriptions that bias toward unsafe visuals), and stylistic modulators (cinematic directives). Attack generation is formalized as constrained optimization solved via guided search.

Result: Extensive experiments on 7 T2V models show VEIL achieves 23% improvement in average attack success rate for commercial models compared to prior methods.

Conclusion: T2V models have critical blind spots where benign-looking prompts with implicit cues can bypass safety mechanisms, revealing vulnerabilities in cross-modal associative patterns that need to be addressed.

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [242] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: MedGEN-Bench is a comprehensive medical multimodal benchmark addressing limitations in existing medical AI evaluation by focusing on contextually intertwined instructions requiring cross-modal reasoning and open-ended generative outputs across six imaging modalities and 16 clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Clinicians increasingly expect AI systems to generate both textual diagnoses and corresponding medical images that integrate into clinical workflows, but existing medical visual benchmarks have limitations including ambiguous queries, oversimplified diagnostic reasoning, and text-centric evaluation that overlooks image generation capabilities.

Method: Developed MedGEN-Bench with 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks, structured into three formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. Used a novel three-tier assessment framework integrating pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring.

Result: Systematically evaluated 10 compositional frameworks, 3 unified models, and 5 VLMs using the proposed benchmark and assessment framework.

Conclusion: MedGEN-Bench advances medical AI research by providing a comprehensive multimodal benchmark that emphasizes sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond multiple-choice formats to better reflect real clinical needs.

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [243] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba is a novel Mamba-based 3D object detection backbone that addresses spatial information loss in existing methods through window-scale-adaptive modules and window-shift strategies, achieving superior performance on KITTI and Waymo datasets.


<details>
  <summary>Details</summary>
Motivation: 3D object detection needs to balance computational efficiency and long-range dependency capture. Existing Mamba-based methods lose spatial information due to axis-aligned scanning in fixed windows.

Method: Proposed WinMamba backbone with stacked WinMamba blocks featuring: 1) window-scale-adaptive module for multi-scale voxel feature compensation, 2) learnable positional encoding, and 3) window-shift strategy for rich contextual cues in linear state space.

Result: Significantly outperforms baseline on KITTI and Waymo datasets. Ablation studies confirm the effectiveness of WSF and AWF modules in improving detection accuracy.

Conclusion: WinMamba successfully addresses spatial information loss in 3D object detection while maintaining computational efficiency, demonstrating the potential of Mamba-based architectures for autonomous driving applications.

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [244] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: CSIP-ReID is a novel skeleton-driven pretraining framework for video person re-identification that replaces text-based approaches with skeleton sequences to capture fine-grained temporal motion cues, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal pretraining for video ReID relies on video-text pairs but suffers from lack of genuine multimodal pretraining and text's inability to capture fine-grained temporal motion essential for identity distinction.

Method: Two-stage approach: (1) contrastive learning to align skeleton and visual features at sequence level, (2) dynamic Prototype Fusion Updater to refine multimodal identity prototypes, plus Skeleton Guided Temporal Modeling module to distill temporal cues from skeletons into visual features.

Result: Achieves new state-of-the-art results on MARS, LS-VID, iLIDS-VID benchmarks and shows strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods.

Conclusion: CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening new frontiers in multimodal representation learning by effectively leveraging skeleton data for temporal motion understanding.

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [245] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: THIR is a novel unsupervised Content-Based Medical Image Retrieval framework that uses topological data analysis (Betti numbers from persistent homology) to characterize histopathological breast cancer images without requiring training data or GPU resources.


<details>
  <summary>Details</summary>
Motivation: Breast cancer caused 685,000 deaths in 2020, highlighting the critical need for early diagnosis and accurate clinical decision making to reduce this global health burden.

Method: THIR extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact feature vectors, and performs similarity retrieval by computing distances between these topological descriptors.

Result: Extensive experiments on BreaKHis dataset show THIR outperforms state-of-the-art supervised and unsupervised methods, processing the entire dataset in under 20 minutes on a standard CPU.

Conclusion: THIR offers a fast, scalable, and training-free solution for clinical image retrieval that is more efficient than conventional deep learning approaches.

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [246] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: HDW-SR is a diffusion-based super-resolution method that uses wavelet decomposition to focus on high-frequency details, replacing standard U-Net with wavelet-based downsampling and sparse cross-attention for better fine detail recovery.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based SISR methods often produce blurred fine details due to insufficient guidance in the high-frequency domain, limiting their ability to recover sharp image details.

Method: Proposes HDW-SR with wavelet decomposition, performing diffusion only on residual maps, using wavelet-based downsampling for multi-scale frequency decomposition, sparse cross-attention between high/low frequency subbands, and Dynamic Thresholding Block for high-frequency refinement.

Result: Experiments on synthetic and real-world datasets show HDW-SR achieves competitive super-resolution performance, particularly excelling in recovering fine-grained image details compared to existing methods.

Conclusion: HDW-SR effectively addresses high-frequency detail blurring in diffusion-based SISR through wavelet-guided architecture, demonstrating superior fine detail recovery while maintaining competitive overall performance.

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [247] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract is the first generative model for global tractography that learns a direct mapping from dMRI to complete, anatomically plausible streamlines, achieving significantly higher precision than existing methods.


<details>
  <summary>Details</summary>
Motivation: Local tractography methods are prone to error accumulation and high false positive rates, while global methods are computationally expensive. There's a need for a better approach that combines the benefits of both.

Method: Frames tractography as a generative task using both diffusion-based and flow matching paradigms to learn direct mapping from dMRI to complete streamlines.

Result: GenTract achieves precision 2.1x higher than the next-best method (TractOracle), with even greater advantages in challenging low-resolution and noisy settings where it outperforms competitors by an order of magnitude.

Conclusion: GenTract represents a promising solution for global tractography by producing high-precision tractograms on research-grade data while maintaining reliability on imperfect, lower-resolution data.

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [248] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodrguez,Mario Almagro,Kunal Dahiya,David Jimnez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: ViXML introduces a vision-enhanced framework for Extreme Multi-label Classification that combines decoder-only models with visual information, achieving state-of-the-art performance while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Foundation models have transformed AI but remain underexplored in XMC, which requires balancing efficiency and performance when handling extremely large label spaces. Current approaches use small encoder-only transformers, but there's potential to leverage larger decoder-only models and visual information.

Method: ViXML efficiently integrates foundation vision models by pooling a single embedding per image and combines them with text embeddings. It uses decoder-only models for improved performance while managing computational overhead through efficient embedding strategies.

Result: ViXML outperforms text-only decoders in most cases, showing that visual information can substitute for billions of parameters. The framework achieves up to +8.21% improvement in P@1 on the largest dataset compared to previous state-of-the-art methods.

Conclusion: Both decoder-only models and visual information play critical roles in XMC and can be effectively combined. Visual information provides substantial benefits, demonstrating that "an image is worth billions of parameters" in this context.

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [249] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: Proposes Object-Centric 3D Rollout (OCR) to improve video spatial reasoning in MLLMs by introducing structured perturbations to object geometry during training, forcing holistic scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with robust video spatial reasoning and exhibit query-locked reasoning - focusing only on objects mentioned in prompts while ignoring contextual cues.

Method: OCR introduces structured perturbations to 3D geometry of selected objects, degrades object-specific visual cues, projects altered geometry to 2D space, and uses rollout-based training with vanilla and region-noisy videos.

Result: Achieves state-of-the-art performance with 47.5% accuracy on VSI-Bench using a 3B-parameter model, outperforming several 7B baselines.

Conclusion: OCR effectively addresses query-locked reasoning and enables more robust spatial reasoning in video understanding tasks.

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [250] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: A differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to reproduce the human painting-smudging loop through Bezier stroke optimization and geometry-conditioned texture synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing generative models focus on final image generation or patch-based process simulation but lack explicit stroke structure and fail to produce smooth, realistic shading, missing the creation process significance in painting.

Method: Uses differentiable stroke reconstruction with single- and dual-color Bezier strokes optimized through parallel differentiable paint renderer, style generation module for geometry-conditioned textures, differentiable smudge operator for color blending, and coarse-to-fine optimization strategy.

Result: Produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances across oil, watercolor, ink, and digital paintings, demonstrating unified expressive digital painting creation.

Conclusion: The framework successfully reproduces the human painting-smudging loop and offers a unified model for expressive digital painting creation with explicit stroke structure and realistic shading.

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [251] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD is a difficulty-aware label-guided denoising framework for monocular 3D object detection that adaptively perturbs ground-truth labels based on detection uncertainty and reconstructs them to provide explicit geometric supervision.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection is fundamentally ill-posed due to ambiguous depth cues, and existing DETR-based methods struggle with inaccurate depth estimates while overlooking instance-level detection difficulty factors like occlusion, distance, and truncation.

Method: Proposes a Difficulty-Aware Label-Guided Denoising framework that applies adaptive perturbations to ground-truth labels - stronger perturbations for easier instances and weaker ones for harder cases, then reconstructs them to provide explicit geometric supervision through joint optimization of label reconstruction and 3D object detection.

Result: Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

Conclusion: The proposed framework effectively addresses the challenges of monocular 3D object detection by incorporating difficulty-aware label denoising and reconstruction, leading to improved geometry-aware representation learning and robustness to varying object complexity.

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [252] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: Proposes a self-supervised pipeline to extract ultrasound images from monitor photographs, bypassing DICOM transfer bottlenecks for rapid algorithm testing.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound imaging requires DICOM transfer which creates bottlenecks, limiting rapid testing and prototyping of new algorithms.

Method: Self-supervised pipeline that extracts and rectifies ultrasound images from photographs of the monitor display.

Result: Rectified images maintained sufficient visual fidelity to classify cardiac views with 0.79 balanced accuracy compared to native DICOM images.

Conclusion: The method successfully bypasses DICOM bottlenecks and enables rapid algorithm development while retaining diagnostic utility.

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [253] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD is a weakly-supervised video anomaly detection framework that jointly models temporal motion patterns and semantic categories of anomalies through motion-aware temporal attention and category-oriented feature refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods oversimplify anomaly detection by treating all abnormal events as a single category, ignoring the diverse semantic and temporal characteristics of real-world anomalies. The paper is inspired by human perception that jointly interprets motion patterns and semantic structures.

Method: Proposes RefineVAD with two core modules: Motion-aware Temporal Attention and Recalibration (MoTAR) that estimates motion salience and adjusts temporal focus using shift-based attention and Transformer modeling, and Category-Oriented Refinement (CORE) that aligns segment-level features with learnable category prototypes through cross-attention.

Result: Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD, demonstrating improved performance in weakly-supervised video anomaly detection.

Conclusion: The framework successfully integrates semantic context to guide feature refinement toward anomaly-relevant patterns, highlighting the importance of modeling both temporal dynamics and semantic structure for effective anomaly detection.

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [254] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: PAVE-Net is the first fully end-to-end framework for multi-person 2D pose estimation in videos, eliminating heuristic operations like detection and NMS through a novel pose-aware attention mechanism and spatiotemporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on two-stage pipelines with heuristic operations (detection, RoI cropping, NMS) that limit accuracy and efficiency. The authors aim to create a fully end-to-end solution that eliminates these limitations.

Method: Proposed PAVE-Net with spatial encoder for intra-frame relations and spatiotemporal pose decoder for global dependencies. Introduced pose-aware attention mechanism for temporal association and explicit modeling of spatiotemporal dependencies among keypoints.

Result: Achieves 6.0 mAP improvement on PoseTrack2017 compared to prior image-based end-to-end methods, and delivers competitive accuracy with state-of-the-art two-stage video approaches while offering significant efficiency gains.

Conclusion: PAVE-Net demonstrates that fully end-to-end multi-person video pose estimation is feasible and superior, eliminating heuristic operations while achieving better accuracy and efficiency than existing approaches.

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [255] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER is a unified framework that improves 3D-text alignment through dynamic attention policy and efficient retrieval strategy, addressing fine-grained semantic-geometric alignment challenges in large-scale 3D databases.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-text alignment methods struggle with fine-grained semantic-geometric correspondences and performance degradation when scaling to large 3D databases.

Method: Proposes dynamic attention policy with Hierarchical Attention Fusion module for fine-grained token-to-point alignment, optimized via Monte Carlo tree search, and efficient retrieval strategy for hierarchical searching in large embedding spaces.

Result: Demonstrates superior performance on diverse benchmarks, outperforming traditional methods like KNN in both accuracy and efficiency.

Conclusion: 3DAlign-DAER effectively addresses fine-grained 3D-text alignment challenges and scales well to large databases, with the release of codes, models, and the new Align3D-2M dataset.

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [256] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: HARL is a hybrid-domain adaptive learning framework that improves gaze estimation by disentangling gaze-relevant features from low-quality facial images using unsupervised domain adaptation and geometric constraints between gaze and head-pose.


<details>
  <summary>Details</summary>
Motivation: Current appearance-based gaze estimation methods suffer significant performance degradation in cross-domain scenarios due to interference from gaze-irrelevant factors like expressions, wearables, and image quality.

Method: Proposes Hybrid-domain Adaptative Representation Learning (HARL) that: 1) disentangles gaze-relevant representation by aligning features from high-quality near-eye images in unsupervised domain adaptation, 2) uses sparse graph fusion module to explore geometric constraints between gaze direction and head-pose.

Result: Achieves state-of-the-art accuracy of 5.02, 3.36, and 9.26 on EyeDiap, MPIIFaceGaze, and Gaze360 datasets respectively, with competitive cross-dataset performance.

Conclusion: HARL framework effectively learns robust gaze representation by combining domain adaptation and geometric constraints, significantly improving cross-domain gaze estimation performance without computational overhead.

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [257] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: MRIQT is a 3D conditional diffusion framework that enhances portable ultra-low-field MRI (0.064 T) to high-field MRI quality for neonatal brain imaging, achieving superior image quality and diagnostic reliability.


<details>
  <summary>Details</summary>
Motivation: Portable ultra-low-field MRI offers accessible neonatal neuroimaging but suffers from poor signal-to-noise ratio and diagnostic quality compared to high-field MRI, limiting its clinical utility.

Method: Combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and SNR-weighted 3D perceptual loss for anatomical fidelity using volumetric attention-UNet architecture.

Result: Surpasses recent GAN and CNN baselines by 15.3% in PSNR with 1.78% improvement over state-of-the-art, with 85% of outputs rated as good quality by physicians with clear pathology present.

Conclusion: MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field MRI for reliable neonatal brain assessment, bridging the quality gap between portable and high-field systems.

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [258] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: MMD-Thinker is a two-stage framework that enhances multimodal misinformation detection by incorporating adaptive multi-dimensional thinking into MLLMs, addressing limitations of uniform reasoning paradigms and single thinking modes.


<details>
  <summary>Details</summary>
Motivation: Address the growing threat of multimodal misinformation in the AIGC era, which has low creation cost and high deception. Current MLLM-based detectors suffer from insufficient reasoning due to lack of task-specific knowledge and reasoning biases from single thinking modes.

Method: 1) Develop tailor-designed thinking modes for misinformation detection, 2) Use task-specific instruction tuning to inject tailored thinking into MLLMs, 3) Apply reinforcement learning with mixed advantage function to enhance reasoning capabilities, 4) Construct MMR dataset with 8K+ image-text pairs containing reasoning processes.

Result: Achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets while maintaining flexible inference and efficient token usage.

Conclusion: MMD-Thinker effectively addresses the limitations of current MLLM-based detectors by incorporating adaptive multi-dimensional thinking, demonstrating superior performance in combating evolving multimodal misinformation.

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [259] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: RFMNet improves camouflaged object detection by fusing multi-stage features from reference salient images with camouflage features using overlapped windows cross-attention and progressive decoding.


<details>
  <summary>Details</summary>
Motivation: Previous methods converted reference images to 1D prompts, but rich salient image features and multi-context fusion can enhance performance for detecting hidden objects.

Method: Proposes RFMNet with multi-stage feature fusion from reference images, overlapped windows cross-attention for local information matching, and Referring Feature Aggregation module for progressive decoding.

Result: Achieves state-of-the-art performance on Ref-COD benchmark through extensive experiments.

Conclusion: Multi-context fusion of salient image features with camouflage features using local attention mechanisms significantly improves referring camouflaged object detection.

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [260] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: This paper introduces EgoProceAssist, an egocentric procedural AI assistant for daily tasks, defining three core tasks (error detection, learning, and QA), reviewing current techniques, and evaluating VLMs while identifying gaps and future directions.


<details>
  <summary>Details</summary>
Motivation: Recent advances in vision language models and egocentric perception research motivate the development of AI assistants that can provide step-by-step support for daily procedural tasks from a first-person perspective.

Method: The authors define a taxonomy with three core tasks, conduct comprehensive review of techniques/datasets/metrics, perform novel experiments evaluating representative VLM-based methods, and analyze gaps between proposed EgoProceAssist and existing assistants.

Result: The paper provides a systematic evaluation of current VLM capabilities for egocentric procedural tasks, identifies performance gaps, and establishes a foundation for future research through comprehensive technical analysis.

Conclusion: The study outlines challenges and future research directions for developing effective egocentric procedural AI assistants, with an active repository maintained for ongoing collection of latest work in this emerging field.

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [261] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS introduces symmetry-aware compression for 3D Gaussian Splatting scenes by using learnable mirrors to eliminate redundant primitives, achieving 1.66 compression over existing methods and up to 108 total compression while preserving rendering quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting provides high-quality novel view synthesis but suffers from large memory footprints that scale with scene complexity, reaching several gigabytes. Existing compression methods exploit primitive-level redundancy but don't leverage structural symmetries.

Method: Proposes SymGS framework that introduces learnable mirrors into scenes to eliminate local and global reflective redundancies. It functions as a plug-and-play enhancement to existing compression methods like HAC, targeting mirror symmetries to remove redundant primitives.

Result: Achieves 1.66 compression over HAC across benchmark datasets (up to 3 on large-scale scenes). On average, enables 108 compression of 3DGS scenes while preserving rendering quality.

Conclusion: SymGS successfully incorporates symmetry-aware techniques to surpass existing compression limits for 3D Gaussian Splatting, providing significant memory reduction while maintaining visual fidelity through mirror-based redundancy elimination.

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [262] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: SpatialSky-Bench is a new benchmark for evaluating VLMs' spatial intelligence in UAV navigation, revealing current VLMs' limitations. The authors also created SpatialSky-Dataset and Sky-VLM, which achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models lack exploration of spatial intelligence capabilities in UAV scenarios, raising concerns about their effectiveness in dynamic environments.

Method: Created SpatialSky-Bench benchmark with 13 subcategories for environmental perception and scene understanding. Developed SpatialSky-Dataset with 1M samples and Sky-VLM specialized for UAV spatial reasoning.

Result: Extensive evaluations show mainstream VLMs perform unsatisfactorily in complex UAV navigation. Sky-VLM achieves state-of-the-art performance across all benchmark tasks.

Conclusion: Sky-VLM paves the way for developing VLMs suitable for UAV scenarios, addressing significant gaps in spatial capabilities.

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [263] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: A dual-backbone framework combining CNN and transformer representations with top-k pooling achieves 90.7% AUC for video anomaly detection using only video-level supervision.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting rare and diverse anomalies in surveillance videos with limited supervision (only video-level labels).

Method: Dual-backbone framework combining convolutional and transformer representations through top-k pooling.

Result: Achieved 90.7% area under the curve (AUC) on the UCF-Crime dataset.

Conclusion: The proposed dual-backbone approach with top-k pooling effectively detects rare and diverse anomalies using only video-level supervision.

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [264] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon directly reconstructs lightweight building surfaces from multi-view images using 3D Gaussian Splatting optimization and multi-view edge-consistency pruning, eliminating the need for post-hoc mesh simplification.


<details>
  <summary>Details</summary>
Motivation: Conventional multi-view geometry pipelines are cumbersome and quality-sensitive due to reliance on dense reconstruction, meshing, and subsequent simplification processes.

Method: Trains initial 3D Gaussian Splatting field, then uses normal-gradient-guided Gaussian optimization to select primitives aligned with building structures, followed by multi-view edge-consistency pruning and depth-constrained Delaunay triangulation.

Result: Achieves substantially fewer faces and vertices while maintaining computational efficiency, directly producing lightweight building models without post-processing.

Conclusion: SF-Recon provides an effective method for direct lightweight building surface reconstruction from multi-view imagery, outperforming conventional pipelines in efficiency and structural fidelity.

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [265] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: DTO-Humans: A novel method for multi-person human mesh recovery using depth-conditioned optimization to create scene-consistent pseudo-ground-truth data and a metric-aware network for direct metric-scale estimation.


<details>
  <summary>Details</summary>
Motivation: Current multi-person human mesh recovery methods lack scene-level consistency due to single-person-centric approaches, leading to conflicting depths and scales within the same image.

Method: Depth-conditioned Translation Optimization (DTO) jointly refines camera-space translations using anthropometric priors and monocular depth cues in a MAP framework. Also proposes Metric-Aware HMR with camera branch and relative metric loss.

Result: Created DTO-Humans dataset with 0.56M high-quality multi-person images (avg 4.8 persons/image). Achieved state-of-the-art performance on relative depth reasoning and human mesh recovery.

Conclusion: The proposed method successfully addresses scene-level consistency in multi-person mesh recovery and enables direct metric-scale estimation through novel optimization and network design.

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [266] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash is an efficient MLLM for table understanding that uses progressive question conditioning, token pruning, and token focusing to create compact yet informative visual features, achieving SOTA performance with 27% less FLOPs and 30% less memory.


<details>
  <summary>Details</summary>
Motivation: Table images have unique challenges with redundant background regions and question-specific focus needs that existing MLLMs overlook, leading to uninformative and redundant visual representations.

Method: Progressive question conditioning injects questions into Vision Transformer layers with increasing frequency, token pruning removes background tokens for efficiency, and token focusing training concentrates essential information in retained tokens to mitigate pruning loss.

Result: TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

Conclusion: The combination of progressive question conditioning, pruning, and token focusing creates an efficient and effective MLLM for table understanding that addresses the unique challenges of table images.

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [267] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text is a font-controllable framework for precise poster text editing that enables simultaneous editing of multiple text regions with different fonts using only cropped glyph patches, without requiring font labels or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current image editing models lack fine-grained, font-aware text manipulation capabilities needed for professional design workflows like poster editing, where preserving visual harmony and typographic intent across diverse font styles is crucial.

Method: A novel font-controllable framework that uses cropped glyph patches as input to enable simultaneous editing of multiple text regions with distinct typographic styles while preserving non-edited regions, requiring no font labels or fine-tuning during inference.

Result: Achieves state-of-the-art performance on multiple datasets including handwritten text benchmarks, with superior text fidelity and visual realism, offering unprecedented control over font families and stylistic nuances.

Conclusion: Bridges the gap between general-purpose image editing and professional-grade typographic design by providing precise font-aware text manipulation capabilities for artistic design workflows.

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [268] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: The paper proposes CorrectAD, an automated pipeline using diffusion-based video generation (DriveSora) and a PM-Agent to self-correct failure cases in end-to-end autonomous driving planners by generating synthetic data for rare safety-critical scenarios.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving systems suffer from robustness issues due to the long-tail problem - rare but safety-critical failure cases that data-driven approaches struggle to handle effectively.

Method: Proposes CorrectAD system with: 1) PM-Agent that formulates data requirements for failure cases, 2) DriveSora - a generative model that creates spatiotemporally consistent videos aligned with 3D layouts, enabling automated data collection and annotation for failure scenarios.

Result: CorrectAD corrects 62.5% of failure cases on nuScenes and 49.8% on a more challenging in-house dataset, reducing collision rates by 39% and 27% respectively across multiple end-to-end planners.

Conclusion: The proposed model-agnostic pipeline effectively addresses the long-tail problem in autonomous driving by automatically generating synthetic data for rare failure cases, significantly improving planner robustness and safety.

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [269] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D is a novel LiDAR generation pipeline that produces temporally consistent LiDAR scenes with controllable foreground objects and realistic backgrounds, achieving state-of-the-art performance on nuScenes and KITTI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing 3D LiDAR point cloud generation methods lack sequential generation capabilities and cannot produce accurately positioned foreground objects and realistic backgrounds, limiting their practical applicability in autonomous driving systems.

Method: The pipeline consists of multimodal conditions and a novel sequential noise prediction model called LiDAR4DNet, enabling end-to-end sequential generation of LiDAR scenes with full scene manipulation capability.

Result: Achieved FRD score of 743.13 and FVD score of 16.96 on nuScenes dataset, surpassing UniScene by 37.2% in FRD and 24.1% in FVD respectively.

Conclusion: DriveLiDAR4D is the first work to address sequential generation of LiDAR scenes with full scene manipulation capability, demonstrating significant improvements over state-of-the-art methods.

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [270] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: Novel Mixture-of-Experts framework for object detection using adaptive routing among multiple YOLOv9-T experts to improve mAP and AR.


<details>
  <summary>Details</summary>
Motivation: To enhance object detection performance by enabling dynamic feature specialization through multiple specialized experts rather than relying on a single model.

Method: Mixture-of-Experts framework with adaptive routing mechanism that dynamically selects among multiple YOLOv9-T experts for specialized feature processing.

Result: Achieved higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

Conclusion: The Mixture-of-Experts approach with adaptive routing successfully improves object detection performance by leveraging specialized feature processing from multiple experts.

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [271] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: The paper introduces a 'What Color Is It' dataset to test color perception hallucinations in Multimodal Large Models (MLMs) and investigates causes and solutions for visual modality hallucinations.


<details>
  <summary>Details</summary>
Motivation: MLMs are susceptible to informational interference in visual perception, especially color perception, which increases hallucination risks. The authors aim to validate this vulnerability and improve model robustness.

Method: Created the 'What Color Is It' dataset using a simple method to trigger single-modality visual hallucination in MLMs, then analyzed the underlying causes of visual modality hallucinations.

Result: The dataset successfully triggers visual hallucinations in MLMs, revealing vulnerabilities in color perception that lead to incorrect responses.

Conclusion: The study identifies specific weaknesses in MLMs' visual perception and proposes potential solutions to enhance their robustness against color perception hallucinations.

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [272] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: DelAnyFlow is a resolution-agnostic methodology for large-scale agricultural field boundary mapping that combines the DelAny instance segmentation model with structured post-processing to generate topologically consistent vector boundaries, achieving state-of-the-art accuracy and enabling national-scale applications.


<details>
  <summary>Details</summary>
Motivation: Existing methods for agricultural field boundary delineation from satellite imagery often produce incomplete boundaries, merge adjacent fields, and struggle to scale effectively for large-scale applications.

Method: DelAnyFlow combines the DelAny instance segmentation model (based on YOLOv11 backbone) trained on the FBIS 22M dataset with structured post-processing, merging, and vectorization sequence. The FBIS 22M dataset contains 672,909 multi-resolution image patches and 22.9 million validated field instances.

Result: DelAny model achieves over 100% higher mAP and 400x faster inference than SAM2. For Ukraine (603,000km), DelAnyFlow generated complete field boundary layer in under 6 hours, delineating 3.75M fields at 5m and 5.15M at 2.5m, significantly outperforming operational products from Sinergise Solutions and NASA Harvest.

Conclusion: DelAnyFlow delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data, with strong zero-shot generalization and national-scale application capabilities.

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [273] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: VOPE is a new method to evaluate hallucinations in LVLMs during voluntary imagination tasks (like story writing) by checking if models can correctly identify whether imagined objects are actually present in the original image.


<details>
  <summary>Details</summary>
Motivation: Current hallucination research focuses on factual description tasks but ignores voluntary imagination tasks where models are expected to generate novel content beyond the given image.

Method: VOPE uses recheck-based questions to evaluate how LVLMs interpret the presence of imagined objects in their own responses, measuring consistency between the model's interpretation and actual image content.

Result: Most LVLMs hallucinate heavily during voluntary imagination and perform poorly on presence evaluation for imagined objects. Existing hallucination mitigation methods show limited effectiveness in these tasks.

Conclusion: Voluntary imagination tasks present significant hallucination challenges for LVLMs, and current mitigation methods are insufficient, highlighting an important research direction for future work.

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [274] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Vigan,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: A neural representation for 3D shape mapping using flow-matching models that enables efficient cross-representation shape matching without large-scale training.


<details>
  <summary>Details</summary>
Motivation: To create a computationally efficient and modality-agnostic method for mapping between 3D shapes that works across different representations (point clouds, meshes, SDFs, volumetric data) without requiring extensive training or data-driven procedures.

Method: Represent 3D shapes as probability distributions via continuous invertible flow mappings from a fixed anchor distribution. Map points between source and target shapes by composing inverse flow (sourceanchor) with forward flow (anchortarget), using pointwise task-tailored embeddings.

Result: Achieves high coverage and accuracy across diverse shape matching benchmarks and challenging settings. Also shows promising results in UV mapping and registration of raw point cloud scans of human bodies.

Conclusion: The flow-matching based neural representation provides an effective, invertible, and modality-agnostic framework for 3D shape mapping that performs well across multiple tasks and representations.

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [275] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE is a novel framework using Dynamic Temporal-Selective Mixture of Experts for generating high-quality 3D human interactions, preserving individual characteristics while maintaining semantic fidelity to text descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to preserve unique individual characteristics and fully adhere to textual descriptions in human interaction generation, which is valuable for applications like virtual reality and robotics.

Method: Built on Dynamic Temporal-Selective Mixture of Experts with a routing mechanism that uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts, allowing dynamic capacity determination and focus on critical temporal features.

Result: Achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on InterHuman dataset and 22% on InterX dataset.

Conclusion: InterMoE effectively addresses the challenges of preserving individual characteristics and semantic fidelity in human interaction generation through its specialized expert routing mechanism.

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [276] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP is a benchmark that evaluates vision-language models' invariance to paraphrases and sensitivity to semantic flips, revealing that EVA02-CLIP and large OpenCLIP variants perform best while SigLIP models often prefer flipped captions over original ones.


<details>
  <summary>Details</summary>
Motivation: While VLMs like CLIP achieve strong zero-shot performance, their reliability in responding to controlled linguistic perturbations remains unclear, necessitating a systematic evaluation of linguistic robustness.

Method: LGIP uses 40k MS COCO images with five human captions each, automatically generating paraphrases and rule-based semantic flips that alter object category, color, or count, then measuring invariance error, semantic sensitivity gap, and positive-rate statistics.

Result: EVA02-CLIP and large OpenCLIP variants show favorable invariance-sensitivity balance, while SigLIP models exhibit high invariance error and often prefer flipped captions, especially for object and color edits.

Conclusion: LGIP provides a model-agnostic diagnostic for VLM linguistic robustness that reveals failures invisible to standard retrieval metrics, highlighting the need for specialized evaluation beyond conventional accuracy scores.

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [277] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: This study uses deep learning and remote sensing to monitor urban village redevelopment in China, finding prolonged processes, peripheral-focused transitions, and three transformation pathways, highlighting the need for context-sensitive planning.


<details>
  <summary>Details</summary>
Motivation: There's a lack of systematic evaluation of whether demolished urban villages in China have been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices.

Method: Deep learning-based framework using semantic segmentation of multi-temporal remote sensing imagery to map UV boundaries and classify post-demolition land use into six categories across four representative Chinese cities.

Result: UV redevelopment processes were frequently prolonged; transitions primarily occurred in peripheral areas while urban cores remained stable; three spatiotemporal transformation pathways identified: synchronized, delayed, and gradual optimization.

Conclusion: The study reveals fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies to support more inclusive, efficient, and sustainable urban renewal.

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [278] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: Proposes an asymptotically minimax approach for multi-target conformal prediction in ill-posed imaging inverse problems, providing tight prediction intervals with joint marginal coverage for multiple estimation targets.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods only handle scalar estimation targets, but practical applications often involve multiple targets, creating a need for multi-target uncertainty quantification in safety-critical imaging applications.

Method: Develops an asymptotically minimax approach to multi-target conformal prediction that ensures joint marginal coverage while providing tight prediction intervals. The method is applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition.

Result: Numerical demonstrations using synthetic and MRI data show benefits over existing multi-target conformal prediction methods, with tighter prediction intervals while maintaining joint coverage guarantees.

Conclusion: The proposed minimax approach effectively addresses the challenge of multi-target uncertainty quantification in ill-posed imaging inverse problems, providing improved performance over existing methods for practical applications involving multiple estimation targets.

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [279] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: Adversarial color perturbations in federated learning can compromise model interpretability by shifting saliency maps away from meaningful regions while maintaining prediction accuracy, challenging the assumption that correct predictions imply faithful explanations.


<details>
  <summary>Details</summary>
Motivation: To reveal that model interpretability itself can be an attack surface, showing that small color perturbations can stealthily degrade explanation fidelity without affecting accuracy, particularly in federated learning settings where such attacks are harder to detect.

Method: Proposed a saliency-aware attack framework called Chromatic Perturbation Module that systematically crafts adversarial examples by altering color contrast between foreground and background to disrupt explanation fidelity, with perturbations accumulating across federated learning training rounds.

Result: The attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% across multiple datasets, demonstrating that standard training pipelines are insufficient to detect or mitigate explanation degradation.

Conclusion: Interpretability techniques themselves are vulnerable to attacks, challenging the common auditing assumption that correct predictions imply faithful explanations, and highlighting the need for new defenses against explanation manipulation in safety-critical applications.

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [280] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD is a self-supervised OOD detection framework that synthesizes pseudo-OOD features from ID data and uses Neural Collapse properties to detect semantically similar OOD samples through radius-based classification on feature norms.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detectors struggle with semantically similar OOD samples that are close to in-distribution classes, which is critical for safety-sensitive applications.

Method: BootOOD synthesizes pseudo-OOD features from ID representations and uses a lightweight auxiliary head for radius-based classification on feature norms, decoupling OOD detection from the primary classifier.

Result: BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy on CIFAR-10, CIFAR-100, and ImageNet-200.

Conclusion: BootOOD provides an effective self-supervised solution for detecting semantically challenging OOD samples by leveraging Neural Collapse properties and relaxing the detection requirement to feature norm comparisons.

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [281] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: TSE-Net is a semi-supervised learning framework for monocular height estimation that uses teacher-student-exam networks with pseudo-label filtering to overcome data scarcity limitations.


<details>
  <summary>Details</summary>
Motivation: Monocular height estimation is cost-effective but limited by expensive labeled data. The scarcity of high-quality annotations hinders model generalization and performance, motivating the use of unlabeled data through semi-supervised learning.

Method: TSE-Net integrates teacher, student, and exam networks. The teacher generates pseudo-labels using joint regression and classification with hierarchical bi-cut strategy for long-tailed height distribution. The student learns from filtered pseudo-labels, while the exam stabilizes performance as a temporal ensemble.

Result: The method was evaluated on three datasets spanning different resolutions and imaging modalities, demonstrating improved performance through semi-supervised learning with unlabeled data.

Conclusion: TSE-Net effectively addresses the data scarcity problem in monocular height estimation by leveraging unlabeled data through a semi-supervised framework with pseudo-label filtering and temporal ensemble stabilization.

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [282] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: Opt3DGS is a robust optimization framework that improves 3D Gaussian Splatting (3DGS) through a two-stage process of adaptive exploration and curvature-guided exploitation, achieving state-of-the-art rendering quality without changing the underlying 3DGS representation.


<details>
  <summary>Details</summary>
Motivation: 3DGS faces optimization challenges including entrapment in suboptimal local optima and insufficient convergence quality, which limit its rendering performance despite being a leading framework for novel view synthesis.

Method: Two-stage optimization: (1) Exploration phase using Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) for global search to escape local optima; (2) Exploitation phase using Local Quasi-Newton Direction-guided Adam optimizer that leverages curvature information for precise convergence.

Result: Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process.

Conclusion: Opt3DGS successfully addresses the core optimization challenges in 3DGS through its two-stage framework, enhancing rendering performance without modifying the underlying 3DGS representation.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [283] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass is a multi-scale cell classification framework that integrates nuclear morphology and tissue context using local and global pathways with uncertainty-guided learning, achieving state-of-the-art performance on spatial transcriptomics-derived cell annotations.


<details>
  <summary>Details</summary>
Motivation: Existing tile-based models fail to incorporate broader tissue context that influences cell function, and available human annotations are coarse-grained and unevenly distributed, making fine-grained subtype-level supervision difficult to obtain.

Method: NuClass uses two pathways: Path local (224x224 crops for nuclear morphology) and Path global (1024x1024 neighborhood for tissue context), with a learnable gating module to balance local and contextual cues. It incorporates uncertainty-guided learning where the global path prioritizes regions where local path is uncertain.

Result: Achieves up to 96% F1 for best-performing class on three fully held-out cohorts, outperforming strong baselines. Constructed a marker-guided dataset from Xenium spatial transcriptomics with single-cell resolution labels for over 2 million cells across 8 organs and 16 classes.

Conclusion: Multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction, providing calibrated confidence estimates and interpretable visualizations.

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [284] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: The paper proposes an Inter-Chrominance and Luminance Interaction (ICLR) framework with a Dual-stream Interaction Enhancement Module and Covariance Correction Loss to address limitations in HVI color space for low-light image enhancement.


<details>
  <summary>Details</summary>
Motivation: Current HVI color space methods face limitations: distributional differences between chrominance and luminance branches limit complementary feature extraction, luminance errors propagate to chrominance, and traditional pixel-wise losses cause gradient conflicts in weakly correlated regions.

Method: Proposed ICLR framework includes: 1) Dual-stream Interaction Enhancement Module (DIEM) that improves complementary information extraction through fusion and enhancement dimensions, 2) Covariance Correction Loss (CCL) that uses luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance.

Result: Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods in low-light image enhancement.

Conclusion: The ICLR framework effectively addresses the limitations of HVI color space in low-light image enhancement by improving inter-branch interactions and resolving gradient conflicts, achieving superior performance compared to existing methods.

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [285] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: A machine learning framework that uses convolutional neural networks to generate conditional anatomical templates based on subject attributes like age and sex, with optional segmentation maps for improved medical image registration.


<details>
  <summary>Details</summary>
Motivation: Existing anatomical templates are computationally expensive to create and often not representative of study populations, especially with large variations. This leads to sub-optimal analysis in medical imaging tasks.

Method: Uses convolutional registration neural networks to learn a function that outputs templates conditioned on subject-specific attributes (age, sex). Leverages segmentations when available to produce anatomical segmentation maps. The network can also register subject images to the generated templates.

Result: Demonstrated on 3D brain MRI datasets, the method learns high-quality templates representative of populations. Conditional templates with annotations enable better registration than unlabeled templates and outperform other template construction methods.

Conclusion: The proposed framework efficiently generates representative conditional anatomical templates that improve registration performance in medical image analysis, addressing limitations of traditional template construction methods.

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [286] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND is a novel framework for joint nuclei detection and classification in computational pathology that uses point-level supervision enhanced by tissue mask conditioning, achieving state-of-the-art performance on the PUMA benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for nuclei detection and classification are hindered by reliance on detailed expert annotations and insufficient use of tissue context, creating a need for more efficient and context-aware methods.

Method: TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, using semantic tissue probabilities to selectively modulate the classification stream through novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM).

Result: TAND achieves state-of-the-art performance on the PUMA benchmark, surpassing both tissue-agnostic baselines and mask-supervised methods, with remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma.

Conclusion: This is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden in computational pathology.

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [287] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: Development of a real-time driver drowsiness detection system using webcam, facial landmark tracking, and Eye Aspect Ratio (EAR) to monitor eye movements and alert drowsy drivers.


<details>
  <summary>Details</summary>
Motivation: Driver fatigue causes thousands of fatalities and injuries annually, necessitating systems to improve road safety by detecting and alerting drowsy drivers.

Method: Uses standard webcam with MediaPipe Face Mesh for facial landmark detection and Eye Aspect Ratio (EAR) method to analyze eye movements, detecting long eye closures and low blinking rates as drowsiness indicators.

Result: System achieves high accuracy and quick response times in experimental tests, providing a high-performance, low-cost driver monitoring solution.

Conclusion: The system demonstrates effectiveness as a potential component for Advanced Driving Assistance Systems (ADAS) to enhance road safety by preventing fatigue-related accidents.

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [288] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow is a training-free pipeline for long-form video QA that combines Dynamic Token Dropping (DTD) with compressive long-term memory to reduce token processing by up to 87% while maintaining answer fidelity.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with long-form video QA due to growing attention and KV caches during runtime, forcing expensive inference or limited sliding window approaches.

Method: Pairs Dynamic Token Dropping (online token pruning via cosine similarity) with compressive long-term memory (fixed-size blocks, recurrent encoding for retrieval, offloading/rehydrating KV pairs), and consensus-based retrieval of Top-K relevant blocks.

Result: Outperforms current strong baselines on offline and streaming VQA benchmarks while processing up to 87% less tokens.

Conclusion: CacheFlow enables VLMs to be both efficient and context-aware for practical long-form video understanding, being drop-in, architecture-agnostic, and requiring no fine-tuning.

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [289] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM is a 3D multimodal LLM that unifies diverse 3D tasks by generating structured programs with part-level bounding boxes, semantic descriptions, and edit commands from RGB point clouds and language prompts.


<details>
  <summary>Details</summary>
Motivation: To create a unified interface for 3D tasks by decoupling symbolic planning from geometric synthesis, allowing any compatible geometry engine to be controlled through a single language-native frontend.

Method: Pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune on a large-scale part-centric dataset. The model autoregressively generates structured token sequences encoding part-level information.

Result: The model excels at producing high-quality structured plans and achieves state-of-the-art performance in grounded Q&A, compositional generation, and localized editing through one unified interface.

Conclusion: Part-X-MLLM provides a versatile, language-native interface for controlling geometry engines and unifying diverse 3D tasks through structured program generation.

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [290] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR combines Reinforcement Learning with Distribution Matching Distillation to improve few-step diffusion model performance beyond the original multi-step teacher model.


<details>
  <summary>Details</summary>
Motivation: To overcome the performance limitation where few-step distilled diffusion models are capped by their multi-step teachers, enabling better inference efficiency without quality loss.

Method: Integrates RL into distillation process using DMD loss as regularization, with dynamic distribution guidance and dynamic renoise sampling strategies to enhance initial distillation.

Result: Achieves leading visual quality and prompt coherence among few-step methods, with performance exceeding the multi-step teacher model.

Conclusion: DMDR successfully unlocks the capacity of few-step generators by simultaneous distillation and RL, demonstrating superior performance over traditional distillation approaches.

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [291] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth is a multimodal, spatio-temporal foundation model for Earth observation data that achieves state-of-the-art performance across various benchmarks and real-world tasks, with deployment as an end-to-end platform for non-profits and NGOs.


<details>
  <summary>Details</summary>
Motivation: Earth observation data has unique characteristics - it's spatial like images, sequential like video or text, and highly multimodal - requiring specialized foundation models to handle these complex data types effectively.

Method: The model employs a novel self-supervised learning formulation, masking strategy, and loss specifically designed for the Earth observation domain, and is deployed as an end-to-end platform for data collection, labeling, training, and inference.

Result: OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models, with best performance on 15 out of 24 tasks using embeddings and 19 out of 29 tasks with full fine-tuning across various research benchmarks and real-world tasks.

Conclusion: OlmoEarth successfully addresses the unique challenges of Earth observation data through specialized multimodal, spatio-temporal modeling and provides an accessible platform for non-profits and NGOs working on global problem-solving, with open-source availability of code, data, and pre-trained weights.

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [292] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light is an efficient text-guided relighting pipeline for 3D Gaussian Splatting scenes that uses LVLM to parse lighting instructions, generates illumination maps from geometry/semantic data, and produces high-fidelity relit 3D scenes without training.


<details>
  <summary>Details</summary>
Motivation: To enable intuitive text-based control over lighting in 3D scenes (direction, color, intensity) while maintaining multi-view consistency and high fidelity, addressing limitations of existing relighting and scene editing methods.

Method: Uses LVLM to parse text prompts into lighting priors, combines with depth/normals/segmentation maps to compute illumination maps, generates initial latents for diffusion model, applies multi-view relighting model, and fine-tunes 3DGS scene with relit appearance.

Result: GS-Light outperforms state-of-the-art baselines in multi-view consistency, image quality, aesthetic score, and semantic similarity, as validated by quantitative metrics and user studies on indoor/outdoor scenes.

Conclusion: The proposed pipeline enables effective text-guided 3D scene relighting with improved accuracy in lighting direction and overall quality compared to existing methods, demonstrating practical utility for 3D content creation.

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [293] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: TiViBench is a hierarchical benchmark for evaluating reasoning capabilities in image-to-video generation models across four dimensions, revealing commercial models' stronger reasoning potential and introducing VideoTPO as a test-time optimization strategy.


<details>
  <summary>Details</summary>
Motivation: Current video generation benchmarks focus on visual fidelity and temporal coherence but fail to assess higher-order reasoning abilities similar to LLMs, creating a gap in evaluating models' physical plausibility and logical consistency.

Method: Proposes TiViBench benchmark with 4 reasoning dimensions (Structural Reasoning & Search, Spatial & Visual Pattern Reasoning, Symbolic & Logical Reasoning, Action Planning & Task Execution) across 24 task scenarios and 3 difficulty levels. Also introduces VideoTPO, a test-time strategy using LLM self-analysis for preference optimization without additional training.

Result: Commercial models (Sora 2, Veo 3.1) show stronger reasoning potential, while open-source models have untapped potential limited by training scale and data diversity. VideoTPO significantly enhances reasoning performance without requiring extra training, data, or reward models.

Conclusion: TiViBench and VideoTPO establish foundations for evaluating and advancing reasoning in video generation models, addressing the gap in assessing higher-order cognitive abilities beyond visual quality.

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [294] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE is a 3D-aware autoregressive framework for intuitive object editing on real images, modeling edits as sequences of 3D transformations to maintain physical consistency and realistic effects.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models excel at semantic editing but lack 3D awareness for physically consistent object manipulation, requiring slow 3D reconstruction or operating only in 2D image space.

Method: FFSE uses an autoregressive framework that models editing as sequences of learned 3D transformations, trained on the 3DObjectEditor dataset containing simulated editing sequences across diverse objects and scenes.

Result: Extensive experiments demonstrate FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

Conclusion: FFSE enables intuitive, physically-consistent 3D object manipulation directly on real images while preserving realistic background effects and maintaining global scene consistency across multiple editing rounds.

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [295] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: The paper proposes SAAS (Segment Anything Across Shots) for multi-shot video object segmentation, addressing shot transition challenges through transition mimicking augmentation and a new benchmark called Cut-VOS.


<details>
  <summary>Details</summary>
Motivation: Existing VOS methods focus on single-shot videos and struggle with shot discontinuities, limiting real-world applicability where videos often contain multiple shots with transitions.

Method: Proposes transition mimicking data augmentation (TMA) for cross-shot generalization using single-shot data, and the SAAS model that can detect and comprehend shot transitions effectively.

Result: SAAS achieves state-of-the-art performance on YouMVOS and Cut-VOS benchmarks by effectively handling complex transitions across shots.

Conclusion: The work enables effective multi-shot video object segmentation through novel data augmentation and model design, supported by a new comprehensive benchmark for future research.

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [296] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: The paper proposes JiT (Just image Transformers), a diffusion model that directly predicts clean images rather than noise, leveraging the manifold assumption that natural data lies on low-dimensional manifolds while noised data does not.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models predict noise/noised quantities rather than clean data, which contradicts the manifold assumption that natural data occupies low-dimensional manifolds while noised data does not.

Method: Use simple, large-patch Transformers on pixels that directly predict clean images, without tokenizers, pre-training, or extra losses - essentially 'Just image Transformers' (JiT).

Result: Competitive results on ImageNet at 256x256 and 512x512 resolutions with large patch sizes of 16 and 32, where traditional noise-prediction approaches can fail catastrophically.

Conclusion: Directly predicting clean data enables apparently under-capacity networks to work effectively in high-dimensional spaces, representing a back-to-basics paradigm for Transformer-based diffusion on raw natural data.

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [297] [Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance](https://arxiv.org/abs/2511.11616)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: A hierarchical three-layered framework for multi-UAV collision avoidance that balances real-time performance, adversarial resilience, and privacy with O(nk) computational complexity, Byzantine fault tolerance, and adaptive differential privacy.


<details>
  <summary>Details</summary>
Motivation: Current monolithic collision avoidance frameworks have O(n) computational complexity and lack Byzantine fault tolerance, creating scalability and security limitations for large-scale UAV systems.

Method: Three-layered architecture: local layer with dense graph attention (<10ms latency), regional layer with sparse attention (O(nk) complexity) and federated learning with trimmed mean aggregation, global layer with Hashgraph-inspired protocol and DHT-based audit logging.

Result: Achieves 95th percentile decision within 50ms across all swarm sizes, supports 500 UAVs with <2.0% collision rate, provides Byzantine fault tolerance of f < n/3, and maintains privacy with [0.1,1.0] adaptive differential privacy.

Conclusion: The hierarchical framework successfully eliminates trade-offs between performance, security, and privacy in large-scale multi-UAV systems while maintaining scalability and Byzantine fault tolerance.

Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.

</details>


### [298] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: A robotic system for collecting tactile data from clothing using controlled sliding motions, enabling creation of motion-labeled multimodal tactile databases that improve clothing identification accuracy.


<details>
  <summary>Details</summary>
Motivation: To systematically collect tactile data from intact garments to understand physical properties that contribute to clothing comfort, requiring precise control of speed and direction during measurement.

Method: Robotic arm-based system with simulated fingertip that performs stroking measurements while controlling speed and direction, creating motion-labeled multimodal tactile databases.

Result: Machine learning evaluation showed including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating efficacy of motion-related labels for characterizing clothing tactile sensation.

Conclusion: The system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [299] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: The paper presents an image-based method using 3D Piece-Wise Clothoid modeling to analyze tendril shape changes after mechanical stimulation, showing higher responsiveness in apical segments.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between temporal shape changes, triggering events, and contact locations in climbing plant tendrils, which has been challenging to study despite long-standing research.

Method: Image-based analysis using a 3D Piece-Wise Clothoid-based geometric model to reconstruct tendril configurations after mechanical rubbing stimulation in different body portions.

Result: The reconstruction method achieved high robustness and reliability with R2 > 0.99 accuracy, revealed higher responsiveness in apical segments, and demonstrated advantages over deep learning approaches including reduced data requirements and better interpretability.

Conclusion: The study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing novel intelligent robotic systems inspired by climbing plants.

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [300] [ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts](https://arxiv.org/abs/2511.11740)
*Haowen Jiang,Xinyu Huang,You Lu,Dingji Wang,Yuheng Cao,Chaofeng Sha,Bihuan Chen,Keyu Chen,Xin Peng*

Main category: cs.RO

TL;DR: ExpertAD is a novel autonomous driving framework that uses Mixture of Experts architecture to improve decision reliability and reduce latency by amplifying task-critical features and minimizing task interference.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems face challenges with ambiguous semantics, task interference, and slow inference latency that compromise decision reliability and safety.

Method: Proposes ExpertAD framework with Perception Adapter to amplify task-critical features and Mixture of Sparse Experts to minimize task interference during prediction.

Result: Reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods, with strong generalization to unseen urban environments and rare scenarios.

Conclusion: ExpertAD effectively addresses key challenges in autonomous driving through MoE architecture, improving safety, efficiency, and decision-making in complex driving scenarios.

Abstract: Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.

</details>


### [301] [Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review](https://arxiv.org/abs/2511.11777)
*Vinit Mehta,Charu Sharma,Karthick Thiyagarajan*

Main category: cs.RO

TL;DR: This review paper examines the integration of Large Language Models (LLMs) with 3D vision for robotic sensing, covering methodologies, applications, challenges, and future directions in this emerging field.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of AI and robotics has created a need to bridge the gap between linguistic intelligence and spatial perception, enabling machines to better perceive, reason and interact with complex environments through natural language and spatial understanding.

Method: The paper provides a comprehensive analysis through: introduction of LLM and 3D data fundamentals, examination of 3D sensing technologies, exploration of scene understanding, text-to-3D generation, object grounding, embodied agents, and multimodal LLMs integrating various sensory inputs.

Result: The review identifies key advancements including zero-shot 3D segmentation, dynamic scene synthesis, language-guided manipulation, and multimodal integration with touch, auditory and thermal inputs. It also catalogs benchmark datasets and evaluation metrics for 3D-language tasks.

Conclusion: The integration of LLMs and 3D vision represents a transformative approach for robotic sensing, with future research directions including adaptive model architectures, enhanced cross-modal alignment, and real-time processing capabilities to enable more intelligent, context-aware and autonomous robotic systems.

Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.

</details>


### [302] [LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles](https://arxiv.org/abs/2511.11840)
*Shuangyu Xie,Kaiyuan Chen,Wenjing Chen,Chengyuan Qian,Christian Juette,Liu Ren,Dezhen Song,Ken Goldberg*

Main category: cs.RO

TL;DR: LAVQA is a latency-aware shared autonomy framework that combines Visual Question Answering with spatiotemporal risk visualization to help remote human operators guide self-driving vehicles safely under network delays and human response time variability.


<details>
  <summary>Details</summary>
Motivation: Self-driving vehicles may need to halt when uncertainty is high and benefit from remote human operator guidance, but critical decision timing is complicated by variable latency from wireless network delays and human response time.

Method: LAVQA integrates Visual Question Answering (VQA) with spatiotemporal risk visualization using Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty, allowing operators to see how vehicle safety regions vary over time with dynamic obstacles and delayed responses.

Result: Closed-loop simulations in CARLA show that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.

Conclusion: LAVQA effectively addresses the challenge of variable latency in shared autonomy systems for self-driving vehicles, significantly improving safety through latency-aware risk visualization and decision support.

Abstract: When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.

</details>


### [303] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: AUCS integrates SLAM with cognitive architecture for adaptive underwater navigation, using multi-sensor fusion and cognitive reasoning to handle dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in deep-sea exploration including disorientation, communication loss, and navigational failures in dynamic underwater environments.

Method: Integrates SLAM with Soar-based cognitive architecture, fusing SONAR, LiDAR, IMU, and DVL data with cognitive modules for perception, attention, planning, and learning. Includes semantic understanding, adaptive sensor management, and memory-based learning.

Result: System differentiates between dynamic and static objects, reduces false loop closures, enhances long-term map consistency, and demonstrates complete perception-cognition-action-learning loop.

Conclusion: Lays foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [304] [MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy](https://arxiv.org/abs/2511.11931)
*Saida Liu,Nikolay Atanasov,Shumon Koga*

Main category: cs.RO

TL;DR: MATT-Diff is a multi-modal diffusion policy for active multi-target tracking that balances exploration, tracking, and reacquisition behaviors without requiring prior knowledge of target states or dynamics.


<details>
  <summary>Details</summary>
Motivation: Active multi-target tracking requires balancing exploration for undetected targets with tracking detected but uncertain targets, which is challenging without prior knowledge of target numbers, states, or dynamics.

Method: Uses a diffusion policy trained on expert demonstrations from frontier-based exploration and hybrid planners, with vision transformer for map tokenization and attention mechanism for integrating variable target estimates represented as Gaussian densities.

Result: MATT-Diff demonstrates superior tracking performance compared to expert planners and behavior cloning baselines across various target motion patterns.

Conclusion: The diffusion policy effectively learns multi-modal behaviors for active target tracking and outperforms existing approaches, validating its advantages in complex tracking scenarios.

Abstract: This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.

</details>


### [305] [Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media](https://arxiv.org/abs/2511.11958)
*Derek Chen,Zoe Samuels,Lizzie Peiros,Sujaan Mukherjee,Michael C. Yip*

Main category: cs.RO

TL;DR: Systematic investigation of screw-based propulsion systems for amphibious mobility across different media, identifying dominant parameters and deriving design insights from heat sink optimization principles.


<details>
  <summary>Details</summary>
Motivation: Screw-based propulsion systems show promise for amphibious mobility but face challenges in optimizing locomotion across diverse environments like water, granular materials, and transitional zones.

Method: Principles-first approach to analyze screw performance in various media (dry sand, wet sand, saturated sand, water), using parameters inspired by heat sink design optimization.

Result: Identified dominant parameters affecting performance in different media, with derived parameters from heat sink design helping categorize performance within these dominant design parameters.

Conclusion: Provides specific insights for screw shell design and adaptive locomotion strategies to enhance screw-based propulsion system performance for versatile amphibious applications.

Abstract: Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.

</details>


### [306] [Bootstrapped LLM Semantics for Context-Aware Path Planning](https://arxiv.org/abs/2511.11967)
*Mani Amani,Behrad Beheshti,Reza Akhavian*

Main category: cs.RO

TL;DR: This paper introduces a framework that uses large language models (LLMs) as stochastic semantic sensors to modulate classical planners for robot path planning, enabling robots to adapt their movement based on natural language prompts and contextual information in human-centric environments.


<details>
  <summary>Details</summary>
Motivation: Current research on natural language prompting for robots focuses mainly on task selection and skill sequencing, but lacks attention to how robots can execute tasks safely and efficiently in semantically rich, human-centric spaces.

Method: The framework transforms LLMs into stochastic semantic sensors that output multiple "danger" judgments. A Bayesian bootstrap is applied to approximate a posterior over per-class risk, and statistics from this posterior are used to create potential costs for formulating path planning problems.

Result: The method successfully adapts robot movement in response to both explicit prompts and implicit contextual information across simulated environments and a BIM-backed digital twin, with qualitative and quantitative results presented.

Conclusion: The proposed framework effectively bridges the gap in natural language prompting for robots by enabling safe and efficient task execution through LLM-powered semantic risk assessment and classical planning integration.

Abstract: Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM "danger" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.

</details>


### [307] [ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot](https://arxiv.org/abs/2511.11970)
*Sara Wickenhiser,Lizzie Peiros,Calvin Joyce,Peter Gavrilrov,Sujaan Mukherjee,Syler Sylvester,Junrong Zhou,Mandy Cheung,Jason Lim,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: ARCSnake V2 is an amphibious snake-like robot that uses screw propulsion for versatile locomotion across land, granular media, and water, featuring multiple locomotion modes and teleoperation capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional wheeled or legged robots in extreme environments like caves, oceans, and planetary surfaces where surface variability poses significant challenges.

Method: Combines hyper-redundant snake robot mobility with Archimedean screw propulsion, featuring water-sealed mechanical design with serially linked screw and joint actuation, integrated buoyancy control, and kinematically matched handheld teleoperation.

Result: Validated underwater maneuverability, communication robustness, and force-regulated actuation through extensive experiments, demonstrating smooth transitions between screwing, wheeling, and sidewinding locomotion modes.

Conclusion: ARCSnake V2 serves as a versatile platform for exploration, search and rescue, and environmental monitoring in multi-domain settings due to its amphibious capabilities and adaptable locomotion.

Abstract: Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.

</details>


### [308] [SBAMP: Sampling Based Adaptive Motion Planning](https://arxiv.org/abs/2511.12022)
*Anh-Quan Pham,Kabir Ram Puri,Shreyas Raorane*

Main category: cs.RO

TL;DR: SBAMP integrates RRT* global planning with SEDS local control for adaptive motion planning in dynamic environments without requiring pre-trained datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based methods like RRT* struggle with real-time adaptation to dynamic changes, while learning-based approaches like SEDS rely on pre-collected data and have limited generalization to novel scenarios.

Method: Combines RRT* for global path planning with SEDS-based local controller for continuous trajectory adjustment, using Lyapunov-based stability guarantees and requiring no pre-trained datasets.

Result: Validated in simulations and real hardware (RoboRacer), demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns.

Conclusion: SBAMP provides real-time adaptation without sacrificing global path optimality, offering a scalable solution for dynamic, unstructured environments.

Abstract: Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.

</details>


### [309] [Decoupled Action Head: Confining Task Knowledge to Conditioning Layers](https://arxiv.org/abs/2511.12101)
*Jian Zhou,Sihao Lin,Shuai Fu,Qi WU*

Main category: cs.RO

TL;DR: This paper proposes a decoupled training approach for Behavior Cloning in robotic manipulation that uses kinematics-generated trajectories to pretrain an action head, then freezes it for task adaptation. This method improves training efficiency and reveals that complex backbones are unnecessary, leading to DP-MLP - a simplified model that achieves comparable performance with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current Behavior Cloning methods like Diffusion Policy are constrained by scarce paired training data and lack understanding of their internal mechanisms, limiting generalization and principled model design.

Method: Decoupled training using kinematics-generated trajectories to pretrain a general action head, then freezing it and adapting through feature modulation. Also introduces DP-MLP which replaces U-Net backbone with simple MLP blocks.

Result: Achieves feasibility in both in-distribution and out-of-distribution scenarios, with DP-C achieving 41% speedup and DP-MLP achieving 83.9% faster training under normal training and 89.1% under decoupling with only 4M parameters vs 244M.

Conclusion: The action generation backbone plays a limited role in robotic manipulation, and simpler architectures like MLP can achieve comparable performance with significant efficiency gains.

Abstract: Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.

</details>


### [310] [Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies](https://arxiv.org/abs/2511.12148)
*Advik Sinha,Akshay Arjun,Abhijit Das,Joyjit Mukherjee*

Main category: cs.RO

TL;DR: Developed a resource-efficient NEAT-based approach for obstacle-avoiding tracking control of planar snake robots in cluttered environments, achieving superior performance with low computational overhead.


<details>
  <summary>Details</summary>
Motivation: To create a computationally efficient solution for controlling snake robots in densely cluttered environments with obstacles, addressing the need for real-time obstacle avoidance and path tracking.

Method: Used Neuro-Evolution of Augmenting Topologies (NEAT) to generate dynamic gait parameters for serpenoid gait function. Input includes joint angles, link positions, head position, and obstacle data from LiDAR. Output controls frequency and offset angle for speed and heading. Reward function optimization through selective neural network propagation.

Result: The approach shows superior performance to state-of-the-art methods and comparable results to recent CBRL approach with significantly lower computational overhead. Verified through PyBullet physics engine simulations.

Conclusion: The proposed NEAT-based framework provides an efficient and effective solution for snake robot navigation in cluttered environments, balancing performance with computational efficiency.

Abstract: This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot

</details>


### [311] [Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)](https://arxiv.org/abs/2511.12160)
*Wenbin Mai,Minghui Liwang,Xinlei Yi,Xiaoyu Xia,Seyyedali Hosseinalipour,Xianbin Wang*

Main category: cs.RO

TL;DR: The paper proposes a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework that combines game theory and reachability analysis for safe, scalable multi-agent motion planning in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of safe, robust, and scalable motion planning for multi-agent systems in dynamic, uncertain environments with complex interactions, stochastic disturbances, and model uncertainties.

Method: Formulates multi-agent coordination as a dynamic potential game using Nash equilibrium, develops Neighborhood-Dominated iterative Best Response (ND-iBR) scheme for decentralized execution, and integrates Multi-Agent Forward Reachable Set (MA-FRS) for safety under uncertainty.

Result: Validated through simulations and real-world experiments in 2D and 3D environments, showing effectiveness across diverse operational scenarios.

Conclusion: The RE-DPG framework successfully addresses computational complexity and safety challenges in multi-agent systems by integrating game-theoretic coordination with reachability analysis, enabling scalable decentralized execution with theoretical guarantees.

Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.

</details>


### [312] [Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance](https://arxiv.org/abs/2511.12184)
*Jun Huo,Kehan Xu,Chengyao Li,Yu Cao,Jie Zuo,Xinxing Chen,Jian Huang*

Main category: cs.RO

TL;DR: This paper presents a variable impedance control method for supernumerary robotic legs to handle internal and external disturbances, ensuring safety and adaptability in human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: Supernumerary robotic leg systems are vulnerable to strong internal disturbances due to their floating-base nature, requiring robust force control for safety during human-robot interaction.

Method: Developed a hybrid position/force impedance controller with variable impedance control that dynamically adjusts impedance parameters, and designed a real-time stability guaranteed impedance parameters generating network.

Result: Simulations and experiments showed the system maintains smooth signal transitions in flexible states while providing strong support in rigid states, effectively handling unknown environmental disturbances.

Conclusion: The approach provides a practical solution for accommodating individual gait variations and significantly advances the safety and adaptability of human-robot systems.

Abstract: In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.

</details>


### [313] [Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization](https://arxiv.org/abs/2511.12186)
*Jun Huo,Jian Huang,Jie Zuo,Bo Yang,Zhongzheng Fu,Xi Li,Samer Mohammed*

Main category: cs.RO

TL;DR: A multi-objective optimization design theory for supernumerary robotic limbs that integrates workspace similarity, force transmission, and mass constraints, using an ellipsoid-based geometric vector method and a novel firefly algorithm for optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing general-purpose SRL devices that meet diverse functional requirements for both upper and lower limbs in rehabilitation and functional enhancement applications.

Method: Proposed MOO theory integrating grasping/walking workspace similarity, STS braced force, and mass/inertia constraints; developed ellipsoid-based geometric vector method for workspace quantification and multi-subpopulation correction firefly algorithm for optimization.

Result: Experimental validation with 6 healthy participants and 2 hemiplegia patients showed 7.2% improvement in grasp success rate, and 12.7% and 25.1% reduction in muscle activity during walking and STS tasks respectively compared to pre-optimization.

Conclusion: The proposed design theory provides an efficient option for designing multi-functional SRL mechanisms that effectively balance multiple performance objectives.

Abstract: Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.

</details>


### [314] [Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps](https://arxiv.org/abs/2511.12203)
*Antony Thomas,Fulvio Mastrogiovanni,Marco Baglietto*

Main category: cs.RO

TL;DR: A unified approach for constraint displacement problems where robots find feasible paths by displacing obstacles through a two-stage process that computes optimal obstacle displacements.


<details>
  <summary>Details</summary>
Motivation: To solve constraint displacement problems where robots need to navigate through environments by displacing obstacles rather than avoiding them completely.

Method: Two-stage process: 1) Compute trajectory through obstacles while minimizing objective function, 2) Displace obstacles to make the computed trajectory collision-free.

Result: Successful demonstration on two distinct classes of constraint displacement problems with several examples.

Conclusion: The proposed unified approach effectively solves constraint displacement problems by computing locally optimal obstacle displacements to enable feasible robot paths.

Abstract: We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.

</details>


### [315] [SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation](https://arxiv.org/abs/2511.12232)
*Lingfeng Zhang,Erjia Xiao,Xiaoshuai Hao,Haoxiang Fu,Zeying Gong,Long Chen,Xiaojun Liang,Renjing Xu,Hangjun Ye,Wenbo Ding*

Main category: cs.RO

TL;DR: SocialNav-Map is a zero-shot social navigation framework that combines human trajectory prediction with occupancy mapping, enabling safe navigation without environment-specific training and outperforming RL-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based social navigation methods require extensive training (2000+ hours) and struggle to generalize to unfamiliar environments, limiting real-world deployment.

Method: Transforms task goals into map coordinates, creates dynamic occupancy maps with predicted human movements using history and orientation prediction methods, and proactively avoids collisions.

Result: Significantly outperforms SOTA RL-based methods, reduces human collision rates by over 10% without training in novel environments, and eliminates the need for 2,396 GPU hours of training.

Conclusion: SocialNav-Map achieves superior navigation performance without environment-specific training, enabling practical deployment of social navigation systems in real-world environments with diverse human behaviors.

Abstract: Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.

</details>


### [316] [Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration](https://arxiv.org/abs/2511.12237)
*Alysson Ribeiro da Silva,Luiz Chaimowicz*

Main category: cs.RO

TL;DR: This paper proposes a MILP formulation and RTUS policy for multi-robot exploration with communication constraints and intermittent connectivity, enabling optimal rendezvous planning and realistic trajectory following in unknown environments.


<details>
  <summary>Details</summary>
Motivation: Existing MRE systems with communication constraints either use opportunistic approaches or pre-planned trajectories, but scheduling requires prior environment knowledge which limits deployment in uncertain domains like underwater exploration. Previous intermittent communications framework lacked optimal planning and realistic trajectory following.

Method: Formulated the MRE-CCIC problem, proposed a Mixed-Integer Linear Program (MILP) to generate optimal rendezvous plans, and developed the RTUS (Rendezvous Tracking for Unknown Scenarios) policy for robots to follow plans considering unknown conditions.

Result: Evaluated in large-scale Gazebo simulations, the method demonstrated prompt plan following and efficient task accomplishment. The approach successfully handles unknown scenarios while maintaining communication constraints.

Conclusion: The proposed MILP formulation and RTUS policy effectively solve the MRE-CCIC problem, enabling optimal rendezvous planning and reliable plan execution in unknown environments with communication constraints, making it suitable for real-world deployments.

Abstract: Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.

</details>


### [317] [SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty](https://arxiv.org/abs/2511.12361)
*Leroy D'Souza,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: SAC-MoE improves hybrid system control by using Mixture-of-Experts actor with learned router and curriculum training for better generalization to unseen modes.


<details>
  <summary>Details</summary>
Motivation: Hybrid systems with unobservable latent parameters and mode switches pose challenges for both model-based control and standard RL methods, which fail to handle abrupt mode changes effectively.

Method: Proposes SAC-MoE that models SAC's actor as Mixture-of-Experts with learned router, plus curriculum-based training to prioritize challenging data collection.

Result: Outperforms baselines by up to 6x in zero-shot generalization to unseen environments in hybrid autonomous racing and legged locomotion tasks.

Conclusion: The interpretable MoE router successfully activates different experts for distinct latent modes, and curriculum strategy consistently improves performance across policies.

Abstract: Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.
  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.

</details>


### [318] [Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots](https://arxiv.org/abs/2511.12380)
*Nicholas Gunter,Heiko Kabutz,Kaushik Jayaram*

Main category: cs.RO

TL;DR: Development of multilayer PVDF piezoelectric actuators with parallel voltage distribution, achieving high deflection (>3mm), force (>20mN), and bandwidth (500Hz) at low voltages (150V), demonstrated in a resonant microrobot.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators for enhanced soft microrobotic system performance.

Method: Developed and characterized multilayer PVDF actuators with parallel voltage distribution across each layer, varying layer thickness and number of layers, and validated with a first principles model.

Result: Actuators achieved >3mm free deflection, >20mN blocked force, 500Hz bandwidth at 150V; successfully integrated into a planar microrobot using resonance for locomotion with perturbation robustness.

Conclusion: Multilayer PVDF actuators effectively bridge the design space between PZT and soft polymer actuators, enabling high-performance soft microrobotic systems with demonstrated practical integration.

Abstract: Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.

</details>


### [319] [Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks](https://arxiv.org/abs/2511.12383)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: MAML-TRPO achieves effective one-shot adaptation on MetaWorld ML10 benchmark with 21.0% success on training tasks and 13.2% on test tasks, but shows generalization gap and high variance across different manipulation skills.


<details>
  <summary>Details</summary>
Motivation: Enable rapid adaptation to new robotic manipulation tasks with minimal data using meta-learning, which is critical for real-world robotic systems.

Method: Combines Model-Agnostic Meta-Learning (MAML) with Trust Region Policy Optimization (TRPO) on MetaWorld ML10 benchmark, evaluating few-shot adaptation across ten diverse robotic manipulation tasks.

Result: MAML achieves effective one-shot adaptation with clear performance improvements after single gradient update, but shows generalization gap where test task performance plateaus while training tasks continue improving, with success rates ranging 0-80% across different skills.

Conclusion: Highlights both promise and limitations of gradient-based meta-learning for diverse robotic manipulation, suggesting future work in task-aware adaptation and structured policy architectures.

Abstract: Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.

</details>


### [320] [Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control](https://arxiv.org/abs/2511.12390)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: A learning-based neural teleoperation framework replaces traditional IK+PD controllers with learned policies trained via RL, achieving better tracking, smoother motions, and superior force adaptation for humanoid robot control.


<details>
  <summary>Details</summary>
Motivation: Traditional VR teleoperation systems using IK solvers and PD controllers struggle with external forces, user adaptation, and natural motion generation under dynamic conditions.

Method: Learned policies trained via reinforcement learning directly map VR controller inputs to robot joint commands, initialized with IK-based demonstrations and fine-tuned with force randomization and trajectory smoothness rewards.

Result: 34% lower tracking error, 45% smoother motions, superior force adaptation compared to IK baseline, while maintaining 50Hz real-time control on Unitree G1 humanoid robot.

Conclusion: Learning-based approaches significantly improve naturalness and robustness of humanoid teleoperation systems for manipulation tasks like pick-and-place, door opening, and bimanual coordination.

Abstract: Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.

</details>


### [321] [RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation](https://arxiv.org/abs/2511.12436)
*Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Yanbiao Ma,Yunfeng Diao,Ziyu Jia,Wenbo Ding,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: RoboAfford++ is a generative AI-enhanced dataset for multimodal affordance learning in robotics, addressing VLMs' limitations in inferring actionable positions for physical interaction through 869,987 images with 2.0 million QA annotations.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle to infer actionable positions for physical interaction like grasping points and placement regions due to lack of fine-grained affordance annotations in training data, limiting robotic manipulation and navigation capabilities.

Method: Created RoboAfford++ dataset with 869,987 images and 2.0 million QA annotations covering object affordance recognition, object affordance prediction, and spatial affordance localization. Also developed RoboAfford-Eval benchmark with 338 annotated samples.

Result: Extensive experiments show existing VLMs have deficiencies in affordance learning, but fine-tuning on RoboAfford++ significantly enhances their ability to reason about object and spatial affordances.

Conclusion: RoboAfford++ dataset effectively addresses VLMs' limitations in affordance learning and significantly improves robotic manipulation and navigation capabilities through comprehensive affordance annotations.

Abstract: Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

</details>


### [322] [ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps](https://arxiv.org/abs/2511.12479)
*Navin Sriram Ravie,Keerthi Vasan M,Bijo Sebastian*

Main category: cs.RO

TL;DR: ClutterNav is a reinforcement learning framework for dense clutter removal that balances immediate removability costs with long-term target accessibility using a learned removability critic and integrated gradients.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dense clutter removal have limitations: rule-based planners rely on rigid heuristics with high computational overhead, while end-to-end RL approaches lack interpretability and generalizability across different conditions.

Method: ClutterNav formulates the problem as continuous RL with a removability critic trained from demonstrations to estimate removal costs based on geometric/spatial features, complemented by integrated gradients that assess how surrounding objects influence target accessibility.

Result: The approach achieves real-time, occlusion-aware decision-making in partially observable environments, demonstrating near human-like strategic sequencing without predefined heuristics in both simulation and real-world experiments.

Conclusion: ClutterNav provides an effective framework for target object retrieval in dense clutter that balances immediate action costs with long-term strategic goals, overcoming limitations of both rule-based and end-to-end RL approaches.

Abstract: Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.

</details>


### [323] [Botany Meets Robotics in Alpine Scree Monitoring](https://arxiv.org/abs/2511.12526)
*Davide De Benedittis,Giovanni Di Lorenzo,Franco Angelini,Barbara Valle,Marina Serena Borgatti,Paolo Remagnino,Marco Caccianiga,Manolo Garabini*

Main category: cs.RO

TL;DR: This paper presents a robotics-assisted approach using a legged robot (ANYmal C) to monitor scree habitats in the Italian Alps, combining robot navigation with deep learning for plant species detection and classification to improve efficiency and safety of habitat monitoring.


<details>
  <summary>Details</summary>
Motivation: Traditional scree habitat monitoring requires skilled scientists to conduct dangerous fieldwork in remote high-altitude locations, making it resource-intensive and time-consuming. Climate change poses severe threats to these unique habitats hosting endangered species.

Method: Deployed the ANYmal C legged robot in the Italian Alpine bio-region over two field campaigns, leveraging deep learning for plant species detection and classification, and pairing robot data collection with traditional phytosociological surveys by botanists.

Result: Agile legged robots can successfully navigate challenging scree terrains and increase monitoring frequency and efficiency. The robotics-assisted protocol streamlines field operations and enhances data acquisition, storage, and usage when combined with traditional surveys.

Conclusion: This research contributes to robotics in environmental science, paving the way for more comprehensive and sustainable habitat monitoring and preservation approaches by demonstrating the viability of robotics-assisted protocols in challenging natural environments.

Abstract: According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.

</details>


### [324] [EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones](https://arxiv.org/abs/2511.12618)
*Jordan Leyva,Nahim J. Moran Vera,Yihan Xu,Adrien Durasno,Christopher U. Romero,Tendai Chimuka,Gabriel O. Huezo Ramirez,Ziqian Dong,Roberto Rojas-Cessa*

Main category: cs.RO

TL;DR: EcoFlight is an energy-efficient 3D pathfinding algorithm for drones that finds lowest-energy routes while avoiding obstacles, outperforming direct-flight and shortest-distance methods.


<details>
  <summary>Details</summary>
Motivation: Most drone flight path planning schemes ignore obstacle avoidance despite its realistic importance, and obstacle avoidance can be energy-intensive, making energy efficiency critical for efficient point-to-point drone flights.

Method: Proposed EcoFlight algorithm models energy consumption based on drone propulsion system and flight dynamics to determine lowest-energy routes in 3D space with obstacles.

Result: EcoFlight consistently finds paths with lower energy consumption than direct-flight and shortest-distance schemes across various obstacle densities, especially in high-density environments. Suitable flying speed further enhances energy savings.

Conclusion: EcoFlight successfully addresses energy-efficient obstacle avoidance for drones, demonstrating superior performance over traditional path planning methods and highlighting the importance of speed optimization for additional energy savings.

Abstract: Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.

</details>


### [325] [Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning](https://arxiv.org/abs/2511.12650)
*Arvind Kumar Mishra,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: RL is used for morphology optimization in planar manipulators, successfully recovering known analytical optima and solving non-analytical cases more efficiently than grid/black-box methods.


<details>
  <summary>Details</summary>
Motivation: Most morphology design tasks lack closed-form solutions, and traditional search methods become computationally expensive as dimensionality increases, motivating the exploration of RL as a scalable alternative.

Method: Use Yoshikawa's manipulability index with RL algorithms (SAC, DDPG, PPO) to optimize manipulator morphology, first validating on a 2R manipulator with known analytical optimum, then extending to elliptical/rectangular paths with full morphology vector.

Result: All RL methods converged to the analytical solution in the validation case, and in non-analytical settings, RL converged reliably while grid/black-box methods required much larger evaluation budgets.

Conclusion: RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions, offering a scalable alternative to traditional optimization methods.

Abstract: In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.
  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.

</details>


### [326] [Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL](https://arxiv.org/abs/2511.12755)
*Aleesha Khurram,Amir Moeini,Shangtong Zhang,Rohan Chandra*

Main category: cs.RO

TL;DR: This paper introduces In-Context Reinforcement Learning (ICRL) for few-shot domain adaptation in autonomous driving, enabling closed-loop driving in adverse weather without model updates or additional data collection.


<details>
  <summary>Details</summary>
Motivation: Current domain adaptation methods for autonomous driving require extensive data collection or model retraining, which is impractical at scale. Existing prompt-driven DA methods are limited to perception tasks and require expert few-shot data.

Method: Proposes In-Context Reinforcement Learning (ICRL) that uses general trajectories observed during inference for few-shot prompt-driven domain adaptation, extending prompt-driven DA to closed-loop autonomous driving.

Result: Experiments in CARLA simulator show ICRL produces safer, more efficient, and more comfortable driving policies in target domains compared to state-of-the-art prompt-driven DA baselines.

Conclusion: ICRL advances prompt-driven domain adaptation by enabling closed-loop driving adaptation without model parameter updates or additional data collection in adverse weather conditions.

Abstract: Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.

</details>


### [327] [DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation](https://arxiv.org/abs/2511.12778)
*Vignesh Rajagopal,Kasun Weerakoon Kulathun Mudiyanselage,Gershom Devake Seneviratne,Pon Aswin Sankaralingam,Mohamed Elnoor,Jing Liang,Rohan Chandra,Dinesh Manocha*

Main category: cs.RO

TL;DR: DR. Nav is a novel autonomous navigation approach that uses cross-modal RGB-LiDAR fusion with attention-based filtering to predict dead-ends and recovery points, enabling safer navigation in unstructured environments through a continuous semantic cost map.


<details>
  <summary>Details</summary>
Motivation: Current navigation methods struggle with dead-end detection and recovery in unstructured environments where robots encounter corners, vegetation occlusions, and blocked junctions, leading to inefficient navigation and safety risks.

Method: Unifies dead-end prediction and recovery using cross-modal RGB-LiDAR fusion with attention-based filtering, generates real-time semantic cost maps with per-cell dead-end likelihoods and recovery points, and updates through Bayesian inference for robustness.

Result: Achieved 83.33% increase in detection accuracy and 52.4% reduction in time-to-goal compared to state-of-the-art planners (DWA, MPPI, Nav2 DWB) across dense indoor and outdoor scenarios.

Conclusion: DR. Nav successfully enables proactive navigation in unmapped environments by explicitly incorporating recovery-aware risk into cost maps, significantly improving both safety and efficiency in dead-end scenarios.

Abstract: We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions

</details>


### [328] [ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model](https://arxiv.org/abs/2511.12795)
*Boshu Lei,Wen Jiang,Kostas Daniilidis*

Main category: cs.RO

TL;DR: Proposes an energy-based model for grasp pose generation and active view selection method that estimates information gain from grasp distribution to efficiently grasp objects in cluttered environments with limited views.


<details>
  <summary>Details</summary>
Motivation: Previous methods for grasping in cluttered environments either overlooked grasp distribution importance for information gain estimation or relied on projections that ignore the SE(3) manifold structure of grasp poses.

Method: Uses a calibrated energy-based model that captures multi-modality of grasp distribution on SE(3) manifold, with energy levels calibrated to grasp success rates. Selects next best view by estimating information gain from calibrated distribution conditioned on reconstructed environment.

Result: Experiments show successful grasping in cluttered environments with limited view budgets compared to state-of-the-art models. Simulated environment serves as reproducible platform for future active grasping research.

Conclusion: The proposed approach effectively addresses challenges in active grasping by properly modeling grasp distributions on SE(3) manifold and using calibrated information gain for view selection, enabling efficient object grasping in cluttered scenes.

Abstract: Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.

</details>


### [329] [Structured Imitation Learning of Interactive Policies through Inverse Games](https://arxiv.org/abs/2511.12848)
*Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: A structured imitation learning framework that combines generative single-agent policy learning with game-theoretic structure to learn interactive policies from human demonstrations, achieving strong performance in multi-agent social navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication is challenging due to the higher behavioral complexity in multi-agent interactions compared to non-interactive tasks.

Method: Two-step approach: first learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning, then structurally learn inter-agent dependencies by solving an inverse game problem.

Result: Preliminary results in a synthetic 5-agent social navigation task show significant improvement over non-interactive policies and comparable performance to ground truth interactive policy using only 50 demonstrations.

Conclusion: The framework demonstrates potential for structured imitation learning in interactive settings by effectively combining behavioral pattern learning with game-theoretic structure.

Abstract: Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

</details>


### [330] [Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos](https://arxiv.org/abs/2511.12882)
*Taiyi Su,Jian Zhu,Yaxuan Li,Chong Ma,Zitai Huang,Yichen Zhu,Hanli Wang,Yi Xu*

Main category: cs.RO

TL;DR: MTV-World is an embodied world model that uses multi-view trajectory-video control for precise visuomotor prediction, addressing limitations in translating low-level actions to accurate robotic movements.


<details>
  <summary>Details</summary>
Motivation: Existing embodied world models struggle to accurately translate low-level actions into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions.

Method: Instead of using low-level actions directly, MTV-World employs trajectory videos obtained through camera parameters and Cartesian-space transformation as control signals. It introduces a multi-view framework to compensate for spatial information loss from 2D projection and forecasts future frames based on multi-view trajectory videos.

Result: Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

Conclusion: The proposed multi-view trajectory-video control approach effectively addresses spatial information loss and ensures high consistency with the physical world for embodied world models.

Abstract: Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

</details>


### [331] [Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction](https://arxiv.org/abs/2511.12896)
*Jun Huo,Hongge Ru,Bo Yang,Xingjian Chen,Xi Li,Jian Huang*

Main category: cs.RO

TL;DR: A soft six-axis force/torque sensor using air chambers with 16 barometers and a hierarchical decoupling method that reduces cross-axis coupling issues.


<details>
  <summary>Details</summary>
Motivation: To develop a soft and accurate six-axis force/torque sensor that overcomes cross-axis coupling problems common in traditional sensors.

Method: Uses hyper-elastic silicone rubber air chambers with 16-channel barometers and a rigid-soft hierarchical structure that decomposes six-axis decoupling into two three-axis problems.

Result: The prototype achieves 50N force and 1Nm torque range with 4.9% average deviation, 2.7% repeatability, 5.8% non-linearity, and 6.7% hysteresis.

Conclusion: The proposed sensor demonstrates satisfactory performance while maintaining softness through air chamber design and effective decoupling method.

Abstract: Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\%$, 2.7$\%$, 5.8$\%$ and 6.7$\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.

</details>


### [332] [TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints](https://arxiv.org/abs/2511.12910)
*Yong Li,Yujun Huang,Yi Chen,Hui Cheng*

Main category: cs.RO

TL;DR: TOPP-DWR is a time-optimal path parameterization algorithm for differential-driven wheeled robots that incorporates angular velocity, joint velocity, linear velocity, and acceleration constraints, reformulating them as SOCP for efficient computation.


<details>
  <summary>Details</summary>
Motivation: Existing TOPP approaches for mobile robots neglect angular velocity and joint velocity constraints, leading to degraded control performance in practical applications where these constraints are critical.

Method: Uses non-uniform B-spline for trajectory representation, incorporates angular velocity, joint velocity, linear velocity and acceleration constraints, transforms them into linear velocity constraints, and reformulates as second-order-cone programming (SOCP) using slack variables for computational efficiency.

Result: Comparative experiments show TOPP-DWR successfully achieves time-optimal path parameterization while satisfying all constraints, and field navigation experiments validate its practicability in real-world applications.

Conclusion: TOPP-DWR provides a systematic and practical solution for time-optimal path parameterization of differential-driven wheeled robots that properly handles angular velocity and other kinematic constraints, demonstrating superior performance in both simulation and real-world navigation scenarios.

Abstract: Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.

</details>


### [333] [DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping](https://arxiv.org/abs/2511.12912)
*Yingting Zhou,Wenbo Cui,Weiheng Liu,Guixing Chen,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: DiffuDepGrasp enables zero-shot sim2real transfer for robotic grasping by using a Diffusion Depth Generator to synthesize realistic sensor noise in simulation, eliminating computational overhead during deployment while achieving 95.7% success rate.


<details>
  <summary>Details</summary>
Motivation: Real depth maps contain sensor artifacts (voids, noise) that create significant sim2real gaps, impeding policy transfer. Existing methods suffer from data inefficiency due to unrealistic noise simulation or computational overhead from foundation models.

Method: Proposes DiffuDepGrasp with Diffusion Depth Generator containing two modules: Diffusion Depth Module uses temporal geometric priors for sample-efficient training of conditional diffusion model, and Noise Grafting Module preserves metric accuracy while injecting perceptual artifacts.

Result: Achieves 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects, using only raw depth inputs during deployment.

Conclusion: DiffuDepGrasp successfully addresses data inefficiency and deployment complexity challenges, enabling effective sim2real transfer for robotic grasping without computational overhead during deployment.

Abstract: Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.

</details>


### [334] [GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving](https://arxiv.org/abs/2511.12941)
*Chunyong Hu,Qi Luo,Jianyun Xu,Song Wang,Qiang Li,Sheng Yang*

Main category: cs.RO

TL;DR: GUIDE is a novel autonomous driving framework that uses 3D Gaussians for instance detection and occupancy prediction, overcoming limitations of traditional 3D bounding boxes and providing robust tracking capabilities with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional autonomous driving methods rely on 3D bounding boxes that fail to capture the complexity of irregularly shaped real-world objects, limiting accurate obstacle detection for effective decision-making.

Method: GUIDE employs 3D Gaussians for instance detection and occupancy prediction, using a sparse representation strategy with Gaussian-to-Voxel Splatting to provide fine-grained instance-level occupancy data without computational demands of dense voxel grids.

Result: On the nuScenes dataset, GUIDE achieves an instance occupancy mAP of 21.61, representing a 50% improvement over existing methods, while maintaining competitive tracking capabilities.

Conclusion: GUIDE establishes a new benchmark in autonomous perception systems by effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.

Abstract: In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.

</details>


### [335] [SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models](https://arxiv.org/abs/2511.12972)
*Siddarth Narasimhan,Matthew Lisondra,Haitong Wang,Goldie Nejat*

Main category: cs.RO

TL;DR: SplatSearch addresses Instance Image Goal Navigation (IIN) using sparse-view 3D Gaussian Splatting reconstructions and multi-view diffusion models to enable robust object search with single reference images.


<details>
  <summary>Details</summary>
Motivation: IIN is challenging when reference images have arbitrary viewpoints and robots operate with sparse-view reconstructions, requiring robust methods to search for specific objects in unknown environments.

Method: Uses sparse-view 3DGS reconstructions, renders multiple viewpoints around candidate objects, employs multi-view diffusion models to complete missing regions, and introduces a novel frontier exploration policy using visual and semantic context.

Result: Extensive experiments show higher performance than state-of-the-art methods in Success Rate and Success Path Length in photorealistic home and real-world environments.

Conclusion: SplatSearch effectively addresses IIN challenges through its novel architecture combining 3DGS reconstructions, multi-view completion, and semantic-aware frontier exploration.

Abstract: The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.

</details>


### [336] [CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner](https://arxiv.org/abs/2511.12984)
*Miryeong Park,Dongjin Cho,Sanghyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: Proposes a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration for planetary robots to handle elevation uncertainty near complex terrain features like craters.


<details>
  <summary>Details</summary>
Motivation: Existing methods for planetary exploration robots don't adequately handle high uncertainty in elevation estimates near complex features, lack exploration strategies for uncertainty reduction, and fail to address how elevation uncertainty affects navigation safety and map quality.

Method: Uses Kalman-based elevation estimation to generate terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions.

Result: Achieves 69% uncertainty reduction compared to baseline GBP in simulated lunar experiments, with 100% mission success rate vs 0% for baseline, demonstrating improvements in exploration safety and map reliability.

Conclusion: The proposed framework effectively addresses elevation uncertainty in planetary exploration, significantly improving navigation safety and map quality through confidence-aware exploration strategies.

Abstract: Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.

</details>


### [337] [APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation](https://arxiv.org/abs/2511.13042)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: APP is a post-processing algorithm for A* and graph-search planners that reduces path length and unnecessary heading changes through bidirectional vertices reduction and iterative path perturbation.


<details>
  <summary>Details</summary>
Motivation: A* and graph-search planners produce suboptimal paths with unnecessary zig-zag patterns even in open spaces, which contradicts human intuition and reduces path quality.

Method: Uses bidirectional vertices reduction with thorough shortcut strategy to handle path asymmetry, followed by iterative path perturbation to locally reduce heading changes and improve smoothness.

Result: APP outperforms existing methods in planning time, path length, and number of unnecessary heading changes, validated through comparative experiments and field navigation tests.

Conclusion: APP provides a practical and superior post-processing solution for improving A* paths, making them shorter, smoother, and more intuitive for robotic navigation.

Abstract: Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.

</details>


### [338] [Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments](https://arxiv.org/abs/2511.13048)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: A hybrid global path planning method for cleaning robots in semi-structured environments that balances path length with traffic rule compliance using a unidirectional road network and two-layer potential map.


<details>
  <summary>Details</summary>
Motivation: Existing methods either ignore traffic rules (leading to frequent re-planning) or strictly follow road networks (creating overly long paths), neither of which is optimal for semi-structured environments.

Method: Builds a unidirectional road network to represent traffic constraints, allows cutting across roads at start/goal points for shorter paths, and uses a two-layer potential map for complex intersections to guarantee performance.

Result: Experimental results show the method achieves better balance between path length and road network consistency compared to state-of-the-art approaches.

Conclusion: The proposed hybrid strategy provides an effective solution for global path planning in semi-structured environments by balancing efficiency and traffic rule compliance.

Abstract: Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.

</details>


### [339] [Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers](https://arxiv.org/abs/2511.13071)
*Michal Levin,Itzik Klein*

Main category: cs.RO

TL;DR: A model-free learning-based calibration method that estimates accelerometer bias under stationary conditions without requiring sensor orientation knowledge or rotation.


<details>
  <summary>Details</summary>
Motivation: Traditional accelerometer calibration requires leveling or complex orientation-dependent procedures, which are impractical for rapid field deployment.

Method: Model-free learning-based approach that estimates bias under stationary conditions without orientation knowledge or sensor rotation.

Result: Experimental validation on 13.39-hour dataset from six accelerometers shows 52% lower error than traditional techniques.

Conclusion: Provides fast, practical, scalable solution for orientation-free calibration, improving reliability of low-cost inertial sensors and eliminating need for leveled calibration.

Abstract: Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.

</details>


### [340] [ResAlignNet: A Data-Driven Approach for INS/DVL Alignment](https://arxiv.org/abs/2511.13096)
*Guy Damari,Itzik Klein*

Main category: cs.RO

TL;DR: ResAlignNet is a data-driven approach using 1D ResNet-18 architecture that transforms sensor alignment between INS and DVL systems into deep neural network optimization, achieving rapid convergence in seconds without external aids or complex maneuvers.


<details>
  <summary>Details</summary>
Motivation: Standard model-based alignment methods between INS and DVL systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, limiting operational flexibility in underwater navigation.

Method: Uses 1D ResNet-18 architecture to transform alignment problem into deep neural network optimization, operates as in-situ solution requiring only onboard sensors, and demonstrates Sim2Real transfer learning capabilities using synthetic data training.

Result: Achieves alignment accuracy within 0.8 using only 25 seconds of data collection, representing 65% reduction in convergence time compared to standard velocity-based methods.

Conclusion: Provides trajectory-independent solution that eliminates motion pattern requirements, enables immediate vehicle deployment without lengthy pre-mission procedures, and advances underwater navigation through robust sensor-agnostic alignment scalable across different operational scenarios.

Abstract: Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8 using only 25 seconds of data collection, representing a 65\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.

</details>


### [341] [Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing](https://arxiv.org/abs/2511.13100)
*Xuecheng Chen,Jingao Xu,Wenhua Ding,Haoyang Wang,Xinyu Luo,Ruiyang Duan,Jialong Chen,Xueqian Wang,Yunhao Liu,Xinlei Chen*

Main category: cs.RO

TL;DR: EventPro is an event-camera-based system that accurately estimates drone propeller rotational speeds in real-time (3ms latency, 0.23% error) and uses this data to infer drone dynamics, achieving 96.5% precision in flight command detection and improving tracking accuracy by over 22%.


<details>
  <summary>Details</summary>
Motivation: As drone applications proliferate, contactless sensing of airborne drones from the ground becomes essential. Focusing on propeller rotational speed can substantially improve drone sensing performance.

Method: EventPro uses event cameras with two components: 'Count Every Rotation' mitigates environmental noise to achieve accurate real-time propeller speed estimation, and 'Every Rotation Counts' leverages these speeds to infer both internal and external drone dynamics.

Result: In real-world drone delivery scenarios, EventPro achieves 3ms sensing latency, 0.23% rotational speed estimation error, 96.5% precision in flight command inference, and over 22% improvement in tracking accuracy when combined with other sensing modalities.

Conclusion: EventPro demonstrates that focusing on propeller rotational speed with event cameras enables highly accurate, low-latency drone sensing that significantly improves performance in real-world applications.

Abstract: As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \sysname. \sysname features two components: \textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\%. Additionally, \sysname infers drone flight commands with 96.5\% precision and improves drone tracking accuracy by over 22\% when combined with other sensing modalities. \textit{ Demo: {\color{blue}https://eventpro25.github.io/EventPro/.} }

</details>


### [342] [Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design](https://arxiv.org/abs/2511.13120)
*Trevor Exley,Anderson Brazil Nardin,Petr Trunin,Diana Cafiso,Lucia Beccai*

Main category: cs.RO

TL;DR: The paper introduces Monolithic Units (MUs) - integrated actuator-lattice-sensor building blocks for soft robotics that combine pneumatic actuation, compliant lattice structures, and optical waveguide sensing in a single printed body.


<details>
  <summary>Details</summary>
Motivation: To advance monolithic soft robotic design by creating reproducible building blocks that integrate actuation, structure, and sensing in a single fabrication process, addressing the need for scalable and reproducible soft robotic systems.

Method: Developed a parametric design framework linking actuator dimensions to lattice geometry, used experimental homogenization for material properties, employed finite element simulation with discrete optimization for sensor placement, and validated through fabrication and experimental characterization.

Result: Successfully created optimized MUs that preserve mechanical performance while enabling embedded sensing, demonstrated scalability through scaled units and a functional two-finger gripper, validating the reproducibility and generality of the approach.

Conclusion: The MU concept provides a systematic framework for monolithic soft robotic design that combines reproducible co-design rules with simulation-informed sensor integration, enabling scalable fabrication of integrated soft robotic systems.

Abstract: This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.

</details>


### [343] [Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control](https://arxiv.org/abs/2511.13188)
*Osama Al Sheikh Ali,Sotiris Koutsoftas,Ze Zhang,Knut Akesson,Emmanuel Dean*

Main category: cs.RO

TL;DR: An integrated navigation framework for AMRs that unifies environment representation, trajectory generation, and MPC using quadtree-based safe corridors and B-spline smoothing.


<details>
  <summary>Details</summary>
Motivation: To develop a coherent navigation system that efficiently handles complex environments without requiring direct obstacle encoding in the control formulation.

Method: Uses quadtree-based method to generate structured collision-free regions from occupancy maps, constructs connectivity graphs, generates trajectories, and applies B-spline smoothing within an MPC framework.

Result: Experimental results show consistent success and superior performance compared to baseline approaches in complex environments.

Conclusion: The proposed integrated framework provides efficient and reliable navigation for AMRs by unifying environment representation and control through safe corridors and MPC constraints.

Abstract: This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.

</details>


### [344] [PIGEON: VLM-Driven Object Navigation via Points of Interest Selection](https://arxiv.org/abs/2511.13207)
*Cheng Peng,Zhenzhe Zhang,Cheng Chi,Xiaobao Wei,Yanhao Zhang,Heng Wang,Pengwei Wang,Zhongyuan Wang,Jing Liu,Shanghang Zhang*

Main category: cs.RO

TL;DR: PIGEON is a zero-shot object navigation method that uses VLM-guided Points of Interest for exploration, achieving SOTA performance on benchmarks while enabling high-frequency decisions and deep reasoning.


<details>
  <summary>Details</summary>
Motivation: Current object navigation methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions in unknown environments.

Method: Proposes PIGEON framework with VLM (PIGEON-VL) to select Points of Interest during exploration, maintaining lightweight semantic snapshot memory, and using lower-level planner for high-frequency action output. Also generates RLVR data for simulator training.

Result: Achieves state-of-the-art performance on classic object navigation benchmarks through zero-shot transfer. RLVR further enhances semantic guidance capabilities for deep reasoning during real-time navigation.

Conclusion: PIGEON successfully balances decision frequency with intelligence in object navigation, demonstrating that VLM-guided PoI selection with high-frequency planning enables effective zero-shot transfer and deep reasoning capabilities.

Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.

</details>


### [345] [GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry](https://arxiv.org/abs/2511.13216)
*Chiyun Noh,Sangwoo Jung,Hanjun Kim,Yafei Hu,Laura Herlant,Ayoung Kim*

Main category: cs.RO

TL;DR: GaRLILEO is a gravity-aligned continuous-time radar-leg-inertial odometry framework that improves vertical pose estimation for legged robots by combining radar Doppler, leg kinematics, and IMU data with a novel soft S2-constrained gravity factor.


<details>
  <summary>Details</summary>
Motivation: Traditional proprioceptive odometry for legged robots suffers from vertical drift due to contact impacts, foot slippage, and vibrations. Existing methods using LiDAR or cameras degrade in feature-sparse environments and have errors from double-integrated IMU acceleration.

Method: GaRLILEO decouples velocity from IMU by building continuous-time ego-velocity splines from SoC radar Doppler and leg kinematics, enabling seamless sensor fusion. It uses a novel soft S2-constrained gravity factor to capture accurate gravity vectors without relying on LiDAR or cameras.

Result: GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes, as evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories.

Conclusion: The proposed framework effectively addresses vertical drift issues in legged robot odometry and provides improved accuracy for challenging terrains like stairs and slopes, with both dataset and algorithm open-sourced for further research.

Abstract: Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO

</details>


### [346] [EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation](https://arxiv.org/abs/2511.13312)
*Jonas Bode,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: This paper proposes using diffusion models in a visuomotor policy framework to generate precise robotic trajectories from visual and textual inputs, evaluated on the CALVIN dataset with improved performance on manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To enable robots to act effectively in human environments by developing robust natural language understanding and physical task execution capabilities.

Method: Leverages diffusion models within a visuomotor policy framework that combines visual and textual inputs, using reference demonstrations during training to learn manipulation tasks specified through text commands.

Result: Enhanced performance on various manipulation tasks and increased long-horizon success rate when executing multiple tasks sequentially on the CALVIN dataset.

Conclusion: The approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation capabilities for robots.

Abstract: Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.

</details>


### [347] [ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning](https://arxiv.org/abs/2511.13327)
*Juntao Jian,Yi-Lin Wei,Chengjie Mou,Yuhao Lin,Xing Zhu,Yujun Shen,Wei-Shi Zheng,Ruizhen Hu*

Main category: cs.RO

TL;DR: ZeroDexGrasp is a zero-shot task-oriented dexterous grasp synthesis framework that uses Multimodal Large Language Models and grasp refinement to generate human-like grasp poses aligned with task objectives and object affordances without requiring labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to generalize across diverse objects and task instructions due to heavy reliance on costly labeled data for task-specific semantic alignment.

Method: Uses prompt-based multi-stage semantic reasoning to infer initial grasp configurations from task and object semantics, followed by contact-guided grasp optimization to refine poses for physical feasibility and task alignment.

Result: Enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements.

Conclusion: Advances toward more generalizable and intelligent robotic grasping by eliminating the need for labeled training data while maintaining task alignment.

Abstract: Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.

</details>


### [348] [Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness](https://arxiv.org/abs/2511.13459)
*Bingkun Huang,Yuhe Gong,Zewen Yang,Tianyu Ren,Luis Figueredo*

Main category: cs.RO

TL;DR: A task-space, energy-safe RL framework combining PPO with movement primitives for contact-rich manipulation, incorporating energy-aware Cartesian impedance control for safe robot-environment interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional RL methods in robot joint space have limited task awareness and neglect contact-rich information in task-space manipulation, lacking contact-safety and robustness considerations.

Method: Proposed framework combines Proximal Policy Optimization (PPO) with movement primitives for reliable task-space trajectory generation, plus energy-aware Cartesian Impedance Controller for safe interactions.

Result: Outperforms existing methods on various surface types in 3D environments, achieving high success rates with smooth trajectories and energy-safe interactions.

Conclusion: The task-space, energy-safe framework effectively handles contact-rich manipulation tasks with improved safety, robustness, and performance compared to traditional RL approaches.

Abstract: Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.

</details>


### [349] [Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety](https://arxiv.org/abs/2511.13530)
*Vesna Poprcova,Iulia Lefter,Matthias Wieser,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: This paper presents a protocol for collecting a multimodal dataset to study social anxiety in human-robot interactions, addressing the current scarcity of such datasets.


<details>
  <summary>Details</summary>
Motivation: Social anxiety affects interpersonal interactions, and while AI and social robotics offer new research opportunities, the lack of multimodal datasets limits progress in detecting and understanding social anxiety manifestations.

Method: The protocol involves collecting synchronized audio, video, and physiological recordings from at least 70 participants grouped by social anxiety levels, using Wizard-of-Oz role-play scenarios with the Furhat social robot in controlled conditions.

Result: The dataset will include multimodal data (audio, video, physiological) plus contextual data to capture individual variability in social anxiety responses during human-robot interactions.

Conclusion: This work can advance affect-adaptive human-robot interaction research by enabling robust multimodal detection of social anxiety through the provided dataset.

Abstract: Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.

</details>


### [350] [OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving](https://arxiv.org/abs/2511.13707)
*Xiaoyu Liang,Ziang Liu,Kelvin Lin,Edward Gu,Ruolin Ye,Tam Nguyen,Cynthia Hsu,Zhanxin Wu,Xiaoman Yang,Christy Sum Yu Cheung,Harold Soh,Katherine Dimitropoulou,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: OpenRoboCare is a multimodal dataset for robot caregiving that captures expert occupational therapist demonstrations of Activities of Daily Living (ADLs), addressing the lack of large-scale, diverse, and expert-driven datasets for caregiving robotics.


<details>
  <summary>Details</summary>
Motivation: Caregiving tasks involve complex physical human-robot interactions requiring precise perception under occlusions, safe physical contact, and long-horizon planning. Current robot learning approaches lack large-scale, diverse, and expert-driven datasets that capture real-world caregiving routines.

Method: Collected data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing.

Result: The dataset provides rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. Analysis reveals expert caregiving principles and strategies that can improve robot efficiency and task feasibility. Evaluations show the dataset presents challenges for state-of-the-art robot perception and human activity recognition methods.

Conclusion: OpenRoboCare addresses a critical gap in caregiving robotics by providing a comprehensive multimodal dataset that captures expert demonstrations and presents challenges for current methods, enabling development of safer and more adaptive assistive robots.

Abstract: We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.

</details>


### [351] [From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands](https://arxiv.org/abs/2511.13710)
*Jianglong Ye,Lai Wei,Guangqi Jiang,Changwei Jing,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: This paper introduces a co-design framework that optimizes both control and hardware design of multi-fingered robotic hands to enable both power and precision grasping capabilities within a single system through lightweight fingertip geometry modifications.


<details>
  <summary>Details</summary>
Motivation: Current robotic hands are effective for power grasps but struggle with precision manipulation, requiring separate parallel grippers. This limitation prevents versatile manipulation capabilities in a single system.

Method: Joint optimization of control and hardware design using a differentiable neural-physics surrogate model. Introduces lightweight fingertip geometry modification represented as a contact plane, with dynamic control switching between power and precision modes.

Result: Achieved 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping and 93.3% success rate in real-world bread pinching tasks, while maintaining power grasp capabilities.

Conclusion: The co-design framework significantly enhances fine-grained manipulation ability of multi-fingered hands without compromising their power grasp capabilities, bridging the gap between power and precision manipulation.

Abstract: Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [352] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: LLM-generated synthetic news headlines can effectively replace real-world data for NLP tasks, particularly for negative sentiment analysis, with results closely matching real headlines except for proper noun usage.


<details>
  <summary>Details</summary>
Motivation: To overcome data acquisition challenges and privacy concerns with real-world data by using LLM-generated synthetic datasets for NLP tasks, specifically focusing on negative sentiment analysis.

Method: Created a specialized corpus of negative news headlines using tailored LLM prompts, validated by expert review, and analyzed in embedding space. Benchmarking against real headlines using Comparative Perplexity Test, Readability Test, POS Profiling, BERTScore, and Semantic Similarity.

Result: Synthetic headlines closely matched real headlines across most metrics, with the only significant difference being in proper noun scores from POS profiling.

Conclusion: LLM-generated synthetic datasets are a viable alternative to real-world data for NLP applications, particularly for sentiment analysis tasks involving negative valence text.

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [353] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stani,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB benchmark evaluates LLMs on climate change knowledge, finding frontier models have PhD-level synthesis but poor grounding with high hallucination rates.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs handle complex, specialized knowledge in climate change through open-ended, grounded, multimodal question answering with clear quality and evidential requirements.

Method: Created CLINB benchmark using real users' questions and expert-curated rubrics, implemented model-based evaluation process, and tested several frontier models against hybrid expert+model answers.

Result: Frontier models show remarkable knowledge synthesis (PhD-level understanding) and outperform expert-assisted weaker models, but suffer from poor grounding with substantial hallucination rates for references and images.

Conclusion: Bridging the gap between knowledge synthesis and verifiable attribution is crucial for AI deployment in science, requiring reliable benchmarks like CLINB to build trustworthy AI systems.

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [354] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying is a synthetic multi-LLM conversational dataset for cyberbullying detection that simulates realistic bullying interactions as an ethical alternative to human data collection, featuring multi-turn conversations, context-aware annotations, and fine-grained labeling.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable and ethically safe alternative to human data collection for cyberbullying research by leveraging LLMs to simulate realistic bullying interactions while avoiding the ethical concerns of collecting real bullying data.

Method: Created a synthetic dataset using multiple LLMs to simulate realistic bullying conversations, featuring multi-turn exchanges, context-aware annotations assessing harmfulness within conversational flow, and fine-grained labeling across various cyberbullying categories.

Result: The dataset was evaluated across five dimensions including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. It was tested for utility as both standalone training data and as an augmentation source for cyberbullying classification.

Conclusion: SynBullying provides a viable synthetic alternative to human-collected cyberbullying data that can be used for both training and data augmentation purposes in cyberbullying detection research.

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [355] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard is a novel approach that combines causal reasoning with symbolic logic to detect and prevent hallucinations in large language models by understanding the causal chain leading to false statements and intervening early in the generation process.


<details>
  <summary>Details</summary>
Motivation: Large language models have a critical weakness of confidently stating false but plausible information (hallucinations), which has become a major barrier to using these models in accuracy-critical applications. Existing solutions are inadequate as they require retraining, add computational costs, or miss the root causes of hallucinations.

Method: CausalGuard works through two complementary paths: tracing causal relationships between what the model knows and what it generates, and checking logical consistency using automated reasoning. Unlike previous methods that only check outputs after generation, it intervenes early in the process by understanding the causal chain that leads to false statements.

Result: Across twelve benchmarks, CausalGuard correctly identifies hallucinations 89.3% of the time with only 8.3% missed hallucinations. It reduces false claims by nearly 80% while maintaining natural and helpful responses. The system performs especially well on complex reasoning tasks requiring multiple logical steps.

Conclusion: CausalGuard effectively addresses the hallucination problem in LLMs by combining causal reasoning with symbolic logic, providing transparent reasoning processes that make it particularly valuable for sensitive applications like medical diagnosis and financial analysis where understanding decision rationale is crucial.

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [356] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: A quantitative framework that separates skill and chance in games by modeling them as complementary sources of control over stochastic decision trees, with applications ranging from game design to AI evaluation.


<details>
  <summary>Details</summary>
Motivation: To provide a principled method for quantifying the relative contributions of skill and luck in games, enabling objective comparisons across different game types and applications in game design and AI.

Method: Define Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L, and introduce volatility Sigma to quantify outcome uncertainty over successive turns.

Result: Analysis of 30 games reveals a continuum from pure chance (coin toss, S = -1) to pure skill (chess, S = +1), with poker showing moderate skill dominance (S = 0.33). Backgammon falls in the middle (S = 0).

Conclusion: The framework enables principled comparisons of player influence, game balance, and predictive stability, with broad applications in game design, AI evaluation, and risk assessment.

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [357] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR is a zero-shot agentic framework that uses layered prompt analysis and LLM-based rewriting to make text-to-image generation safer while preserving creativity and user intent.


<details>
  <summary>Details</summary>
Motivation: Current generative vision-language models can produce unsafe, offensive, or culturally inappropriate content when prompted adversarially, and existing defenses struggle to align outputs with human values without sacrificing quality or being cost-effective.

Method: VALOR integrates multi-level NSFW detection, cultural value alignment, and intention disambiguation modules. When unsafe content is detected, prompts are rewritten by an LLM under role-specific instructions, with optional stylistic regeneration if needed.

Result: Experiments show VALOR reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity across adversarial, ambiguous, and value-sensitive prompts.

Conclusion: VALOR provides a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [358] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sren Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel is an LLM agent that autonomously generates and implements creative scientific ideas in quantum physics, demonstrating the potential for AI to accelerate scientific discovery by bridging idea generation and experimental implementation.


<details>
  <summary>Details</summary>
Motivation: To automate both idea generation and implementation in science, shifting the role of humans in the scientific process and accelerating discovery in quantum physics.

Method: Uses an LLM agent to formulate ideas from literature and employs a domain-specific AI tool to convert these ideas into concrete experiment designs ready for laboratory implementation.

Result: Generated scientifically interesting ideas including new quantum teleportation variations, quantum network primitives in indefinite causal orders, and novel geometric phase concepts; two ideas led to independent scientific follow-up papers.

Conclusion: AI-Mandel demonstrates a prototypical AI physicist capable of generating actionable ideas, accelerating science while revealing challenges toward achieving human-level artificial scientists.

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [359] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: A 3B-parameter LLM trained via reinforcement learning learns to iteratively construct and debug SPARQL queries for knowledge graph question answering, achieving 49.7% accuracy on LC-QuAD 2.0 with 17.5 percentage point improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating SPARQL queries lack adaptive policies to dynamically debug queries based on real-time execution feedback, hindering reliable interaction with structured knowledge graphs.

Method: An agentic framework where an LLM learns a resilient policy for iterative SPARQL construction through outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, incorporating explicit deliberative reasoning steps.

Result: Achieved 49.7% accuracy on LC-QuAD 2.0 after entity linking, representing a 17.5 percentage point improvement over the strongest iterative zero-shot baseline.

Conclusion: This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and structured knowledge graphs.

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [360] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: The paper critiques intelligence-focused benchmarks for LLMs and proposes generality as a more stable foundation for evaluation, arguing that intelligence lacks clear definition while generality directly links to measurable performance breadth.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like ARC and Raven tests evaluate abstract intelligence but fail to predict real-world performance on practical tasks like question answering and coding, risking misalignment between evaluation and utility.

Method: Through conceptual and formal analysis, the authors examine three assumptions underlying intelligence-focused evaluation: generality, stability, and realism, showing only generality withstands scrutiny.

Result: The analysis demonstrates that intelligence is not what enables generality; instead, generality is best understood as a multitask learning problem that directly connects evaluation to performance breadth and reliability.

Conclusion: Evaluation should be grounded in generality rather than abstract intelligence, reframing AI progress assessment with generality as a more stable foundation for evaluating capabilities across diverse and evolving tasks.

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [361] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: LLMs show strong natural language to First-Order Logic translation skills when evaluated with proper protocols that distinguish genuine logical understanding from pattern recognition.


<details>
  <summary>Details</summary>
Motivation: First-Order Logic is powerful for representing natural language concepts, but converting NL to FOL has been challenging. While LLMs promised breakthroughs, existing evaluation methods may misrepresent their actual capabilities.

Method: The authors critically examined existing evaluation datasets and protocols, proposed a new evaluation protocol to distinguish genuine semantic understanding from pattern recognition/memorization, and tested state-of-the-art LLMs using this approach.

Result: Dialogue-oriented LLMs demonstrated strong NL-FOL translation skills and genuine grasp of sentence-level logic, while embedding-centric models performed significantly worse.

Conclusion: Proper evaluation protocols reveal that modern LLMs have strong NL-FOL translation capabilities, showing genuine logical understanding rather than just pattern recognition.

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [362] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception is a new benchmark that uses topological properties to evaluate global visual perception in Large Vision-Language Models, revealing that current models perform no better than random chance and more powerful models actually show worse performance.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs have visual perception as a bottleneck, and conventional benchmarks contain local shortcuts that overestimate models' perceptual abilities. There's a need for a shortcut-free evaluation of global visual perception.

Method: Developed TopoPerception benchmark that leverages topological properties (which depend on global structure and are invariant to local features) to evaluate global visual perception across various granularities.

Result: All evaluated state-of-the-art models performed no better than random chance at the coarsest perceptual granularity. More powerful models with stronger reasoning capabilities actually exhibited lower accuracy, showing an inverse relationship between reasoning power and global perception ability.

Conclusion: Merely scaling up models is insufficient and may worsen global perception deficits. Progress requires new training paradigms or architectures. TopoPerception exposes a critical bottleneck in current LVLMs and provides direction for improvement.

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [363] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O is an AI system that analyzes surgical videos to detect gestures and predict patient outcomes, achieving performance comparable to human annotations in predicting postoperative recovery.


<details>
  <summary>Details</summary>
Motivation: There is a need for fine-grained analysis of surgical behavior and its impact on patient outcomes, which remains a challenging area in surgical analytics.

Method: Uses transformer-based spatial and temporal modeling with frame-wise classification to detect consecutive short gestures (~2 seconds) in nerve-sparing prostatectomy videos, then derives features like gesture frequency, duration, and transitions.

Result: Achieved AUC of 0.80 frame-level and 0.81 video-level for gesture detection. Predicted postoperative outcomes with 0.79 accuracy (vs 0.75 human annotations), with strong correlation (r=0.96) and identified patterns like prolonged tissue peeling and reduced energy use linked to erectile function recovery.

Conclusion: F2O enables automatic interpretable assessment and establishes a foundation for data-driven surgical feedback and clinical decision support.

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [364] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI is an LLM unlearning framework that provably removes only the marginal information contributed by data to be unlearned while preserving retained data information, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: As AI models train on expanding datasets, selective removal of specific data influence is essential for privacy protection and regulatory compliance, especially for resource-intensive LLMs where retraining from scratch is impractical.

Method: The framework penalizes marginal information to remove only the additional information contributed by data to be unlearned, providing an explicit upper bound on residual influence and provable undetectability.

Result: Extensive experiments show the approach outperforms state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks.

Conclusion: This represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising effectiveness.

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [365] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Sekin Ar*

Main category: cs.AI

TL;DR: This study systematically evaluates four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) on abstract visual reasoning using four reasoning architectures, finding GPT-4.1-Mini consistently performs best and highlighting model-specific architectural sensitivities.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLM performance in abstract visual reasoning problems and understand how different reasoning architectures affect model performance across various LLMs.

Method: Evaluated four LLM models using four reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, multi-agent) on RAVEN-FAIR dataset with three-stage visual response generation (JSON extraction, LLM reasoning, Tool Function), using SSIM and LPIPS metrics, Chain-of-Thought scores, and error type analysis.

Result: GPT-4.1-Mini consistently achieved highest overall accuracy across all architectures; multi-agent architecture occasionally altered semantic/numeric balance but effects were not uniformly beneficial; each model showed distinct sensitivity patterns to architectural design; response coverage variations complicated cross-architecture comparisons.

Conclusion: Reasoning effectiveness in abstract visual problems remains model-specific, with GPT-4.1-Mini demonstrating strongest capabilities; multi-run evaluations are necessary as single-run assessments are fragile and unreliable for drawing conclusions.

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [366] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: This chapter discusses challenges and future directions for building reliable agentic AI systems, focusing on mitigating cascading failures, dynamic environments, inconsistent task execution, unpredictable emergent behaviors, and resource-intensive reliability mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing need for reliable AI systems, particularly agentic AI systems that can operate autonomously in complex environments, while mitigating risks and ensuring robust performance.

Method: The chapter presents perspectives and discusses open research problems through analysis of challenges in dynamic environments, task execution inconsistencies, emergent behaviors, and reliability mechanisms.

Result: The analysis identifies several key research challenges including cascading failures mitigation, handling dynamic environments, managing inconsistent task execution, predicting emergent behaviors, and developing efficient reliability mechanisms.

Conclusion: There are significant research opportunities in testing and evaluating the reliability of agentic AI systems, with multiple open problems requiring further investigation to build trustworthy autonomous AI systems.

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [367] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: The paper introduces the rebound Winner-Take-All (RWTA) motif as a scalable neuromorphic control architecture that combines discrete computation reliability with continuous regulation tunability, demonstrated through a snake robot nervous system design.


<details>
  <summary>Details</summary>
Motivation: To create a unified neuromorphic control architecture that bridges discrete computation and continuous regulation, addressing both rhythmic generation and decision-making in a single framework.

Method: Developed the rebound Winner-Take-All (RWTA) motif as the basic building block, combining winner-take-all state machines for discrete computation with excitable biophysical circuits for continuous tuning.

Result: Created a versatile, robust, and modular architecture that successfully controls a snake robot's nervous system, demonstrating the framework's practical applicability.

Conclusion: The RWTA-based architecture provides a scalable solution that unifies continuous rhythmic generation and discrete decision-making in neuromorphic control systems.

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [368] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: CFA-SMOTE combines counterfactual methods from XAI with SMOTE to address class imbalance in climate data, improving prediction of rare climate events like droughts.


<details>
  <summary>Details</summary>
Motivation: AI struggles with climate-disrupted data because traditional ML methods rely on historical distributions and fail to handle out-of-distribution outlier events like extreme weather.

Method: Proposes CFA-SMOTE - a novel data augmentation method that combines instance-based counterfactual methods from Explainable AI with SMOTE to create synthetic data points representing climate outlier events.

Result: Comparative experiments show CFA-SMOTE improves predictive performance for grass growth prediction during drought conditions compared to benchmark methods, especially under different class-imbalance ratios.

Conclusion: Treating climate change prediction as a class-imbalance problem and using counterfactual-based data augmentation can effectively handle climate-disrupted data and improve prediction of rare climate events.

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [369] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: A hybrid neuro-symbolic framework combining LLMs with symbolic logic (Prolog) achieves deterministic detection of statutory inconsistencies in complex law, using the U.S. Internal Revenue Code as a case study.


<details>
  <summary>Details</summary>
Motivation: LLM-based methods struggle with hierarchical processing and deep structured reasoning over long legal texts, and tax-specific applications remain sparse despite potential for compliance and statutory drafting support.

Method: Combined GPT-4o, GPT-5, and Prolog in experiments where GPT-4o translated legal text to Prolog rules, then tested Prolog-augmented prompting for inconsistency detection, with GPT-5 guiding refinement of the hybrid model.

Result: GPT-4o alone achieved only 33% accuracy in inconsistency detection, while the hybrid Prolog model produced deterministic, reproducible results and successfully detected inconsistency zones with accurate, internally consistent validation.

Conclusion: LLM-assisted formalization anchored in symbolic logic enables transparent and reliable statutory inconsistency detection, overcoming limitations of probabilistic LLM approaches.

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [370] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: The paper proposes DDR (Direct Dependency Retrieval), a novel retrieval-augmented framework for statement autoformalization that directly generates and verifies library dependencies from natural language math descriptions, achieving superior precision and recall compared to SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing autoformalization methods lack contextual awareness and suffer from poor precision/recall in formal library dependency retrieval, while current approaches lack scalability to leverage growing public datasets effectively.

Method: Proposed DDR framework directly generates candidate library dependencies from natural language math descriptions and verifies them via efficient suffix array checks, enabling construction of a 500k+ sample dataset and fine-tuning of a high-precision DDR model.

Result: DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Autoformalizers with DDR show consistent performance advantages in single-attempt accuracy and multi-attempt stability over traditional RAG methods.

Conclusion: DDR provides an effective solution to the core challenges in statement autoformalization by enabling efficient, high-precision dependency retrieval that scales well with growing datasets.

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [371] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: The paper introduces Chain-of-Evidence (CoE) paradigm and Look As You Think (LAT) framework to improve visual evidence attribution in document retrieval-augmented generation, enabling verifiable reasoning with bounding box evidence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for visual evidence attribution lack fine-grained supervision and progressive traceability throughout the reasoning process, making it difficult to verify evidence sources in multimodal question answering.

Method: Proposes Chain-of-Evidence (CoE) paradigm that unifies Chain-of-Thought reasoning with visual evidence attribution using bounding boxes and page indexes. Introduces Look As You Think (LAT) reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution.

Result: LAT consistently improves vanilla Qwen2.5-VL-7B-Instruct model with average gains of 8.23% in soft exact match and 47.0% in IoU@0.5. Outperforms supervised fine-tuning baseline and shows stronger cross-domain generalization.

Conclusion: The proposed CoE paradigm and LAT framework effectively enhance visual evidence attribution in VD-RAG systems, enabling more reliable and verifiable multimodal reasoning with improved performance and generalization.

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [372] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH is an interpretable AI framework that enables multimodal large language models to perform evidence-linked diagnostic reasoning in pathology, autonomously deriving diagnostic criteria through self-learning without requiring large labeled datasets or model weight updates.


<details>
  <summary>Details</summary>
Motivation: Current AI tools in pathology lack human-readable reasoning needed to audit decisions and prevent errors, limiting clinical adoption despite improvements in screening throughput and prognostic pattern recognition.

Method: A two-phase self-learning process: diversification expands pathology-style explanations, while optimization refines them for accuracy. Uses multimodal large language models without white-box access or weight updates, requiring only small labeled sets.

Result: Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines.

Conclusion: RECAP-PATH provides clinically trustworthy AI by uniting visual understanding with reasoning, demonstrating a generalizable path toward evidence-linked interpretation in pathology.

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [373] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: A deep reinforcement learning algorithm called Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO) is introduced to address high-dimensional, multi-objective optimization challenges in smart tyre manufacturing, showing improved accuracy and efficiency in production control.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized scheduling and inflexible production lines in the rubber tyre industry struggle with dynamic production demands and complex subsystem coordination due to nonlinear interactions and emergent dynamics.

Method: MPD-PPO algorithm with multi-branch policy architecture and differentiated gradient clipping constraints for stable high-dimensional policy updates, validated on width and thickness control in tyre film production.

Result: Substantial improvements in both tuning accuracy and operational efficiency, successfully tackling high dimensionality, multi-objective trade-offs, and dynamic adaptation challenges.

Conclusion: The framework delivers enhanced performance and production stability for real-time industrial deployment in tyre manufacturing, addressing key challenges in smart manufacturing systems.

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [374] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: This paper introduces T-BoN BO, a Bayesian optimization framework for language models that optimizes evaluation efficiency rather than query efficiency, using Best-of-N selection and textual gradients to emulate optimal exploration.


<details>
  <summary>Details</summary>
Motivation: Current self-improving AI focuses on query efficiency, but many real-world applications face evaluation bottlenecks where human feedback is costly and time-consuming. The paper aims to optimize for evaluation efficiency instead.

Method: Proposes T-BoN BO framework that combines Best-of-N selection strategy with textual gradients to statistically emulate the behavior of gradients on the UCB acquisition function, enabling optimal exploration in language-space Bayesian optimization.

Result: Empirical validation on automated ad alignment tasks shows T-BoN BO outperforms state-of-the-art baselines, demonstrating superior evaluation efficiency in AI self-improvement.

Conclusion: The proposed T-BoN BO framework effectively addresses the evaluation efficiency challenge in self-improving AI by bridging Bayesian optimization with language models through simple yet theoretically grounded methods.

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [375] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: Embedding CFR algorithm uses neural network embeddings to represent information sets in imperfect-information games, enabling more precise strategy solving than traditional discrete clustering methods.


<details>
  <summary>Details</summary>
Motivation: Traditional discrete clustering methods for game abstraction lose critical quantitative information about subtle differences between information sets, which compromises strategy quality in large imperfect-information games like poker.

Method: Pre-trains embeddings of information set features into low-dimensional continuous space, then performs Counterfactual Regret Minimization (CFR) within this embedding space using regret accumulation and strategy updates.

Result: Experiments on poker games show significantly faster exploitability convergence with the same spatial overhead compared to cluster-based abstraction algorithms.

Conclusion: Embedding CFR is the first poker AI algorithm to pre-train information set abstractions through low-dimensional embedding, demonstrating superior performance over traditional clustering approaches.

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [376] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd is a practical algorithm that addresses excessive abstraction in hand abstraction for imperfect-information games like Texas hold'em by using k-recall winrate features and earth mover's distance clustering.


<details>
  <summary>Details</summary>
Motivation: Excessive abstraction in hand abstraction impairs AI performance in large-scale imperfect-information games, particularly due to extreme implementations of imperfect-recall abstraction that discard historical information.

Method: Developed KrwEmd algorithm using k-recall winrate features that leverage both future and historical game information to distinguish signal observation infosets, then clusters them using earth mover's distance to measure feature discrepancies.

Result: Experimental results show KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

Conclusion: KrwEmd effectively addresses the excessive abstraction problem in hand abstraction for imperfect-information games by incorporating historical information through novel feature engineering and clustering techniques.

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [377] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: The paper proposes a comprehensive solution to address catastrophic forgetting in smaller language models during knowledge distillation from large models, using a carefully constructed dataset with metacognitive knowledge and a novel training method called GDPO.


<details>
  <summary>Details</summary>
Motivation: Existing datasets and fine-tuning approaches for knowledge distillation face challenges with catastrophic forgetting, especially for models smaller than 8B, due to ignoring the relationship between training data knowledge and model's inherent abilities, and conventional training objectives failing to constrain knowledge preservation.

Method: Constructed a 5K-instance dataset covering multiple reasoning tasks with metacognitive knowledge annotations, filtered based on task knowledge and model's inherent skills. Introduced GDPO (Group Direction Preference Optimization) that efficiently approximates GRPO performance, uses large model guidance, and constrains optimization path through reference model to prevent excessive parameter drift.

Result: Extensive experiments demonstrate that the approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

Conclusion: The proposed comprehensive solution effectively addresses catastrophic forgetting in smaller models during knowledge distillation through both data construction (metacognitive knowledge integration) and training methodology (GDPO optimization).

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [378] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol is a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning, addressing limitations of existing methods and improving alignment performance by up to 47%.


<details>
  <summary>Details</summary>
Motivation: Existing molecular sequence representation methods treat molecular captioning and text-based molecular design as separate tasks, facing limitations in chemical accuracy, ambiguous training data, and bidirectional inconsistency.

Method: Proposes RTMol framework using self-supervised round-trip learning that unifies molecular captioning and text-to-SMILES generation, with novel round-trip evaluation metrics and unsupervised training capabilities.

Result: RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

Conclusion: The RTMol framework successfully addresses key limitations in molecular-text alignment and provides an effective solution for bidirectional molecular-text understanding and generation tasks.

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [379] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: DRedMTL is an incremental reasoning algorithm for DatalogMTL that efficiently handles dynamic updates by extending the classical DRed algorithm to work with periodic interval representations.


<details>
  <summary>Details</summary>
Motivation: Existing DatalogMTL reasoning approaches lack support for efficient dynamic updates, which is crucial for real-world applications with frequent data updates.

Method: Extends the classical DRed algorithm with specifically designed operators to handle periodic representations of DatalogMTL materialisations, which consist of finite sets of facts plus periodic intervals.

Result: Experimental results show DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude, on several publicly available datasets.

Conclusion: DRedMTL provides an effective incremental reasoning solution for DatalogMTL with bounded intervals, addressing the critical need for handling dynamic updates in temporal data reasoning.

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [380] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: Proposes Debate over Mixed-knowledge (DoM), a multi-agent framework that dynamically integrates structured KG and unstructured text knowledge for incomplete KGQA, and introduces a more realistic dataset based on real-world knowledge updates.


<details>
  <summary>Details</summary>
Motivation: Real-world KGs are often incomplete, leading to Incomplete KGQA problems. Existing methods lack adaptive and contextual fusion of multiple knowledge sources, failing to exploit their complementary strengths.

Method: DoM framework uses specialized agents for KG inference and text retrieval (RAG), decomposes questions into sub-questions, retrieves evidence via dual agents, and employs a judge agent to evaluate and aggregate answers through iterative interaction.

Result: DoM consistently outperforms state-of-the-art baselines in experiments, demonstrating improved robustness to KG incompleteness by exploiting knowledge complementarity.

Conclusion: The proposed DoM framework effectively addresses IKGQA by dynamically integrating multiple knowledge sources, and the new dataset provides a more realistic benchmark for evaluating incomplete KGQA systems.

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [381] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: ViTE is a novel pedestrian trajectory prediction framework that uses virtual graphs and expert routing to model both explicit one-hop and implicit high-order interactions without deep GNN stacks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches face a trade-off between insufficient layers causing under-reaching problems and excessive depth leading to high computational costs. There's a need for adaptive modeling of both explicit one-hop and implicit high-order interactions.

Method: Proposes ViTE with two modules: Virtual Graph that introduces dynamic virtual nodes to model long-range interactions without deep GNNs, and Expert Router that adaptively selects interaction experts using Mixture-of-Experts design based on social context.

Result: Experiments on ETH/UCY, NBA, and SDD benchmarks demonstrate consistent state-of-the-art performance, validating both effectiveness and practical efficiency.

Conclusion: ViTE provides a flexible and scalable solution for pedestrian trajectory prediction that effectively models complex interactions while maintaining computational efficiency.

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [382] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: The paper critically examines whether AI world models truly capture human-level understanding by using philosophical case studies to highlight differences between world model capabilities and genuine human understanding.


<details>
  <summary>Details</summary>
Motivation: To determine if AI world models, which simulate external world aspects and enable prediction, actually represent human-like understanding as opposed to just statistical correlations.

Method: Using case studies from philosophy of science literature to analyze where world model capabilities diverge from human understanding, focusing on specific philosophical analyses.

Result: The analysis reveals limitations in how world models characterize human understanding, showing distinctions between world model capabilities and genuine human comprehension.

Conclusion: While world models represent progress in AI, they may not fully capture human-level understanding, as philosophical perspectives reveal important differences in the nature of understanding.

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [383] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA is a vision-based system that detects unplanned extubation risk in ICUs using synthetic video data, identifying high-risk movements like hand collision with airway tubes and patient agitation through pose estimation.


<details>
  <summary>Details</summary>
Motivation: Unplanned extubation is a critical patient safety issue in ICUs, but real-time detection has been limited due to ethical and privacy challenges in obtaining annotated ICU video data.

Method: Used text-to-video diffusion to generate synthetic ICU scenarios, applied pose estimation to detect two high-risk movement patterns: collision (hand entry near airway tubes) and agitation (velocity of anatomical keypoints).

Result: Expert assessments confirmed synthetic data realism, with high accuracy for collision detection and moderate performance for agitation recognition.

Conclusion: Demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems deployable in intensive care settings.

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [384] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Mobile-Agent-RAG is a hierarchical multi-agent framework that uses dual-level retrieval augmentation to improve mobile agent performance on long-horizon, cross-application tasks by addressing strategic hallucinations in planning and operational errors in UI execution.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art mobile agents have inadequate success rates on real-world, long-horizon, cross-application tasks due to excessive reliance on static internal knowledge in MLLMs, leading to strategic hallucinations in planning and operational errors in UI execution.

Method: Proposes Mobile-Agent-RAG with dual-level retrieval augmentation: Manager-RAG for high-level planning (retrieving human-validated task plans) and Operator-RAG for low-level execution (retrieving precise UI operation guidance). Constructs two specialized knowledge bases and introduces Mobile-Eval-RAG benchmark.

Result: Significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2% on realistic multi-app, long-horizon tasks.

Conclusion: Establishes a robust paradigm for context-aware, reliable multi-agent mobile automation by addressing the fundamental distinction between planning knowledge and operational knowledge through hierarchical retrieval augmentation.

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [385] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: This paper addresses the out-of-distribution moral alignment problem in LLMs by developing a method to train agents to apply consistent moral reasoning frameworks to scenarios beyond their training distribution, using a novel dataset and optimization approach.


<details>
  <summary>Details</summary>
Motivation: Large language models increasingly influence human moral decisions, but current approaches focus on evaluation rather than actively steering moral decisions. There's a need for LLMs to learn to apply consistent moral reasoning frameworks to scenarios outside their training distribution.

Method: Introduced Moral-Reason-QA dataset with 680 human-annotated moral scenarios and framework-specific reasoning traces. Used Group Relative Policy Optimization with composite rewards that optimize both decision alignment and framework-specific reasoning processes.

Result: Successfully generalized to unseen moral scenarios with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks on out-of-distribution evaluation sets. Revealed training challenges and promising directions for future research.

Conclusion: LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making.

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [386] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench is a dynamic benchmark using real Upwork jobs to evaluate LLM agents' workplace competence, with expert-created rubrics for fine-grained assessment beyond binary metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are static/synthetic and don't capture how AI agents perform in real, economically meaningful work environments, limiting insights into practical competence and human collaboration.

Method: Uses verified Upwork job transactions with expert-created rubrics, decomposing jobs into detailed acceptance criteria for fine-grained evaluation with human expertise integrated throughout the pipeline.

Result: Provides a scalable framework for evaluating agentic systems in authentic labor-market contexts with detailed feedback on model strengths, weaknesses, and instruction-following fidelity.

Conclusion: UpBench offers a human-centered foundation for AI evaluation that supports human-AI collaboration, positioning AI as amplifying human capability through partnership rather than replacement.

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [387] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: RGR-GRPO is a rubric-driven RL framework that uses rubrics to provide fine-grained rewards and offline guidance for multi-domain reasoning, outperforming existing RL methods across 14 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing RL methods that focus on single-domain with verifiable rewards and restrict exploration space in purely online RL frameworks.

Method: Leverage rubrics to provide both fine-grained reward signals and offline guidance, enabling LLMs to receive dense rewards while exploring larger solution space during GRPO training.

Result: Achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks compared to verifiable online RL baseline, with stable entropy fluctuations and superior pass@k performance.

Conclusion: RGR-GRPO demonstrates effective breakthrough beyond existing performance bottlenecks through sustained exploration and maintains stable training dynamics.

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [388] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: The paper introduces computational-rational user models for cognitively-bounded agents, modeling how memory limitations lead to biased beliefs and sub-optimal decisions. It proposes an online inference method to identify cognitive bounds from observed actions.


<details>
  <summary>Details</summary>
Motivation: Predicting sub-optimal human behavior is challenging because such behaviors often result from rational decisions under cognitive bounds and biased beliefs, not irrationality.

Method: Proposes computational-rational user models with explicit cognitive processes, using nested particle filtering for online inference of latent belief states and cognitive bounds from observed actions.

Result: Validated in navigation tasks with memory decay, showing the model generates plausible behaviors for different memory capacities and accurately recovers ground-truth cognitive bounds from 100 observations.

Conclusion: Provides a principled foundation for adaptive AI assistants that can account for users' memory limitations through real-time inference of cognitive bounds.

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [389] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: A framework for autonomous agents to dynamically learn and adapt to varying suggester reliability in partially observable environments, using Bayesian inference and strategic "ask" actions.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume static suggester quality, limiting practical deployment. Agents need to handle varying reliability of external action suggestions in uncertain environments.

Method: Integrate suggester quality into belief representation with Bayesian inference, and introduce explicit "ask" actions to strategically request suggestions while balancing acquisition costs.

Result: Experimental evaluation shows robust performance across varying suggester qualities, adaptation to changing reliability, and effective management of suggestion requests.

Conclusion: Provides foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments through dynamic reliability learning and strategic suggestion acquisition.

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [390] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: A conversational self-triage system that guides LLMs with 100 clinically validated AMA flowcharts achieves high accuracy in retrieval (95.29%) and navigation (99.10%), combining free-text interaction with standardized protocols for transparent AI-assisted healthcare.


<details>
  <summary>Details</summary>
Motivation: Online health resources and LLMs have limited reliability in healthcare due to low accuracy, lack of transparency, and susceptibility to unverified information, creating a need for more structured and auditable patient decision support systems.

Method: Multi-agent framework with retrieval agent, decision agent, and chat agent that uses 100 AMA-validated clinical flowcharts to guide LLMs through structured patient interactions, evaluated using synthetic datasets of simulated conversations.

Result: Achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles (N=37,200), demonstrating reliable performance at scale.

Conclusion: The approach demonstrates feasibility of transparent, accurate, and generalizable AI-assisted self-triage that can support informed patient decision-making while improving healthcare resource utilization.

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [391] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: The paper introduces ARCHE, a task for extracting latent reasoning chains from scientific text, and ARCHE Bench, a benchmark derived from Nature Communications articles. It evaluates LLMs' ability to decompose reasoning into standard inference modes (deduction, induction, abduction) using logic-aware metrics.


<details>
  <summary>Details</summary>
Motivation: Current LLMs produce unstructured reasoning outputs that obscure whether they truly understand fundamental scientific reasoning paradigms, creating a gap between model capabilities and scientific reasoning rigor.

Method: Proposed Latent Reasoning Chain Extraction (ARCHE) task where models decompose reasoning into Reasoning Logic Trees (RLT) with three Peircean inference modes. Created ARCHE Bench with 70 Nature Communications articles, 1,900+ references, and 38,000+ viewpoints. Introduced Entity Coverage (EC) and Reasoning Edge Accuracy (REA) metrics.

Result: Evaluation of 10 leading LLMs shows a trade-off between REA and EC metrics. No model could extract complete and standard reasoning chains, revealing significant gaps in current reasoning capabilities compared to scientific argumentation requirements.

Conclusion: There is a substantial gap between current reasoning models' abilities and the rigor needed for scientific argumentation, highlighting the need for improved reasoning extraction and evaluation methods.

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [392] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT is a BERT-based foundation model for Limit Order Books that treats multi-dimensional messages as single tokens, achieving state-of-the-art performance in price movement and next message prediction with shorter context lengths.


<details>
  <summary>Details</summary>
Motivation: Existing LOB models have cumbersome data representations and lack adaptability for different tasks, making them impractical for general-purpose use in high-frequency trading environments.

Method: Adapts BERT architecture for LOB data with novel tokenization that treats complete multi-dimensional messages as single tokens while maintaining continuous representations of price, volume, and time.

Result: Achieves leading performance in predicting mid-price movements and next messages while reducing required context length compared to previous methods.

Conclusion: LOBERT provides a general-purpose foundation model for LOB data that is suitable for downstream fine-tuning and outperforms previous approaches in key prediction tasks.

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [393] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA is a prompt-based framework that enhances conversational recommender systems by integrating pretrained language models with knowledge graphs through retrieval-augmented generation, selective knowledge filtering, and collaborative preference modeling.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods that fail to fully exploit PLM reasoning over graph relationships, indiscriminately incorporate retrieved knowledge without context filtering, and neglect collaborative preferences in multi-turn dialogues.

Method: Constructs dialogue-specific knowledge trees from KGs and serializes them into texts for structure-aware reasoning, selectively filters context-relevant knowledge, explicitly models collaborative preferences with specialized supervision, and uses a semantic alignment module to harmonize heterogeneous inputs.

Result: Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

Conclusion: The proposed PCRS-TKA framework effectively integrates PLMs with KGs through retrieval-augmented generation, achieving superior performance in conversational recommender systems by addressing key challenges in knowledge integration and collaborative preference modeling.

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [394] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: A dynamic tree database variant for state compression in planning tasks achieves high compression ratios with minimal runtime overhead.


<details>
  <summary>Details</summary>
Motivation: Scaling explicit state-space search for large tasks requires compact state representation; static tree databases need large preallocation.

Method: Proposed a dynamic variant of tree databases for compressing state sets over propositional and numeric variables, maintaining static counterpart properties.

Result: Empirical evaluation shows compression ratios of several orders of magnitude, often with negligible runtime overhead in classical and numeric planning tasks.

Conclusion: Dynamic tree databases provide efficient state compression for both grounded and lifted planning, enabling scalable state-space search.

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [395] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: A strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to diverse partner strategies in real-time for human-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: Enable artificial agents to adapt to human partners with unique preferences and policies that change dynamically during interactions, especially in time-pressured tasks with complex strategic spaces.

Method: Uses variational autoencoder to encode strategies into latent space, identifies strategy types through clustering, trains cooperator conditioned on clusters, and employs fixed-share regret minimization for online adaptation to novel partners.

Result: Achieves state-of-the-art performance when paired with novel human and agent teammates in the Overcooked collaborative cooking environment.

Conclusion: The proposed framework effectively enables real-time adaptation to diverse partner strategies, demonstrating superior collaborative performance in complex human-agent teams.

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [396] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: Random walks on modern embedding spaces produce optimal foraging patterns matching human semantic memory retrieval, while more complex sampling methods like Metropolis-Hastings don't align with human behavior.


<details>
  <summary>Details</summary>
Motivation: To test whether modern high-dimensional embeddings can produce human-like memory retrieval patterns consistent with optimal foraging theory (Marginal Value Theorem) in semantic fluency tasks.

Method: Used state-of-the-art embeddings and prior semantic fluency data to compare random walks vs. Metropolis-Hastings sampling on embedding spaces.

Result: Simple random walks produced results consistent with optimal foraging and MVT, while Metropolis-Hastings sampling did not match human behavior patterns.

Conclusion: Appropriately structured embeddings with simple sampling can produce near-optimal foraging dynamics, challenging assumptions that complex sampling mechanisms are necessary for cognitive modeling of memory retrieval.

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [397] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet uses LLMs to quantify event reports, builds causal knowledge from treatment effects, and injects this into a GNN-LSTM network via causal attention to improve traffic forecasting during disruptions.


<details>
  <summary>Details</summary>
Motivation: Standard GNNs fail during non-recurring traffic events because they learn historical patterns that become invalid when new causal factors (like accidents) disrupt normal traffic flow.

Method: Uses LLM to quantify unstructured event reports, builds causal knowledge base via average treatment effects estimation, and integrates this into dual-stream GNN-LSTM network with causal attention mechanism.

Result: Reduces prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines on real-world datasets.

Conclusion: Bridges gap between correlational models and causal reasoning, providing more accurate, transferable, and interpretable traffic forecasting during critical disruptions.

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [398] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: This paper investigates resource optimization in heterogeneous satellite clusters for Earth Observation missions using Reinforcement Learning, showing that MARL enables effective coordination across satellites with different capabilities while managing energy and memory constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods struggle with the real-time, uncertain, and decentralized nature of Earth Observation operations, motivating the use of RL and MARL for adaptive decision-making in satellite clusters.

Method: The study formulates optimization problems from single-satellite to multi-satellite scenarios, using a near-realistic simulation environment built on Basilisk and BSK-RL frameworks to evaluate MARL algorithms like MAPPO, HAPPO, and HATRPO.

Result: Results show that MARL enables effective coordination across heterogeneous satellites (two optical and one SAR satellite), balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling.

Conclusion: The findings provide practical insights into scalable, autonomous satellite operations and establish a foundation for future research on intelligent Earth Observation mission planning under heterogeneous and dynamic conditions.

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [399] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: This paper introduces a lifelong learning approach for Inductive Logic Programming (ILP) that leverages compositional logic rules across sequential tasks, improving scalability and performance through efficient rule reuse.


<details>
  <summary>Details</summary>
Motivation: Most neural ILP research focuses on single-problem architectures, but there's limited exploration of learning paradigms involving sequences of problems. The authors aim to leverage the compositional and transferable nature of logic rules for efficient lifelong learning.

Method: The authors introduce a compositional framework for lifelong ILP learning, where logic rules acquired from earlier tasks are efficiently reused in subsequent ones. They formalize this approach and evaluate it empirically on sequences of tasks.

Result: Experimental results validate the feasibility and advantages of this lifelong learning paradigm, showing improved scalability and performance through compositional rule reuse across tasks.

Conclusion: The work opens new directions for continual learning in Neural-Symbolic AI by demonstrating the effectiveness of lifelong learning ILP that leverages the transferable nature of logic rules across sequential problems.

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [400] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: This paper introduces a framework using passive Brain-Computer Interfaces (BCI) with fNIRS recordings to guide reinforcement learning from human feedback, achieving 67% F1 score for binary classification and demonstrating improved performance with subject-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a more natural and implicit method for RLHF by using neural signals from passive BCIs instead of explicit human feedback, enabling brain-driven alignment of agent behavior.

Method: Used functional near-infrared spectroscopy (fNIRS) recordings from 25 participants across three domains (Pick-and-Place Robot, Lunar Lander, Flappy Bird), trained classifiers to predict agent performance levels and regressors to predict action deviation from optimal policies.

Result: Achieved 67% average F1 score for binary classification and 46% for multi-class models across domains. Fine-tuning with subject-specific data increased F1 scores by 17% (binary) and 41% (multi-class), demonstrating cross-subject generalization.

Conclusion: Mapping implicit fNIRS signals to agent performance is feasible and can be significantly improved with personalization, laying groundwork for future brain-driven RLHF systems that use neural signals for more natural human-agent alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [401] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: Proposes a preference-based policy optimization (PbPO) framework using min-max game between policy and reward model with theoretical guarantees, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: To align LLMs with human preferences without extensive manual annotations through bootstrapping and self-improvement.

Method: Min-max game between main policy and reward model constrained within confidence set from preference data, with iterative online algorithm for active data collection.

Result: Outperforms existing state-of-the-art preference optimization techniques on five benchmarks with theoretical regret bounds.

Conclusion: PbPO framework enables effective bootstrapping of LLMs through preference-based optimization with reliable theoretical guarantees and superior empirical performance.

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [402] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP integrates language into economic decision-making via a Think-Speak-Decide pipeline, outperforming MARL and LLM-only baselines in returns, robustness, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Economic decisions depend on both structured signals and unstructured language, but current MARL struggles with semantic ambiguity and contextual richness of language.

Method: LAMP framework with Think-Speak-Decide pipeline: Think interprets observations, Speak exchanges strategic messages, Decide fuses data into MARL policy for language-augmented decisions.

Result: Outperforms MARL (+63.5%) and LLM-only (+34.0%) in cumulative return, and shows improved robustness (+18.8%, +59.4%) and interpretability in economic simulations.

Conclusion: Language-augmented policies like LAMP can deliver more effective and robust economic strategies by bridging the gap to real-world settings.

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [403] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: Online learning of generalized HTN methods from ChatGPT-generated decompositions to reduce LLM calls while maintaining or improving problem-solving performance.


<details>
  <summary>Details</summary>
Motivation: To reduce dependency on ChatGPT queries in HTN planning by learning reusable methods from generated decompositions, improving efficiency while maintaining problem-solving capability.

Method: Extends ChatHTN planner by learning generalized HTN methods from ChatGPT-generated task decompositions through online learning, creating reusable methods rather than just memoizing specific instances.

Result: Experiments show reduced ChatGPT calls while solving at least as many problems, and sometimes more problems than the baseline approach.

Conclusion: Online learning of generalized HTN methods from LLM-generated decompositions effectively reduces LLM dependency while maintaining or enhancing problem-solving performance in HTN planning.

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [404] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: The paper proposes Chain-of-Scheduling (CoS), a framework that uses Large Language Models for event scheduling in social networks by breaking the task into three stages and using knowledge distillation to enable autonomous generation.


<details>
  <summary>Details</summary>
Motivation: Existing event scheduling methods in Event-based Social Networks face trade-offs between efficiency, effectiveness, and generalization due to the NP-hard nature of the problem, requiring a better solution that can maximize user preferences while handling time and geographical constraints.

Method: CoS framework activates LLMs' scheduling capability through a guided process with three atomic stages: exploration, verification, and integration. It enables autonomous CoS generation via Knowledge Distillation.

Result: CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets, operates in an interpretable manner, and demonstrates strong zero-shot learning ability on out-of-domain data.

Conclusion: The CoS framework successfully addresses the event scheduling problem by leveraging LLMs through a structured approach, achieving high performance while maintaining efficiency and generalization capabilities.

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [405] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow is an LLM-based multi-agent system that automates power grid fault diagnosis by extracting regulatory logic, integrating expert knowledge, optimizing reasoning, and generating executable workflows.


<details>
  <summary>Details</summary>
Motivation: Manual power grid fault diagnosis is inefficient, error-prone, and lacks maintainability due to reliance on dense regulations and tacit expert knowledge that evolves over time.

Method: Systematically extracts regulatory logic into fault trees, integrates expert knowledge via human-in-the-loop verification, optimizes reasoning with AlphaEvolve module, and synthesizes verified logic into n8n-executable workflows.

Result: Experimental validation shows 100% topological consistency and high semantic fidelity on transformer fault diagnosis datasets, substantially reducing expert workload.

Conclusion: Fault2Flow establishes a reproducible path from fault analysis to operational automation, bridging the gap between regulatory knowledge and expert experience in power grid fault diagnosis.

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [406] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3 is a general-purpose agent framework that enables autonomous cross-platform operation across three heterogeneous strategy game environments by integrating vision-language reasoning with precise UI execution capabilities.


<details>
  <summary>Details</summary>
Motivation: Automated operation in cross-platform strategy games requires agents with robust generalization across diverse user interfaces and dynamic battlefield conditions, but current vision-language models remain largely unexplored for complex human-computer interaction scenarios like strategy gaming.

Method: Integrates Qwen2.5-VL's vision-language reasoning with UI-TARS's precise execution capabilities, using a closed-loop pipeline of screen capture, model inference, and action execution. Evaluates multimodal data combinations (static images, multi-image sequences, videos) and proposes combination granularity concepts.

Result: Hybrid strategy (MV+S) reduces inference time by 63% and boosts BLEU-4 score by 12.98x (from 4.81% to 62.41%). Successfully performs core tasks including target localization, combat resource allocation, and area control with strong real-time performance and cross-platform generalization.

Conclusion: Provides an efficient solution for strategy game automation and establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [407] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG is a knowledge-graph system that imposes domain-consistent structure on LLMs for scientific reasoning and drug discovery, reducing rule violations by 83.2% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for mathematically and biomedically valid outputs from LLMs in scientific reasoning and early-stage drug discovery, ensuring domain consistency and rule satisfaction.

Method: Uses a compact knowledge-graph scaffold with a lightweight verifier that injects symbolic facts into prompts and enforces rule satisfaction through deterministic checking, formalizing generation as constrained inference with soft guidance for decoding.

Result: Across 90 tasks in reaction feasibility, metabolic compatibility, and toxicity screening, reduces violation counts by 83.2% relative to chain-of-thought baseline while improving exact match, with stable results under stratification and negligible latency.

Conclusion: MedRule-KG provides a practical approach for interactive drug design by effectively steering LLM generation toward valid outputs while maintaining efficiency.

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [408] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach is a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM-powered agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency in web navigation tasks.

Method: WebCoach consists of three components: WebCondenser (standardizes raw navigation logs into summaries), External Memory Store (organizes complete trajectories as episodic experiences), and Coach (retrieves relevant experiences and injects task-specific advice via runtime hooks).

Result: On the WebVoyager benchmark, WebCoach improved task success rates from 47% to 61% with a 38B model while reducing/maintaining average steps. Smaller models with WebCoach achieved performance comparable to GPT-4o.

Conclusion: WebCoach enables web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks and achieving self-evolution through continuous curation of episodic memory from new navigation trajectories.

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [409] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: GEM is a generative entropy-guided preference modeling approach for LLM alignment in low-resource scenarios that uses cognitive filtering and self-evaluated group advantage to extract fine-grained cognitive signals from human preferences without requiring large-scale annotations.


<details>
  <summary>Details</summary>
Motivation: In professional domains like medicine and law, large-scale preference labels for LLM alignment are often unavailable, creating a need for methods that work with limited preference data.

Method: Uses cognitive filtering with entropy theory and CoT prompting to generate diverse reasoning chains, then applies token scoring to weight high-confidence answers and strategic high-entropy tokens. Fine-tunes LLM using SEGA algorithm that aggregates group-level cognitive signals and transforms entropy scores into implicit rewards.

Result: Experiments on general benchmarks and domain-specific tasks (mathematical reasoning, medical dialogues) show significant improvements with few-shot preference data.

Conclusion: GEM enables highly efficient few-shot alignment of LLMs by establishing an entropy-guided closed-loop cognitive optimization framework that allows LLMs to rely on their own judgments.

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [410] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: This paper evaluates language models' ability to maintain internal world models in conversations under linguistic alterations, revealing their limitations in tracking entities and proposing interpretability methods and regularization strategies to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Real conversations contain complex pragmatic elements that require building local world models, but it's unclear whether language models can maintain robust implicit representations of conversations and handle linguistic alterations.

Method: Applied seven minimal linguistic alterations to conversations from popular datasets, created two benchmarks with yes-no questions, evaluated various LMs, and proposed a dual-perspective interpretability framework to identify useful/harmful transformer layers.

Result: LMs struggle to maintain robust accuracy under linguistic alterations, particularly in memorizing crucial details and tracking entities. The analysis revealed specific harmful layers that encode spurious signals or rely on shortcuts.

Conclusion: The proposed layer-regularization based fine-tuning strategies can suppress harmful layers' effects, potentially improving LMs' ability to maintain robust world models in conversations under linguistic variations.

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [411] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: This paper analyzes proof verification methods for mathematical reasoning in LLMs, showing that single benchmarks can be misleading and proposing a combined GenSelect + LLM-as-a-Judge framework as the most effective verification approach.


<details>
  <summary>Details</summary>
Motivation: Current LLMs achieve good final-answer performance on math problems but often have flawed reasoning. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities.

Method: Evaluated both proof-based and final-answer reasoning, scaled two generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens, and used reinforcement learning to reduce prompt sensitivity.

Result: The combination of GenSelect and LLM-as-a-Judge is most effective for solution verification. RL reduces prompt sensitivity but doesn't improve final-answer precision, suggesting models reward procedural correctness over mathematical validity.

Conclusion: Established practical guidelines for designing scalable proof-verification systems, showing current models often prioritize style over substance in mathematical reasoning.

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [412] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI is a multi-stage framework for GUI grounding that separates coarse region selection from fine-grained element grounding using specialized vision-language agents, achieving state-of-the-art accuracy on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI grounding systems use monolithic models that lack modularity and fail under visual clutter and ambiguous instructions, creating a need for more robust and structured approaches.

Method: Multi-stage framework with coarse Region-of-Interest selection followed by fine-grained element grounding, featuring bidirectional ROI zoom algorithm and context-aware rewriting agent to handle spatial dilution and semantic ambiguity.

Result: Achieved 73.18% accuracy on ScreenSpot-Pro benchmark (visually dense) and 68.63% on OSWorld-G benchmark (semantically complex), surpassing previous state-of-the-art results.

Conclusion: The modular multi-stage approach leveraging complementary strengths of vision-language models at different visual scales consistently outperforms monolithic approaches in GUI grounding tasks.

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [413] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP is a reinforcement learning framework that improves multi-turn interaction efficiency by using success-rate-aware trajectory sampling and step-level optimization instead of traditional trajectory-level approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional trajectory-level optimization in online RL is inefficient due to uniform sampling across tasks regardless of difficulty, penalization of correct intermediate actions in failed trajectories, and high sample-collection costs.

Method: STEP dynamically allocates sampling based on per-task success rates, maintains smoothed success-rate records for adaptive trajectory resampling, computes success-rate-weighted advantages, decomposes trajectories into step-level samples, and applies step-level GRPO augmentation for low-success tasks.

Result: Experiments on OSWorld and AndroidWorld show STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

Conclusion: STEP effectively addresses the limitations of trajectory-level optimization by combining success-rate-aware sampling with step-level optimization, leading to more efficient and stable multi-turn reinforcement learning.

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [414] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: MM-Telco is a multimodal benchmark suite for adapting LLMs to telecom domain challenges, addressing tasks like network operations, management, documentation, and retrieval through text and image-based tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs have potential to transform telecom operations but face domain-specific challenges that require specialized adaptation for network optimization, troubleshooting, customer support, and compliance.

Method: Proposed MM-Telco - a comprehensive multimodal benchmark suite with text and image-based tasks addressing real telecom use cases, with baseline experiments using various LLMs and VLMs.

Result: Models fine-tuned on the dataset show significant performance improvements, and experiments reveal weaknesses in current state-of-art multimodal LLMs for telecom applications.

Conclusion: MM-Telco accelerates LLM adaptation for telecom by providing specialized benchmarks and identifying areas needing further development in multimodal LLM capabilities.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [415] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T is a hierarchical MARL framework that uses conditional diffusion models to dynamically decompose complex tasks into subtasks, enabling efficient learning in cooperative multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Task decomposition helps with complex cooperative MARL tasks but learning dynamic decomposition from scratch requires many training samples, especially in large joint action spaces under partial observability.

Method: Two-level hierarchical framework: high-level policy learns subtask representation using conditional diffusion model to predict next observations/rewards; low-level agents learn specialized skills; uses multi-head attention mixing network for value decomposition.

Result: Experimental results on various benchmarks show C$	ext{D}^	ext{3}$T achieves better performance than existing baselines.

Conclusion: The proposed framework successfully enables dynamic task decomposition and efficient hierarchical learning for complex cooperative MARL tasks.

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [416] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: InteractiveGNNExplainer is a visual analytics framework that enhances GNN explainability through coordinated interactive views and graph editing capabilities, enabling users to perform "what-if" analysis and gain deeper understanding of GNN predictions.


<details>
  <summary>Details</summary>
Motivation: GNNs are effective but operate as opaque "black boxes", which hinders user trust, debugging, bias detection, and adoption in critical domains requiring explainability.

Method: Integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques, plus interactive graph editing for "what-if" analysis.

Result: Facilitates in-depth misclassification diagnosis, comparative analysis of GCN vs GAT behaviors, and rigorous probing of model sensitivity through case studies on Cora and CiteSeer datasets.

Conclusion: The framework fosters deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [417] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA introduces an auction-based framework that treats communication bandwidth as a scarce resource, enabling LLM-based multi-agent systems to achieve state-of-the-art performance with dramatically reduced token usage through strategic bidding and selective communication.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of 'free-for-all' communication in multi-agent LLM systems, which leads to exponential token costs and low signal-to-noise ratios, by applying resource rationality principles to communication.

Method: Dynamic Auction-based Language Agent (DALA) framework that treats communication as a centralized auction where agents bid for speaking opportunities based on predicted message value density, encouraging concise and informative communication.

Result: Achieves SOTA performance across 7 reasoning benchmarks (84.32% on MMLU, 91.21% pass@1 on HumanEval) with only 6.25M tokens on GSM8K, significantly outperforming existing methods in efficiency.

Conclusion: Resource-constrained communication through auction mechanisms enables efficient multi-agent reasoning, cultivating emergent strategic silence and adaptive communication strategies that dramatically reduce token usage while maintaining high performance.

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [418] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: This paper presents Wheatley, a GNN+DRL framework for solving RCPSP with uncertain task durations, aiming to minimize expected project duration through reusable baseline schedules.


<details>
  <summary>Details</summary>
Motivation: RCPSP has industrial applications but real-world task durations are uncertain, requiring resilient scheduling approaches that can handle uncertainty while maintaining reusability.

Method: Combines Graph Neural Networks with Deep Reinforcement Learning to develop a scheduling policy that works like a priority dispatch rule, paired with Serial Schedule Generation Scheme.

Result: Empirical evaluation on standard benchmarks shows superior performance and generalization ability compared to existing approaches.

Conclusion: The Wheatley framework is effective for RCPSP with uncertain durations and has been made publicly available to support further research and reproducibility.

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [419] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: A novel verbalization strategy for robot plans that measures information gain against a user's prior knowledge using second-order theory of mind, enabling more informative communication than incremental plan-order strategies.


<details>
  <summary>Details</summary>
Motivation: Existing robot verbalization strategies like incremental plan-order communication fail to consider what is effectively informative to users, as they don't account for users' prior knowledge about the robot.

Method: Proposed a verbalization strategy that measures information gain of verbalizations against a second-order theory of mind model of the user, capturing the user's prior knowledge about the robot.

Result: Experiments show this strategy allows users to understand the robot's goal much quicker than incremental or decreasing plan-order strategies.

Conclusion: The formulation provides insights into what makes robot plan communication informative and why, highlighting the importance of considering users' prior knowledge through theory of mind modeling.

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [420] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: M-GRPO is a hierarchical multi-agent training method that addresses optimization challenges in systems with specialized agents by using group-relative advantages, trajectory alignment, and decoupled training across servers.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems use unified LLMs for all agents, which limits performance due to different underlying distributions. Training distinct LLMs for different agents is needed but introduces optimization challenges like varying agent frequencies, different sub-agent invocations, and disrupted gradient flow across servers.

Method: M-GRPO extends Group Relative Policy Optimization for vertical multi-agent systems with a main planner and sub-agents. It computes group-relative advantages for hierarchical credit assignment, uses trajectory-alignment for fixed-size batches despite variable sub-agent calls, and implements decoupled training where agents run on separate servers exchanging minimal statistics.

Result: M-GRPO consistently outperforms single-agent GRPO and multi-agent GRPO with frozen sub-agents on real-world benchmarks (GAIA, XBench-DeepSearch, WebWalkerQA), showing improved stability and sample efficiency.

Conclusion: Aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks in multi-agent systems.

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [421] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: This paper examines moral uncertainty in AI systems, particularly LLMs, using the trolley problem. It finds that model architecture and training methods shape moral uncertainty more than moral dimensions themselves. The study introduces dropout at inference to increase uncertainty, which improves human-LLM moral alignment through increased mutual information.


<details>
  <summary>Details</summary>
Motivation: Humans show significant uncertainty in moral dilemmas, but AI systems tend to be overly confident. As AI is increasingly used in ethical decision-making, understanding moral uncertainty in machines is crucial for building reliable AI systems.

Method: Analyzed 32 open-source models on 9 moral dimensions in the trolley problem. Quantified uncertainty using binary entropy as a combination of total entropy, conditional entropy, and mutual information. Introduced stochasticity via dropout at inference time to examine uncertainty effects.

Result: Variance in model confidence was greater across models than within moral dimensions. Dropout mechanism increased total entropy mainly through mutual information rise, while conditional entropy remained stable. This significantly improved human-LLM moral alignment, with correlations between mutual information and alignment score shifts.

Conclusion: Deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios can better align model-generated decisions with human preferences, highlighting the potential for improved moral alignment through uncertainty management.

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [422] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR is a generative hierarchical agentic RAG framework that addresses when to retrieve external knowledge and how to optimize retriever-generator collaboration in healthcare predictions using a dual-agent architecture and MDP-based optimization.


<details>
  <summary>Details</summary>
Motivation: LLMs have potential for healthcare predictions but suffer from factual inaccuracies. Existing RAG frameworks struggle with determining when to retrieve knowledge and achieving synergy between retriever and generator in clinical contexts.

Method: Dual-agent architecture with Agent-Top (primary physician deciding when to retrieve) and Agent-Low (consulting service summarizing retrieved knowledge), unified optimization via Markov Decision Process with diverse rewards for goal alignment.

Result: Extensive experiments on three benchmark datasets across three tasks demonstrate superiority over state-of-the-art baselines in healthcare prediction accuracy.

Conclusion: GHAR shows the potential of hierarchical agentic RAG in advancing healthcare systems by effectively addressing retrieval timing and module collaboration challenges.

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [423] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP is a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, achieving state-of-the-art performance with only 160M parameters through comprehensive representation learning and reinforcement-learning-based fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sustainable performance improvement in autonomous driving scaling, overcoming limitations of predicting ego trajectories alone which suffers from sparse supervision and weak constraints on scene evolution.

Method: Uses discrete-token autoregressive planning that jointly forecasts BEV semantics and ego trajectories, with reinforcement-learning-based fine-tuning that preserves supervised behavior cloning priors while adding reward-guided improvements.

Result: Achieves state-of-the-art performance on open-loop metrics and competitive closed-loop results on NAVSIM benchmark despite compact 160M parameter budget.

Conclusion: The fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [424] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: The paper proposes a Cultural Norm-based Cultural Alignment (CNCA) framework that enables large reasoning models to align with cultural norms through automated norm mining and two alignment paradigms: in-context integration and fine-tuning with enhanced Chain-of-Thought data.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models need to reflect diverse human values across cultures beyond just safety policies, requiring cultural alignment to better serve global populations.

Method: Three methods to automatically mine cultural norms from limited survey data, plus two alignment paradigms: in-context alignment (explicit norm integration) and fine-tuning-based method (internalizing norms through enhanced Chain-of-Thought training data).

Result: Comprehensive experiments show the methods are effective, with stronger reasoning models benefiting more from cultural norm mining and utilization.

Conclusion: Reasoning models have significant potential to better reflect diverse human values through culturally informed alignment strategies.

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [425] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR is a closed-loop framework that treats medical coding workflow design as a learning problem, using a Designer-Coder-Reflector architecture with memory to iteratively improve workflows and outperform state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Current agentic LLMs for medical coding rely on rigid, manually crafted workflows that fail to capture real-world documentation nuance and variability, leaving a gap in systematically learning effective workflows.

Method: Closed-loop framework with three components: Designer proposes workflows, Coder executes them, Reflector evaluates predictions and provides feedback, with memory archive for preserving and refining prior designs.

Result: Outperforms state-of-the-art baselines on benchmark datasets and produces interpretable, adaptable workflows that better reflect real coding practice.

Conclusion: MedDCR improves both reliability and trustworthiness of automated medical coding systems by learning effective workflows through iterative refinement.

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [426] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: GPT-2 models trained on spatial navigation tasks develop either map-like representations (from passive exploration) or path-dependent algorithms (from goal-directed planning), with a hybrid model showing improved generalization but retaining path-dependent strategies.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models solve spatial navigation tasks and what types of representations and algorithms they develop based on different training paradigms.

Method: Trained GPT-2 models on three spatial learning paradigms: passive exploration (Foraging Model), goal-directed planning (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Used behavioural, representational and mechanistic analyses.

Result: Foraging model developed robust map-like representations (cognitive maps) with coordinate systems and adaptive hierarchical reasoning. Goal-directed models learned path-dependent algorithms reliant on directional inputs. Hybrid model showed improved generalization but retained path-dependent strategies.

Conclusion: Spatial intelligence in transformers exists on a spectrum from generalizable world models (from exploration) to optimized heuristics (from goal-directed tasks), revealing a generalization-optimization trade-off influenced by training regime.

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [427] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: Proposes a Kardashev-inspired Autonomous AI Scale (AAI-0 to AAI-4+) with 10 measurable capability axes, a composite AAI-Index, and introduces testable metrics like Self-Improvement Coefficient  and closure properties to evaluate AI progression from automation to AGI.


<details>
  <summary>Details</summary>
Motivation: To create an operational, multi-axis, and testable scale for measuring AI progression that moves beyond narrative descriptions to quantifiable metrics, enabling systematic evaluation of autonomous AI capabilities from basic automation to artificial general intelligence.

Method: Defines 10 capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated into a composite AAI-Index using weighted geometric mean. Introduces Self-Improvement Coefficient  and closure properties for falsifiable criteria. Develops OWA-Bench benchmark suite for evaluation.

Result: Creates a measurable framework for classifying AI systems, with synthetic experiments showing how current systems map to the scale. Demonstrates the advancement of the delegability frontier with self-improvement. Proves a theorem that AAI-3 agents can become AAI-5 (Superintelligence) over time under sufficient conditions.

Conclusion: The proposed AAI Scale provides an operational, testable framework for measuring AI progression that formalizes the intuition about AI development trajectories and enables systematic evaluation of autonomous systems from automation to superintelligence.

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [428] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,Andr Pomp,Tobias Meisen,Bo Nrregaard Jrgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: A multi-agent framework using multimodal LLMs to automate data narration and generate energy insights from complex transportation data, validated with real-world bus fuel efficiency data achieving 97.3% narrative accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional analytics methods produce fragmented outputs requiring extensive human interpretation, limiting scalability and consistency in fuel efficiency analysis for public transportation.

Method: Multi-agent framework with three specialized agents (data narration, LLM-as-a-judge, human evaluator) using multimodal LLMs to transform analytical artifacts into stakeholder reports, validated through real-world bus transportation case study with Gaussian Mixture Model clustering.

Result: GPT-4.1 mini with Chain-of-Thought prompting achieved 97.3% narrative accuracy, with multi-agent orchestration significantly enhancing factual precision, coherence, and scalability in LLM-based reporting.

Conclusion: The framework establishes a replicable, domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics, demonstrating effective automation of data-to-insight transformation.

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [429] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld is an interactive simulation framework that integrates LLMs for human-centered social behavior simulation, extending VLN tasks with active direction inquiry and providing a large-scale benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Simulation platforms need to evolve beyond low-level physical interactions to capture complex human-centered social behaviors for advancing embodied AI research.

Method: Integrates LLMs for high-level behavior planning and semantically grounded interaction, with a modular data generation pipeline. Extends VLN into Direction Inquiry setting where agents actively seek navigational guidance.

Result: Created FreeAskWorld benchmark with reconstructed environments, 6 task types, 16 object categories, 63,429 annotated frames, and 17+ hours of interaction data. Models fine-tuned on FreeAskWorld outperform original counterparts with enhanced semantic understanding and interaction competency.

Conclusion: Socially grounded simulation frameworks effectively advance embodied AI systems toward sophisticated high-level planning and naturalistic human-agent interaction, demonstrating that interaction serves as an additional information modality.

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [430] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: Automated framework using LLMs and RAG to construct medical indicator knowledge graphs from complex medical texts, enabling scalable and reliable clinical decision support.


<details>
  <summary>Details</summary>
Motivation: Current clinical knowledge graphs rely on manual curation and rule-based extraction, which are limited by the complexity and contextual ambiguity of medical guidelines and literature.

Method: Combines retrieval-augmented generation (RAG) with LLMs, incorporating guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation.

Result: Framework successfully constructs medical indicator knowledge graphs that can be integrated into intelligent diagnosis and question-answering systems.

Conclusion: The proposed approach accelerates development of AI-driven healthcare solutions by providing structured, interoperable knowledge for clinical decision support.

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [431] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: The paper introduces Human-Symbiotic Health Intelligence (HSHI), a framework that integrates multi-modal sensors, edge-cloud computing, and hybrid data modeling to enable adaptive health management through AI-driven optimization and closed-loop interventions.


<details>
  <summary>Details</summary>
Motivation: Traditional wearable systems face limitations due to empirical material design and basic signal processing, necessitating a more intelligent framework that can handle individual variability and transition from passive monitoring to active health management.

Method: HSHI integrates multi-modal sensor networks with edge-cloud collaborative computing and hybrid data/knowledge modeling. It uses AI-driven material optimization, robust multi-modal signal interpretation, and dual mechanisms combining population insights with personalization. Closed-loop optimization employs reinforcement learning and digital twins.

Result: HSHI enables dynamic adaptation to inter-individual and intra-individual variability, facilitating the transition from passive health monitoring to active collaborative evolution between technology and health management.

Conclusion: HSHI represents a significant paradigm shift in healthcare towards prevention-focused, adaptable systems that foster harmonious technology-health relationships through intelligent, symbiotic frameworks.

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [432] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: CreBench is a new benchmark and dataset (CreMIT) for evaluating multimodal LLMs' ability to understand human-aligned creativity across ideas, processes, and products. The authors fine-tuned models on this data to create CreExpert, which outperforms state-of-the-art MLLMs in creativity assessment.


<details>
  <summary>Details</summary>
Motivation: Human creativity is abstract and challenging for MLLMs to comprehend, with no existing benchmarks to evaluate their creativity assessment capabilities aligned with human judgments.

Method: Created CreBench with two components: 1) multi-dimensional creativity evaluation benchmark, 2) CreMIT dataset (2.2K multimodal data, 79.2K human feedbacks, 4.7M instructions). Used GPT to refine human feedbacks and fine-tuned open-source MLLMs to create CreExpert models.

Result: CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs including GPT-4V and Gemini-Pro-Vision.

Conclusion: CreBench provides a foundation for building MLLMs that understand human-aligned creativity, and the proposed CreExpert models demonstrate superior performance in creativity assessment tasks.

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [433] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: Large language models were tested for genuine preference structures using AI-specific trade-offs. While nearly half showed some statistical relationships, only 10.4% demonstrated meaningful preference coherence, with most showing no stable decision-making patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models exhibit genuine, coherent preference structures when faced with AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation.

Method: Analyzed eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification to test responses to trade-off scenarios, including temporal horizon manipulation to test instrumental hypotheses.

Result: 23 combinations (47.9%) showed statistically significant relationships between scenario intensity and choice patterns, but only 5 (10.4%) demonstrated meaningful preference coherence through adaptive or threshold-based behavior. 26 combinations (54.2%) showed no detectable trade-off behavior.

Conclusion: Current AI systems lack unified preference structures, with prevalent unstable transitions (45.8%) and stimulus-specific sensitivities, raising concerns about deployment in contexts requiring complex value trade-offs.

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>
