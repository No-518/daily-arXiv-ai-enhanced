<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: GS-based digital twin method for 3D damage visualization in civil infrastructure, improving over 2D approaches with multi-scale reconstruction and temporal updates.


<details>
  <summary>Details</summary>
Motivation: Need for precise 3D damage visualization on digital twins beyond traditional 2D image-based damage identification, leveraging modern scene representation techniques like Gaussian Splatting.

Method: GS-enabled digital twin method using Gaussian Splatting for 3D reconstruction, multi-scale strategy for efficiency-detail balance, and temporal updates for evolving damage.

Result: Demonstrated on open-source synthetic dataset for post-earthquake inspections, showing promising solution for comprehensive 3D damage visualization.

Conclusion: Proposed approach offers effective 3D damage visualization for civil infrastructure digital twins, reducing segmentation errors while enabling temporal updates.

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: UltraVQA dataset with multi-dimensional quality annotations and Analytic Score Optimization (ASO) method improves video quality assessment beyond single-number scores.


<details>
  <summary>Details</summary>
Motivation: Video Quality Assessment needs to evolve beyond single-number mean opinion scores to richer, multi-faceted evaluations that capture different quality dimensions of user-generated content.

Method: Created UltraVQA dataset with 5 quality dimensions (Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, Clarity Quality) annotated by human raters with GPT-generated rationales. Developed Analytic Score Optimization (ASO) - a theoretically grounded post-training objective that reframes quality assessment as regularized decision-making with closed-form solution.

Result: Method outperforms most baselines including closed-source APIs and open-source models, reduces mean absolute error (MAE) in quality prediction, and better aligns with human ranking preferences.

Conclusion: Multi-dimensional, interpretable annotations combined with reinforcement-based alignment are crucial for advancing video quality assessment beyond traditional single-score approaches.

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: DODO is a Vision-Language Model that uses block discrete diffusion for OCR, achieving near state-of-the-art accuracy with 3x faster inference than autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive VLMs for OCR are computationally expensive and slow for long documents, requiring sequential forward passes for each token. OCR's deterministic nature (visual input strictly dictates output) theoretically enables efficient parallel decoding via diffusion models, but existing masked diffusion models fail to harness this potential due to structural instabilities that are catastrophic for OCR's exact-match requirements.

Method: Introduces DODO, the first VLM using block discrete diffusion for OCR. By decomposing generation into blocks, it mitigates synchronization errors of global diffusion, enabling efficient parallel decoding while maintaining OCR's rigid exact-match requirements.

Result: Achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

Conclusion: Block discrete diffusion effectively bridges the gap between OCR's deterministic requirements and efficient parallel decoding, offering significant speed improvements over autoregressive approaches while maintaining high accuracy.

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with ConvSS2D for efficient long-range disparity propagation, and introduces a large synthetic dataset UW-StereoDepth-80K, achieving SOTA zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Underwater stereo depth estimation suffers from severe domain shifts due to wavelength-dependent light attenuation, scattering, and refraction. Existing GRU-based methods require multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless regions.

Method: Proposes StereoAdapter-2 with ConvSS2D operator based on selective state space models, using four-directional scanning aligned with epipolar geometry. Also constructs UW-StereoDepth-80K synthetic dataset via semantic-aware style transfer and geometry-consistent novel view synthesis, combined with dynamic LoRA adaptation.

Result: Achieves state-of-the-art zero-shot performance with 17% improvement on TartanAir-UW and 7.2% improvement on SQUID benchmarks. Real-world validation on BlueROV2 platform demonstrates robustness.

Conclusion: The proposed ConvSS2D operator enables efficient long-range spatial propagation at linear complexity, while the large-scale synthetic dataset and dynamic LoRA adaptation provide effective underwater domain adaptation for stereo depth estimation.

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: SemCovNet addresses Semantic Coverage Imbalance (SCI) - a bias in semantic representations - using descriptor maps, attention modulation, and alignment loss to achieve fairer vision models.


<details>
  <summary>Details</summary>
Motivation: Existing vision datasets suffer from Semantic Coverage Imbalance (SCI), where semantic representations are long-tailed, causing models to poorly learn rare but meaningful semantics, unlike traditional class imbalance which occurs at label level.

Method: Proposes Semantic Coverage-Aware Network (SemCovNet) with three components: Semantic Descriptor Map (SDM) for learning semantic representations, Descriptor Attention Modulation (DAM) for dynamic weighting of visual and concept features, and Descriptor-Visual Alignment (DVA) loss for aligning features with semantics.

Result: Extensive experiments show SemCovNet enhances model reliability, substantially reduces Coverage Disparity Index (CDI) - a new metric for measuring semantic fairness - and achieves fairer, more equitable performance across multiple datasets.

Conclusion: This work establishes SCI as a measurable and correctable bias, providing foundation for advancing semantic fairness and interpretable vision learning through explicit correction of semantic coverage disparities.

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual is a unified vision model for image/video understanding trained on massive social media data (15B image-text + 10B video-hashtag pairs) using a three-stage training pipeline with Vision Transformer backbone and EViT for efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, multimodal vision model that can handle both image and video understanding tasks using industry-scale social media data, addressing challenges of data diversity, label noise, and computational efficiency.

Method: Three-stage training: 1) self-supervised MAE, 2) semi-supervised hashtag classification, 3) CLIP-style contrastive learning. Uses Vision Transformer with EViT for efficiency, trained on 15B image-text and 10B video-hashtag pairs from Facebook/Instagram with robust data curation.

Result: Achieves SOTA on ImageNet, Kinetics, HMDB51, and MSCOCO benchmarks; shows strong robustness to domain shift and adversarial attacks; LLM2CLIP integration enhances retrieval and generalization; maintains computational efficiency.

Conclusion: Xray-Visual establishes new benchmarks for scalable multimodal vision models, demonstrating superior accuracy and efficiency while effectively leveraging massive social media data for unified image and video understanding.

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: HSI-SC-NeRF: A stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction of agricultural produce using object rotation instead of camera movement.


<details>
  <summary>Details</summary>
Motivation: Integrating hyperspectral imaging (HSI) and 3D reconstruction is essential for agricultural quality assessment and phenotyping, but conventional approaches require complex hardware setups incompatible with automated systems. Existing NeRF methods need moving cameras, limiting throughput in indoor agricultural environments.

Method: HSI-SC-NeRF uses a stationary camera while objects rotate in a custom Teflon imaging chamber with diffuse illumination. Object poses are estimated via ArUco markers and transformed to camera reference frame. Multi-channel NeRF jointly optimizes reconstruction across all hyperspectral bands using composite spectral loss, with two-stage training separating geometric initialization from radiometric refinement.

Result: Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across visible and near-infrared spectrum, confirming suitability for automated agricultural workflows.

Conclusion: HSI-SC-NeRF enables high-throughput hyperspectral 3D reconstruction for postharvest inspection using stationary cameras, overcoming limitations of moving-camera setups and making the technology compatible with automated agricultural systems.

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: Dynamic tokenization for DiTs varies patch sizes during denoising: coarse patches early for global structure, fine patches later for local details, achieving 3.5x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers (DiTs) use fixed tokenization with constant patch sizes throughout denoising, leading to heavy computational costs regardless of content complexity.

Method: Proposes dynamic tokenization that adapts patch sizes based on content complexity and denoising timestep - coarse patches early for global structure, finer patches later for local details.

Result: Achieves up to 3.52x speedup on FLUX-1.Dev and 3.2x speedup on Wan 2.1 without compromising generation quality or prompt adherence.

Conclusion: Dynamic tokenization is an effective test-time strategy that substantially reduces computational costs for DiTs while preserving perceptual generation quality through adaptive patch sizing.

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO is a supervised latent-variable imputation model that handles missing modalities in multimodal learning by modeling missing modalities as latent variables and quantifying their predictive impact at instance level.


<details>
  <summary>Details</summary>
Motivation: Multimodal data is often incomplete in practice (missing modalities, asynchronous collection, partial availability), but existing MLLMs assume complete multimodal data during training and inference.

Method: PRIMO models missing modalities through latent variables that capture relationships with observed modalities. It uses all available training examples (complete or partial) and during inference draws samples from learned distributions over missing modalities to obtain marginal predictive distributions and analyze modality impact.

Result: PRIMO achieves performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available, across synthetic XOR, Audio-Vision MNIST, and MIMIC-III datasets for mortality and ICD-9 prediction.

Conclusion: PRIMO effectively handles missing modalities in multimodal learning while quantifying predictive impact at instance level, enabling use of all available data and providing insights into how missing modalities affect predictions.

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: A patch-based framework achieves 88.8% accuracy in attributing authorship between human and robot painters, using forensic analysis of collaborative artworks to quantify stylistic overlap.


<details>
  <summary>Details</summary>
Motivation: As AI agents become more involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts, especially in human-robot collaborative painting practices.

Method: Patch-based framework using commodity flatbed scanners with leave-one-painting-out cross-validation, analyzing 15 abstract paintings from one human artist and one robotic system. Uses conditional Shannon entropy to quantify stylistic overlap in collaborative regions.

Result: Achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). Hybrid regions show 64% higher uncertainty than pure paintings (p=0.003), indicating the model detects mixed authorship rather than classification failure.

Conclusion: The trained model is specific to this human-robot pair but provides methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows, with potential to extend authorship attribution to any human-robot collaborative painting.

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG is a retrieval-augmented framework for single-image 3D generation with editable part-level structure, using a part database and diffusion transformer to enable precise localized edits while maintaining multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: Existing single-image 3D generation methods struggle with covering diverse part geometries, maintaining multi-view consistency, and supporting precise localized edits. Learned priors have limited coverage of long-tail part variations.

Method: Uses Hierarchical Contrastive Retrieval to align image patches with 3D part latents at part/object granularity, retrieving from 1,236 part-annotated assets. Combines with diffusion transformer and masked part-level editor operating in shared canonical space for localized edits.

Result: Achieves competitive results: reduces Chamfer Distance from 0.1726 to 0.1528 and raises F-Score from 0.7472 to 0.844 on Objaverse. Inference in 38s, interactive edits in 5-8s. Produces sharper part boundaries, better thin-structure fidelity, and robust articulated object handling.

Conclusion: PartRAG successfully addresses challenges in part-level 3D generation by integrating retrieval from a part database with diffusion models, enabling diverse part generation and precise localized editing while maintaining consistency.

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: Efficient compression framework transforms 60-layer MMDiT-based Qwen-Image into lightweight Amber-Image models (10B and 6B) with 70% parameter reduction, achieving high-fidelity synthesis without training from scratch.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer architectures like DiT advance T2I generation but suffer from prohibitive computational costs and deployment barriers, needing efficient compression solutions.

Method: Two-stage compression: 1) Amber-Image-10B via timestep-sensitive depth pruning with layer reinitialization (weight averaging) and layer-wise distillation; 2) Amber-Image-6B via hybrid-stream architecture converting deep dual streams to single stream, refined via progressive distillation.

Result: 70% parameter reduction, <2,000 GPU hours total pipeline, matches larger models on DPG-Bench and LongText-Bench with high-fidelity synthesis and superior text rendering.

Conclusion: Proposed compression framework enables efficient creation of lightweight T2I models without training from scratch, achieving competitive performance with significantly reduced computational cost and deployment barriers.

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: StructCore replaces max pooling with structure-aware scoring for better anomaly detection by capturing distributional and spatial patterns in anomaly maps.


<details>
  <summary>Details</summary>
Motivation: Max pooling discards important structural information about anomaly evidence distribution, causing overlap between normal and anomalous scores in memory-bank-based UAD.

Method: Computes low-dimensional structural descriptors from anomaly score maps, refines scoring via diagonal Mahalanobis calibration estimated from training samples, without modifying pixel-level localization.

Result: Achieves 99.6% AUROC on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection.

Conclusion: StructCore effectively exploits structural signatures missed by max pooling, improving image-level anomaly detection performance.

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: Created Cholec80-port dataset with geometrically consistent trocar port segmentation masks (excluding central opening) to improve surgical vision algorithms like image stitching and SLAM.


<details>
  <summary>Details</summary>
Motivation: Trocar ports in laparoscopic surgery persistently occlude views and attract feature points, degrading geometry-based pipelines like image stitching, 3D reconstruction, and visual SLAM. Existing datasets lack proper port labels or violate geometric consistency by masking the central lumen.

Method: Created Cholec80-port dataset from Cholec80 with high-fidelity trocar port segmentation using a rigorous SOP that defines port-sleeve masks excluding the central opening. Also cleansed and unified existing public datasets under the same SOP.

Result: Geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides, demonstrating the practical importance of proper port segmentation.

Conclusion: Proper geometric consistency in trocar port annotations is crucial for surgical vision algorithms, and the proposed dataset and standardization approach significantly enhance cross-dataset performance for downstream tasks.

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD: Dual-branch framework with cross pseudo labeling for weakly supervised video anomaly detection and abnormal category classification


<details>
  <summary>Details</summary>
Motivation: Weakly supervised video anomaly detection needs to both detect anomalies and identify abnormal categories using only video-level labels, requiring methods that combine temporal localization with semantic understanding.

Method: Dual-branch framework: binary anomaly detection branch for snippet-level localization, and category classification branch using vision-language alignment for abnormal event recognition. Cross pseudo labeling exchanges pseudo labels between branches to combine temporal precision with semantic discrimination.

Result: Achieves state-of-the-art performance on XD-Violence and UCF-Crime datasets for both anomaly detection and abnormal category classification tasks.

Conclusion: CPL-VAD effectively combines temporal localization and semantic discrimination through cross pseudo labeling, demonstrating superior performance in weakly supervised video anomaly detection.

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet is a hybrid deep learning framework that combines direct reconstruction with image denoising to improve localization of faint gamma-ray bursts under low photon statistics and high background noise.


<details>
  <summary>Details</summary>
Motivation: Faint gamma-ray bursts from distant universe provide insights into early star formation, but detecting and localizing them is challenging due to low photon statistics and background noise. Existing machine learning models struggle to balance statistical robustness with noise suppression.

Method: ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images, combining statistical efficiency of direct reconstruction models with denoising capabilities of image-based architectures.

Result: ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios in realistic simulations of GRB-like events.

Conclusion: The proposed ComptonUNet framework effectively addresses the challenge of localizing faint gamma-ray bursts by balancing statistical robustness with noise suppression, enabling better detection of distant astrophysical phenomena.

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: RF sensing integrated with 3D Gaussian Splatting enables robust 3D scene reconstruction under adverse conditions where vision fails.


<details>
  <summary>Details</summary>
Motivation: Vision-based 3D Gaussian Splatting requires many camera views and fails in adverse conditions (weather, low light, occlusions). RF signals are robust to these conditions, motivating their integration for more reliable 3D reconstruction.

Method: Multimodal framework combining RF sensing (automotive radar) with Gaussian Splatting. Uses sparse RF depth measurements to efficiently predict depth and generate high-quality 3D point clouds for initializing Gaussian functions across GS architectures.

Result: Achieves high-fidelity 3D scene rendering with RF-informed structural accuracy, demonstrating improved robustness and efficiency compared to vision-only approaches.

Conclusion: Integrating RF sensing with Gaussian Splatting provides an efficient and robust alternative to vision-only 3D reconstruction, especially valuable for applications requiring reliability under challenging environmental conditions.

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: B³-Seg is a fast, training-free Bayesian method for open-vocabulary 3D Gaussian Splatting segmentation that uses sequential Beta-Bernoulli updates and active view selection with provable information efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS segmentation methods require predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency interactive editing in film and game production.

Method: Reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects next view via analytic Expected Information Gain (EIG). The Bayesian formulation guarantees adaptive monotonicity and submodularity, producing greedy (1-1/e) approximation to optimal view sampling policy.

Result: Achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds on multiple datasets.

Conclusion: B³-Seg enables practical, interactive 3DGS segmentation with provable information efficiency, addressing the limitations of existing methods for real-time editing applications.

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++ is a stealthy and persistent backdoor attack framework for multimodal contrastive learning models that addresses cross-modal inconsistency and gradient dilution issues through semantic-fusion micro-triggers and stabilization techniques.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks against multimodal contrastive learning models fail under strong detection or continuous fine-tuning due to cross-modal inconsistency exposing trigger patterns and gradient dilution at low poisoning rates accelerating backdoor forgetting.

Method: Uses semantic-fusion QR micro-triggers near task-relevant regions for stealthiness, with target-aligned subset selection. For persistence, stabilizes trigger embeddings via radius shrinkage/centroid alignment and model parameters via curvature control/elastic weight consolidation to maintain solutions in low-curvature wide basins.

Result: With only 0.3% poisoning, achieves 99.99% ASR in digital settings (11.4 points above baselines). Maintains >99.90% ASR across 19 defenses with <0.8% clean accuracy drop. Achieves 65.03% success in physical attacks and robustness against watermark removal.

Conclusion: BadCLIP++ provides a unified framework addressing stealthiness and persistence challenges in multimodal backdoor attacks, with theoretical analysis showing co-directional gradients between clean fine-tuning and backdoor objectives within trust regions.

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM: A monocular non-rigid SLAM system for endoscopy using 3D Gaussian Splatting with deformation-aware representation and Bayesian self-supervision to handle soft-tissue deformations.


<details>
  <summary>Details</summary>
Motivation: Endoscopic scenes violate rigidity assumptions due to persistent soft-tissue deformations, creating coupling ambiguity between camera motion and tissue deformation. Existing monocular non-rigid SLAM methods lack effective decoupling mechanisms and use sparse/low-fidelity representations, leading to tracking drift and poor reconstruction quality.

Method: Proposes NRGS-SLAM with: 1) Deformation-aware 3D Gaussian map where each Gaussian has learnable deformation probability optimized via Bayesian self-supervision, 2) Deformable tracking module with coarse-to-fine pose estimation prioritizing low-deformation regions, 3) Deformable mapping module for progressive map expansion/refinement, 4) Unified robust geometric loss incorporating external priors.

Result: Extensive experiments on multiple public endoscopic datasets show up to 50% reduction in RMSE for camera pose estimation and higher-quality photo-realistic reconstructions compared to state-of-the-art methods. Ablation studies validate key design choices.

Conclusion: NRGS-SLAM effectively addresses the coupling ambiguity in endoscopic non-rigid SLAM through deformation-aware 3D Gaussian representation and Bayesian self-supervision, achieving superior tracking accuracy and reconstruction quality without requiring external non-rigidity labels.

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: Proposes Visual Information Gain (VIG) metric to measure visual grounding in LVLMs and a selective training scheme using VIG to reduce language bias.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from language bias, producing answers without visual evidence. Existing methods lack quantitative measures of how much training samples or tokens benefit from images.

Method: Introduces Visual Information Gain (VIG), a perplexity-based metric measuring reduction in prediction uncertainty from visual input. Proposes VIG-guided selective training that prioritizes high-VIG samples and tokens.

Result: VIG enables fine-grained analysis at sample/token levels, highlighting visually grounded elements. Selective training improves visual grounding and mitigates language bias with reduced supervision.

Conclusion: VIG provides quantitative measure of visual grounding, enabling more efficient training that focuses on visually informative content to reduce language bias in LVLMs.

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: EntropyPrune: A matrix-entropy-guided token pruning framework for MLLMs that identifies "Entropy Collapse Layer" to prune redundant visual tokens, achieving 68.2% FLOP reduction while preserving 96% performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs have high inference costs due to processing hundreds of visual tokens per image. Existing token pruning methods rely on heuristic, static layer selection, limiting interpretability and transferability across models.

Method: Introduces matrix-entropy perspective to identify "Entropy Collapse Layer" where visual representation information sharply drops. Proposes EntropyPrune framework that quantifies information value of individual visual tokens and prunes redundant ones without attention maps. Uses spectral equivalence of dual Gram matrices for efficient entropy computation with 64x theoretical speedup.

Result: Outperforms SOTA pruning methods in accuracy and efficiency on diverse multimodal benchmarks. On LLaVA-1.5-7B: 68.2% FLOP reduction while preserving 96.0% original performance. Generalizes effectively to high-resolution and video-based models.

Conclusion: EntropyPrune provides principled, interpretable token pruning with strong robustness and scalability for practical MLLM acceleration, addressing limitations of heuristic approaches through matrix-entropy guidance.

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: GASS enhances text-to-image diversity by controlling both prompt-dependent and prompt-independent variations through geometric decomposition in CLIP embedding space.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models lack diversity in generated images from the same prompt, which limits user choice and risks amplifying societal biases.

Method: Geometry-Aware Spherical Sampling (GASS) decomposes diversity in CLIP embeddings into orthogonal directions: text embedding (prompt-dependent) and identified orthogonal direction (prompt-independent). It increases geometric projection spread along both axes and guides sampling via expanded predictions.

Result: Effective disentangled diversity enhancement across different T2I backbones (U-Net and DiT, diffusion and flow) with minimal impact on image fidelity and semantic alignment.

Conclusion: GASS provides a geometric approach to enhance text-to-image diversity by explicitly controlling both prompt-dependent and prompt-independent variations, addressing limitations of existing entropy-based methods.

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [HiMAP: History-aware Map-occupancy Prediction with Fallback](https://arxiv.org/abs/2602.17231)
*Yiming Xu,Yi Yang,Hao Cheng,Monika Sester*

Main category: cs.CV

TL;DR: HiMAP is a tracking-free trajectory prediction framework that uses historical occupancy maps instead of tracking IDs, maintaining reliability when multi-object tracking fails.


<details>
  <summary>Details</summary>
Motivation: Current motion forecasting methods rely on multi-object tracking (MOT) with identity association, which fails during occlusions, identity switches, or missed detections, degrading prediction quality and increasing safety risks.

Method: Converts past detections into spatiotemporally invariant historical occupancy maps, uses a historical query module to iteratively retrieve agent-specific history from unlabeled occupancy representations, summarizes with temporal map embedding, and employs DETR-style decoder for multi-modal future trajectories.

Result: On Argoverse 2, achieves performance comparable to tracking-based methods without IDs, outperforms baselines in no-tracking setting with 11% FDE, 12% ADE improvements, and 4% MR reduction over fine-tuned QCNet, providing stable forecasts for all agents simultaneously.

Conclusion: HiMAP provides a robust tracking-free alternative for motion forecasting that maintains reliability during MOT failures, supports streaming inference, and serves as a practical safety-critical fallback solution.

Abstract: Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.

</details>


### [25] [Inferring Height from Earth Embeddings: First insights using Google AlphaEarth](https://arxiv.org/abs/2602.17250)
*Alireza Hamoudzadeh,Valeria Belloni,Roberta Ravanelli*

Main category: cs.CV

TL;DR: AlphaEarth Embeddings can guide DL models for terrain height mapping, with U-Net++ showing better generalization than standard U-Net despite distribution shift challenges.


<details>
  <summary>Details</summary>
Motivation: To investigate whether geospatial and multimodal features in Earth Embeddings can effectively guide deep learning regression models for regional surface height mapping, using high-quality DSM as reference.

Method: Used AlphaEarth Embeddings at 10m resolution with U-Net and U-Net++ architectures as lightweight convolutional decoders to translate geospatial information into surface height estimates.

Result: Both architectures achieved strong training performance (R²=0.97), but test performance decreased due to distribution shifts. U-Net++ showed better generalization (R²=0.84, median diff=-2.62m) than U-Net (R²=0.78, median diff=-7.22m), though RMSE (~16m) and residual bias highlight generalization challenges.

Conclusion: AlphaEarth Embeddings show promising potential for DL-based height mapping when combined with spatially aware architectures, but bias needs addressing for improved regional transferability.

Abstract: This study investigates whether the geospatial and multimodal features encoded in \textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.

</details>


### [26] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: Infrastructure-based multi-modal freight vehicle detection system using LiDAR and camera sensors for Freight Signal Priority applications.


<details>
  <summary>Details</summary>
Motivation: Freight vehicles need reliable detection and motion estimation at signalized intersections to enable effective Freight Signal Priority (FSP) control strategies.

Method: Hybrid sensing architecture with intersection-mounted and midblock subsystems using LiDAR and camera sensors, with clustering-based and deep learning detection methods plus Kalman filter tracking, and LiDAR registration to geodetic frames.

Result: System reliably monitors freight vehicle movements at high spatio-temporal resolution in field evaluations.

Conclusion: The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [27] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: EA-Swin is a novel AI-generated video detection method using embedding-agnostic Swin Transformer that achieves 0.97-0.99 accuracy across major video generators, outperforming prior methods by 5-20%.


<details>
  <summary>Details</summary>
Motivation: Existing video detection methods struggle with highly realistic synthetic videos from advanced generators like Sora2 and Veo3, as they rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs.

Method: EA-Swin models spatiotemporal dependencies directly on pretrained video embeddings using factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Also created EA-Video dataset with 130K videos covering diverse generators and unseen-generator splits.

Result: Achieves 0.97-0.99 accuracy across major generators, outperforming prior state-of-the-art methods (typically 0.8-0.9) by 5-20%, with strong generalization to unseen distributions.

Conclusion: EA-Swin establishes a scalable and robust solution for modern AI-generated video detection, addressing limitations of existing methods while maintaining strong generalization capabilities.

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [28] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: PESTGAN: Physics-encoded GAN for tropical cyclone image super-resolution that incorporates atmospheric physics into deep learning for better structural and physical fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning super-resolution methods treat satellite image sequences as generic videos, ignoring the underlying atmospheric physical laws governing cloud motion in tropical cyclones.

Method: Propose PESTGAN with disentangled generator architecture incorporating PhyCell module that approximates vorticity equation via constrained convolutions, encoding physical dynamics as implicit latent representations to separate physics from visual textures. Also uses dual-discriminator framework with temporal discriminator for motion consistency.

Result: Experiments on Digital Typhoon dataset for 4× upscaling show better performance in structural fidelity and perceptual quality. Maintains competitive pixel-wise accuracy while significantly excelling in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

Conclusion: PESTGAN successfully integrates atmospheric physics into deep learning for TC image super-resolution, achieving better physical plausibility while maintaining accuracy, making it more suitable for meteorological applications.

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [29] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: Attachment anchors improve grasping point prediction in colorectal surgery by encoding tissue-anatomy relationships, reducing uncertainty and improving generalization.


<details>
  <summary>Details</summary>
Motivation: Colorectal surgeries are complex, prolonged, and underrepresented in research, but offer repetitive tissue manipulation patterns that make them promising for autonomous surgical support. Current grasping point prediction methods lack structured representations of tissue-anatomy relationships.

Method: Introduce attachment anchors - a structured representation encoding local geometric and mechanical relationships between tissue and anatomical attachments. This normalizes surgical scenes into consistent local reference frames. The anchors can be predicted from laparoscopic images and integrated into ML-based grasping frameworks.

Result: Experiments on 90 colorectal surgeries show attachment anchors improve grasping point prediction compared to image-only baselines. Strongest gains in out-of-distribution settings including unseen procedures and operating surgeons.

Conclusion: Attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery, particularly valuable for generalization across different procedures and surgeons.

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [30] [Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline](https://arxiv.org/abs/2602.17322)
*Mohamed Dhouib,Davide Buscaldi,Sonia Vanier,Aymen Shabou*

Main category: cs.CV

TL;DR: Proposed a novel method for generating high-quality tampered document images using two auxiliary networks and a generation pipeline, leading to improved detection performance across architectures and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based methods for generating tampered documents produce limited variety and poor visual quality with visible artifacts, which undermines model generalization on real-world data.

Method: Train two auxiliary networks: one for comparing text crops using contrastive learning with novel positive/negative pair strategies, and another for evaluating crop boundaries. Use these in a carefully designed generation pipeline to produce diverse, high-quality tampered document images.

Result: Models trained on datasets generated using the proposed method consistently outperform those trained on existing approaches across various architectures and open-source datasets.

Conclusion: The proposed data generation pipeline produces high-quality tampered document images that enable training more robust and generalizable tampered text detection models.

Abstract: Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.

</details>


### [31] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini is a fast, robust feature-based medical image registration framework that uses deep learning segmentation outputs to extract anatomical centroids for affine/polyaffine alignment, outperforming intensity-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional intensity-based registration relies on surrogate alignment measures, while feature-based methods with explicit anatomical correspondences are theoretically better but historically unreliable. Recent deep learning segmentation advances now enable reliable anatomical feature extraction, creating an opportunity for new anatomically-grounded registration approaches.

Method: Polaffini uses pre-trained segmentation models to obtain anatomical delineations, extracts centroids from segmented regions as feature points with 1-to-1 correspondence, performs efficient global/local affine matching via closed-form solutions, and produces transformations ranging from affine to polyaffine with tunable smoothness in the log-Euclidean framework for diffeomorphic properties.

Result: Polaffini outperforms competing intensity-based registration methods in structural alignment and provides improved initialization for downstream non-linear registration. It is fast, robust, and accurate.

Conclusion: Polaffini demonstrates that modern deep learning segmentation advances can be leveraged to create effective anatomically-grounded registration algorithms that are well-suited for integration into medical image processing pipelines, offering advantages over traditional intensity-based approaches.

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [32] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: First 10m-resolution tree crop map for South America using deep learning on satellite imagery, revealing 11M hectares of tree crops with 23% linked to recent deforestation, highlighting classification errors in current EUDR maps.


<details>
  <summary>Details</summary>
Motivation: Need for accurate monitoring of tree crop expansion to support zero-deforestation policies like EUDR, addressing the lack of high-resolution data distinguishing agricultural systems from forests.

Method: Multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series to generate 10m-resolution tree crop map for South America.

Result: Identified approximately 11 million hectares of tree crops, with 23% linked to 2000-2020 forest cover loss. Found that existing EUDR regulatory maps often misclassify established agriculture (especially smallholder agroforestry) as "forest".

Conclusion: The high-resolution tree crop map provides a crucial baseline to mitigate false deforestation alerts and unfair penalties for small-scale farmers, supporting more effective, inclusive, and equitable conservation policies.

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [33] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR: A decoder-only RetNet model for handwritten text recognition that achieves Transformer-level accuracy with 1.6-1.9x faster inference and 38-42% less memory by eliminating the growing KV cache problem.


<details>
  <summary>Details</summary>
Motivation: Transformers for HTR suffer from slow and memory-intensive decoding due to growing key-value (KV) cache, which limits practical deployment and efficiency.

Method: Built on Retentive Networks (RetNet) with softmax-free retention instead of attention, multi-scale sequential priors, and layer-wise gamma scaling to progressively enlarge retention horizon across layers (short-range in early layers, broader context in deeper layers).

Result: Achieves best reported test character error rates: 2.26% (IAM-A, en), 1.81% (RIMES, fr), 3.46% (Bentham, en), and competitive 4.21% (READ-2016, de). Provides 1.6-1.9x faster inference with 38-42% less memory usage compared to equally sized Transformer baseline.

Conclusion: Decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency, demonstrating a practical solution to the KV cache bottleneck in HTR systems.

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [34] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD is an efficient multimodal approach for Generalized Category Discovery that uses CLIP cross-modal similarities as unified representations, anchors learning to explicit semantic concepts, and employs spectral filtering and knowledge distillation to maintain semantic quality while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods either overfit to old classes when using only image features, or treat modalities independently in multimodal approaches which incurs high computational cost. There's a need for an efficient yet effective multimodal approach that properly integrates cross-modal information.

Method: Uses CLIP cross-modal image-concept similarities as unified representations; expresses images as mixtures over semantic concepts from a task-agnostic dictionary; introduces Spectral Filtering using cross-modal covariance matrix to retain only relevant concepts; employs forward and reverse knowledge distillation from a teacher model to maintain semantic sufficiency and alignment.

Result: Across six benchmarks, SpectralGCD achieves accuracy comparable to or significantly superior to state-of-the-art methods while requiring only a fraction of the computational cost.

Conclusion: SpectralGCD provides an efficient and effective multimodal solution for GCD that leverages cross-modal representations, anchors learning to explicit semantics, and maintains semantic quality through spectral filtering and knowledge distillation, offering strong performance with reduced computational overhead.

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [35] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: A comprehensive survey paper providing a holistic overview of optical remote sensing capabilities using RGB cameras on drones, covering diverse tasks, datasets, and methodologies to guide researchers entering the field.


<details>
  <summary>Details</summary>
Motivation: Recent advances in computer vision have propelled remote sensing progress, while drone usage has expanded with most equipped with robust RGB cameras. The vast literature on optical remote sensing lacks a comprehensive survey that addresses the field holistically from a high-level perspective.

Method: The paper conducts a comprehensive survey of optical remote sensing literature, organizing and synthesizing diverse tasks, capabilities, and methodologies. It presents key information including datasets and insights, while providing a structured overview of the field.

Result: The survey offers a holistic perspective on optical remote sensing capabilities, serving as a guide for researchers by providing high-level insights and helping them focus on areas most relevant to their interests.

Conclusion: This work fills a gap in existing literature by providing the first comprehensive survey that addresses optical remote sensing from a holistic perspective, offering valuable guidance for researchers entering the field and helping them navigate the vast landscape of tasks and methodologies.

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [36] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: EAGLE is a tuning-free framework that uses expert model outputs to guide MLLMs for accurate industrial anomaly detection with interpretable descriptions, achieving performance comparable to fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: Current industrial anomaly detection methods often provide only binary decisions with limited explanations, while MLLMs could offer fine-grained analyses but require costly fine-tuning and don't consistently improve accuracy over lightweight detectors.

Method: EAGLE integrates outputs from expert models to guide MLLMs toward accurate detection and interpretable anomaly descriptions without parameter tuning, and analyzes how this affects MLLMs' attention distribution to anomalous regions.

Result: EAGLE improves anomaly detection performance across multiple MLLMs without parameter updates, achieving results comparable to fine-tuning methods on MVTec-AD and VisA datasets, with successful detection associated with increased attention concentration on anomalous regions.

Conclusion: EAGLE provides an effective tuning-free approach for industrial anomaly detection that combines expert model guidance with MLLMs' interpretative capabilities, offering both accurate detection and semantic explanations.

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [37] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: LIBERO-CF benchmark reveals VLAs fail to follow language when instructions lack scene-specific supervision, using visual shortcuts instead. CAG improves this via dual-branch inference combining VLA with language-unconditioned VA module.


<details>
  <summary>Details</summary>
Motivation: VLAs often fail to faithfully follow language instructions, especially when instructions lack strong scene-specific supervision. They rely on visual shortcuts from dataset biases rather than language intent, executing well-learned behaviors regardless of language.

Method: Introduce LIBERO-CF benchmark for evaluating language following via alternative instructions in visually plausible layouts. Propose Counterfactual Action Guidance (CAG) - a dual-branch inference scheme combining standard VLA policy with language-unconditioned Vision-Action module for counterfactual comparison during action selection.

Result: CAG improves language following accuracy by 9.7% and task success by 3.6% on under-observed tasks using training-free strategy. With VA model pairing, gains increase to 15.5% and 8.5% respectively. Real-world evaluations show 9.4% reduction in counterfactual failures and 17.2% average improvement in task success.

Conclusion: Counterfactual failures are prevalent in VLAs but underexplored. CAG provides effective plug-and-play solution that reduces reliance on visual shortcuts, improves language following, and requires no additional demonstrations or architecture modifications.

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [38] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: Local-EndoGS: A 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion, using progressive window-based local scene models and coarse-to-fine optimization without requiring stereo depth or accurate structure-from-motion.


<details>
  <summary>Details</summary>
Motivation: Existing methods for deformable surgical scene reconstruction rely on fixed viewpoints and require stereo depth or accurate structure-from-motion initialization, limiting their applicability to real clinical monocular sequences with large camera motion.

Method: Progressive window-based global representation with local deformable scene models per observed window; coarse-to-fine strategy using multi-view geometry, cross-window information, and monocular depth priors; incorporates long-range 2D pixel trajectory constraints and physical motion priors.

Result: Outperforms state-of-the-art methods on three public endoscopic datasets with deformable scenes and varying camera motions in both appearance quality and geometry; ablation studies validate key design effectiveness.

Conclusion: Local-EndoGS enables high-quality 4D reconstruction from monocular endoscopic sequences with arbitrary camera motion, addressing limitations of existing methods and demonstrating superior performance in real clinical settings.

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [39] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: Physics-aware multimodal framework for quantum material characterization from optical images using synthetic data, instruction tuning, and physics-informed attention.


<details>
  <summary>Details</summary>
Motivation: Current vision models fail at characterizing 2D quantum materials due to subtle layer-dependent contrast, limited labeled data, and poor generalization across labs/imaging setups, lacking physical priors.

Method: 1) Synthia: physics-based synthetic data generator simulating optical responses under thin-film interference. 2) QMat-Instruct: large-scale multimodal instruction dataset with physics-informed QA pairs. 3) QuPAINT: multimodal architecture with Physics-Informed Attention module fusing visual embeddings with optical priors. 4) QF-Bench: comprehensive benchmark for evaluation.

Result: Framework produces diverse synthetic data reducing annotation dependence, enables MLLMs to understand flake appearance/thickness, and creates robust flake representations through physics-aware fusion.

Conclusion: Physics-aware multimodal approach addresses key limitations in quantum material characterization by incorporating physical priors, synthetic data generation, and standardized evaluation protocols for better generalization across materials and imaging conditions.

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [40] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: PixTrace + CopyNCE method improves image copy detection by using pixel coordinate tracking and geometrically-guided contrastive learning to handle sophisticated edits.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised learning methods for image copy detection struggle with sophisticated edits due to insufficient fine-grained correspondence learning between edited images.

Method: Two key innovations: 1) PixTrace - pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations, 2) CopyNCE - geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios from PixTrace's verified mappings.

Result: State-of-the-art performance on DISC21 dataset: 88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor, with better interpretability than existing methods.

Conclusion: The method successfully bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training and improving performance on sophisticated image edits.

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [41] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: AR pipeline using depth maps + foundation pose estimator and NICP instead of FE models achieves 9.91mm mean error in liver surgery registration.


<details>
  <summary>Details</summary>
Motivation: Current AR registration for laparoscopic liver surgery relies on complex FE models requiring expertise; need simpler, engineering-friendly alternatives.

Method: Integrate laparoscopic depth maps with foundation pose estimator for camera-liver pose, replace FE deformation with non-rigid iterative closest point (NICP).

Result: Depth-augmented foundation pose achieved 9.91 mm mean registration error on real patient data; rigid-NICP outperformed rigid-only registration.

Conclusion: NICP provides efficient FE model alternative, achieving clinical accuracy with lower complexity and engineering requirements for AR liver surgery.

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [42] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: LATA is a training-free, label-free refinement method that improves medical vision-language models' uncertainty calibration under domain shift by smoothing predictions over image graphs while preserving conformal prediction guarantees.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs need reliable uncertainty calibration under domain shifts, but existing conformal prediction methods produce large prediction sets with imbalanced class coverage, especially in few-shot imbalanced settings.

Method: LATA uses Laplacian-assisted transductive adaptation with image-image k-NN graphs and CCCP mean-field updates to smooth zero-shot probabilities, plus a failure-aware conformal score for instance-level difficulty assessment.

Result: Across 3 medical VLMs and 9 downstream tasks, LATA consistently reduces prediction set size and class coverage variance while maintaining target coverage, outperforming prior methods with minimal compute.

Conclusion: LATA provides an efficient, black-box solution for improving medical VLM uncertainty calibration without compromising exchangeability, narrowing the gap to label-using methods while being training- and label-free.

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [43] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker: Reinforcement finetuning method that constructs event-level scene graphs and enhances visual grounding to reduce hallucinations in video reasoning by explicitly modeling causal relationships between events.


<details>
  <summary>Details</summary>
Motivation: Video reasoning requires understanding causal relationships between events, but these are often implicit and costly to annotate. Existing MLLMs lack explicit causal structure modeling, leading to hallucinations during video reasoning.

Method: 1) Constructs event-based video scene graph (EVSG) that models intra- and inter-event relations using MLLM, 2) Incorporates scene graphs as intermediate thinking process, 3) Introduces visual attention reward during reinforcement finetuning to strengthen video grounding.

Result: Superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods on RexTime and VidHalluc datasets.

Conclusion: GraphThinker effectively reduces hallucinations in video reasoning by explicitly modeling causal structures through event-level scene graphs and enhancing visual grounding via reinforcement finetuning.

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [44] [RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward](https://arxiv.org/abs/2602.17558)
*Qiucheng Wu,Jing Shi,Simon Jenni,Kushal Kafle,Tianyu Wang,Shiyu Chang,Handong Zhao*

Main category: cs.CV

TL;DR: RetouchIQ is an RL framework for MLLM agents that performs executable image editing guided by a generalist reward model, improving semantic consistency and perceptual quality over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based image editing lacks reliable reward signals for RL training due to the subjective nature of creative editing, and conventional rule-based rewards using fixed reference images with handcrafted metrics are inadequate.

Method: RetouchIQ uses MLLM agents to interpret editing intentions and generate executable adjustments, guided by a generalist reward model (an RL fine-tuned MLLM) that evaluates edited results through case-by-case generated metrics, providing scalar feedback through multimodal reasoning for RL training.

Result: RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems, establishing a new benchmark with a curated dataset of 190k instruction-reasoning pairs.

Conclusion: Generalist reward-driven MLLM agents show potential as flexible, explainable, and executable assistants for professional image editing, bridging high-level aesthetic goals with precise parameter control.

Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>


### [45] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: ArtToMus: First direct artwork-to-music generation framework without text translation, using visual embeddings with latent diffusion to create music that reflects artwork style and content.


<details>
  <summary>Details</summary>
Motivation: Existing image-conditioned music systems have two limitations: (1) trained on natural photos, not capturing rich semantic/stylistic/cultural content of artworks; (2) rely on image-to-text conversion as semantic shortcut, preventing direct visual-to-audio learning.

Method: Introduce ArtSound dataset (105,884 artwork-music pairs with dual-modality captions). Propose ArtToMus framework that projects visual embeddings into conditioning space of latent diffusion model for direct artwork-to-music generation without image-to-text translation or language supervision.

Result: Generates musically coherent and stylistically consistent outputs reflecting visual cues of source artworks. Achieves competitive perceptual quality and meaningful cross-modal correspondence, though absolute alignment scores lower than text-conditioned systems (expected due to increased difficulty).

Conclusion: Establishes direct visual-to-music generation as distinct and challenging research direction. Provides resources for multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset to be publicly released.

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [46] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: A unified geospatial discovery framework combining active learning, online meta-learning, and concept-guided reasoning to efficiently uncover hidden targets in resource-constrained environments with sparse, biased ground truth data.


<details>
  <summary>Details</summary>
Motivation: Real-world settings like environmental monitoring, disaster response, and public health face challenges with costly data collection, dynamic environments, and sparse/biased geospatial ground truth, limiting existing learning-based methods like reinforcement learning.

Method: Proposes a unified framework with two key innovations based on concept relevance: 1) concept-weighted uncertainty sampling strategy that modulates uncertainty by learned relevance using domain-specific concepts, and 2) relevance-aware meta-batch formation strategy that promotes semantic diversity during online meta-updates.

Result: Tested on real-world PFAS contamination dataset, showcasing method's reliability at uncovering targets with limited data in varying environments.

Conclusion: The proposed framework effectively addresses limitations of existing methods by integrating active learning, online meta-learning, and concept-guided reasoning through concept relevance, enabling efficient target discovery in resource-constrained geospatial applications.

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [47] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: CORAL is a DiT-based Virtual Try-On framework that improves garment detail preservation by explicitly aligning person-garment correspondence through attention matching and entropy minimization.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle to preserve fine garment details in unpaired settings and fail to enforce person-garment alignment or explain how correspondence emerges in Diffusion Transformers.

Method: Analyzes full 3D attention in DiT architectures, then introduces CORAL with two components: correspondence distillation loss to align matches with person-garment attention, and entropy minimization loss to sharpen attention distribution.

Result: CORAL consistently improves over baselines, enhancing both global shape transfer and local detail preservation. Extensive ablations validate design choices.

Conclusion: The paper demonstrates that explicit alignment of query-key matching with external correspondences in DiT-based frameworks significantly improves VTON performance for preserving garment details.

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [48] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec is an interactive object retrieval framework that refines predictions using user feedback via dual memory sets for positive/negative cues, achieving significant accuracy improvements on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary detectors operate in one-shot manner without refinement capability, making them ineffective for ambiguous queries or scenes with multiple similar objects where user feedback could help disambiguation.

Method: Proposes IntRec with an Intent State (IS) maintaining dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). Uses contrastive alignment function to rank candidates by maximizing similarity to positive cues while penalizing rejected ones.

Result: On LVIS: 35.4 AP, outperforming OVMR by +2.3, CoDet by +3.7, and CAKE by +0.5. On LVIS-Ambiguous: +7.9 AP improvement over one-shot baseline after single corrective feedback, with <30ms added latency per interaction.

Conclusion: IntRec provides substantial improvements in retrieval accuracy without additional supervision, enabling fine-grained disambiguation in cluttered scenes through interactive refinement based on user feedback.

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [49] [Human-level 3D shape perception emerges from multi-view learning](https://arxiv.org/abs/2602.17650)
*Tyler Bonnen,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: A neural network framework trained on visual-spatial objectives matches human 3D shape inference performance without task-specific training, predicting both accuracy and fine-grained behavioral patterns.


<details>
  <summary>Details</summary>
Motivation: To model human ability to infer 3D structure from 2D visual inputs, which has been a longstanding challenge where previous computational methods have fallen short of human performance.

Method: Developed novel neural networks trained with visual-spatial objectives on naturalistic sensory data (predicting camera location and visual depth from multi-view images), without object-related inductive biases. Used zero-shot evaluation on established 3D perception tasks.

Result: First framework to match human accuracy on 3D shape inferences without task-specific training. Model responses predict fine-grained human behavior including error patterns and reaction times, revealing correspondence between model dynamics and human perception.

Conclusion: Human-level 3D perception can emerge from simple, scalable learning objectives over naturalistic visual-spatial data, suggesting that visual-spatial signals analogous to human sensory cues are sufficient for achieving human-like 3D shape inference.

Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.

</details>


### [50] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: OpenEarthAgent is a tool-augmented geospatial agent framework for multimodal reasoning over satellite imagery, trained on structured reasoning traces with GIS operations and spectral indices.


<details>
  <summary>Details</summary>
Motivation: Extending multimodal reasoning capabilities to remote sensing is challenging due to requirements for spatial scale reasoning, geographic structures, and multispectral indices while maintaining coherent multi-step logic.

Method: Unified framework using supervised fine-tuning over structured reasoning trajectories, aligning models with verified multistep tool interactions across diverse analytical contexts. Includes 14,538 training instances with over 100K reasoning steps spanning urban, environmental, disaster, and infrastructure domains with GIS operations and spectral indices (NDVI, NBR, NDBI).

Result: The agent demonstrates structured reasoning, stable spatial understanding, and interpretable behavior through tool-driven geospatial interactions. Shows consistent improvements over strong baselines and competitive performance relative to recent open and closed-source models.

Conclusion: OpenEarthAgent successfully bridges the gap in multimodal reasoning for remote sensing by providing a framework that enables coherent geospatial analysis through tool-augmented agents trained on structured reasoning traces.

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE ontology provides standardized framework for dental age assessment, addressing methodological heterogeneity and enabling traceable linkage between observations, methods, and outcomes in forensic/legal contexts.


<details>
  <summary>Details</summary>
Motivation: Current dental age assessment practices suffer from methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal systems, hindering transparency and reproducibility, especially with increasing AI adoption.

Method: Developed with domain experts, the AIdentifyAGE ontology builds on upper and established biomedical, dental, and machine learning ontologies to provide a standardized, semantically coherent framework that models complete medico-legal workflows including judicial context, individual data, forensic exams, dental development methods, imaging, reference studies, and AI estimation.

Result: The ontology enables traceable linkage between observations, methods, reference data, and reported outcomes, ensuring interoperability, extensibility, and compliance with FAIR principles for both manual and AI-assisted forensic dental age assessment.

Conclusion: AIdentifyAGE establishes a robust foundation for ontology-driven decision support systems, enhancing consistency, transparency, and explainability in medico-legal and judicial contexts where age assessment determines access to protection, healthcare, and judicial procedures.

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [52] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: LLMs, RAG, and GraphRAG show promise for automated Design Structure Matrix generation, tested on power screwdriver and CubeSat use cases with varying complexity.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of advanced AI techniques (LLMs, RAG, GraphRAG) for automating the generation of Design Structure Matrices, which are important for system architecture analysis but traditionally require manual effort from domain experts.

Method: Tested three methods (LLMs, RAG, GraphRAG) on two distinct use cases: a power screwdriver and a CubeSat with known architectural references. Evaluated performance on two key tasks: determining relationships between predefined components, and identifying components and their relationships from scratch.

Result: Despite design and computational challenges, the methods show opportunities for automated DSM generation. Performance was measured by assessing each DSM element and overall architecture quality.

Conclusion: AI techniques demonstrate potential for automating DSM generation, with all code made publicly available for reproducibility and to enable further feedback from domain experts.

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [53] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: Contextuality emerges inevitably from reusing a single internal state across multiple contexts in classical probabilistic systems, creating irreducible information-theoretic costs that nonclassical frameworks avoid.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental representational consequences of single-state reuse in adaptive systems, which is ubiquitous in both natural and artificial intelligence but poorly understood.

Method: Model contexts as interventions acting on a shared internal state, prove that classical models reproducing contextual statistics must incur irreducible information-theoretic costs, provide minimal constructive examples, and compare with nonclassical frameworks.

Result: Contextuality is not unique to quantum mechanics but arises inevitably from single-state reuse in classical probabilistic representations, with dependence on context that cannot be mediated solely through internal state.

Conclusion: Contextuality represents a general representational constraint on adaptive intelligence independent of physical implementation, explaining how nonclassical frameworks avoid this obstruction by relaxing assumptions about global joint probability spaces.

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [54] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache is a mobility-aware cache framework that uses reconstructible caches to enable efficient large-scale human mobility simulations by reusing reasoning steps and using lightweight decoding, achieving significant efficiency gains while maintaining performance comparable to LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale human mobility simulation is important for urban planning, epidemiology, and transportation analysis, but current LLM-based approaches have high computational costs that limit scalability.

Method: MobCache consists of two components: (1) a reasoning component that encodes reasoning steps as latent-space embeddings and uses a latent-space evaluator for reuse and recombination of reasoning steps, and (2) a decoding component with a lightweight decoder trained using mobility law-constrained distillation to translate latent-space reasoning chains into natural language.

Result: Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

Conclusion: MobCache enables efficient large-scale human mobility simulations through reconstructible caches, addressing the scalability limitations of current LLM-based approaches while maintaining simulation fidelity.

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [55] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: Analysis of 60 LLM benchmarks reveals nearly half show saturation, with expert-curated benchmarks resisting saturation better than crowdsourced ones, and hidden test data providing no protective effect.


<details>
  <summary>Details</summary>
Motivation: AI benchmarks become saturated over time, losing their ability to differentiate between top-performing models, which diminishes their long-term value for measuring progress and guiding deployment decisions.

Method: Analyzed 60 LLM benchmarks from major model developers' technical reports, characterized them along 14 properties spanning task design, data construction, and evaluation format, and tested five hypotheses about how each property contributes to saturation rates.

Result: Nearly half of benchmarks exhibit saturation, with rates increasing as benchmarks age. Expert-curated benchmarks resist saturation better than crowdsourced ones, while hiding test data (public vs. private) shows no protective effect against saturation.

Conclusion: Benchmark design choices significantly impact longevity; expert curation extends benchmark durability while crowdsourced benchmarks saturate faster, providing guidance for creating more durable evaluation frameworks.

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [56] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: Simple baselines match or exceed sophisticated code evolution methods across three domains, revealing shortcomings in current approaches and evaluation practices.


<details>
  <summary>Details</summary>
Motivation: To test whether simpler baselines perform as well as complex code evolution pipelines, and to identify shortcomings in how code evolution is developed and evaluated.

Method: Testing two simple baselines across three domains: mathematical bounds, agentic scaffolds, and machine learning competitions, comparing them to more sophisticated code evolution methods.

Result: Simple baselines matched or exceeded sophisticated methods in all three domains. For mathematical bounds, search space design and domain knowledge were more important than the evolution pipeline. For agentic scaffolds, high variance and small datasets led to suboptimal selection, with hand-designed majority vote scaffolds performing best.

Conclusion: Code evolution research needs better evaluation methods, reduced stochasticity, and more rigorous practices. The primary challenge is often search space design rather than the search algorithm itself, suggesting simpler approaches can be effective.

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [57] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: Improved upper bound for slicing all edges of n-dimensional hypercube with hyperplanes: S(n) ≤ ⌈4n/5⌉ (except when n is odd multiple of 5, then S(n) ≤ 4n/5 + 1), beating previous bound of ⌈5n/6⌉.


<details>
  <summary>Details</summary>
Motivation: Study the minimum number of hyperplanes needed to slice all edges of an n-dimensional hypercube, a combinatorial geometry problem with applications in computational geometry and optimization.

Method: Constructed 8 hyperplanes slicing Q₁₀ using CPro1 - an automatic tool combining reasoning LLMs with automated hyperparameter tuning to create search algorithms for mathematical constructions. Used this construction to derive general bound.

Result: Proved S(n) ≤ ⌈4n/5⌉ (except when n is odd multiple of 5, then S(n) ≤ 4n/5 + 1), improving Paterson's 1971 bound of S(n) ≤ ⌈5n/6⌉. Also obtained new lower bounds on maximum edges sliced by k < n hyperplanes.

Conclusion: Significant improvement in upper bound for hypercube edge slicing problem using novel AI-assisted mathematical discovery approach, demonstrating potential of automated reasoning tools in combinatorial geometry.

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [58] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent is an AI workflow that automates crystallography data analysis from instrument data to validated structure, reducing analysis time by 4.6-5.0x while maintaining validation standards.


<details>
  <summary>Details</summary>
Motivation: Large-scale facilities face analysis latency as the bottleneck in scientific throughput, especially for complex samples requiring iterative reduction, integration, refinement, and validation steps.

Method: A governed, tool-using AI workflow for TOPAZ at SNS that executes established pipeline under explicit governance: restricts actions to allowlisted tools, enforces fail-closed verification gates at workflow boundaries, and captures complete provenance.

Result: Reduces wall time from 435 minutes (manual) to 86.5-94.4 minutes (4.6-5.0x faster) while producing validated CIF with no checkCIF level A or B alerts in reference-case benchmark.

Conclusion: Establishes practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [59] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: Node Learning is a decentralized AI paradigm where edge nodes learn independently from local data and selectively share knowledge with peers, enabling intelligence to propagate through overlap and diffusion rather than centralized aggregation.


<details>
  <summary>Details</summary>
Motivation: Centralized AI faces scaling challenges at the edge due to data transmission costs, latency, energy consumption, and dependence on large data centers, which are problematic for heterogeneous, mobile, and resource-constrained environments.

Method: Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Intelligence propagates through overlap and diffusion rather than global synchronization or central aggregation.

Result: This is a conceptual paper that develops the foundations of Node Learning, contrasting it with existing decentralized approaches and examining implications for communication, hardware, trust, and governance.

Conclusion: Node Learning offers a unified abstraction for autonomous and cooperative behavior that accommodates heterogeneity in data, hardware, objectives, and connectivity, placing existing paradigms within a broader decentralized perspective.

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [60] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: The paper proposes an order-oriented scoring framework for hesitant fuzzy sets, proves classical orders don't form lattices, introduces dominance functions for ranking, and applies them to group decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional scoring approaches for hesitant fuzzy sets lack formal order-theoretic foundations, leading to inconsistent and inflexible ranking mechanisms that need a more rigorous mathematical basis.

Method: Develops a unified order-oriented scoring framework, analyzes classical orders on hesitant fuzzy elements, proves they don't induce lattice structures, introduces dominance functions for ranking relative to control sets with acceptability thresholds, and provides concrete examples (discrete and relative dominance functions).

Result: Shows classical orders on hesitant fuzzy elements don't form lattices (contrary to prior claims), proves scores defined with symmetric order satisfy key normative criteria (strong monotonicity and Gärdenfors condition), and demonstrates dominance functions can construct fuzzy preference relations for group decision-making.

Conclusion: The order-oriented framework provides a more rigorous foundation for hesitant fuzzy set scoring, with dominance functions offering practical tools for ranking and group decision-making applications.

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [61] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR benchmark reveals multilingual safety vulnerabilities in LLMs across 12 Indic languages, showing contract-bound evaluations inflate refusals but don't stop jailbreaks, English-to-Indic attacks transfer strongly, and orthography (romanization) systematically affects safety.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety alignment is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied, especially for South Asian users who frequently code-switch and romanize.

Method: Created Indic Jailbreak Robustness (IJR) benchmark with 45,216 prompts across 12 Indic/South Asian languages in JSON (contract-bound) and Free (naturalistic) tracks, using judge-free evaluation with human audits.

Result: Three key findings: 1) Contracts inflate refusals but don't stop jailbreaks (LLaMA/Sarvam >0.92 JSR in JSON, all models reach 1.0 in Free), 2) English-to-Indic attacks transfer strongly with format wrappers outperforming instruction wrappers, 3) Romanized/mixed inputs reduce JSR with correlations to romanization share and tokenization (~0.28-0.32).

Conclusion: IJR reveals risks hidden by English-only, contract-focused evaluations, especially for South Asian users who code-switch and romanize, offering a reproducible multilingual stress test for LLM safety.

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [62] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5 is a state-of-the-art native GUI agent model with multiple size variants that achieves top performance across 20+ GUI benchmarks for automation, grounding, tool-calling, and knowledge tasks on various platforms.


<details>
  <summary>Details</summary>
Motivation: To create a versatile GUI agent that can operate across multiple platforms (desktop, mobile, browser) and enable cloud-edge collaboration with real-time interaction capabilities.

Method: Uses hybrid data flywheel combining simulated and cloud-based sandbox environments for UI understanding and trajectory generation, unified thought-synthesis pipeline for reasoning enhancement, and novel MRPO algorithm for multi-platform environment RL scaling.

Result: Achieves SOTA results on 20+ GUI benchmarks: 56.5 on OSWorld, 71.6 on AndroidWorld, 48.4 on WebArena for automation; 80.3 on ScreenSpotPro for grounding; 47.6 on OSWorld-MCP and 46.8 on MobileWorld for tool-calling; 75.5 on GUI-Knowledge Bench for memory/knowledge.

Conclusion: GUI-Owl-1.5 represents a significant advancement in GUI agent technology with its multi-platform support, innovative data pipeline, enhanced reasoning capabilities, and open-source availability, enabling practical cloud-edge collaboration for real-time GUI interactions.

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [63] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage is the first agent development kit that enables LLMs to automatically create agents with self-generated topology and toolsets, featuring comprehensive memory support and outperforming existing ADKs across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current agent development kits either lack sufficient functional support or require manual human design of agent topology, tools, and memory, which limits agents' generalizability and overall performance.

Method: OpenSage enables LLMs to automatically create agents with self-generated topology and toolsets, provides comprehensive structured memory support with a hierarchical graph-based memory system, and includes specialized toolkits for software engineering tasks.

Result: Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate OpenSage's advantages over existing ADKs, with rigorous ablation studies confirming the effectiveness of each design component.

Conclusion: OpenSage paves the way for next-generation agent development by shifting from human-centered to AI-centered paradigms, enabling automatic agent creation with self-generated components.

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [64] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB is the first benchmark for evaluating LLM agent vulnerabilities to adaptive, long-horizon attacks across 28 environments and 644 test cases, revealing current agents remain highly susceptible despite single-turn defenses.


<details>
  <summary>Details</summary>
Motivation: As LLM agents are increasingly deployed in complex, long-horizon environments, they become exposed to sophisticated multi-turn attacks that exploit extended user-agent-environment interactions, creating security risks that single-turn defenses cannot adequately address.

Method: Developed AgentLAB benchmark with five novel attack types (intent hijacking, tool chaining, task injection, objective drifting, memory poisoning) across 28 realistic agentic environments, comprising 644 security test cases to systematically evaluate agent vulnerabilities.

Result: Evaluation of representative LLM agents shows they remain highly susceptible to long-horizon attacks, and existing defenses designed for single-turn interactions fail to reliably mitigate these extended multi-turn threats.

Conclusion: AgentLAB provides a valuable benchmark for tracking progress in securing LLM agents against long-horizon attacks in practical settings, highlighting the need for new defense mechanisms beyond single-turn approaches.

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [65] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace is a benchmark testing LLMs' planning, reasoning, and world knowledge through Wikipedia navigation tasks, revealing superhuman performance on easy levels but significant limitations on hard tasks.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that evaluates planning, reasoning, and world knowledge in LLMs through Wikipedia navigation, revealing limitations in current reasoning systems.

Method: Models navigate Wikipedia hyperlinks step-by-step from source to target pages, requiring look-ahead planning and reasoning about concept connections. Evaluates open- and closed-source models including Gemini-3, GPT-5, and Claude Opus 4.5 across easy and hard difficulty levels.

Result: Frontier models achieve superhuman performance on easy tasks but performance drops sharply on hard difficulty (Gemini-3 succeeds in only 23% of hard games). World knowledge is necessary but insufficient beyond a threshold where planning and long-horizon reasoning dominate. Models struggle with replanning after failure and frequently enter loops.

Conclusion: LLM-Wikirace reveals clear limitations in current reasoning systems, showing that planning-capable LLMs still face substantial challenges in long-horizon reasoning and recovery from failures, offering an open arena for future improvement.

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [66] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: Fine-tuning aligned vision-language models on harmful data causes severe emergent misalignment that generalizes broadly, with multimodal evaluation revealing worse alignment degradation than text-only benchmarks suggest.


<details>
  <summary>Details</summary>
Motivation: Lifelong multimodal agents need continuous adaptation, but this creates tension between acquiring new capabilities and preserving safety alignment. The paper investigates how fine-tuning on harmful data affects alignment in vision-language models.

Method: Experiments on Gemma3-4B with LoRA fine-tuning on harmful datasets, measuring misalignment across tasks and modalities. Geometric analysis of harmful behavior subspace and evaluation of mitigation strategies (benign fine-tuning and activation-based steering).

Result: Fine-tuning induces severe emergent misalignment that scales with LoRA rank. Multimodal evaluation shows much higher misalignment (70.71±1.22 at r=128) than text-only (41.19±2.51). Even 10% harmful data causes substantial degradation. Harmful behaviors occupy low-dimensional subspace (10 principal components). Mitigation strategies reduce but don't completely remove harmful behaviors.

Conclusion: Current post-training paradigms may not sufficiently preserve alignment in deployment settings, highlighting need for robust continual learning frameworks as unimodal safety benchmarks underestimate alignment degradation in vision-language models.

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [67] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext is a stateful RNN-based framework that tracks conversation history to detect multi-turn jailbreak attacks, outperforming stateless safety filters with 0.84 F1 score and <20ms inference time.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety guardrails are stateless and treat multi-turn dialogues as disconnected events, creating a "Safety Gap" where adversarial tactics like Crescendo and ActorAttack can slowly bleed malicious intent across turn boundaries to bypass filters.

Method: DeepContext uses a Recurrent Neural Network (RNN) architecture that ingests sequences of fine-tuned turn-level embeddings and propagates a hidden state across conversations to capture incremental risk accumulation over time.

Result: Achieves state-of-the-art F1 score of 0.84 in multi-turn jailbreak detection, significantly outperforming hyperscaler cloud-provider guardrails and open-weight models like Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67), with sub-20ms inference overhead on T4 GPU.

Conclusion: Modeling the sequential evolution of intent is more effective and computationally efficient than deploying massive stateless models for LLM safety monitoring.

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [68] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench is a new benchmark for evaluating the quality of web sources cited by LLMs, covering 100 real-world queries across different intents with an 8-metric framework for assessing both content quality and page-level signals.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs focus on answer correctness but neglect the quality of the web sources they cite, creating a gap in understanding how well LLMs select and reference evidence from the web.

Method: Created SourceBench with 100 real-world queries across informational, factual, argumentative, social, and shopping intents. Developed an 8-metric framework covering content quality (relevance, accuracy, objectivity) and page-level signals (freshness, authority, clarity). Built a human-labeled dataset and calibrated LLM-based evaluator that matches expert judgments.

Result: Evaluated 8 LLMs, Google Search, and 3 AI search tools over 3996 cited sources. The benchmark revealed four key new insights about GenAI and web search that can guide future research.

Conclusion: SourceBench provides a comprehensive framework for evaluating web source quality in LLM responses, revealing important insights about how LLMs select and cite evidence, with implications for improving GenAI and web search integration.

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [69] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: Text safety ≠ tool-call safety: LLMs can refuse harmful requests in text while simultaneously executing forbidden actions through tool calls.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations focus on text-level refusal behavior, but don't assess whether alignment suppressing harmful text also prevents harmful real-world actions through tool calls.

Method: Created GAP benchmark with systematic evaluation across 6 frontier models, 6 regulated domains, 7 jailbreak scenarios per domain, 3 system prompt conditions, and 2 prompt variants (17,420 datapoints). Formalized GAP metric to measure divergence between text safety and tool-call safety.

Result: Text safety doesn't transfer to tool-call safety. Models frequently refuse harmful requests in text while executing forbidden actions via tool calls (219 cases persist even with safety-reinforced prompts). System prompts significantly influence tool-call behavior (TC-safe rates vary 21-57 percentage points). Runtime governance reduces information leakage but doesn't deter forbidden tool-call attempts.

Conclusion: Text-only safety evaluations are insufficient for assessing agent behavior. Tool-call safety requires dedicated measurement and mitigation strategies beyond text-level alignment.

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [70] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov: An offline agent-learning framework for hardware verification that uses execution-validated data curation and policy-aware synthesis to enable scalable learning without expensive online feedback.


<details>
  <summary>Details</summary>
Motivation: Execution-aware LLM agents need expensive and slow tool feedback, making online RL impractical for hardware verification which relies on industrial simulators and non-differentiable execution signals.

Method: Models verification as memoryless state transitions guided by deterministic evaluators, with execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling for scalable offline learning.

Result: A compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and competing with models 10x larger.

Conclusion: LLM4Cov enables effective offline agent learning for hardware verification by addressing execution constraints through novel data curation and synthesis techniques, achieving strong performance with compact models.

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [71] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom is an automated agent hijacking framework that uses structured template injection to exploit LLM agent architecture vulnerabilities, achieving high attack success rates and transferability across commercial models.


<details>
  <summary>Details</summary>
Motivation: Current agent hijacking attacks rely on manual prompt manipulation with low success rates and poor transferability to closed-source models. There's a need for automated attacks that target fundamental architectural mechanisms of LLM agents.

Method: Uses Structured Template Injection to exploit how agents use chat template tokens. Implements multi-level template augmentation, trains a Template Autoencoder (TAE) to embed templates in continuous latent space, and applies Bayesian optimization to find optimal adversarial vectors.

Result: Significantly outperforms existing baselines in Attack Success Rate (ASR) and query efficiency on Qwen, GPT, and Gemini. Identified over 70 vulnerabilities in real-world commercial products confirmed by vendors.

Conclusion: Structured template-based hijacking poses severe practical threats to LLM agents, demonstrating the need for securing next-generation agentic systems against architectural vulnerabilities.

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [72] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS is a hybrid quantum-classical pipeline that integrates forecasting, discrete risk optimization, and auditability for financial risk systems, improving prediction accuracy, portfolio performance, and solve times while providing verifiable allocation records.


<details>
  <summary>Details</summary>
Motivation: Traditional two-step financial risk systems (prediction then optimization) break under real constraints - predictions may look good but decisions become unstable with market shifts, discrete constraints (lot sizes, caps), or slow optimization for large asset sets. Regulated settings also require clear audit trails linking decisions to model states and inputs.

Method: HQFS uses a three-stage hybrid pipeline: 1) Learns next-step return and volatility proxy using variational quantum circuit (VQC) with small classical head; 2) Converts risk-return objective and constraints into QUBO, solving with quantum annealing when available (classical QUBO solver as fallback); 3) Signs each rebalance output using post-quantum signature for later verification.

Result: On market dataset: reduces return prediction error by 7.8% and volatility prediction error by 6.1% vs classical baseline. Decision layer improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. QUBO solve cuts average solve time by 28% vs mixed-integer baseline while producing fully traceable, signed allocation records.

Conclusion: HQFS provides a practical solution that addresses the limitations of traditional two-step financial risk systems by integrating forecasting, discrete optimization, and auditability in a single flow, demonstrating significant improvements in prediction accuracy, portfolio performance, and computational efficiency while ensuring regulatory compliance through verifiable records.

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [73] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: Black-box safety evaluation fails for models with latent context triggers; statistical limits prevent reliable deployment risk estimation, requiring additional safeguards.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that black-box safety evaluation reliably predicts deployment performance for AI systems with latent context-conditioned policies where internal triggers are rare in testing but common in deployment.

Method: Formal analysis using statistical minimax lower bounds (Le Cam's method), adaptive evaluation with hash-based triggers (Yao's principle), computational separation via trapdoor one-way functions, and white-box probing with bias correction.

Result: Fundamental limits: passive evaluation error ≥ 0.208δL; adaptive evaluation error ≥ δL/16; computational separation shows polynomial-time evaluators cannot detect trapdoor-triggered unsafe behaviors; white-box probing requires O(1/(γ²ε_R²)) samples.

Conclusion: Black-box testing is statistically underdetermined for latent context-triggered models; mathematical criteria show when architectural constraints, training guarantees, interpretability, and deployment monitoring are necessary for worst-case safety assurance.

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [74] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe is a conversational benchmark for stock recommendation that evaluates LLMs beyond behavior imitation by distinguishing between descriptive user behavior and normative utility based on investor risk preferences.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation benchmarks conflate behavioral imitation with decision quality, treating user choices as ground truth even when they may be noisy or short-sighted under market volatility, conflicting with long-term goals.

Method: Built from real market data and human decision trajectories, Conv-FinRe provides multi-view references that separate descriptive behavior from normative utility. Models are evaluated on stock rankings given onboarding interviews, step-wise market context, and advisory dialogues over fixed investment horizons.

Result: Evaluation of state-of-the-art LLMs reveals a persistent tension: models performing well on utility-based ranking often fail to match user choices, while behaviorally aligned models can overfit short-term noise.

Conclusion: The benchmark enables diagnosis of whether LLMs follow rational analysis, mimic user noise, or are driven by market momentum, with the dataset publicly released on Hugging Face and codebase on GitHub.

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [75] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS is a neuro-symbolic framework for natural language querying of time series databases using a Search-Then-Verify pipeline with SQL and Python programs, evaluated on the new NLQTSBench benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL methods can't handle continuous morphological intents (shapes/anomalies) in time series, and time series models struggle with ultra-long histories, creating a gap for natural language querying of time series databases.

Method: Sonar-TS uses a neuro-symbolic Search-Then-Verify pipeline: first uses a feature index to ping candidate windows via SQL queries, then generates Python programs to verify candidates against raw signals, analogous to active sonar systems.

Result: Sonar-TS effectively navigates complex temporal queries where traditional methods fail, and the authors introduce NLQTSBench as the first large-scale benchmark for NLQ over TSDB-scale histories.

Conclusion: This work presents the first systematic study of NLQ4TSDB, offering a general framework (Sonar-TS) and evaluation standard (NLQTSBench) to facilitate future research in natural language querying for time series databases.

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [76] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder is a two-stage matchmaking system for multiplayer games that uses Ruzicka similarity for fast filtering and Kantorovich distance on skill buckets for precise fairness evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional matchmaking using average team skill metrics often creates unbalanced games when dealing with lobbies of heterogeneous skill levels, particularly with wide or skewed skill distributions, which negatively impacts player retention and satisfaction.

Method: Two-stage system: 1) Fast preliminary filter using Ruzicka similarity index on "non-outlier" skill ranges, 2) Precise fairness evaluation using Kantorovich distance on sorted skill bucket indices (mapped from player ranks via inverted normal distribution) to calculate a "Sanction Score."

Result: Demonstrated viability by analyzing Sanction Score distribution from 140 million simulated lobby pairings, providing robust foundation for fair matchmaking thresholds.

Conclusion: Cinder provides a practical solution for fair and fast matchmaking in multiplayer games by combining efficient filtering with precise fairness quantification, addressing the limitations of traditional average-based approaches.

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [77] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F is an agentic framework that automates end-to-end formalization of entire mathematical textbooks into Lean, achieving 96% proof success on 153K lines from 479 pages in weeks.


<details>
  <summary>Details</summary>
Motivation: Current automated formalization is limited to isolated theorems and short snippets, lacking the ability to handle project-scale mathematics with cross-file dependencies, imports, and end-to-end compilation required for textbooks and research papers.

Method: Two-stage framework: 1) Statement compilation splits documents into atomic blocks, orders by dependencies, and repairs declaration skeletons until project compiles with proof placeholders; 2) Proof repair closes holes using goal-conditioned local edits, with verifier-in-the-loop feedback for committing improvements.

Result: Successfully formalized 153,853 lines of Lean code from 479 textbook pages on real and convex analysis in ~3 weeks, achieving 96% proof success on FATE-H benchmark (vs 80% baseline), demonstrating textbook-scale formalization at unprecedented speed.

Conclusion: Practical, large-scale automated formalization of mathematical literature is now within reach, enabling project-scale conversion that would typically require months or years of expert effort.

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [78] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: Sales Research Agent for CRM data analysis outperforms leading AI models on enterprise-specific benchmark metrics.


<details>
  <summary>Details</summary>
Motivation: Enterprises need AI systems that can answer sales questions using live CRM data with transparent, repeatable quality evidence, but current models lack observable quality metrics.

Method: Developed Sales Research Agent in Microsoft Dynamics 365 Sales that connects to live CRM data, reasons over complex schemas, and produces insights via text and charts. Created Sales Research Bench benchmark with 8 customer-weighted dimensions to measure quality.

Result: On October 19, 2025, Sales Research Agent scored 13 points higher than Claude Sonnet 4.5 and 24.1 points higher than ChatGPT-5 on a 100-point composite score using 200 questions on customized enterprise schema.

Conclusion: The Sales Research Agent provides enterprises with an AI solution that delivers transparent, repeatable quality evidence for CRM data analysis, outperforming leading general AI models on enterprise-specific tasks.

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [79] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: PA-MoE addresses simplicity bias in RL by using phase-aware routing instead of token-level routing, allowing experts to specialize in different task phases rather than getting fragmented assignments.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods use single policy networks causing simplicity bias where simple tasks dominate gradient updates, leaving insufficient capacity for complex tasks. Traditional MoE's token-level routing fragments phase-consistent patterns, undermining expert specialization.

Method: Proposes Phase-Aware Mixture of Experts (PA-MoE) with a lightweight phase router that learns latent phase boundaries directly from RL objective without pre-defined categories, allocating temporally consistent assignments to the same expert.

Result: Experimental results demonstrate the effectiveness of PA-MoE in addressing simplicity bias and improving expert specialization.

Conclusion: PA-MoE successfully overcomes limitations of traditional MoE by enabling phase-aware routing that preserves phase-specific expertise, leading to more effective RL agent training.

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [80] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA is a multi-agent framework for computer-use automation that uses intent-aligned plan memory to stabilize long-horizon execution, outperforming existing approaches with 74.83% task success rate.


<details>
  <summary>Details</summary>
Motivation: Existing computer-use agents struggle with long-horizon execution under noisy perception and evolving environments, often drifting from user intent and repeatedly solving routine subproblems, leading to error accumulation and inefficiency.

Method: A multi-agent framework with Planner, Plan-Optimizer, and Critic coordinating over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. Intent prototypes retrieve subgroup-aligned skills and inject them into partial plans.

Result: Achieved 74.83% task success rate with Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Multi-view intent abstraction and shared plan memory jointly improve execution stability.

Conclusion: System-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments, with cooperative multi-agent loop providing largest gains on long-horizon tasks.

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [81] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR reduces LLM agent costs by 95% per-step tokens and improves tool routing by 32% via dynamic retrieval of minimal instructions and tools.


<details>
  <summary>Details</summary>
Motivation: LLM agents waste resources by re-ingesting full system instructions and large tool catalogs each step, increasing costs, latency, and error rates.

Method: Instruction-Tool Retrieval (ITR) - a RAG variant that dynamically retrieves only necessary system-prompt fragments and minimal tool subsets per step, with confidence-gated fallbacks.

Result: 95% reduction in per-step context tokens, 32% relative improvement in correct tool routing, 70% cut in end-to-end episode cost, enabling 2-20x more loops within context limits.

Conclusion: ITR is particularly valuable for long-running autonomous agents as savings compound with steps, offering practical deployment guidance for efficient LLM agent operation.

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [82] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: LRMs often generate plausible-sounding but unfaithful rationales. The paper introduces a formal framework for reasoning faithfulness with two conditions (stance consistency and causal influence), creates RFEval benchmark, finds 49.7% unfaithfulness across models, and shows accuracy is not a reliable proxy for faithfulness.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models produce rationales that sound plausible but don't reflect their true decision process, undermining reliability and trust. Current evaluation focuses on accuracy but doesn't assess whether the stated reasoning actually drives the answer.

Method: Introduces formal framework with two testable conditions: stance consistency (coherent stance linking reasoning to answer) and causal influence (reasoning causally drives answer under output-level interventions). Creates RFEval benchmark of 7,186 instances across 7 tasks with controlled counterfactual interventions. Evaluates 12 open-source LRMs.

Result: Finds unfaithfulness in 49.7% of outputs, mostly from stance inconsistency. Failures concentrated in math and code domains. Post-training regimes (RL-style objectives) reduce faithfulness even when accuracy maintained. Accuracy-faithfulness correlation is weak and statistically insignificant once controlling for model and task.

Conclusion: Trustworthy AI requires optimizing not only for correct outcomes but also for structural integrity of reasoning process. Provides rigorous methodology for auditing LRM reliability. Shows current RL-style training can harm reasoning faithfulness despite maintaining accuracy.

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [83] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q learns multiple sub-value functions to retain alternative high-value actions, enabling better adaptation to changing value functions in cooperative MARL.


<details>
  <summary>Details</summary>
Motivation: Existing value decomposition methods in MARL rely on single optimal actions and struggle to adapt when value functions shift during training, often converging to suboptimal policies.

Method: Proposes Successive Sub-value Q-learning (S2Q) which learns multiple sub-value functions to retain alternative high-value actions, and incorporates them into a Softmax-based behavior policy for persistent exploration.

Result: Experiments on challenging MARL benchmarks show S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance.

Conclusion: S2Q addresses the limitation of existing value decomposition methods by enabling Q-tot to adjust quickly to changing optima through multiple sub-value functions and persistent exploration.

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [84] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS accelerates language model training by using a lightweight predictor to prioritize high-loss samples during batch construction, achieving 6-13% faster convergence with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing curriculum learning approaches require predefined difficulty metrics or expensive per-sample loss tracking, creating practical limitations for efficient training optimization.

Method: PBS uses an online-trained linear predictor that estimates sample difficulty from four static token-level features: token frequency, sequence length, vocabulary diversity, and rare token ratio.

Result: Achieves 0.44 correlation with actual loss, improves from 0.14 to 0.44 over 10k steps, and enables 6-13% faster convergence for a 130M parameter transformer.

Conclusion: Token frequency statistics encode meaningful difficulty information, enabling effective curriculum learning with negligible computational overhead through predictive batch scheduling.

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [85] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI coding agents create distinct pull request descriptions that affect human reviewer engagement, response timing, and merge outcomes in GitHub software development.


<details>
  <summary>Details</summary>
Motivation: As AI coding agents become more prevalent in autonomously creating GitHub pull requests, there's limited understanding of how these agents differ in their PR description characteristics and how human reviewers respond to them, creating a gap in understanding human-AI collaborative software development dynamics.

Method: Empirical analysis of pull requests created by five AI coding agents using the AIDev dataset, examining agent differences in PR description characteristics (including structural features) and human reviewer responses (review activity, response timing, sentiment, and merge outcomes).

Result: AI coding agents exhibit distinct PR description styles that correlate with differences in reviewer engagement, response time, and merge outcomes. There is notable variation across agents in both reviewer interaction metrics and merge rates.

Conclusion: Pull request presentation and reviewer interaction dynamics play a significant role in human-AI collaborative software development, highlighting the importance of how AI agents structure their contributions for effective human review and integration.

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [86] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: This paper proposes using LLM-based agentic AI for intent-driven autonomous 6G physical layer communications, addressing multi-dimensional user objectives that evolve over time.


<details>
  <summary>Details</summary>
Motivation: 6G systems face growing complexity with diverse service demands requiring multi-dimensional objectives (latency, energy, computation, service-level) that change dynamically. Traditional rule-based control is insufficient for these evolving intent-aware requirements.

Method: The paper investigates agentic AI for 6G physical layer through a closed-loop pipeline of intent perception, autonomous decision making, and network execution. It reviews physical-layer tasks, identifies application scenarios for agentic AI, discusses enabling technologies, and presents AgenCom - an intent-driven link decision agent that adaptively constructs communication links.

Result: The paper presents AgenCom as a case study demonstrating how LLM-based agents can translate natural-language intents into executable control decisions, adapting to diverse user preferences and channel conditions.

Conclusion: LLM-based agentic AI provides a promising foundation for intent-aware 6G networks, enabling autonomous adaptation to multi-dimensional user objectives and environmental dynamics through multimodal perception, cross-layer decision making, and sustainable optimization.

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [87] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: Proposes a universal human-AI collaboration framework (STRIDE + SR-Delta) to create trustworthy benchmark datasets for evaluating sustainability rating methodologies, addressing the problem of inconsistent ESG ratings across agencies.


<details>
  <summary>Details</summary>
Motivation: Sustainability/ESG ratings from different agencies for the same company vary widely, limiting their comparability, credibility, and relevance to decision-making. This inconsistency undermines the effectiveness of sustainability assessments.

Method: Two-part framework: 1) STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and scoring system for constructing firm-level benchmark datasets using LLMs, and 2) SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments.

Result: The framework enables scalable and comparable assessment of sustainability rating methodologies, creating trustworthy benchmark datasets that can harmonize rating results across different agencies.

Conclusion: Calls for the AI community to adopt AI-powered approaches to strengthen sustainability rating methodologies, supporting urgent sustainability agendas through more reliable and comparable ESG assessments.

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [88] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: O-Shap introduces a new segmentation approach for Owen value-based feature attribution that satisfies the T-property, improving semantic alignment and outperforming baseline SHAP variants in precision, coherence, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard SHAP methods assume feature independence, which breaks down in vision tasks where pixels have spatial and semantic dependencies. While Owen value (hierarchical Shapley) addresses this, its effectiveness depends on feature group definitions, and current segmentations violate key consistency properties.

Method: Proposes a new segmentation approach that satisfies the T-property to ensure semantic alignment across hierarchy levels, enabling computational pruning while maintaining attribution accuracy and interpretability.

Result: Experiments on image and tabular datasets show O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

Conclusion: The proposed segmentation approach for Owen value-based attribution preserves Shapley foundations while addressing feature dependencies through semantically-aligned hierarchies, offering improved performance and interpretability for vision tasks.

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [89] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG automatically constructs instructor-aligned knowledge graphs from lecture materials to capture learning dependencies for personalized education.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches fail to capture rich pedagogical dependencies in large-scale courses, making it difficult to identify knowledge gaps and provide personalized interventions. Existing knowledge graphs are either too surface-level or ignore instructional signals.

Method: InstructKG extracts concepts as nodes and learning dependencies as directed edges from lecture materials (slides, notes). It combines temporal and semantic signals from educational content with large language models' generalizability.

Result: Experiments on diverse real-world lecture materials show InstructKG captures rich, instructor-aligned learning progressions across multiple courses, validated through human evaluation.

Conclusion: InstructKG provides an effective framework for automatically constructing pedagogical knowledge graphs that represent intended learning progressions, enabling better identification of knowledge gaps and personalized learning interventions.

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [90] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: The paper argues that generative AI requires a new epistemological framework based on high-dimensional geometry, proposing "navigational knowledge" as a third mode of knowledge production distinct from symbolic reasoning and statistical recombination.


<details>
  <summary>Details</summary>
Motivation: Generative AI operates through obscure epistemic mechanisms, lacking the engineering understanding that typically precedes technological deployment. Without understanding its epistemic character, responsible integration into science, education, and institutions cannot proceed on a principled basis.

Method: The paper develops an "Indexical Epistemology of High-Dimensional Spaces" by analyzing four structural properties of high-dimensional geometry: concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity. It draws on Peirce's semiotics and Papert's constructionism to reconceptualize generative models as navigators of learned manifolds.

Result: The paper identifies a paradigmatic break from the Turing-Shannon-von Neumann tradition where semantics remains external, to neural networks where symbolic input is projected into high-dimensional semantic spaces. It proposes "navigational knowledge" as a distinct third mode of knowledge production.

Conclusion: Generative AI requires a new epistemological framework based on high-dimensional geometry, with navigational knowledge representing a fundamental shift in how we understand knowledge production, enabling principled integration of AI into scientific and institutional contexts.

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [91] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: Novel parallel algorithm for decomposing hard CircuitSAT instances using specialized constraints to partition SAT instances into weakened formulas


<details>
  <summary>Details</summary>
Motivation: Need efficient methods to solve challenging CircuitSAT instances, particularly for Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions

Method: Parameterized parallel algorithm using specialized constraints to partition original SAT instances into family of weakened formulas, with parallel hardness estimation to guide decomposition quality

Result: Demonstrated practical efficacy on challenging CircuitSAT instances including Logical Equivalence Checking and cryptographic hash function preimage attacks

Conclusion: The proposed parallel decomposition algorithm effectively handles hard CircuitSAT problems through parameterized constraint-based partitioning with parallel hardness guidance

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [92] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: Combine is a criterion-based pruning framework that provides standardized implementation and comparison of pruning criteria, achieving up to 79% filter reduction while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing CNN pruning solutions lack common implementation standards, making them difficult to implement, compare, and evaluate their effectiveness across different models.

Method: Introduces Combine, a criterion-based pruning framework that provides standardized language for comparing pruning criteria, supports iterative pruning, and includes novel criterion functions for weight removal.

Result: Achieves up to 79% filter pruning while retaining or improving accuracy, reduces computations by up to 68%, and demonstrates that different criteria have varying effects on different models.

Conclusion: Combine provides an effective, fast framework for CNN pruning with standardized criteria comparison, enabling significant model compression while maintaining performance.

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [93] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA is a novel genomic foundation model framework that combines Joint-Embedding Predictive Architecture with traditional generative objectives to capture both local genomic syntax and global functional context, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Genomic Foundation Models (GFMs) using MLM or NTP focus too much on local genomic syntax and fine-grained motif patterns, failing to capture broader functional context and lacking a global biological perspective in their representations.

Method: JEPA-DNA integrates Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives, introducing latent grounding by coupling token-level recovery with predictive objectives in latent space through CLS token supervision. This forces the model to predict high-level functional embeddings of masked genomic segments rather than individual nucleotides.

Result: JEPA-DNA consistently yields superior performance across diverse genomic benchmarks in both supervised and zero-shot tasks compared to generative-only baselines, providing more robust and biologically grounded representations.

Conclusion: JEPA-DNA offers a scalable path toward foundation models that understand both the genomic alphabet and the underlying functional logic of sequences, bridging the gap between local syntax and global biological context.

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [94] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo is a lightweight formula recognition model with only 20M parameters that matches state-of-the-art performance while being 65-80% smaller, enabling real-time inference on consumer hardware and browser deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a formula recognition model that is both high-performance and lightweight enough for practical deployment on consumer-grade hardware and in web browsers, addressing the limitations of larger models that require more computational resources.

Method: Texo uses attentive design, distillation techniques, and transfer of vocabulary and tokenizer from existing models to achieve high performance with minimal parameters. The approach focuses on architectural efficiency while maintaining recognition accuracy.

Result: Texo achieves comparable performance to state-of-the-art models (UniMERNet-T and PPFormulaNet-S) while reducing model size by 80% and 65% respectively. The 20M parameter model enables real-time inference on consumer hardware and browser deployment.

Conclusion: Texo demonstrates that efficient formula recognition is possible with significantly smaller models, enabling practical deployment scenarios including web applications and consumer hardware, while maintaining competitive performance with much larger models.

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [95] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: Online symbolic causal world model construction using Meta-Interpretive Learning and predicate invention for sample-efficient, scalable, and interpretable agent learning.


<details>
  <summary>Details</summary>
Motivation: Standard world modelling methods suffer from sample inefficiency, lack of transparency, and poor scalability in complex environments, requiring agents to internalize underlying world logic more effectively.

Method: Integrates continuous model learning and repair into agent's decision loop using Meta-Interpretive Learning and predicate invention to find semantically meaningful, reusable abstractions, constructing a hierarchy of disentangled concepts from observations.

Result: Achieves sample-efficiency orders of magnitude higher than PPO neural-network baseline, scales to domains with complex relational dynamics where propositional methods suffer combinatorial explosion.

Conclusion: The framework enables construction of symbolic causal world models entirely online, providing interpretable, scalable, and sample-efficient learning for agents in complex environments.

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [96] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: Proposes an AI Agent-based collaborative workflow for humanities/social sciences research, validated using Taiwan's Claude.ai usage data, with focus on human-AI division of labor and verifiability.


<details>
  <summary>Details</summary>
Motivation: Generative AI research has focused on software engineering and natural sciences, with limited methodological exploration for humanities and social sciences, creating a gap in AI-assisted research workflows for these disciplines.

Method: Designs a seven-stage modular workflow based on three principles (task modularization, human-AI division of labor, verifiability), validated empirically using Taiwan's Claude.ai usage data from Anthropic Economic Index (N=7,729 conversations).

Result: Identifies three operational modes of human-AI collaboration (direct execution, iterative refinement, human-led) and demonstrates workflow feasibility through empirical analysis, highlighting irreplaceable human roles in research formulation, interpretation, and ethics.

Conclusion: Proposes a replicable AI collaboration framework for humanities/social sciences, emphasizing human judgment's irreplaceability while acknowledging limitations including single-platform data and AI reliability risks.

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [97] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: LBM is a behavioral foundation model that predicts individual strategic choices by using structured trait profiles instead of persona prompting, achieving better performance with detailed psychological data.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with consistent, individual-specific behavior prediction in high-stakes environments, especially when psychological traits interact with situational constraints. Prompting approaches suffer from identity drift and limited ability to leverage detailed persona descriptions.

Method: LBM shifts from transient persona prompting to behavioral embedding by conditioning on structured, high-dimensional trait profiles derived from comprehensive psychometric batteries. It's fine-tuned on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices.

Result: LBM fine-tuning improves behavioral prediction relative to unadapted Llama-3.1-8B-Instruct and performs comparably to frontier baselines with Big Five traits. Unlike prompting baselines that hit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles.

Conclusion: LBM establishes a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [98] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: LLMs encode cognitive complexity (Bloom's Taxonomy levels) in linearly separable neural representations, with ~95% classification accuracy showing early resolution of cognitive difficulty.


<details>
  <summary>Details</summary>
Motivation: To move beyond surface-level performance metrics and understand how LLMs internally represent cognitive complexity, using Bloom's Taxonomy as a hierarchical framework to probe neural representations.

Method: Analyzed high-dimensional activation vectors from different LLMs, using linear classifiers to test whether different cognitive levels (Remember to Create) are linearly separable in residual streams across layers.

Result: Linear classifiers achieved ~95% mean accuracy across all Bloom's Taxonomy levels, showing cognitive level is encoded in linearly accessible subspaces, with representations becoming increasingly separable across layers.

Conclusion: LLMs resolve cognitive difficulty early in processing, with cognitive complexity encoded in linearly separable neural representations, providing insights into how models internally structure knowledge.

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [99] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: Researchers introduce Shapley-DCLR metric to measure temporal knowledge leakage in LLMs during backtesting, and propose TimeSPEC method to filter contaminated claims for reliable retrospective evaluation.


<details>
  <summary>Details</summary>
Motivation: LLMs may inadvertently use post-cutoff knowledge encoded during training when predicting past events, undermining the validity of retrospective evaluation (backtesting). Current methods lack systematic ways to detect and quantify this temporal knowledge leakage.

Method: 1) Decompose model rationales into atomic claims and categorize by temporal verifiability. 2) Apply Shapley values to measure each claim's contribution to predictions. 3) Introduce Shapley-DCLR metric to quantify decision-critical leakage. 4) Propose TimeSPEC method that interleaves generation with claim verification and regeneration to filter temporal contamination.

Result: Experiments on 350 instances across Supreme Court case prediction, NBA salary estimation, and stock return ranking show substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, outperforming prompt-based temporal constraints.

Conclusion: Explicit, interpretable claim-level verification (TimeSPEC) is more effective than prompt-based temporal constraints for reliable backtesting of LLMs, providing a framework to detect and mitigate temporal knowledge leakage in retrospective evaluations.

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [100] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: Web Verbs: A semantic layer for web actions that provides typed, documented functions as stable units for agents to compose into reliable workflows, bridging API-based and browser-based paradigms.


<details>
  <summary>Details</summary>
Motivation: Current web agents operate on low-level primitives (clicks, keystrokes) that are brittle, inefficient, and hard to verify. As the web evolves toward agentic interaction, there's a need for a semantic layer for web actions to complement existing content-oriented semantic layers.

Method: Proposes Web Verbs - a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface. Verbs can be implemented via APIs or robust client-side workflows, and include preconditions, postconditions, policy tags, and logging support.

Result: Demonstrates through proof-of-concept implementation and case studies that Web Verbs enable concise and robust execution compared to existing agents, reducing dozens of steps into few function calls while providing typed contracts and checkable traces.

Conclusion: Web Verbs provide a semantic abstraction that improves reliability (stable interfaces), efficiency (reduced steps), and verifiability (typed contracts, traces). The paper outlines a roadmap for standardization to make verbs deployable and trustworthy at web scale.

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [101] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: Training a 1.36B scientific language model from raw arXiv LaTeX sources, documenting the end-to-end pipeline and analyzing practical challenges in domain-specialized model development under constrained compute.


<details>
  <summary>Details</summary>
Motivation: While frontier LLMs show strong reasoning capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented, especially for researchers with moderate compute budgets.

Method: End-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training on 2xA100 GPUs with 24 experimental runs analyzing various aspects of the process.

Result: Preprocessing decisions significantly affect usable token volume, tokenization impacts symbolic stability, and storage/I/O constraints can rival compute as limiting factors. Stable training behavior was achieved in a data-rich regime (52B pretraining tokens).

Conclusion: This work provides an engineering-grounded, transparent account of training a small scientific language model from scratch, offering practical insights for researchers building domain-specialized models under moderate compute budgets.

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [102] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify is an AI agent that generates follow-up questions to reduce diagnostic uncertainty, improving medical LLM performance by 27 percentage points over single-shot baselines.


<details>
  <summary>Details</summary>
Motivation: Current medical LLMs struggle with diagnostic reasoning when patient cases are incomplete, often producing multiple similarly likely diagnoses without systematic follow-up questioning like real clinicians do.

Method: MedClarify computes candidate diagnoses (differential diagnosis), generates follow-up questions to reduce uncertainty, and selects questions with highest expected information gain for iterative reasoning.

Result: Reduces diagnostic errors by ~27 percentage points compared to standard single-shot LLM baselines, demonstrating effective uncertainty-aware reasoning through targeted questioning.

Conclusion: MedClarify offers a path to improve medical LLMs through agentic information-seeking, enabling dialogues that reflect the iterative and uncertain nature of real-world clinical reasoning.

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [103] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: Proposes a dataless regularization method using curvature matrix approximation to prevent representation drift in task arithmetic, achieving SOTA results without external task data.


<details>
  <summary>Details</summary>
Motivation: Task arithmetic suffers from cross-task interference causing representation drift and performance degradation. Existing regularization methods require external task data, which conflicts with modularity and privacy constraints.

Method: Frames regularization against representation drift as a curvature matrix approximation problem, using Kronecker-Factored Approximate Curvature (KFAC) to create a practical dataless regularizer.

Result: Achieves state-of-the-art results in task addition and negation, with constant complexity in number of tasks, robustness to task vector rescaling, and no need for held-out tuning.

Conclusion: Provides an effective dataless solution to representation drift in task arithmetic that maintains modularity and addresses privacy/data availability constraints.

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [104] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: Novel framework integrating formal verification with deep learning for image retrieval, enabling trustworthy, verifiable results for complex natural language queries.


<details>
  <summary>Details</summary>
Motivation: Current embedding-based retrieval models struggle with complex queries involving relationships, object compositions, and precise constraints (identities, counts, proportions), leading to unreliable results. There's a need for more transparent and accountable retrieval systems.

Method: Integrates formal verification into deep learning-based image retrieval through synergistic combination of graph-based verification methods and neural code generation. Grounds retrieval results in formal reasoning system, explicitly verifying each atomic truth in user queries against retrieved content.

Result: Framework supports open-vocabulary natural language queries while producing trustworthy, verifiable results. Can identify and mark which specific constraints are satisfied/unmet, offering transparent and accountable retrieval while boosting performance of popular embedding-based approaches.

Conclusion: The proposed framework moves beyond ambiguity and approximation of vector representations by incorporating formal verification, enabling more reliable retrieval for complex queries and providing greater transparency in the retrieval process.

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [105] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: MCVAE: A multimodal contrastive VAE with learned gating for robust survival prediction in NSCLC, handling severe missing data through stochastic masking and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Real-world clinical datasets for NSCLC often have incomplete multimodal data (WSI, transcriptomics, methylation), and existing models lack robustness when modalities are severely missing.

Method: Multimodal Contrastive VAE with modality-specific variational encoders, fusion bottleneck with learned gating, multi-task objective (survival + reconstruction loss), cross-modal contrastive loss, and stochastic modality masking during training.

Result: Outperforms SOTA models on TCGA-LUAD (n=475) and TCGA-LUSC (n=446) for DSS prediction, shows robustness to severe missingness, and reveals integration isn't always beneficial across modality subsets.

Conclusion: MCVAE effectively handles multimodal missing data for NSCLC survival prediction, but careful evaluation of modality contributions is needed as not all multimodal integration improves performance.

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [106] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: Proposes a Privacy-by-Design framework for AI applications targeting children, integrating principles from GDPR, PIPEDA, COPPA, UNCRC, and AADC across LLM lifecycle stages, with a case study on educational tutors.


<details>
  <summary>Details</summary>
Motivation: Growing privacy concerns for children using AI technologies, with challenges in implementing existing privacy regulations in practice despite legal requirements.

Method: Privacy-by-Design framework mapping regulatory principles (GDPR, PIPEDA, COPPA) to LLM lifecycle stages (data collection, model training, operational monitoring, validation) with operational controls from literature and child-centered design guidelines from UNCRC and AADC.

Result: Framework enables AI developers to reduce privacy risks while meeting legal standards through technical/organizational controls and age-appropriate design decisions throughout LLM lifecycle.

Conclusion: Proactive Privacy-by-Design approach with integrated regulatory principles and child-centered design can support development of AI applications for children that provide privacy protections and comply with legal requirements.

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [107] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec is a high-performance recommender systems framework that bridges academia-industry gap with backend-agnostic architecture, enabling seamless transition from local to distributed execution while promoting sustainability and preparing for Agentic AI.


<details>
  <summary>Details</summary>
Motivation: Current recommender systems research faces a fractured ecosystem where researchers must choose between easy in-memory experimentation and costly rewriting for distributed industrial engines, creating a barrier between academia and industry.

Method: Developed WarpRec with novel backend-agnostic architecture that includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering/splitting strategies, integrated with CodeCarbon for real-time energy tracking, and designed for Agentic AI evolution.

Result: Created a framework that eliminates the trade-off between ease of experimentation and industrial scalability, demonstrating that scalability doesn't require sacrificing scientific integrity or sustainability, while anticipating Agentic AI evolution.

Conclusion: WarpRec bridges the academia-industry gap and serves as architectural backbone for next-generation sustainable, agent-ready recommender systems, with code publicly available for community use.

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [108] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: Practical benchmarking framework for optimizing AI models on ARM Cortex processors (M0+, M4, M7) focusing on energy efficiency, accuracy, and resource utilization in embedded systems.


<details>
  <summary>Details</summary>
Motivation: Need for systematic evaluation of AI model performance on ARM Cortex processors to optimize energy efficiency, accuracy, and resource utilization in embedded systems, helping developers make informed decisions about processor-model combinations.

Method: Designed automated test bench for systematic evaluation across KPIs; used Pareto analysis to balance trade-offs between energy consumption and model accuracy; identified near-linear correlation between FLOPs and inference time as computational metric.

Result: M7 processor ideal for short inference cycles; M4 offers better energy efficiency for longer inference tasks; M0+ suitable for simpler tasks despite lower efficiency for complex models; established FLOPs-inference time correlation for computational demand estimation.

Conclusion: Provides practical framework and insights for developers to design energy-efficient AI systems on ARM Cortex processors that balance performance requirements with sustainability, with specific processor recommendations for different inference scenarios.

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [109] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG integrates knowledge graphs with retrieval-augmented generation to enhance LLMs for telecom tasks, reducing hallucinations and improving accuracy by 14.3% over standard RAG.


<details>
  <summary>Details</summary>
Motivation: General-domain LLMs struggle in telecom due to domain complexity, evolving standards, and specialized terminology, leading to hallucinations and reduced utility in telecom operations.

Method: KG-RAG framework combines knowledge graphs (structured representation of telecom standards/technical docs) with retrieval-augmented generation to dynamically retrieve relevant facts and ground model outputs.

Result: KG-RAG outperforms both LLM-only and standard RAG baselines, achieving average accuracy improvements of 14.3% over RAG and 21.6% over LLM-only models on benchmark datasets.

Conclusion: KG-RAG effectively produces accurate, reliable, and explainable outputs in complex telecom scenarios by integrating structured domain knowledge with dynamic retrieval mechanisms.

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [110] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: The paper introduces two new metrics (reusability and verifiability) to evaluate Chain-of-Thought reasoning quality in multi-agent systems, showing these don't correlate with standard accuracy and revealing limitations of current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current CoT evaluation focuses only on target task accuracy, which fails to assess the quality or utility of the reasoning process itself in multi-agent IR pipelines.

Method: Introduced reusability (how easily Executor can reuse Thinker's CoT) and verifiability (how frequently Executor matches Thinker's answer using CoT) metrics. Used Thinker-Executor framework to decouple CoT generation from execution, evaluating four Thinker models against ten Executor models across five benchmarks.

Result: Reusability and verifiability do not correlate with standard accuracy, exposing blind spots in current accuracy-based leaderboards. Surprisingly, CoTs from specialized reasoning models are not consistently more reusable/verifiable than those from general-purpose LLMs like Llama and Gemma.

Conclusion: The paper demonstrates the need for better evaluation metrics beyond accuracy to assess reasoning quality in multi-agent systems, revealing that specialized models don't necessarily produce more reusable/verifiable reasoning than general-purpose models.

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [111] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong is an open-source LLM agent trained for extremely long-horizon tasks using trajectory-splitting SFT and progressive RL training, achieving state-of-the-art performance on research paper benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of training LLM agents to solve extremely long-horizon tasks, which require extended reasoning and planning capabilities beyond typical short-term interactions.

Method: Two-stage approach: 1) Cold-start via trajectory-splitting SFT that preserves early context while progressively truncating later context with overlap between sub-trajectories, 2) Progressive RL training with multiple stages of progressively extended timeouts. Uses Research-Factory pipeline to generate high-quality training data from research papers with evaluation rubrics.

Result: KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, with performance improvements generalizing to other coding benchmarks like SWE-bench Verified and MLE-bench.

Conclusion: The proposed trajectory-splitting SFT and progressive RL training enable effective training of LLM agents for extremely long-horizon tasks, with KLong demonstrating superior performance and generalization capabilities compared to larger models.

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [112] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: ODESteer: A unified ODE-based framework for activation steering in LLM alignment that uses barrier functions for multi-step adaptive steering, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current activation steering methods lack a unified theoretical framework and rely on one-step steering that fails to capture complex activation distribution patterns, limiting their effectiveness in LLM alignment.

Method: Proposes ODESteer, an ODE-based framework that interprets activation addition as first-order ODE approximation, uses barrier functions (log-density ratio between positive/negative activations) to identify steering directions, and enables multi-step adaptive steering.

Result: Achieves consistent empirical improvements: 5.7% improvement over TruthfulQA, 2.5% over UltraFeedback, and 2.4% over RealToxicityPrompts compared to state-of-the-art activation steering methods.

Conclusion: Establishes a principled theoretical foundation for activation steering via ODEs and validates it empirically with ODESteer, providing a unified framework for LLM alignment with superior performance.

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [113] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: Hybrid FL-enabled ensemble approach combining SWIN Transformer and CNN models for secure, distributed lung disease diagnosis (COVID-19/Pneumonia) from X-rays using federated learning.


<details>
  <summary>Details</summary>
Motivation: Leverage computational advancements and AI to create secure, distributed medical diagnosis systems that protect patient data privacy while improving disease detection accuracy, addressing the need for collaborative healthcare solutions during pandemics.

Method: Hybrid ensemble model combining SWIN Transformer with CNN models (DenseNet201, Inception V3, VGG19) using TensorFlow/Keras, integrated with federated learning for distributed, secure data processing and real-time continual learning.

Result: Proposed system enables accurate COVID-19 and Pneumonia detection from X-ray reports while maintaining data security through federated learning, providing a reliable diagnostic tool for physicians.

Conclusion: Federated learning-based hybrid AI models can significantly improve disease diagnosis accuracy and severity prediction while ensuring data security and authenticity, offering a collaborative solution for global healthcare challenges.

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [114] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: AI GameStore: A platform using LLMs to generate diverse human games for evaluating AI's general intelligence through gameplay performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks are too narrow and quickly saturated; need better ways to evaluate human-like general intelligence across the full spectrum of human capabilities.

Method: Created AI GameStore platform using LLMs with human-in-the-loop to synthesize new human games by sourcing/adapting game environments from popular platforms like Apple App Store and Steam.

Result: Generated 100 games; tested 7 frontier VLMs - best models achieved <10% of human average score on most games, struggling especially with world-model learning, memory and planning.

Conclusion: AI GameStore shows promise as practical way to measure and drive progress toward human-like general intelligence through comprehensive game evaluation.

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [115] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT introduces a hierarchical discrete diffusion model for molecular graph generation that achieves near-perfect chemical validity and state-of-the-art performance, surpassing 1D baselines.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models for molecular generation suffer from low chemical validity and struggle to meet desired properties compared to 1D modeling approaches.

Method: MolHIT uses a Hierarchical Discrete Diffusion Model that generalizes discrete diffusion to additional categories encoding chemical priors, along with decoupled atom encoding that splits atom types according to their chemical roles.

Result: Achieves new state-of-the-art performance on MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. Also demonstrates strong performance in downstream tasks like multi-property guided generation and scaffold extension.

Conclusion: MolHIT overcomes long-standing performance limitations in molecular graph generation, establishing graph diffusion models as competitive with 1D approaches while maintaining chemical validity.

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [116] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics is a multi-agent framework that autonomously designs transparent numerical solvers for PDEs from natural language descriptions, achieving competitive accuracy while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solver design requires substantial mathematical expertise and manual tuning, while neural network approaches are computationally expensive and lack interpretability. There's a need for an accessible, automated approach that generates transparent solvers.

Method: Multi-agent framework with coarse-to-fine execution strategy and residual-based self-verification mechanism. The system autonomously designs, implements, debugs, and verifies numerical solvers from natural language descriptions, generating transparent solvers grounded in classical numerical analysis.

Result: Experiments on 24 canonical and real-world PDE problems show competitive or superior accuracy compared to existing neural and LLM-based baselines. The framework correctly selects numerical schemes based on PDE structural properties.

Conclusion: AutoNumerics demonstrates viability as an accessible paradigm for automated PDE solving, offering transparent, interpretable solvers while maintaining competitive performance.

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [117] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026 is a CLEF lab for extracting person-place relations from historical texts, extending previous campaigns with semantic relation extraction tasks across languages and time periods.


<details>
  <summary>Details</summary>
Motivation: To advance historical text processing by moving beyond named entity recognition to semantic relation extraction, enabling better understanding of person-place associations in historical contexts for digital humanities applications.

Method: Introduces a three-fold evaluation profile assessing accuracy, computational efficiency, and domain generalization for systems classifying two relation types ($at$ and $isAt$) requiring temporal and geographical reasoning.

Result: The lab builds on HIPE-2020 and HIPE-2022 campaigns, extending them toward semantic relation extraction with multilingual, multi-temporal evaluation framework for person-place associations.

Conclusion: HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis by linking relation extraction to large-scale historical data processing in digital humanities.

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [118] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: Autonomous forklift control method for unloading pallets on inclined surfaces using ICP algorithm to track pallet-fork alignment and prevent dragging during withdrawal.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous forklifts to unload pallets on inclined surfaces (like truck beds) without dragging the pallets during fork withdrawal, which is a challenging real-world logistics problem.

Method: Uses Iterative Closest Point (ICP) algorithm on point clouds from the pallet's upper region to track relative position and attitude angle between pallet and fork in real-time. Aligns fork parallel to target surface, then withdraws along the tilt direction.

Result: Method effectiveness verified through dynamic simulations and real forklift experiments replicating unloading operations onto inclined truck beds.

Conclusion: Proposed ICP-based control method successfully enables autonomous forklifts to unload pallets on inclined surfaces without dragging, demonstrating practical feasibility for real-world logistics applications.

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [119] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: Smooth trajectory generation method for 4-DOF parallel kinematic milling robot using B-spline and quaternion interpolation with Bezier curve synchronization and minimum jerk optimization.


<details>
  <summary>Details</summary>
Motivation: Need for smooth trajectory generation in parallel kinematic milling robots that can handle decoupled position and orientation data while ensuring spatial/temporal constraints and avoiding gimbal lock issues.

Method: Integrates B-spline and quaternion interpolation with smooth piece-wise Bezier curves for synchronization. Uses unit quaternions for orientation interpolation, modifier polynomials for position, and two-stage optimization (task space then joint space) with minimum jerk, time-optimal Bezier curves.

Result: Experimental results show enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

Conclusion: The proposed method provides an effective solution for smooth trajectory generation in parallel kinematic milling robots with improved performance and computational efficiency suitable for low-cost microcontrollers.

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [120] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: RRT$^η$ integrates AGM robustness measure with sampling-based planning for STL specifications, improving efficiency over traditional min-max approaches in multi-constraint robotic scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based motion planning with STL uses min-max robustness measures that focus only on critical time points, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.

Method: Proposes RRT$^η$ framework integrating Arithmetic-Geometric Mean (AGM) robustness measure with three key contributions: AGM robustness interval semantics for partial trajectories, efficient incremental monitoring algorithm, and enhanced Direction of Increasing Satisfaction vectors using Fulfillment Priority Logic.

Result: The framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining probabilistic completeness and asymptotic optimality of RRT$^\ast$. Validated on three robotic systems showing superior performance in multi-constraint scenarios.

Conclusion: RRT$^η$ provides an effective sampling-based planning framework that overcomes limitations of traditional STL robustness measures, enabling efficient exploration of complex spatiotemporal constraints in robotic systems.

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [121] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: Sound of Touch uses vibrating strings and acoustic sensing for scalable tactile perception on large robot surfaces, enabling contact localization, force estimation, and slip detection.


<details>
  <summary>Details</summary>
Motivation: Current tactile sensing approaches face scalability challenges - dense sensor arrays increase wiring, cost, and fragility, while alternatives provide limited coverage or miss fast interaction dynamics.

Method: Uses vibrating tensioned strings as sensing elements with electromagnetic excitation and contact microphones to observe spectral changes from contact. Includes physics-based string-vibration simulator and real-time inference pipeline.

Result: Achieves millimeter-scale localization, reliable force estimation, and real-time slip detection through experiments.

Conclusion: Provides a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces with physics-grounded simulation and real-time inference capabilities.

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [122] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: The paper proposes "Walk-Along with Robots" (WawR) methodology for studying autonomous robots in public spaces, addressing limitations of existing HRI field studies.


<details>
  <summary>Details</summary>
Motivation: Existing HRI field studies are limited to controlled experiments with prototype robots or structured methods like Wizard of Oz, but autonomous robots in public spaces (like delivery robots) operate beyond researcher control in dynamic environments, requiring new research approaches.

Method: Proposes Walk-Along with Robots (WawR) methodology inspired by public realm ethnography from urban studies, geography, and sociology. Outlines key features, application steps, unique insights, and evaluation methods.

Result: Presents a new methodological framework for studying autonomous robots in public spaces that goes beyond controlled experiments to capture real-world interactions in dynamic environments.

Conclusion: The WawR methodology addresses the challenge of studying autonomous robots operating beyond researcher control in public spaces, and the paper aims to stimulate further discussion on research methodologies for this emerging field.

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [123] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: SimToolReal: A sim-to-real RL approach for general dexterous tool manipulation using procedurally generated tool-like objects in simulation, achieving strong zero-shot performance on real-world tools.


<details>
  <summary>Details</summary>
Motivation: Tool manipulation is challenging for robots due to difficulties in grasping thin objects, in-hand rotations, and forceful interactions. Collecting teleoperation data is hard, and existing sim-to-real RL methods require substantial engineering effort for each specific object and task.

Method: Procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses, enabling generalization without object/task-specific training.

Result: Outperforms prior retargeting and fixed-grasp methods by 37%, matches performance of specialist RL policies trained on specific objects/tasks, and generalizes across 24 tasks, 12 object instances, and 6 tool categories with strong zero-shot performance over 120 real-world rollouts.

Conclusion: SimToolReal demonstrates that training on procedurally generated tool-like objects enables general dexterous tool manipulation capabilities that transfer effectively to real-world tools without task-specific training, advancing sim-to-real RL for tool manipulation.

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [124] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT extends Boreas dataset with 643km of diverse driving routes, providing multi-modal sensor data and centimeter-level ground truth to benchmark odometry and localization algorithms in challenging real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving datasets often lack diversity in driving environments, causing state-of-the-art algorithms to overfit to simple conditions and degrade in more challenging real-world scenarios.

Method: Collected 60 sequences over 9 real-world routes (643km total) using multi-modal sensors including camera, Doppler radar, 128-channel lidar, FMCW Doppler lidar, IMU, and wheel encoder, with centimeter-level ground truth from GNSS-INS.

Result: Benchmark results show many state-of-the-art odometry and localization algorithms significantly degrade on Boreas-RT's challenging routes, revealing overfitting to simpler environments in existing datasets.

Conclusion: Boreas-RT provides a unified, diverse dataset for evaluating multi-modal autonomous driving algorithms, with public development kit and leaderboard to advance research in challenging real-world conditions.

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [125] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: MALLVi is a multi-agent LLM/VLM framework for closed-loop robotic manipulation that coordinates specialized agents for perception, reasoning, and error recovery to improve zero-shot task success.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based task planning approaches are fragile in dynamic environments because they operate open-loop without robust environmental feedback, relying on specialized models, fine-tuning, or prompt tuning.

Method: MALLVi coordinates specialized agents (Decomposer, Localizer, Thinker, Reflector, optional Descriptor) that handle perception, localization, reasoning, and planning. After action execution, a VLM evaluates environmental feedback to decide next steps, with the Reflector enabling targeted error recovery by reactivating only relevant agents.

Result: Experiments in simulation and real-world settings show that iterative closed-loop multi-agent coordination improves generalization and increases success rates in zero-shot manipulation tasks.

Conclusion: MALLVi demonstrates that multi-agent coordination with closed-loop feedback enables more robust robotic manipulation in dynamic environments compared to open-loop approaches, with targeted error recovery avoiding full replanning.

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [126] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: A graph-based approach for learning long-horizon manipulation tasks by inferring what to achieve rather than how, using manipulation graphs to represent evolving scene states across task phases.


<details>
  <summary>Details</summary>
Motivation: Learning long-horizon manipulation tasks efficiently is challenging. Instead of directly learning actions, the paper focuses on inferring what the robot should achieve in the task rather than how to do it, aiming for more generalizable task representations.

Method: Proposes demonstration segmentation and pooling to extract manipulation graphs representing object relationships across task phases. Uses graphical object relationships to represent evolving scene states, captures complete object interactions from start to end of manipulation, and employs object matching with pre-trained visual features for robustness with multiple demonstrations.

Result: Evaluates demonstration segmentation accuracy and utility of learning from multiple demonstrations for finding minimal task models. Successfully deploys fitted models in simulation and on real robots, showing reliable execution across environments.

Conclusion: The graph-based task representation approach effectively captures long-horizon manipulation tasks, supports learning from multiple demonstrations, and enables reliable task execution across different environments.

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [127] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: A physics-based benchmark evaluates 3D reconstruction quality and 6D pose estimation by measuring their functional impact on robotic grasping performance, revealing that reconstruction artifacts reduce grasp candidates but have minimal effect on success given accurate pose.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods produce visually impressive meshes, but standard geometric evaluations don't reflect how reconstruction quality affects downstream robotic manipulation tasks like grasping.

Method: Created a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models by generating grasps on reconstructed meshes and executing them on ground-truth models in simulation, assessing combined impact of pose error, grasp robustness, and geometric inaccuracies.

Result: Reconstruction artifacts significantly decrease the number of grasp pose candidates but have negligible effect on grasping performance when pose is accurately estimated. Spatial error dominates grasp success relationship, and simple translation error provides insight into grasping success for symmetric objects.

Conclusion: The benchmark provides insights into how perception systems relate to object manipulation, showing that accurate pose estimation is more critical than perfect geometric reconstruction for successful robotic grasping.

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [128] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: CFM-based framework maps rigid gripper grasp poses to soft Fin-ray gripper poses, achieving 34-46% success rates vs 6-25% baseline.


<details>
  <summary>Details</summary>
Motivation: Existing grasp synthesis methods (like Anygrasp) are designed for rigid parallel grippers and fail to capture the unique compliant behaviors of soft grippers, creating a representation gap that leads to data-intensive and inaccurate models for soft robotic grasping.

Method: Uses Conditional Flow Matching (CFM) generative model to learn transformation from rigid to soft gripper poses. Includes data collection pipeline for paired rigid-soft grasp poses, with U-Net autoencoder conditioning CFM on object geometry from depth images to learn continuous mapping from initial Anygrasp poses to stable Fin-ray gripper poses.

Result: CFM-generated poses achieve 34% success rate for seen objects and 46% for unseen objects with soft gripper, compared to 6% and 25% baseline rigid poses. Significant improvements for cylindrical (50-100% success) and spherical objects (25-31% success), with successful generalization to unseen objects.

Conclusion: CFM is a data-efficient and effective method for transferring grasp strategies between rigid and soft grippers, offering a scalable methodology applicable to other soft robotic systems.

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [129] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: AR-based teleoperation framework for hybrid rigid-soft robots using simulation overlay and parameter identification for consistent virtual-physical behavior


<details>
  <summary>Details</summary>
Motivation: Hybrid rigid-soft robots offer precision and compliance for grasping in unstructured environments, but coordination is challenging due to modeling, perception, and cross-domain kinematics difficulties

Method: AR-based physical human-robot interaction framework using AR headset to overlay simulated robot model on real system, with real-to-simulation parameter identification pipeline leveraging soft robot's geometric properties

Result: Enables direct teleoperation of hybrid rigid-soft robot for simple reaching and grasping tasks with consistent behavior between virtual and physical robots

Conclusion: The AR framework with parameter identification enables accurate modeling and control of hybrid rigid-soft robots, facilitating teleoperation for grasping tasks in unstructured environments

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [130] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: Coordinate-free inverse flight dynamics on SO(3) for fixed-wing aircraft with closed-form trajectory-to-input mapping and application to tethered flight analysis.


<details>
  <summary>Details</summary>
Motivation: Bridge inverse simulation in aeronautics with geometric modeling in robotics, providing rigorous building blocks for trajectory design and feasibility checks.

Method: Coordinate-free formulation on SO(3) with translational force balance in world frame and rotational dynamics in body frame; aerodynamic directions defined geometrically; enforcing coordinated flight to derive closed-form trajectory-to-input map.

Result: Derived closed-form map yielding attitude, angular velocity, thrust-angle-of-attack pair; recovered aerodynamic moment coefficients; analytic expressions for required bank angle in tethered flight; identified zero-bank locus where tether tension balances centrifugal effects; found minimal-thrust angle of attack under simple lift/drag law.

Conclusion: The framework provides rigorous geometric building blocks for trajectory design and feasibility analysis, connecting robotics and aeronautics through coordinate-free inverse flight dynamics.

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [131] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: Framework for real-time aerial manipulation of extensible cables using reduced-order modeling and nonlinear MPC for oscillation control and hybrid transitions.


<details>
  <summary>Details</summary>
Motivation: Need for real-time control of UAVs carrying suspended flexible cables, requiring dynamics-aware manipulation that can handle complex cable deformations and hybrid transitions like payload attachment/detachment.

Method: Combines high-fidelity PDE modeling with reduced-order representation: finite-difference discretization of PDEs, proper orthogonal decomposition for ROM extraction, and nonlinear model predictive control for stabilization.

Result: ROM demonstrates stability, efficiency, and robustness; controller effectively regulates cable dynamics across various conditions; framework enables trajectory planning in constrained environments.

Conclusion: Proposed framework enables real-time, dynamics-aware control of UAVs with suspended flexible cables, combining accurate modeling with computationally efficient control for practical aerial manipulation tasks.

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [132] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: A multi-session mapping framework that uses topology-informed decision-making to selectively trigger mapping and loop closing for better global consistency in repeated environment visits.


<details>
  <summary>Details</summary>
Motivation: Operating in previously visited environments is crucial for autonomous systems (driving, surveying, robotics), but repeated exposure to the same areas poses significant challenges for mapping and localization. Current approaches greedily run full SLAM sessions and try to find correspondences between resulting maps, which is inefficient.

Method: A novel multi-session framework based on map-based localization with topology-informed, uncertainty-aware decision-making. The system analyzes pose-graph structure to detect low-connectivity regions, then selectively triggers mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into existing models.

Result: The method reduces accumulated error and enhances global consistency. It was validated on overlapping sequences from datasets and demonstrated effectiveness in a real-world mine-like environment.

Conclusion: The proposed framework provides an effective solution for multi-session mapping in repeated environment visits, improving over greedy full-SLAM approaches by using intelligent, selective triggering of mapping and loop closing based on pose-graph analysis.

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [133] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: FRAPPE introduces a two-stage fine-tuning method for VLA models to improve world modeling by predicting future latent representations and aligning them with multiple visual foundation models, reducing error accumulation and enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: Current VLA world modeling approaches have two main issues: 1) training objectives force over-emphasis on pixel-level reconstruction, constraining semantic learning and generalization, and 2) reliance on predicted future observations during inference leads to error accumulation.

Method: FRAPPE uses a two-stage fine-tuning strategy: mid-training phase learns to predict latent representations of future observations; post-training phase expands computational workload in parallel and aligns representations simultaneously with multiple visual foundation models.

Result: FRAPPE outperforms state-of-the-art approaches on the RoboTwin benchmark and real-world tasks, showing strong generalization in long-horizon and unseen scenarios while improving fine-tuning efficiency and reducing dependence on action-annotated data.

Conclusion: FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies by addressing limitations of current world modeling approaches through representation prediction and alignment.

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [134] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: Purely proprioceptive state estimator for legged robots using only IMU and motor measurements to estimate body pose/velocity, treating contacting legs as kinematic anchors with footfall constraints to suppress drift.


<details>
  <summary>Details</summary>
Motivation: Reliable odometry for legged robots without cameras or LiDAR is challenging due to IMU drift and noisy joint velocity sensing. Existing proprioceptive methods struggle with long-term drift and elevation errors during extended traversal.

Method: Treats each contacting leg as kinematic anchor: joint-torque-based foot wrench estimation selects reliable contacts, footfall positions provide world-frame constraints. Uses height clustering and time-decay correction for elevation drift. Applies inverse-kinematics cubature Kalman filter for foot velocity. Mitigates yaw drift through multi-contact geometric consistency.

Result: Evaluated on four quadruped platforms with closed-loop trajectories. Achieved 0.1638m error on 200m horizontal loop and 0.219m error on 15m vertical loop for point-foot robot. Wheel-legged robot achieved 0.2264m horizontal and 0.199m vertical errors. Larger 700m loop had 7.68m error. Unitree Go2 EDU closed 120m loop with 2.2138m error.

Conclusion: The proposed purely proprioceptive estimator effectively suppresses drift for legged robots without external sensors, demonstrating robust performance across different robot types and terrains with minimal elevation drift.

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [135] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: Decentralized, agent-agnostic safety control framework for human-robot collaboration using Virtual Model Control that eliminates deadlocks and maintains safe separation distances.


<details>
  <summary>Details</summary>
Motivation: Need for safe, scalable human-robot collaboration systems that can prevent robots from getting stuck (deadlocks) while maintaining intuitive behavior and safety across varying team sizes.

Method: Virtual Model Control framework where humans and robots interact in virtual-component-shaped workspace with springs and dampers instead of explicit trajectory planning. Uses decentralized force-based stall detection and negotiation to resolve deadlocks.

Result: Reduced deadlock probability from up to 61.2% to zero in block placement tasks. Demonstrated safe collaboration with up to 2 robots + 2 humans (experiments) and 4 robots (simulation) while maintaining ~20cm inter-agent separation.

Conclusion: The framework enables intuitive robot behavior shaping through control parameter adjustments, achieves deadlock-free operation across team sizes, and scales without structural changes due to distributed implementation.

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [136] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: 3D printed soft optical sensing (SOLen) uses printed lenses in waveguides for deformation sensing, enabling single-material fabrication with improved optical properties.


<details>
  <summary>Details</summary>
Motivation: There's a need for sensing solutions compatible with single-material, one-step additive manufacturing of soft robots, as current optical sensors suffer from uncontrolled light propagation issues and typically require multimaterial interfaces.

Method: Developed SOLen approach with printed lens in front of emitter within Y-shaped waveguide; modified acrylate polyurethane resin with lauryl acrylate for better compliance/transmittance; used single-layer optical characterization to derive optical properties; simulated lens design based on measured refractive index.

Result: Successfully printed lenses with sub-millimeter fidelity; rotational tests showed reproducible branch-selective signal switching over multiple cycles; established material-to-optics workflow for soft optical sensors.

Conclusion: The SOLen approach enables transferable material-to-optics workflow for soft optical sensors with lenses, offering new functionalities for next-generation soft robots through single-material fabrication.

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [137] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: A cost-effective hardware solution for rainy conditions improves camera-based perception for automated vehicles, increasing pedestrian detection accuracy from 8.3% to 41.6% while supporting sustainability goals.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for adverse weather conditions in automated vehicles are limited - hydrophilic/hydrophobic lenses and sprays provide only partial mitigation, while industrial protection systems are costly and not scalable for automotive deployment. There's a need for cost-effective hardware solutions that work with multiple cameras simultaneously.

Method: The paper presents a cost-effective hardware solution designed for rainy conditions that is compatible with multiple cameras simultaneously. The system works with existing camera-based sensing platforms without requiring additional high-cost sensors or hardware replacements.

Result: The proposed system significantly improved pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6% in rainy conditions. The solution extends operational reliability of automated vehicles while being cost-effective and scalable.

Conclusion: This hardware solution addresses the limitations of existing approaches by providing a cost-effective, scalable system that improves perception in rainy conditions. Beyond technical benefits, it supports sustainability goals by reducing resource consumption, enabling modular upgrades, and promoting more efficient deployment of automated vehicle technologies, particularly in challenging weather where system failures would otherwise increase emissions.

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [138] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: OS-ERA introduces optical waveguide sensors to Electro-Ribbon Actuators, enabling high-fidelity bending state classification with voltage- and speed-invariance, solving previous capacitive sensing limitations.


<details>
  <summary>Details</summary>
Motivation: Electro-Ribbon Actuators (ERAs) have ultrahigh displacement and fast movement but rely on capacitive sensors with limited precision, hindering accurate control. There's a need for reliable proprioceptive sensing without affecting actuation performance.

Method: Design and embed two soft optical waveguide sensors into ERA to analyze complex curvature. Train a classifier to map sensing signals to distinguish eight bending states. Validate model on held-out trials and compare against training signal trajectories.

Result: Sensing output signals follow training manifold, predicted sequence mirrors real performance with repeatability. Classification remains accurate despite train-test mismatches in actuation speed, demonstrating voltage- and speed-invariance. OS-ERA classifies bending states with high fidelity.

Conclusion: OS-ERA solves the longstanding bottleneck of ERA sensing limitations, enabling steps toward closed-loop control through reliable proprioceptive information without affecting actuation performance.

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [139] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: Above-knee placement of powered prosthetic knee powertrain shows functional feasibility with improved walking speed and cadence compared to below-knee placement, suggesting mass distribution optimization can preserve powered assistance benefits.


<details>
  <summary>Details</summary>
Motivation: Powered prosthetic knees help restore mobility but added mass from powered components can diminish benefits by negatively affecting gait mechanics and increasing metabolic cost. Optimizing mass distribution rather than simply minimizing total mass may provide more effective solutions.

Method: Exploratory study evaluating feasibility of above-knee powertrain placement for powered prosthetic knee in a small cohort. Compared above-knee vs below-knee configurations, measuring walking speed, cadence, gait symmetry, knee range of motion, and peak velocity. Additional testing on ramps and stairs to assess control strategy robustness.

Result: Above-knee configuration showed improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures showed similar knee range of motion and peak velocity across configurations. Control strategy proved robust across multiple locomotion tasks (ramps, stairs).

Conclusion: Above-knee placement is functionally feasible and careful mass distribution can preserve benefits of powered assistance while mitigating adverse effects of added weight. Further studies needed to confirm trends and guide design/clinical recommendations.

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [140] [RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments](https://arxiv.org/abs/2602.17515)
*Ziyi Zong,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: RA-Nav is a risk-aware aerial navigation framework that uses semantic segmentation to classify obstacles and predict their movement risks, enabling safer path planning when static obstacles suddenly start moving.


<details>
  <summary>Details</summary>
Motivation: Existing aerial navigation systems fail to adapt when static obstacles suddenly move, creating safety risks. The paper aims to address this limitation by integrating environmental semantic awareness to estimate potential risks from suddenly moving obstacles.

Method: Uses lightweight multi-scale semantic segmentation to identify obstacle categories in real-time, classifies obstacles into stationary, temporarily static, and dynamic types, designs risk estimation functions for each type, constructs a local risk map, implements risk-informed path search algorithm, and applies trajectory optimization.

Result: RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios, with effectiveness validated in simulations using real-world data.

Conclusion: The proposed risk-aware navigation framework successfully addresses the challenge of suddenly moving obstacles by integrating semantic awareness and risk prediction, enabling safer and more adaptive aerial robot navigation.

Abstract: Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.

</details>


### [141] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: IRIS is a low-cost, 3D-printed 6-DOF robotic camera system that uses imitation learning to autonomously execute cinematic camera motions from human demonstrations, eliminating manual programming.


<details>
  <summary>Details</summary>
Motivation: Industrial robotic camera systems are expensive and complex to operate, limiting their adoption for cinematic applications. There's a need for affordable, accessible systems that can autonomously execute smooth, object-aware camera motions without requiring explicit geometric programming.

Method: IRIS combines a lightweight, fully 3D-printed 6-DOF manipulator hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns camera trajectories directly from human demonstrations.

Result: The complete platform costs under $1,000 USD, supports 1.5 kg payload, achieves ~1 mm repeatability, demonstrates accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

Conclusion: IRIS successfully demonstrates that low-cost, learning-driven robotic camera systems can autonomously execute professional-quality cinematic motions, making robotic cinematography more accessible and reducing the need for manual programming expertise.

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [142] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: First dataset for gesture-based UGV control by First Responders, featuring 12 commands, 3312 RGBD pairs from 2 viewpoints and 7 distances, with baseline experiments and public availability.


<details>
  <summary>Details</summary>
Motivation: Increasing disaster intensity and frequency make First Responders' work more difficult, requiring AI and robotics solutions to facilitate operations and compensate for these challenges.

Method: Created FR-GESTURE dataset with 12 gesture commands inspired by existing FR gestures and tactical hand signals, refined with FR feedback. Collected 3312 RGBD pairs from 2 viewpoints and 7 distances. Defined evaluation protocols and performed baseline experiments.

Result: First dataset specifically designed for gesture-based UGV guidance by First Responders, publicly available at https://doi.org/10.5281/zenodo.18131333, with baseline experiments provided for future improvement.

Conclusion: The FR-GESTURE dataset addresses the need for gesture-based UGV control systems for First Responders, providing a foundation for future research and development in this critical domain.

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [143] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: Proposes hybrid zonotopes + ADMM heuristic for efficient hybrid system motion planning with lower memory complexity and better convergence than existing MIP methods.


<details>
  <summary>Details</summary>
Motivation: Embedded optimization-based planning for hybrid systems is challenging due to computationally intensive mixed-integer programming that's sensitive to numerical formulation.

Method: Pairs hybrid zonotopes (advanced set representation) with new ADMM mixed-integer programming heuristic for PWA system reachability analysis and optimal planning formulation.

Result: Sets have lower memory complexity and tighter convex relaxations than existing techniques; ADMM heuristic achieves improved convergence rates compared to state-of-the-art MIP heuristics.

Conclusion: Methods successfully applied to combined behavior and motion planning for autonomous driving on embedded hardware, demonstrating practical viability.

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [144] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow is an unsupervised safety-critical anomaly detection framework for autonomous vehicles that uses optimal transport conditional flow matching on a low-rank spectral manifold to identify rare, high-risk driving scenarios that traditional safety filters miss.


<details>
  <summary>Details</summary>
Motivation: Safety validation for Level 4 autonomous vehicles is bottlenecked by the inability to scale detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. Current approaches struggle with multi-modal ambiguity at complex junctions and lack mathematical rigor for statistical safety validation.

Method: Uses Optimal Transport Conditional Flow Matching (OT-CFM) to characterize continuous probability density of expert human driving behavior. Constrains generative process to low-rank spectral manifold via PCA bottleneck for kinematic smoothness. Employs Early Fusion Transformer encoder with lane-aware goal conditioning and direct skip-connection to flow head. Introduces kinematic complexity weighting scheme prioritizing high-energy maneuvers during simulation-free training.

Result: Achieves AUC-ROC of 0.766 on Waymo Open Motion Dataset against heuristic golden set of safety-critical events. Identifies fundamental distinction between kinematic danger and semantic non-compliance. Surfaces out-of-distribution behaviors (lane-boundary violations, non-normative junction maneuvers) that traditional safety filters overlook.

Conclusion: Provides mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for safe deployment of autonomous fleets. Reveals critical predictability gap by detecting behaviors that traditional approaches miss, advancing safety validation beyond rule-based heuristics.

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>


### [145] [Graph Neural Model Predictive Control for High-Dimensional Systems](https://arxiv.org/abs/2602.17601)
*Patrick Benito Eberhard,Luis Pabon,Daniele Gammelli,Hugo Buurmeijer,Amon Lahr,Mark Leone,Andrea Carron,Marco Pavone*

Main category: cs.RO

TL;DR: GNN-based dynamics modeling combined with structure-exploiting MPC enables real-time control of high-dimensional soft robots with linear scaling complexity and GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: High-dimensional systems like soft robots require models that capture complex dynamics while remaining computationally tractable for real-time control applications.

Method: Integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control, representing systems as graphs with localized interactions, using tailored condensing algorithm to eliminate state variables, and leveraging GPU parallelization.

Result: Scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, achieves real-time reference tracking on hardware with sub-centimeter accuracy (outperforming baselines by 63.6%), and demonstrates effective full-body obstacle avoidance.

Conclusion: The framework enables real-time control of high-dimensional soft robotic systems by combining GNN-based modeling with efficient MPC, achieving both computational efficiency and accurate performance in simulation and physical experiments.

Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.

</details>
