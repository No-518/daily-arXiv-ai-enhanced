<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.RO](#cs.RO) [Total: 27]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR is a semantic decoupling framework that protects face privacy by separating identity from appearance, generating adversarial faces that look different to humans but maintain machine identity, with password-based restoration for authorized access.


<details>
  <summary>Details</summary>
Motivation: As facial recognition becomes deeply integrated into online services, there's a critical need to decouple identity information from visual representations during image storage/transmission to protect privacy while maintaining functionality.

Method: SIDeR decomposes facial images into machine-recognizable identity features and visually perceptible appearance components, uses semantic-guided recomposition in diffusion model latent space, incorporates momentum-driven unrestricted perturbation optimization and semantic-visual balancing to generate diverse adversarial faces.

Result: Achieves 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality on CelebA-HQ and FFHQ datasets.

Conclusion: SIDeR effectively protects face privacy by generating visually anonymous adversarial faces that maintain machine identity consistency, with password-based restoration capability for authorized access, demonstrating superior performance over existing methods.

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [2] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack is a plug-and-play graph-theoretic loss function that enhances multi-object tracking by optimizing tracking-specific objectives through differentiable learning, improving identity preservation and spatiotemporal consistency without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based MOT methods require redesigning tracking architectures. There's a need for a universal training objective that can enhance tracking performance across different models without modifying their architectures, addressing the challenge of integrating detection accuracy, identity preservation, and spatiotemporal consistency.

Method: UniTrack uses differentiable graph representation learning to create a unified loss function that optimizes tracking-specific objectives. It integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss that can be plugged into existing MOT systems without architectural modifications.

Result: UniTrack demonstrates consistent improvements across diverse tracking models (Trackformer, MOTR, FairMOT, ByteTrack, GTR, MOTE) and multiple benchmarks, achieving up to 53% reduction in identity switches, 12% IDF1 improvements, and GTR achieving 9.7% MOTA gains on SportsMOT.

Conclusion: UniTrack provides an effective plug-and-play solution for enhancing multi-object tracking performance through unified differentiable learning, offering significant improvements across various models and benchmarks without requiring architectural changes to existing systems.

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [3] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: VISTA improves Vision-Language-Action models by strengthening visual conditioning through preference optimization and latent-space distillation, reducing vision-action misalignment without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Extending pretrained VLMs to action space causes vision-action misalignment where action predictions show weak dependence on current visual state, leading to unreliable outputs. Successful rollouts consistently show stronger visual dependence than failed ones.

Method: Two-stage training: 1) Align action prediction with visual input via preference optimization on track-following surrogate task, 2) Transfer enhanced alignment to instruction-following tasks through latent-space distillation during supervised finetuning.

Result: Improves both visual conditioning and task performance for discrete OpenVLA, and yields consistent gains when extended to continuous OpenVLA-OFT setting, all without architectural modifications or additional data collection.

Conclusion: Explicitly strengthening visual conditioning in VLA models through the proposed training framework effectively addresses vision-action misalignment and improves model reliability and performance.

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [4] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: Survey paper exploring various strategies for accurate food portion estimation from images, focusing on overcoming 2D-to-3D size estimation challenges in dietary assessment.


<details>
  <summary>Details</summary>
Motivation: Image-based dietary assessment is crucial for health monitoring and chronic disease prevention, but suffers from the fundamental limitation of estimating 3D food size from 2D images.

Method: The paper explores multiple strategies including: auxiliary inputs (depth maps, multi-view images), model-based approaches (template matching), and deep learning methods (monocular images or combinations with auxiliary inputs).

Result: The paper provides a comprehensive survey and analysis of different approaches to bridge the 2D-to-3D gap in food portion estimation from images.

Conclusion: Various strategies have been developed to address the critical limitation of 3D size estimation from 2D food images, with deep learning approaches showing particular promise for accurate portion estimation in dietary assessment applications.

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [5] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: VCR method identifies visual concepts in LMMs to audit medical AI performance gaps across demographic subgroups


<details>
  <summary>Details</summary>
Motivation: Need reliable auditing methods for ML models in safety-critical healthcare domains to uncover model shortcomings and performance disparities

Method: Visual Concept Ranking (VCR) method identifies important visual concepts in LMMs, generates hypotheses about visual feature dependencies, and validates with manual interventions

Result: LMMs show unexpected performance gaps between demographic subgroups on medical tasks; VCR successfully identifies visual concepts that explain these disparities

Conclusion: VCR provides an effective auditing approach for understanding and addressing performance disparities in multimodal medical AI models

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [6] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: CLEAR-HPV is an interpretable MIL framework that discovers morphological concepts in HPV-related cancer histopathology without requiring concept labels, reducing high-dimensional embeddings to compact concept-fraction vectors while maintaining predictive power.


<details>
  <summary>Details</summary>
Motivation: Current attention-based multiple instance learning (MIL) models for HPV status prediction in head and neck/cervical cancers provide limited morphological interpretability despite strong slide-level prediction performance.

Method: CLEAR-HPV restructures MIL latent space using attention to enable unsupervised concept discovery, automatically identifies morphological concepts (keratinizing, basaloid, stromal), generates spatial concept maps, and represents slides as compact concept-fraction vectors.

Result: The framework reduces high-dimensional feature spaces (e.g., 1536 dimensions) to only 10 interpretable concepts while preserving predictive information, and generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC datasets.

Conclusion: CLEAR-HPV provides compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models in whole-slide histopathology, addressing the interpretability limitations of current approaches.

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [7] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: ARGaze: Autoregressive gaze estimation for online first-person video that predicts current gaze using visual features and recent gaze history, achieving state-of-the-art performance on egocentric benchmarks.


<details>
  <summary>Details</summary>
Motivation: Online egocentric gaze estimation lacks explicit head/eye signals and must infer visual attention from sparse cues. Gaze exhibits strong temporal continuity during goal-directed activities, where recent gaze provides powerful prior for predicting future gaze.

Method: ARGaze reformulates gaze estimation as sequential prediction using transformer decoder. At each timestep, it predicts current gaze by conditioning on: (1) current visual features, and (2) fixed-length Gaze Context Window of recent gaze target estimates. This enforces causality and enables bounded-resource streaming inference.

Result: Achieves state-of-the-art performance across multiple egocentric benchmarks under online evaluation. Extensive ablations validate that autoregressive modeling with bounded gaze history is critical for robust prediction.

Conclusion: Autoregressive modeling with gaze context window effectively leverages temporal continuity for online egocentric gaze estimation, outperforming previous methods while maintaining causality and efficient streaming inference.

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [8] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: AirGlove improves vision-based hand tracking for gloved hands by generalizing learned glove representations to new glove designs with limited data.


<details>
  <summary>Details</summary>
Motivation: Existing sensor-based hand tracking has accuracy issues affected by sensor quality and calibration, while vision-based models trained on bare hands perform poorly on gloved hands due to appearance differences.

Method: Proposes AirGlove which leverages existing glove data to generalize learned glove representations to new glove designs with limited training data, evaluated under zero-shot and fine-tuning setups.

Result: AirGlove effectively generalizes hand pose models to new glove designs and achieves significant performance improvements over existing approaches across multiple sensing gloves.

Conclusion: AirGlove addresses the appearance gap problem in vision-based hand tracking for gloved hands and enables better generalization to new glove designs with limited data.

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [9] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: SHaSaM is a novel combinatorial approach using submodular optimization for fairness-aware representation learning that mines hard samples to address data imbalance and uses submodular mutual information to minimize sensitive attribute influence while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks inherit social and demographic biases from training data, leading to unfair predictions based on sensitive attributes like race, age, and gender. Existing methods struggle with inherent data imbalance between attribute groups and inadvertently emphasize sensitive attributes, worsening both fairness and performance.

Method: Two-stage approach: 1) SHaSaM-MINE uses submodular subset selection to mine hard positives and negatives, mitigating data imbalance. 2) SHaSaM-LEARN introduces combinatorial loss functions based on Submodular Conditional Mutual Information to maximize decision boundaries between target classes while minimizing influence of sensitive attributes.

Result: Experiments on CelebA and UTKFace show state-of-the-art results: up to 2.7 points improvement in model fairness (Equalized Odds) and 3.5% gain in Accuracy, achieving these within fewer epochs compared to existing methods.

Conclusion: SHaSaM provides a unified formulation that restricts models from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance, offering an effective combinatorial solution to bias mitigation in deep learning.

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [10] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: A diffusion-based image-to-image pipeline that reverses underwater degradations using synthetic corruption and trained on 2.5k awareness photography images achieves high perceptual consistency with 11M parameters.


<details>
  <summary>Details</summary>
Motivation: Underwater photography suffers from reduced contrast, spatial blur, and wavelength-dependent color distortions that obscure marine life vibrancy and require heavy post-processing.

Method: Developed an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation.

Result: Achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

Conclusion: The diffusion-based approach effectively reverses underwater image degradations with good perceptual quality and generalization using relatively small models and datasets.

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [11] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: ShapePuri is a novel adversarial defense framework that uses shape guidance and appearance debiasing to achieve over 80% robust accuracy on AutoAttack benchmark, surpassing previous methods while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial attacks, and existing defense methods like diffusion-based purification suffer from high computational costs and information loss. There's a need for more efficient and effective defense strategies.

Method: ShapePuri integrates two components: 1) Shape Encoding Module (SEM) that provides dense geometric guidance using Signed Distance Functions (SDF) to align model representations with stable structural invariants, and 2) Global Appearance Debiasing (GAD) module that mitigates appearance bias through stochastic transformations.

Result: Achieves 84.06% clean accuracy and 81.64% robust accuracy under AutoAttack protocol, becoming the first defense framework to surpass the 80% threshold on this benchmark. The approach is scalable, efficient, and preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

Conclusion: ShapePuri provides an effective adversarial defense solution that leverages shape guidance and appearance debiasing to achieve state-of-the-art robustness while maintaining computational efficiency and prediction stability.

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [12] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: PoseGaussian: A pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis that uses body pose as both structural prior and temporal cue, achieving real-time rendering at 100 FPS with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in dynamic human scene synthesis including articulated motion and severe self-occlusion, while improving robustness and generalization beyond prior works that only use pose as condition or for warping.

Method: Uses body pose as structural prior fused with color encoder for depth refinement, and as temporal cue processed by dedicated pose encoder for temporal consistency. Integrated into fully differentiable, end-to-end trainable pipeline that embeds pose signals into both geometric and temporal stages.

Result: Achieves real-time rendering at 100 FPS while maintaining Gaussian Splatting efficiency. State-of-the-art performance on ZJU-MoCap, THuman2.0, and in-house datasets with PSNR 30.86, SSIM 0.979, LPIPS 0.028.

Conclusion: PoseGaussian effectively leverages pose information in both geometric and temporal domains to achieve high-fidelity human novel view synthesis with real-time performance and superior perceptual quality and structural accuracy.

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [13] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: Video generative models repurposed as temporally-aware reward models via energy-based formulation, achieving SOTA with 30K human annotations (6-65× fewer than VLM approaches).


<details>
  <summary>Details</summary>
Motivation: Current video reward modeling relies on Vision-Language Models (VLMs) that struggle with subtle temporal dynamics, creating a need for temporally-aware alternatives.

Method: Transform video generation models into energy-based reward models using contrastive training with synthetic negative videos created via latent-space perturbations (temporal slicing, feature swapping, frame shuffling).

Result: Achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human annotations (6-65× fewer than VLM-based approaches).

Conclusion: Video generative models can be effectively repurposed as temporally-aware reward models, providing superior performance with dramatically reduced annotation requirements.

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [14] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: A unified framework combining explicit semantic representations and implicit detail encoding achieves state-of-the-art rate-perception performance for ultra-low bitrate neural compression.


<details>
  <summary>Details</summary>
Motivation: Existing neural codecs struggle at ultra-low bitrates, and current generative compression methods face a tradeoff between semantic faithfulness (explicit methods) and perceptual realism (implicit methods).

Method: A training-free framework that integrates explicit high-level semantics (conditioning diffusion models) with implicit fine-grained details (reverse-channel coding), plus a plug-in encoder for flexible distortion-perception tradeoff control.

Result: Achieves state-of-the-art rate-perception performance, outperforming existing methods by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on Kodak, DIV2K, and CLIC2020 datasets respectively.

Conclusion: The proposed unified framework successfully bridges the gap between explicit and implicit representations, enabling superior compression performance at ultra-low bitrates without the traditional tradeoff between semantic faithfulness and perceptual realism.

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [15] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: E.M.Ground is a novel Video LLM for Temporal Video Grounding that improves event localization by focusing on holistic event perception rather than just matching start/end frames.


<details>
  <summary>Details</summary>
Motivation: Existing Video LLMs for Temporal Video Grounding rely heavily on exact timestamp matching of start/end frames, which fails to capture semantic continuity and event integrity, leading to localization ambiguities.

Method: Three key innovations: 1) Special <event> token that aggregates information from all frames of a query event for semantic continuity; 2) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities; 3) Multi-grained frame feature aggregation to enhance matching reliability and compensate for compression loss.

Result: Extensive experiments on benchmark datasets show E.M.Ground consistently outperforms state-of-the-art Video LLMs by significant margins.

Conclusion: E.M.Ground addresses the limitations of existing temporal video grounding methods by focusing on holistic event perception, leading to more accurate and reliable temporal localization.

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [16] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: MPA improves cross-domain few-shot segmentation by progressively adapting from source to target domains using hybrid data augmentation and dual-chain multi-view prediction.


<details>
  <summary>Details</summary>
Motivation: Existing cross-domain few-shot segmentation methods struggle due to limited target domain samples, weak initial few-shot capability in target domains, and large domain gaps that hinder effective adaptation.

Method: Multi-view Progressive Adaptation (MPA) with two components: (1) Hybrid Progressive Augmentation that generates increasingly diverse and complex views through cumulative strong augmentations, and (2) Dual-chain Multi-view Prediction that leverages these views through sequential and parallel learning paths with extensive supervision.

Result: MPA outperforms state-of-the-art methods by a large margin (+7.0%) in extensive experiments, demonstrating effective adaptation of few-shot capability to target domains.

Conclusion: MPA successfully addresses cross-domain few-shot segmentation challenges by progressively adapting few-shot capability through data augmentation and multi-view prediction strategies, achieving robust and accurate adaptation to target domains.

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [17] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: Proposes Conditional Point Sparsification (CPS), a training-free method that adaptively sparsifies dense point prompts for SAM in cross-domain few-shot segmentation, improving performance on medical and satellite images.


<details>
  <summary>Details</summary>
Motivation: Existing SAM-based few-shot segmentation methods rely on dense point matching between reference and target images, but these perform poorly in cross-domain scenarios (medical/satellite images) due to large domain shifts disrupting SAM's learned point-image interactions.

Method: Conditional Point Sparsification (CPS) - a training-free approach that uses reference exemplars with ground-truth masks to adaptively sparsify dense matched points, providing more reliable prompts for SAM in cross-domain settings.

Result: Extensive experiments show CPS outperforms existing training-free SAM-based methods across diverse cross-domain few-shot segmentation datasets.

Conclusion: Point density is crucial for SAM's performance in cross-domain settings, and adaptive point sparsification guided by reference exemplars effectively addresses the limitations of dense point prompts in cross-domain few-shot segmentation.

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [18] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: Combines local neighbor-aware patch features with normalizing flow model and adapter module for anomaly detection in die casting, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Surface defects in die casting impede quality control, and while computer vision techniques have been explored for defect detection, there's a gap between generic pretrained feature extractors and industrial product images that needs to be addressed.

Method: Proposes a method combining local neighbor-aware patch features with a normalizing flow model, introducing an adapter module to bridge the gap between generic pretrained feature extractors and industrial product images for more efficient and accurate anomaly detection.

Result: Achieves 99.28% image-level AUROC on MVTec AD (20% error reduction), 96.48% on VisA dataset (28.2% error reduction), and 95.77% accuracy on proprietary die casting dataset without requiring anomalous samples for training.

Conclusion: The method demonstrates the potential of computer vision and deep learning techniques to advance inspection capabilities for the die casting industry through improved anomaly detection without needing anomalous training samples.

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [19] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: Active label cleaning method for denoising crowdsourced medical image annotations, achieving 67.18% AP50 with 73.30% cost reduction compared to full expert annotation.


<details>
  <summary>Details</summary>
Motivation: Automated detection of electron dense deposits in glomerular disease is limited by scarce high-quality labeled data. Crowdsourcing reduces annotation cost but introduces label noise that degrades model performance.

Method: Proposes an active label cleaning approach using active learning to select the most valuable noisy samples for expert re-annotation. Includes a Label Selection Module that leverages discrepancies between crowdsourced labels and model predictions for sample selection and instance-level noise grading.

Result: Achieves 67.18% AP50 on private dataset, an 18.83% improvement over training on noisy labels. Performance reaches 95.79% of full expert annotation while reducing annotation cost by 73.30%.

Conclusion: Provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources by efficiently denoising crowdsourced datasets through active label cleaning.

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [20] [RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation](https://arxiv.org/abs/2602.05257)
*Diya He,Qingchen Liu,Cong Zhang,Jiahu Qin*

Main category: cs.CV

TL;DR: RFM-Pose accelerates category-level 6D object pose estimation using flow-matching generative models and reinforcement learning, reducing computational costs while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current score-based generative models for object pose estimation suffer from high sampling costs and inefficiency, limiting their practical application in real-time scenarios like virtual reality and embodied intelligence.

Method: Proposes RFM-Pose framework that: 1) Uses flow-matching generative models to generate pose candidates along optimal transport paths, 2) Casts sampling as a Markov decision process and applies proximal policy optimization to fine-tune the sampling policy, 3) Interprets flow field as learnable policy and maps estimator to value network for joint optimization of pose generation and hypothesis scoring.

Result: Experiments on REAL275 benchmark show RFM-Pose achieves favorable performance while significantly reducing computational cost. The approach also adapts well to object pose tracking, attaining competitive results in that setting.

Conclusion: RFM-Pose provides an efficient framework for category-level 6D object pose estimation that balances performance and computational efficiency, with potential applications in real-time systems and pose tracking.

Abstract: Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.

</details>


### [21] [ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network](https://arxiv.org/abs/2602.05262)
*Junzhou Li,Manqi Zhao,Yilin Gao,Zhiheng Yu,Yin Li,Dongsheng Jiang,Li Xiao*

Main category: cs.CV

TL;DR: ReGLA is a lightweight hybrid network combining efficient convolutions with ReLU-based gated linear attention to balance accuracy and latency for high-resolution images, achieving 80.85% Top-1 accuracy on ImageNet with only 4.98ms latency at 512px.


<details>
  <summary>Details</summary>
Motivation: Transformer-based architectures often suffer from excessive latency when processing high-resolution images, creating a need for lightweight models that can balance accuracy and computational efficiency.

Method: ReGLA integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling, featuring three key innovations: Efficient Large Receptive Field (ELRF) module, ReLU Gated Modulated Attention (RGMA) module, and multi-teacher distillation strategy.

Result: ReGLA-M achieves 80.85% Top-1 accuracy on ImageNet-1K at 224px with only 4.98ms latency at 512px, and outperforms similarly scaled iFormer models by 3.1% AP on COCO object detection and 3.6% mIoU on ADE20K semantic segmentation.

Conclusion: ReGLA establishes itself as a state-of-the-art solution for high-resolution visual applications by effectively balancing accuracy and latency through its hybrid architecture and innovative modules.

Abstract: Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.

</details>


### [22] [Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2602.05271)
*Shengqin Jiang,Xiaoran Feng,Yuankai Qi,Haokui Zhang,Renlong Hang,Qingshan Liu,Lina Yao,Quan Z. Sheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Freeze feature extractor, fine-tune prototypes with dual-calibration offsets for better discriminative power in few-shot class-incremental learning.


<details>
  <summary>Details</summary>
Motivation: Traditional FSCIL methods with frozen feature extractors suffer from representation bias, while prompt-based tuning has limited capacity due to extreme data scarcity. The paper argues the real challenge is optimizing decision regions in a static feature space rather than acquiring new features.

Method: Proposes prototype fine-tuning framework that evolves static centroids into dynamic, learnable components using dual-calibration method with class-specific and task-aware offsets to improve discriminative capacity for incremental classes.

Result: Extensive results demonstrate superior performance across multiple benchmarks while requiring minimal learnable parameters.

Conclusion: Freezing feature extractors while fine-tuning prototypes is an effective approach for FSCIL, with the dual-calibration method synergistically improving prototype discriminative capacity for ongoing incremental classes.

Abstract: Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.

</details>


### [23] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: Magic-MM-Embedding: Efficient multimodal embedding models using visual token compression and progressive training for state-of-the-art performance with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs show promise for universal multimodal retrieval but face practical limitations due to high computational costs from processing many visual tokens, hindering real-world deployment.

Method: Two synergistic pillars: (1) Efficient MLLM architecture with visual token compression to reduce latency/memory, and (2) Multi-stage progressive training: continue pretraining → contrastive pretraining with hard negative mining → task-aware fine-tuning using MLLM-as-a-Judge for data curation.

Result: Model outperforms existing methods by a large margin while being more inference-efficient, achieving state-of-the-art performance in universal multimodal embedding.

Conclusion: Magic-MM-Embedding successfully addresses the efficiency-performance trade-off in multimodal retrieval, offering a practical solution for real-world applications through architectural optimization and sophisticated training strategy.

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [24] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: Fast-SAM3D accelerates 3D reconstruction by addressing multi-level heterogeneity in the pipeline, achieving 2.67× speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: SAM3D enables scalable open-world 3D reconstruction but suffers from prohibitive inference latency, and generic acceleration strategies fail due to neglecting the pipeline's inherent multi-level heterogeneity.

Method: Training-free framework with three heterogeneity-aware mechanisms: Modality-Aware Step Caching to decouple structural evolution from layout updates, Joint Spatiotemporal Token Carving to focus refinement on high-entropy regions, and Spectral-Aware Token Aggregation to adapt decoding resolution.

Result: Fast-SAM3D achieves up to 2.67× end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation.

Conclusion: The work presents the first systematic investigation into SAM3D's inference dynamics and demonstrates that addressing multi-level heterogeneity enables significant acceleration while maintaining quality, making efficient 3D reconstruction more practical.

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [25] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: FlashBlock improves block diffusion efficiency by reusing stable attention outputs from tokens outside current blocks, reducing computation and KV cache access without quality loss.


<details>
  <summary>Details</summary>
Motivation: Block diffusion still has substantial overhead from repeatedly computing attention over growing KV caches in long-context settings, despite its efficiency improvements.

Method: Identifies cross-step redundancy in block diffusion attention, proposes FlashBlock to cache and reuse stable attention outputs from tokens outside current blocks, and combines with sparse attention as complementary residual reuse.

Result: Achieves up to 1.44× higher token throughput and up to 1.6× reduction in attention time with negligible impact on generation quality for diffusion language models and video generation.

Conclusion: FlashBlock effectively reduces computational overhead in block diffusion by exploiting attention stability across diffusion steps, offering orthogonal improvements to existing sparse attention techniques.

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [26] [Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning](https://arxiv.org/abs/2602.05321)
*Dongki Jung,Jaehoon Choi,Adil Qureshi,Somi Jeong,Dinesh Manocha,Suyong Yeon*

Main category: cs.CV

TL;DR: Wid3R is a feed-forward neural network for wide FOV visual geometry reconstruction that supports fisheye and panoramic cameras without requiring rectification or undistortion.


<details>
  <summary>Details</summary>
Motivation: Prior 3D reconstruction methods are limited to pinhole cameras and rectified images, requiring careful calibration and undistortion, which restricts their applicability to real-world scenarios with wide FOV cameras like fisheye or panoramic systems.

Method: Uses ray representation with spherical harmonics and a novel camera model token within the network to enable distortion-aware 3D reconstruction, supporting various wide FOV camera types including 360 imagery.

Result: Demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D benchmark. First multi-view foundation model supporting feed-forward 3D reconstruction directly from 360 imagery.

Conclusion: Wid3R provides a generalizable solution for wide FOV 3D reconstruction that overcomes limitations of perspective-only methods, enabling practical applications with fisheye and panoramic cameras without requiring complex calibration procedures.

Abstract: We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.

</details>


### [27] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: MTPano is a multi-task panoramic foundation model that uses perspective foundation models to generate pseudo-labels for panoramic data, then processes rotation-invariant and rotation-variant tasks separately with a dual-branch architecture to handle geometric distortions.


<details>
  <summary>Details</summary>
Motivation: Panoramic scene understanding is challenging due to limited high-resolution multi-task annotations, geometric distortions in equirectangular projections, and coordinate system discrepancies that prevent direct adaptation of perspective foundation models.

Method: 1) Label-free training using perspective patches projected from panoramas to generate pseudo-labels from off-the-shelf foundation models; 2) Panoramic Dual BridgeNet with separate branches for rotation-invariant (depth, segmentation) and rotation-variant (surface normals) tasks; 3) Geometry-aware modulation layers with absolute position and ray direction priors; 4) ERP token mixers and gradient truncation to handle distortions while enabling beneficial cross-task sharing.

Result: MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

Conclusion: The proposed MTPano framework successfully addresses panoramic scene understanding challenges through a label-free training pipeline, task categorization, and specialized architecture design that leverages perspective priors while handling spherical geometry distortions.

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [28] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: PAIR framework reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs, achieving better structural and semantic preservation while removing harmful content.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods focus on removing unsafe concepts without providing guidance toward safe alternatives, leading to failure in preserving structural and semantic consistency between original and erased generations.

Method: PAIR uses unsafe-safe pairs generated from unsafe inputs while preserving structural/semantic fidelity. It includes: (1) Paired Semantic Realignment that maps target concepts to semantically aligned safe anchors, and (2) Fisher-weighted Initialization for DoRA that initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs to encourage safe alternatives while suppressing unsafe concepts.

Result: Extensive experiments show PAIR significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

Conclusion: The PAIR framework successfully transforms concept erasure from simple removal to consistency-preserving semantic realignment, enabling fine-grained erasure that removes only targeted concepts while maintaining overall semantic consistency.

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [29] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: APEX introduces adaptive prototype learning with MDL-based complexity optimization and posterior-aware scoring to fix fundamental flaws in existing OOD detection methods, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based OOD detection methods suffer from two fundamental flaws: 1) Static Homogeneity Assumption (fixed representational resources for all classes), and 2) Learning-Inference Disconnect (discarding prototype quality knowledge at inference), which limit model capacity and performance.

Method: APEX uses a Two-Stage Repair process with two key innovations: 1) Adaptive Prototype Manifold (APM) that applies Minimum Description Length principle to automatically determine optimal prototype complexity K_c* per class, resolving prototype collision; 2) Posterior-Aware OOD Scoring (PAOS) that quantifies prototype quality (cohesion and separation) to bridge learning-inference gap.

Result: Comprehensive experiments on benchmarks like CIFAR-100 validate APEX's superiority, achieving new state-of-the-art performance in OOD detection.

Conclusion: APEX successfully addresses fundamental limitations in prototype-based OOD detection through adaptive prototype complexity optimization and quality-aware scoring, demonstrating significant performance improvements over existing methods.

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [30] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: HIVE introduces a multimodal latent reasoning framework that enables "slow thinking" through hierarchical visual cues injection, moving beyond traditional language-centric chains of thought for more robust and efficient reasoning.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs rely on inefficient "fast thinking" paradigms with end-to-end generation or language-centric chains of thought, which are verbose, prone to hallucinations, and lack robust integration of multimodal signals.

Method: HIVE recursively extends transformer blocks to create internal loops for iterative reasoning refinement, injectively grounding the process with hierarchical visual cues from global scene context to fine-grained regional details directly into latent representations.

Result: Extensive evaluations show that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

Conclusion: Robust multimodal reasoning should evolve within a latent space with seamless integration of multimodal signals, and HIVE's approach of hierarchical visual cues injection enables more deliberate, grounded reasoning without superficial textual rationales.

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [31] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: D-KNN framework addresses Simplicity Paradox in OOD detection by decoupling semantic and structural features via orthogonal decomposition, achieving SOTA performance on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing SOTA OOD detection models suffer from Simplicity Paradox: they're sensitive to semantically subtle OOD samples but blind to structurally distinct yet semantically simple samples or sensor noise. This is caused by Semantic Hegemony in feature space where semantic components dominate structural signals.

Method: Proposes D-KNN, a training-free plug-and-play geometric decoupling framework that uses orthogonal decomposition to separate semantic components from structural residuals, with dual-space calibration to reactivate sensitivity to weak residual signals.

Result: D-KNN establishes new SOTA on CIFAR and ImageNet benchmarks. Reduces FPR95 from 31.3% to 2.3% for Simplicity Paradox cases, and boosts AUROC from 79.7% to 94.9% for sensor noise detection.

Conclusion: The Simplicity Paradox in OOD detection stems from Semantic Hegemony in feature space, which can be effectively addressed through geometric decoupling of semantic and structural components, enabling robust detection across diverse OOD scenarios.

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [32] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: CityGenAgent: A natural language-driven framework for hierarchical procedural generation of high-quality 3D cities using interpretable block and building programs with two-stage learning (SFT + RL).


<details>
  <summary>Details</summary>
Motivation: Automated 3D city generation is crucial for autonomous driving, VR, and embodied AI, but existing methods lack high-fidelity assets, controllability, and manipulation capabilities.

Method: Decomposes city generation into Block Program and Building Program components. Uses two-stage learning: 1) Supervised Fine-Tuning for valid program generation, 2) Reinforcement Learning with Spatial Alignment and Visual Consistency rewards for spatial reasoning and text-visual alignment.

Result: CityGenAgent achieves superior semantic alignment, visual quality, and controllability compared to existing methods, supporting natural language editing and manipulation.

Conclusion: Establishes a robust foundation for scalable 3D city generation with improved fidelity, controllability, and natural language interaction capabilities.

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [33] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: SAIL enables diffusion models to self-improve with minimal human feedback, using only 6% of preference data needed by existing methods, by having models generate samples, self-annotate preferences, and iteratively refine themselves.


<details>
  <summary>Details</summary>
Motivation: Aligning diffusion models with human preferences is challenging when reward models are unavailable and collecting large-scale preference datasets is expensive. The paper asks whether effective alignment can be achieved using only minimal human feedback by leveraging the latent capabilities within diffusion models themselves.

Method: SAIL (Self-Amplified Iterative Learning) is a closed-loop framework where diffusion models act as their own teachers. Starting from a small seed set of human-annotated preference pairs, the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. A ranked preference mixup strategy balances exploration with adherence to initial human priors to prevent catastrophic forgetting.

Result: SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using only 6% of the preference data required by existing approaches. The framework reveals that diffusion models possess remarkable self-improvement capabilities that can effectively replace both large-scale human annotation and external reward models.

Conclusion: Diffusion models have inherent self-improvement capabilities that, when properly harnessed through frameworks like SAIL, can achieve effective alignment with human preferences using minimal feedback, eliminating the need for both extensive human annotation and external reward models.

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [34] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: VLMs struggle with visual reasoning, especially on abstract puzzles (28% accuracy) vs natural images (45% accuracy). Most failures (56%) stem from perception issues, not reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess whether Vision Language Models (VLMs) can reliably perform nonverbal visual reasoning, given recent progress in multimodal AI systems.

Method: Introduce VRIQ benchmark with abstract puzzle-style and natural-image reasoning tasks. Use diagnostic probes to analyze perception vs reasoning failures, and fine-grained probes targeting specific perception categories (shape, count, position, 3D/depth).

Result: VLMs perform near random on abstract puzzles (28% accuracy) and weakly on natural tasks (45% accuracy). Tool-augmented reasoning shows only modest improvements. 56% of failures come from perception alone, 43% from both perception and reasoning, and only 1% from reasoning alone.

Conclusion: Current VLMs remain unreliable abstract reasoners, primarily due to perception limitations rather than reasoning deficiencies. The benchmark provides a principled basis for improving visual reasoning in multimodal systems.

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [35] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: Dolphin-v2 is an enhanced document parsing model that handles both digital-born and photographed documents with improved layout analysis, finer-grained element detection, and parallel processing for efficiency.


<details>
  <summary>Details</summary>
Motivation: The document parsing field is fragmented with specialized models requiring complex selection, and existing approaches fail to handle distorted/photographed documents effectively due to reliance on axis-aligned bounding boxes.

Method: Two-stage approach: 1) Joint document type classification (digital-born vs photographed) with layout analysis and reading order prediction; 2) Hybrid parsing - photographed documents parsed holistically as complete pages, digital-born documents parsed element-wise in parallel using layout anchors.

Result: +14.78 points overall improvement on OmniDocBench, 91% error reduction on photographed documents, maintains efficient inference through parallel processing, evaluated on DocPTBench, OmniDocBench, and RealDoc-160 benchmark.

Conclusion: Dolphin-v2 substantially improves document parsing by handling both document types effectively, introducing finer-grained detection with semantic attributes, and preserving code block indentation while maintaining efficiency.

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [36] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: Proposed Memory-Execute-Review framework for Visual Language Navigation achieves state-of-the-art performance by balancing success rate and generalization, outperforming both supervised fine-tuning and training-free methods across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods struggle to achieve both high success rate and good generalization simultaneously - supervised fine-tuning methods have higher SR but poor generalization, while training-free methods generalize better but have lower SR.

Method: Memory-Execute-Review framework with three components: hierarchical memory module for information support, execute module for routine decision-making, and review module for handling abnormal situations and behavior correction.

Result: Achieved absolute SR improvements of 7% (TF) and 5% (ZS) across 4 datasets; 8% and 6% improvements on HM3D_v0.1 and HM3D_OVON under ZS; surpassed all SFT methods on MP3D and HM3D_OVON with 5% and 2% SR improvements while maintaining better generalization.

Conclusion: The Memory-Execute-Review framework successfully addresses the trade-off between success rate and generalization in VLN, achieving comprehensive leadership in both metrics across challenging datasets.

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [37] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: Proposes Parallel Swin Transformer-Enhanced Med2Transformer for MRI-to-CT synthesis to enable MRI-only radiotherapy planning, improving anatomical fidelity and dosimetric accuracy.


<details>
  <summary>Details</summary>
Motivation: MRI lacks electron density information needed for radiotherapy dose calculation, forcing combined MRI-CT workflows that increase registration uncertainty and complexity. Synthetic CT generation from MRI would enable MRI-only planning but is challenging due to nonlinear MRI-CT relationships and anatomical variability.

Method: Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture integrating convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Uses multi-scale shifted window attention with hierarchical feature aggregation.

Result: Experiments on public and clinical datasets show higher image similarity and improved geometric accuracy compared to baselines. Dosimetric evaluation shows clinically acceptable performance with mean target dose error of 1.69%.

Conclusion: The proposed method enables accurate MRI-to-CT synthesis for radiotherapy planning, potentially eliminating the need for separate CT acquisitions and reducing workflow complexity while maintaining dosimetric accuracy.

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [38] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: IndustryShapes is a new RGB-D benchmark dataset of industrial tools and components for 6D pose estimation, featuring realistic industrial settings and challenging object properties to bridge lab research with real-world manufacturing applications.


<details>
  <summary>Details</summary>
Motivation: To address the gap between lab-based research and real-world industrial deployment by providing a realistic benchmark dataset for 6D pose estimation. Previous datasets focused on household items, synthetic data, or controlled lab environments, lacking industrial relevance and challenging properties needed for manufacturing applications.

Method: Created a comprehensive RGB-D dataset with five new industrial object types captured in realistic assembly settings. The dataset includes diverse scene complexity (simple to challenging), single/multiple objects, and multiple instances of same objects. Organized in two parts: classic set (4.6k images, 6k annotated poses) and extended set with additional modalities including RGB-D static onboarding sequences - a first in the field.

Result: The dataset provides a realistic testbed for benchmarking instance-level and novel object 6D pose estimation methods. Evaluation on state-of-the-art methods for instance-based and novel object pose estimation, object detection, and segmentation shows there is room for improvement in industrial applications.

Conclusion: IndustryShapes bridges the gap between academic research and industrial deployment by providing the first comprehensive industrial tool dataset with realistic settings and challenging properties, enabling better benchmarking and advancement of 6D pose estimation methods for manufacturing robotics.

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [39] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: Statistical flow matching for dataset distillation: aligns synthetic data statistics with original data flows, achieving SOTA performance with 10x lower memory and 4x faster runtime.


<details>
  <summary>Details</summary>
Motivation: Previous gradient matching methods for dataset distillation are computationally expensive, requiring loading thousands of real images and multiple augmentation passes at each step, leading to high memory and runtime overhead.

Method: Proposes statistical flow matching that aligns constant statistical flows from target class centers to non-target class centers in original data. Also introduces classifier inheritance strategy that reuses original dataset's classifier with lightweight linear projector.

Result: Achieves comparable or better performance than state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Classifier inheritance provides substantial performance gains with minimal storage.

Conclusion: Statistical flow matching offers a stable, efficient supervised learning framework for dataset distillation that significantly reduces computational overhead while maintaining or improving performance.

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [40] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: PIRATR is an end-to-end 3D object detection framework that jointly estimates 6-DoF poses and parametric attributes from point clouds, enabling robotic applications with simulation-trained models that generalize to real-world data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between low-level geometric reasoning and actionable world models for robotic applications. Current 3D detection methods often focus only on geometric localization without capturing task-relevant parametric properties of objects that robots need to interact with effectively.

Method: PIRATR extends PI3DETR to perform parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point clouds. It uses modular, class-specific heads that can be easily extended to new object types without pipeline redesign. The system is trained entirely in synthetic environments.

Result: Validated on an automated forklift platform with three diverse object categories (crane grippers, loading platforms, and pallets), PIRATR achieves a detection mAP of 0.919 on real outdoor LiDAR scans without additional fine-tuning, demonstrating effective generalization from synthetic to real data.

Conclusion: PIRATR establishes a new paradigm of pose-aware, parameterized perception that bridges geometric reasoning with actionable world models. This paves the way for scalable, simulation-trained perception systems deployable in dynamic robotic environments.

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [41] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MAD framework uses VAE-learned disentangled latent space to enable biologically plausible pathomics feature editing via conditional diffusion, avoiding unrealistic artifacts by respecting feature correlations.


<details>
  <summary>Details</summary>
Motivation: Pathomics features are often difficult to interpret across clinical contexts, and existing conditional diffusion models assume feature independence, which is violated by correlated pathomics features, leading to unrealistic artifacts when editing features.

Method: Proposes Manifold-Aware Diffusion (MAD) framework that uses VAE to learn disentangled latent space, regularizes feature trajectories within this space to maintain biological plausibility, then guides conditional diffusion model with optimized features for high-fidelity image synthesis.

Result: Experiments show MAD successfully navigates pathomics feature manifold during editing, outperforms baseline methods in conditional feature editing while preserving structural coherence.

Conclusion: MAD enables controllable and biologically plausible cell nuclei editing by respecting intrinsic feature correlations through manifold-aware regularization, advancing explainable pathomics biomarkers.

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [42] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: SparseVideoNav introduces video generation models for Beyond-the-View Navigation, achieving 27x speed-up and 2.5x success rate over LLM baselines by generating sparse future trajectories instead of full videos.


<details>
  <summary>Details</summary>
Motivation: Current vision-language navigation relies on detailed instructions that contradict real-world autonomy needs. Agents should navigate with simple high-level intents, requiring Beyond-the-View Navigation where agents locate unseen targets without step-by-step guidance.

Method: Proposes SparseVideoNav that introduces video generation models for BVN tasks, generating sparse future trajectories spanning 20-second horizons instead of full videos, achieving sub-second inference with 27x speed-up.

Result: Achieves 2.5x success rate over state-of-the-art LLM baselines on BVN tasks, works in challenging night scenes, and demonstrates 27x speed-up compared to unoptimized video generation approaches.

Conclusion: Video generation models are uniquely suitable for BVN due to inherent long-horizon supervision, and SparseVideoNav enables practical real-world deployment through sparse trajectory generation, marking significant advancement in autonomous navigation.

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [43] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: TSBOW is a new traffic surveillance dataset with over 32 hours of real-world data featuring 48k manual and 3.2M semi-labeled frames across 8 vehicle/pedestrian classes, specifically designed for occluded vehicle detection under extreme weather conditions.


<details>
  <summary>Details</summary>
Motivation: Existing traffic surveillance datasets are limited to mild weather conditions (light haze, rain, snow) and fail to address extreme weather events intensified by global warming, which degrade CCTV quality and increase accident rates.

Method: Created TSBOW dataset by collecting over 32 hours of real-world traffic data from densely populated urban areas, with manual annotation of 48,000+ frames and semi-labeling of 3.2M+ frames across 8 traffic participant classes, establishing object detection benchmarks.

Result: Produced comprehensive dataset with varied road types, scales, and viewpoints that highlights challenges of occlusions and adverse weather, establishing a benchmark for occluded vehicle detection under diverse annual weather scenarios.

Conclusion: TSBOW serves as critical resource for advancing Intelligent Transportation Systems, underscores potential of CCTV-based traffic monitoring, and paves way for new research and applications in extreme weather conditions.

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [44] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: Proposed Geometry-guided virtual Outlier Synthesis (GOS) for OOD detection under long-tailed distributions without external data, using vMF distribution sampling and Dual-Granularity Semantic Loss to outperform sota methods.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods for long-tailed distributions rely on external datasets (like 80 Million Tiny Images) which are impractical due to high acquisition costs and privacy concerns. Need a data-free solution.

Method: Geometry-guided virtual Outlier Synthesis (GOS) models statistical properties using von Mises-Fisher distribution on hypersphere to locate low-likelihood annulus and perform directional sampling of virtual outliers. Combined with Dual-Granularity Semantic Loss using contrastive learning to maximize distinction between ID features and synthesized boundary outliers.

Result: Extensive experiments on benchmarks like CIFAR-LT demonstrate the method outperforms state-of-the-art approaches that utilize external real images.

Conclusion: Proposed framework completely eliminates reliance on external datasets while maintaining superior OOD detection performance under long-tailed distributions through virtual outlier synthesis and semantic contrastive learning.

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [45] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: InterPrior is a framework that learns unified generative controllers for humanoid loco-manipulation through imitation pretraining and RL finetuning, enabling generalization to unseen objects and contexts.


<details>
  <summary>Details</summary>
Motivation: Humans don't plan whole-body interactions explicitly but use high-level affordances, with coordination emerging from physical/motor priors. Scaling such priors is needed for humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination.

Method: Two-stage approach: 1) Large-scale imitation pretraining distills a full-reference imitation expert into a goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. 2) RL finetuning with data augmentation (physical perturbations) consolidates latent skills into a valid manifold, improving generalization to unseen goals and initializations.

Result: Creates a motion prior that generalizes beyond training data, enabling interactions with unseen objects. Demonstrates effectiveness for user-interactive control and potential for real robot deployment.

Conclusion: InterPrior provides a scalable framework for learning unified generative controllers that can compose and generalize loco-manipulation skills while maintaining physically coherent whole-body coordination, bridging imitation learning with reinforcement learning for robust humanoid control.

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [46] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: Proposes Disco framework for dense cell segmentation using adjacency-aware collaborative coloring, addressing limitations of 2-coloring methods in complex tissues with odd-length cycles.


<details>
  <summary>Details</summary>
Motivation: Existing cell segmentation methods struggle with complex, dense cellular regions. Graph coloring methods show promise but haven't been validated in real-world scenarios with dense overlaps and complex topologies. Most real-world cell graphs are non-bipartite with odd-length cycles, making simple 2-coloring insufficient.

Method: Disco framework combines data-driven topological labeling with constrained deep learning. Uses "Explicit Marking" to transform topological challenges into learnable classification by recursively decomposing cell graphs and isolating conflict sets. "Implicit Disambiguation" resolves ambiguities by enforcing feature dissimilarity between instances for separable representations.

Result: Released GBC-FS 2025 dataset with complex dense nuclear arrangements. Systematic analysis revealed most real-world cell graphs are non-bipartite with high prevalence of odd-length cycles (triangles). Proposed Disco framework addresses these challenges.

Conclusion: Simple 2-coloring theory is insufficient for complex tissues, while higher-chromaticity models cause redundancy. Disco's adjacency-aware collaborative coloring provides effective solution for dense cell instance segmentation in complex real-world scenarios.

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [47] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: NeVStereo: A unified framework combining NeRF-based novel view synthesis with stereo depth estimation to jointly deliver camera poses, multi-view depth, rendering, and surface reconstruction from casual RGB-only inputs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches have limitations - feed-forward 3D reconstruction systems don't output novel view synthesis, while neural rendering methods assume fixed camera poses and are sensitive to pose errors. There's no single framework that provides accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views.

Method: NeVStereo combines NeRF-based novel view synthesis for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and iterative refinement that updates both depth and radiance field to improve geometric consistency.

Result: Achieves consistently strong zero-shot performance across indoor, outdoor, tabletop, and aerial benchmarks: up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing methods.

Conclusion: NeVStereo successfully addresses the limitations of existing approaches by providing a unified framework that delivers accurate camera poses, multi-view depth, high-quality novel view synthesis, and surface reconstruction from casual RGB-only inputs, while mitigating common NeRF-based issues like surface stacking and artifacts.

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [48] [Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications](https://arxiv.org/abs/2602.05426)
*Wahyu Rahmaniar,Kenji Suzuki*

Main category: cs.CV

TL;DR: Multi-AD is a CNN-based unsupervised anomaly detection model using squeeze-and-excitation blocks for attention, knowledge distillation for feature transfer, and a teacher-student architecture with multi-scale feature integration to detect anomalies across medical and industrial domains.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of annotated data in cross-domain applications like anomaly detection, which is crucial for early disease diagnosis in medicine and defect detection in industry, requiring robust unsupervised methods.

Method: Uses CNN with squeeze-and-excitation blocks for channel-wise attention, knowledge distillation to transfer features from teacher to student, discriminator network to enhance normal/anomaly distinction, and multi-scale feature integration at inference. Teacher-student architecture maintains consistent high-dimensional feature representation.

Result: Outperformed state-of-the-art models, achieving best average AUROC: image-level (81.4% medical, 99.6% industrial) and pixel-level (97.0% medical, 98.4% industrial) across brain MRI, liver CT, retina OCT, and MVTec AD datasets.

Conclusion: Multi-AD demonstrates strong generalization across medical and industrial domains, making it effective for real-world applications where annotated data is scarce, with superior performance in both image-level and pixel-level anomaly detection tasks.

Abstract: Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.

</details>


### [49] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: LD-SLRO uses latent diffusion models to restore distorted fringe patterns on reflective surfaces, improving 3D reconstruction accuracy by suppressing specular artifacts.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction of glossy, highly reflective objects using fringe projection profilometry is challenging due to specular reflection and indirect illumination that distort or erase fringe patterns.

Method: Proposes LD-SLRO: encodes phase-shifted fringe images to latent representations, uses these as conditional inputs to a latent diffusion model that probabilistically suppresses reflection artifacts and recovers lost fringe information. Includes specular reflection encoder, time-variant channel affine layer, and attention modules.

Result: Method improves both fringe quality and 3D reconstruction accuracy, reducing average root-mean-squared error from 1.8176 mm to 0.9619 mm compared to state-of-the-art methods.

Conclusion: LD-SLRO effectively addresses the challenge of 3D reconstruction for reflective objects by using latent diffusion models to restore distorted fringe patterns, offering high flexibility in input/output configuration and superior performance.

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [50] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: Stable Velocity framework reduces variance in flow matching training and accelerates sampling by leveraging variance regimes analysis.


<details>
  <summary>Details</summary>
Motivation: Flow matching's reliance on single-sample conditional velocities creates high-variance training targets that destabilize optimization and slow convergence, particularly near the prior distribution.

Method: Proposes Stable Velocity framework with: 1) Stable Velocity Matching (StableVM) for unbiased variance reduction in training, 2) Variance-Aware Representation Alignment (VA-REPA) for adaptive supervision in low-variance regimes, and 3) Stable Velocity Sampling (StableVS) for finetuning-free acceleration using closed-form simplifications in low-variance regimes.

Result: Extensive experiments on ImageNet 256×256 and large pretrained models (SD3.5, Flux, Qwen-Image, Wan2.2) show consistent training efficiency improvements and >2× faster sampling in low-variance regimes without quality degradation.

Conclusion: The Stable Velocity framework successfully addresses flow matching's variance issues, improving both training stability and sampling efficiency across diverse diffusion models.

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [51] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: Parametric 3D mesh models for synthetic defect generation in metal casting, enabling scalable synthetic training data with pixel-perfect annotations for machine learning-based defect detection.


<details>
  <summary>Details</summary>
Motivation: Need for automated defect detection in quality control using non-destructive testing, requiring large amounts of high-quality training data for machine learning approaches, which can be addressed through synthetic data generation.

Method: Develop parametric 3D mesh models of various defect types that can be added to object geometry to create synthetic defective objects, then use physically-based Monte Carlo simulation of inspection methods to generate synthetic data resembling real inspection data.

Result: Variable and arbitrarily large synthetic datasets can be generated with inclusion of rarely occurring defects in sufficient quantity, along with parallel creation of pixel-perfect annotations for training machine learning models.

Conclusion: The parametric defect modeling approach enables scalable synthetic data generation for training machine learning-based defect detection systems, applicable to various non-destructive testing methods beyond visual surface inspection.

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [52] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: Training-free feature caching accelerates video diffusion models but loses quality; step-distillation works for images but fails for videos. This paper introduces learnable feature caching and conservative distillation to achieve 11.8× acceleration without quality loss.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion model acceleration methods have limitations: feature caching causes semantic/detail loss with compression, and step-distillation (effective for images) degrades severely for videos. Combining these methods fails due to sparser sampling steps in distilled models.

Method: Introduces distillation-compatible learnable feature caching using a lightweight neural predictor instead of training-free heuristics to better capture high-dimensional feature evolution. Also proposes conservative Restricted MeanFlow approach for stable, lossless distillation on large-scale video models.

Result: Achieves 11.8× acceleration while preserving generation quality. Extensive experiments demonstrate effectiveness of the proposed method.

Conclusion: The paper successfully addresses limitations of existing acceleration methods by combining learnable feature caching with conservative distillation, pushing acceleration boundaries for video diffusion models without compromising quality.

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [53] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: Proposes an attention-retaining framework for continual learning in Vision Transformers that prevents catastrophic forgetting by constraining attention drift through gradient masking.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting remains a critical challenge in continual learning. The paper identifies attention drift in Vision Transformers as a primary source of forgetting, where attention to previously learned visual concepts shifts significantly after learning new tasks.

Method: A two-step attention-retaining framework: 1) Extract attention maps of previous tasks using layer-wise rollout and generate instance-adaptive binary masks; 2) When learning new tasks, apply these masks to zero out gradients associated with previous attention regions, preventing disruption of learned concepts. Enhanced with gradient scaling for compatibility with modern optimizers.

Result: The method effectively mitigates catastrophic forgetting and preserves visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse continual learning scenarios.

Conclusion: Attention drift in Vision Transformers is a key cause of catastrophic forgetting, and constraining this drift through gradient masking effectively preserves learned knowledge while enabling continual learning of new tasks.

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [54] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: SOMA-1M is a large-scale, pixel-level aligned SAR-optical dataset with 1.3M image pairs across 12 land cover categories and multiple resolutions (0.5m-10m), enabling training of multimodal foundation models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmark datasets have limitations like single spatial resolution, insufficient scale, and poor alignment accuracy, making them inadequate for training multi-scale foundation models that need to leverage complementary SAR and optical imagery.

Method: Created SOMA-1M dataset with over 1.3M georeferenced image pairs (512x512 pixels) from Sentinel-1, PIESAT-1, Capella Space, and Google Earth. Used a rigorous coarse-to-fine image matching framework to achieve pixel-level alignment across 12 land cover categories with global multi-scale coverage.

Result: Supervised training on SOMA-1M significantly enhances performance across four hierarchical vision tasks (image matching, fusion, SAR-assisted cloud removal, cross-modal translation). Achieved SOTA in multimodal remote sensing image matching with evaluation of over 30 mainstream algorithms.

Conclusion: SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models, addressing critical limitations of existing datasets and enabling cross-modal collaborative processing.

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [55] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: A 2014 PhD thesis report evaluating feature detectors/descriptors for fisheye images in self-calibration context, providing PFSeq dataset but not proposing new algorithms.


<details>
  <summary>Details</summary>
Motivation: Address the chicken-and-egg problem in fisheye camera self-calibration: need accurate projection model for optimal feature detection, but need good features to compute that model. Focus on finding best features for fisheye images from car-mounted zenith cameras for visual odometry and stereovision in urban scenes.

Method: Created PFSeq (Photorealistic Fisheye Sequence) dataset, conducted comprehensive experiments comparing existing feature detectors and descriptors for fisheye images in self-calibration context. Work done as part of PhD thesis on 3D modeling from fisheye video data.

Result: Provides detailed bibliography and PFSeq dataset (available at DOI: 10.57745/DYIVVU), with experimental evaluation of feature detectors/descriptors for fisheye images. Results represent 2014 state-of-the-art evaluation.

Conclusion: This is a draft report from 2014 PhD work providing dataset and feature evaluation for fisheye image processing, not peer-reviewed or updated with current state-of-the-art, but offers valuable historical perspective and dataset for fisheye camera research.

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [56] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: VGGT-Motion is a calibration-free monocular SLAM system that addresses scale drift in long sequences through motion-aware submap construction and anchor-driven Sim(3) registration, achieving efficient kilometer-scale operation.


<details>
  <summary>Details</summary>
Motivation: Existing calibration-free monocular SLAM systems suffer from severe scale drift on long sequences due to motion-agnostic partitioning that breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment methods are computationally expensive.

Method: 1) Motion-aware submap construction using optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. 2) Anchor-driven direct Sim(3) registration that uses context-balanced anchors for search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. 3) Lightweight submap-level pose graph optimization with linear complexity for global consistency.

Result: VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM on kilometer-scale trajectories.

Conclusion: The proposed VGGT-Motion system effectively addresses scale drift in long sequences through motion-aware partitioning and efficient registration, enabling robust and scalable calibration-free monocular SLAM for kilometer-scale operation.

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [57] [Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification](https://arxiv.org/abs/2602.05522)
*Jeongbin You,Donggun Kim,Sejun Park,Seungsang Oh*

Main category: cs.CV

TL;DR: Mapper-GIN: A lightweight topology-inspired pipeline that partitions point clouds into overlapping regions using Mapper algorithm, constructs region graphs, and performs graph classification with GIN for robust 3D point cloud classification.


<details>
  <summary>Details</summary>
Motivation: Instead of scaling up backbones or relying on specialized data augmentation for robust 3D point cloud classification, the paper explores whether structural abstraction alone can improve robustness through topology-inspired decomposition.

Method: Mapper-GIN partitions point clouds into overlapping regions using Mapper algorithm (PCA lens, cubical cover, density-based clustering), constructs a region graph from overlaps, and performs graph classification with Graph Isomorphism Network (GIN).

Result: On ModelNet40-C corruption benchmark, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters, outperforming heavier architectures requiring additional mechanisms.

Conclusion: Region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition, demonstrating that simple topology-inspired decomposition can provide strong corruption robustness without heavy architectures.

Abstract: Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.

</details>


### [58] [Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains](https://arxiv.org/abs/2602.05527)
*Ben Isselmann,Dilara Göksu,Andreas Weinmann*

Main category: cs.CV

TL;DR: DINO-pretrained Vision Transformers transfer well across microscopy domains, with domain-relevant SSL (Human Protein Atlas) outperforming both ImageNet and target-domain pretraining for protein localization classification.


<details>
  <summary>Details</summary>
Motivation: Microscopy datasets are often too small for robust deep learning, and it's unclear how well self-supervised representations transfer across different microscopy domains with varying staining protocols and channel configurations.

Method: Investigate cross-domain transferability using DINO-pretrained Vision Transformers on OpenCell protein localization dataset. Generate embeddings from three backbones: ImageNet-1k, Human Protein Atlas (HPA), and OpenCell. Evaluate by training supervised classification head on OpenCell labels.

Result: All pretrained models transfer well. HPA-pretrained model achieves best performance (mean macro F1-score = 0.8221 ± 0.0062), slightly outperforming DINO trained directly on OpenCell (0.8057 ± 0.0090). ImageNet-pretrained also performs well.

Conclusion: Large-scale domain-relevant SSL representations generalize effectively to related microscopy datasets, enabling strong downstream performance even with limited task-specific labeled data.

Abstract: Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.

</details>


### [59] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: SSG is a training-free inference-time guidance method that ensures VAR models maintain their intended coarse-to-fine hierarchy by emphasizing high-frequency semantic residuals not explained by earlier scales.


<details>
  <summary>Details</summary>
Motivation: VAR models can drift from their intended coarse-to-fine hierarchy during inference due to limited capacity and accumulated errors, causing train-inference discrepancy that degrades generation quality.

Method: Proposes Scaled Spatial Guidance (SSG) that isolates semantic residuals (high-frequency content not explained by coarser scales) using Discrete Spatial Enhancement (DSE) - a frequency-domain procedure to sharpen and better isolate these residuals, then guides generation toward the intended hierarchy.

Result: SSG yields consistent gains in fidelity and diversity while preserving low latency, works broadly across VAR models with discrete visual tokens regardless of tokenization design or conditioning modality.

Conclusion: SSG reveals untapped efficiency in coarse-to-fine image generation by mitigating train-inference discrepancy through principled frequency-domain guidance, maintaining the intended hierarchy while ensuring global coherence.

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [60] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: Systematic evaluation shows camera-LiDAR fusion (DAL) outperforms single-modality approaches for 3D person detection, especially in challenging scenarios, though it remains vulnerable to sensor misalignment and certain corruptions.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D person detection is critical for safety applications like robotics, industrial monitoring, and surveillance. Most existing research focuses on autonomous driving, so there's a need to explore detection performance in diverse indoor/outdoor scenes using the JRDB dataset.

Method: Systematic evaluation of 3D person detection using camera-only (BEVDepth), LiDAR-only (PointPillars), and camera-LiDAR fusion (DAL) on the JRDB dataset. Analysis includes performance under varying occlusion and distance levels, plus robustness testing against sensor corruptions and misalignments.

Result: Fusion-based approach (DAL) consistently outperforms single-modality models, particularly in challenging scenarios. However, DAL remains sensitive to sensor misalignment and certain LiDAR-based corruptions. Camera-based BEVDepth showed lowest performance and was most affected by occlusion, distance, and noise.

Conclusion: Sensor fusion is important for enhanced 3D person detection, but ongoing research is needed to address vulnerabilities to sensor misalignment and corruptions in these systems.

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [61] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: FastVMT accelerates video motion transfer by reducing computational redundancy in Diffusion Transformers through local attention masking and gradient reuse, achieving 3.43x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing video motion transfer methods using Diffusion Transformers (DiT) suffer from computational inefficiency. While some methods try to accelerate DiT computations, they fail to address structural sources of inefficiency related to motion and gradient patterns in video generation.

Method: Two key innovations: 1) Local attention masking to address motion redundancy by restricting attention to local neighborhoods since frame-to-frame motion is small and smooth, 2) Gradient reuse optimization that exploits slow-changing gradients along the diffusion trajectory to skip unnecessary computations.

Result: FastVMT achieves an average 3.43x speedup compared to previous methods while maintaining visual fidelity and temporal consistency of generated videos.

Conclusion: By identifying and addressing two types of computational redundancy (motion and gradient redundancy) in Diffusion Transformers for video motion transfer, FastVMT demonstrates significant speed improvements without compromising output quality.

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [62] [ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors](https://arxiv.org/abs/2602.05572)
*Zhenxiao Liang,Ning Zhang,Youbao Tang,Ruei-Sung Lin,Qixing Huang,Peng Chang,Jing Xiao*

Main category: cs.CV

TL;DR: ShapeGaussian is a template-free 4D human reconstruction method that achieves high-fidelity results from monocular videos by integrating vision priors to overcome limitations of both generic and template-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations: generic reconstruction methods (like 4DGS) struggle with high-deformation human motion without multi-view cues, while template-based approaches (like HUGS using SMPL) are susceptible to pose estimation errors leading to unrealistic artifacts. There's a need for a method that combines high-fidelity reconstruction with robustness to pose estimation errors.

Method: Two-step pipeline: 1) Learn coarse deformable geometry using pretrained models that estimate data-driven priors, 2) Refine geometry using a neural deformation model to capture fine-grained dynamic details. Leverages 2D vision priors to mitigate pose estimation artifacts and uses multiple reference frames to resolve 2D keypoint invisibility issues in a template-free manner.

Result: Extensive experiments show ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.

Conclusion: ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust 4D human scene reconstructions from casual monocular videos, overcoming limitations of both generic and template-based approaches.

Abstract: We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.

</details>


### [63] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: ViGT is a calibration-free autonomous driving model that estimates continuous 3D occupancy fields from surround-view cameras using self-supervised learning on image-LiDAR pairs, achieving SOTA performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To create a foundational geometric model for autonomous driving that prioritizes scalability, simplicity, and generalization across diverse sensor configurations without requiring manual annotations or calibration.

Method: Uses a calibration-free architecture with Visual Implicit Geometry Transformer to estimate continuous 3D occupancy fields in BEV from multiple camera views, trained self-supervised using synchronized image-LiDAR pairs from five large-scale datasets.

Result: Achieves state-of-the-art performance on pointmap estimation with best average rank across baselines, and comparable performance to supervised methods on Occ3D-nuScenes benchmark.

Conclusion: ViGT demonstrates effective geometric understanding for autonomous driving through scalable, calibration-free architecture with self-supervised training, providing a common representation for multiple geometric tasks across diverse sensor setups.

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [64] [A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features](https://arxiv.org/abs/2602.05574)
*Mengyu Li,Ingibjörg Kristjánsdóttir,Thilo van Eimeren,Kathrin Giehl,Lotta M. Ellingsen,the ASAP Neuroimaging Initiative*

Main category: cs.CV

TL;DR: Hybrid CNN-ML framework using multi-modal MRI data achieves high AUC scores (0.95, 0.86, 0.92) for differentiating atypical Parkinsonian disorders from Parkinson's disease and between subtypes.


<details>
  <summary>Details</summary>
Motivation: Atypical Parkinsonian Disorders (APD) like PSP and MSA are often misdiagnosed as Parkinson's disease in early stages due to overlapping symptoms. There's a critical need for reliable imaging biomarkers for early differential diagnosis.

Method: Hybrid framework combining convolutional neural networks (CNNs) with machine learning techniques using multi-modal input: T1-weighted MRI, segmentation masks of 12 deep brain structures, and corresponding volumetric measurements.

Result: Achieved promising classification performance with AUC scores: 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA.

Conclusion: Fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes, potentially enabling more reliable early-stage diagnosis and timely interventions in clinical practice.

Abstract: Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.

</details>


### [65] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: LocateEdit-Bench: A large-scale dataset for benchmarking AI-generated image forgery localization methods against instruction-based editing, with 231K edited images from 4 editing models covering 3 edit types.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated forgery localization methods focus on inpainting-based manipulations but are ineffective against the latest instruction-based editing paradigms, creating a critical gap in detection capabilities.

Method: Created LocateEdit-Bench dataset with 231K edited images using 4 cutting-edge editing models across 3 common edit types, with detailed dataset analysis and development of multi-metric evaluation protocols.

Result: Established a foundation for benchmarking localization methods against instruction-driven image editing, enabling assessment of existing methods and facilitating development of future forgery localization approaches.

Conclusion: The dataset bridges the critical gap between evolving image editing technologies and detection methods, providing necessary tools to keep pace with instruction-based editing and advance forgery localization research.

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [66] [LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578)
*Junyang Chen,Xiangbo Lv,Zhiqiang Kou,Xingdong Sheng,Ning Xu,Yiguo Qiao*

Main category: cs.CV

TL;DR: LoGoSeg is an efficient single-stage framework for open-vocabulary semantic segmentation that integrates object existence priors, region-aware alignment, and dual-stream fusion to improve spatial alignment and reduce hallucinations without needing external proposals or extra datasets.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary segmentation methods using vision-language models suffer from imprecise spatial alignment due to image-level pretraining, leading to mismatched segmentations in ambiguous scenes. They also lack strong object priors and region-level constraints, causing object hallucination and missed detections.

Method: LoGoSeg introduces three key innovations: (1) object existence prior that dynamically weights relevant categories using global image-text similarity to reduce hallucinations, (2) region-aware alignment module for precise region-level visual-textual correspondences, and (3) dual-stream fusion mechanism combining local structural information with global semantic context. It's a single-stage framework that eliminates need for external mask proposals, additional backbones, or extra datasets.

Result: Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate competitive performance and strong generalization in open-vocabulary settings.

Conclusion: LoGoSeg provides an efficient solution for open-vocabulary semantic segmentation that addresses key limitations of existing methods by integrating object priors and region-level alignment while maintaining computational efficiency through its single-stage design.

Abstract: Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.

</details>


### [67] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: The paper introduces a Geometric Observability Index (GOI) that quantifies how individual image features influence camera pose estimation on SE(3), unifying classical sensitivity analysis with influence function theory on Lie groups.


<details>
  <summary>Details</summary>
Motivation: Classical sensitivity tools fail to explain how individual image features influence pose estimates or why dynamic/inconsistent observations disproportionately distort modern SLAM and SfM systems. There's a gap in understanding per-feature sensitivity on the Lie group SE(3).

Method: Extends influence function theory to matrix Lie groups and derives an intrinsic perturbation operator for left-trivialized M-estimators on SE(3). Develops Geometric Observability Index (GOI) through spectral decomposition of the curvature operator along observable subspace directions.

Result: GOI quantifies single measurement contributions through curvature and Lie algebraic structure, revealing correspondence between weak observability and amplified sensitivity. In population regime, GOI coincides with Fisher information geometry, providing single-measurement analogue of Cramer-Rao bound.

Conclusion: GOI provides geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability. The curvature spectrum and GOI yield lightweight diagnostic signals for identifying dynamic features and weak observability without modifying existing SLAM architectures.

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [68] [A Mixed Reality System for Robust Manikin Localization in Childbirth Training](https://arxiv.org/abs/2602.05588)
*Haojie Cheng,Chang Liu,Abhiram Kanneganti,Mahesh Arjandas Choolani,Arundhati Tushar Gosavi,Eng Tat Khoo*

Main category: cs.CV

TL;DR: MR system for childbirth training combines virtual guidance with tactile manikin interaction, enabling independent practice with haptic feedback. System uses calibrated RGB-D camera for real-time visual integration and achieves higher training effectiveness than VR.


<details>
  <summary>Details</summary>
Motivation: Medical students face limited practical experience in vaginal births due to shortened clinical rotations, patient reluctance, and unpredictable labor. Need to reduce clinician instructional burden while enhancing trainee learning efficiency.

Method: Mixed reality system combining virtual guidance with tactile manikin interaction. Uses passthrough capability of commercial HMDs with calibrated external RGB-D camera. Implements coarse-to-fine localization pipeline: first aligns maternal manikin with fiducial markers, then registers pre-scanned neonatal head. Enables spatially accurate overlay of virtual guiding hands for following expert trajectories.

Result: System achieves accurate and stable manikin localization on standalone headset without external computing. User study with 83 fourth-year medical students showed MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance compared to VR training, and was consistently preferred by trainees.

Conclusion: MR-based childbirth training system effectively enhances medical student training by combining virtual guidance with tactile interaction, enabling independent practice with authentic haptic feedback while reducing reliance on continuous expert supervision.

Abstract: Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.

</details>


### [69] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: EgoPoseVR: A real-time egocentric full-body pose estimation framework for VR that fuses headset motion cues with RGB-D observations using cross-attention fusion and kinematic optimization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current head-mounted camera-based approaches for VR pose estimation suffer from temporal instability, inaccurate lower-body estimation, and lack of real-time performance, limiting practical VR embodiment applications.

Method: End-to-end framework integrating HMD motion cues with egocentric RGB-D observations via dual-modality fusion pipeline. Uses spatiotemporal encoder for frame/joint representations, cross-attention fusion, and kinematic optimization module with HMD constraints. Trained on large-scale synthetic dataset of 1.8M aligned frames.

Result: Outperforms state-of-the-art egocentric pose estimation models. User study shows significantly higher ratings in accuracy, stability, embodiment, and intention for future use compared to baselines.

Conclusion: EgoPoseVR enables robust full-body pose tracking for VR without additional sensors or room-scale tracking, offering practical solution for accurate VR embodiment.

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [70] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: CAViT replaces static MLPs in Vision Transformers with dynamic channel-wise self-attention, creating a dual-attention architecture that improves performance while reducing parameters and computational cost.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers rely on static MLPs for channel mixing that lack adaptability to input content, limiting their ability to dynamically recalibrate feature representations based on global image context.

Method: CAViT introduces a dual-attention architecture where each Transformer block performs spatial self-attention followed by channel-wise self-attention, replacing the static MLP with a dynamic, attention-based mechanism for feature interaction.

Result: CAViT outperforms standard ViT baseline by up to +3.6% accuracy across five benchmark datasets spanning natural and medical domains, while reducing parameter count and FLOPs by over 30%.

Conclusion: The attention-driven token mixing strategy enhances representational expressiveness without increasing depth or complexity, producing sharper and semantically meaningful activation patterns as validated by qualitative attention maps.

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [71] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: Proposes a robust multi-instance fitting method for non-classical models using model-to-data error estimator and meta-heuristic optimization.


<details>
  <summary>Details</summary>
Motivation: Existing robust fitting methods focus on classical models (lines, circles, planes) and single instances, lacking solutions for multiple instances of non-classical models (spiral curves, procedural models, free-form surfaces) from noisy data.

Method: Formulates multi-instance fitting as optimization with novel estimator based on model-to-data error (handles outliers without predefined threshold) and meta-heuristic optimizer for global optimum search.

Result: Method effectively reconstructs multiple instances of various non-classical models, demonstrated through experimental results.

Conclusion: Proposed approach successfully addresses multi-instance fitting for non-classical models with robust outlier handling and global optimization.

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [72] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: XSIM is a sensor simulation framework for autonomous driving that extends 3DGUT splatting with rolling-shutter modeling and addresses spherical camera challenges through phase modeling and dual-opacity Gaussian representations.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGUT splatting methods struggle with spherical cameras (like LiDARs) in autonomous driving due to cyclic projection issues and time discontinuities at azimuth boundaries, leading to incorrect particle projection. There's a need for better sensor simulation that handles complex distortions in dynamic driving environments.

Method: Extends 3DGUT splatting with generalized rolling-shutter modeling. Introduces phase modeling to handle temporal and shape discontinuities at azimuth borders for spherical cameras. Proposes extended 3D Gaussian representation with two distinct opacity parameters to resolve geometry-color distribution mismatches.

Result: Consistently outperforms strong recent baselines and achieves state-of-the-art performance across Waymo Open Dataset, Argoverse 2, and PandaSet. Provides enhanced scene representations with improved geometric consistency and photorealistic appearance.

Conclusion: XSIM offers a unified sensor simulation framework for autonomous driving that effectively addresses spherical camera challenges and provides realistic sensor modeling for dynamic environments, with publicly available source code.

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [73] [ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing](https://arxiv.org/abs/2602.05629)
*Jianlei Chi,Yuzhen Wu,Jiaxuan Hou,Xiaodong Zhang,Ming Fan,Suhui Sun,Weijun Dai,Bo Li,Jianguo Sun,Jun Sun*

Main category: cs.CV

TL;DR: ROMAN is a novel scenario generation approach for ADS testing that combines multi-head attention with traffic law weighting to generate high-risk violation scenarios, outperforming existing tools in violation count and diversity.


<details>
  <summary>Details</summary>
Motivation: Current ADS testing approaches have limited ability to generate complex, high-risk law-breaking scenarios and fail to account for complex multi-vehicle interactions and critical situations, posing safety risks for autonomous vehicle deployment.

Method: ROMAN combines a multi-head attention network to model interactions among vehicles, traffic signals, and other factors with a traffic law weighting mechanism that uses an LLM-based risk weighting module to evaluate violations based on severity and occurrence dimensions.

Result: ROMAN surpassed state-of-the-art tools ABLE and LawBreaker with 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while maintaining greater scenario diversity and successfully generating violation scenarios for every clause of input traffic laws.

Conclusion: ROMAN enables more thorough and targeted ADS evaluation by effectively generating high-risk violation scenarios, identifying more violations than existing approaches, and covering all traffic law clauses, making it a valuable tool for comprehensive autonomous vehicle safety testing.

Abstract: Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.

</details>


### [74] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: UniSurg is a video-native foundation model that shifts from pixel-level reconstruction to latent motion prediction for surgical video analysis, achieving state-of-the-art performance across multiple surgical tasks.


<details>
  <summary>Details</summary>
Motivation: Current foundation models for surgical video analysis waste capacity on low-level visual details (smoke, reflections, fluids) rather than focusing on semantic structures essential for surgical understanding.

Method: Built on V-JEPA architecture with three innovations: 1) motion-guided latent prediction, 2) spatiotemporal affinity self-distillation, 3) feature diversity regularization. Pretrained on UniSurg-15M dataset (3,658 hours from 50 sources across 13 anatomical regions).

Result: Significantly outperforms SOTA methods: +14.6% F1 on EgoSurgery, +10.3% on PitVis for workflow recognition; 39.54% mAP-IVT on CholecT50 for action triplet recognition; plus improvements in skill assessment, polyp segmentation, and depth estimation across 17 benchmarks.

Conclusion: UniSurg establishes a new standard for universal, motion-oriented surgical video understanding by focusing on semantic motion patterns rather than low-level visual details.

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [75] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: Nuance-level personality modeling outperforms facet and trait-level approaches for personality recognition from audiovisual data, reducing error by up to 74%.


<details>
  <summary>Details</summary>
Motivation: Traditional personality recognition relies on broad trait scores as ground truth, but similar trait scores can manifest through diverse, context-dependent behaviors, limiting generalization. The authors explore more granular hierarchical levels (facets and nuances) of the Big-Five model to improve predictive accuracy.

Method: Used the UDIVA v0.5 dataset and trained a transformer-based model with cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms to predict personality at different hierarchical levels (trait, facet, nuance).

Result: Nuance-level models consistently outperformed facet and trait-level models, reducing mean squared error by up to 74% across different interaction scenarios.

Conclusion: More granular personality modeling at the nuance level significantly enhances personality recognition from audiovisual interaction data compared to traditional trait-level approaches.

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [76] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: ShapeUP is a scalable 3D editing framework that uses supervised latent-to-latent translation with image conditioning to achieve precise, consistent 3D manipulation while leveraging pretrained 3D foundation models.


<details>
  <summary>Details</summary>
Motivation: Existing 3D editing methods face trade-offs between visual controllability, geometric consistency, and scalability. Optimization-based methods are slow, multi-view 2D propagation suffers from visual drift, and training-free latent manipulation can't benefit from scaling due to frozen priors.

Method: ShapeUP formulates 3D editing as supervised latent-to-latent translation within native 3D representation. It trains on triplets (source 3D shape, edited 2D image, edited 3D shape) using a 3D Diffusion Transformer (DiT) to learn direct mapping, enabling image-as-prompt control with implicit mask-free localization.

Result: ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering fine-grained visual control over local/global edits while maintaining strict structural consistency with original assets.

Conclusion: ShapeUP presents a robust and scalable paradigm for native 3D content creation that leverages pretrained 3D foundation models while adapting them to editing through supervised training, overcoming limitations of existing 3D editing frameworks.

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [77] [Poster: Camera Tampering Detection for Outdoor IoT Systems](https://arxiv.org/abs/2602.05706)
*Shadi Attarha,Kanaga Shanmugi,Anna Förster*

Main category: cs.CV

TL;DR: Two approaches for detecting tampered images from smart cameras: rule-based and deep-learning methods, with trade-offs between accuracy and resource requirements.


<details>
  <summary>Details</summary>
Motivation: Smart cameras in outdoor settings are vulnerable to tampering (vandalism/environmental conditions), but detecting tampering is challenging with still images (no continuous video frames). Need effective detection methods for real-world scenarios.

Method: Proposed two approaches: 1) Rule-based method using predefined rules/algorithms, 2) Deep-learning-based method using neural networks. Compared performance in terms of accuracy, computational demands, and training data requirements.

Result: Deep-learning model provides higher accuracy, while rule-based method is more suitable for resource-limited scenarios where prolonged calibration is impractical. Created publicly available datasets with normal, blurred, and rotated images for development/evaluation.

Conclusion: Both methods have practical applications depending on scenario requirements - deep learning for high accuracy when resources allow, rule-based for constrained environments. Public datasets address resource gap for camera tampering detection research.

Abstract: Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.

</details>


### [78] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: A multi-task learning framework for point-supervised temporal action localization that introduces three self-supervised temporal understanding tasks to enhance the model's ability to understand temporal relationships among frames.


<details>
  <summary>Details</summary>
Motivation: Existing point-supervised temporal action localization approaches only use point-supervised snippet-level classification without explicit modeling of temporal relationships among frames. Understanding temporal relationships is crucial for localizing full action frames since it helps models understand how actions are defined.

Method: Proposes a multi-task learning framework with three self-supervised temporal understanding tasks: (1) Action Completion, (2) Action Order Understanding, and (3) Action Regularity Understanding. These tasks leverage point supervision to enhance the model's temporal understanding capability for action localization.

Result: Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

Conclusion: This is the first attempt to explicitly explore temporal consistency for point-supervised action localization, showing that understanding temporal relationships among frames significantly benefits action localization performance.

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [79] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: AGFF-Embed is a novel MLLM embedding method that adaptively fuses global and fine-grained semantic information through multiple embeddings and explicit gradient amplification for enhanced multimodal understanding.


<details>
  <summary>Details</summary>
Motivation: Current multimodal embeddings (CLIP-based and MLLM-based) only capture global semantics, but complex real-world scenarios require hybrid perception of both global and fine-grained elements, necessitating a fusion mechanism.

Method: AGFF-Embed prompts MLLMs to generate multiple embeddings focusing on different semantic dimensions, then adaptively aggregates them. It incorporates Explicit Gradient Amplification (EGA) for in-batch hard negatives enhancement without dataset editing.

Result: Evaluation on MMEB and MMVP-VLM benchmarks shows AGFF-Embed achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

Conclusion: The proposed AGFF-Embed method successfully addresses the limitation of existing multimodal embeddings by adaptively fusing global and fine-grained information, demonstrating superior performance across comprehensive benchmarks.

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [80] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: DepthPrior uses depth information as prior knowledge to improve small object detection without modifying detector architectures, achieving significant performance gains through depth-based loss weighting, stratification, and confidence thresholding.


<details>
  <summary>Details</summary>
Motivation: Small and distant objects are challenging to detect due to scale variation, low resolution, and background clutter. While depth information can help, existing approaches require complex architectural modifications. There's a need for a method that leverages depth without changing detector architectures.

Method: DepthPrior framework uses depth as prior knowledge rather than fused feature. It includes: Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training to focus on small/distant objects, and Depth-Aware Confidence Thresholding (DCT) during inference to adjust confidence thresholds based on object depth.

Result: Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) show up to +9% mAP_S and +7% mAR_S for small objects, with inference recovery rates as high as 95:1 (true vs. false detections).

Conclusion: DepthPrior effectively improves small object detection using depth as prior knowledge without requiring additional sensors, architectural changes, or performance costs, making it a practical solution for safety-critical applications.

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [81] [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](https://arxiv.org/abs/2602.05737)
*Luca Ciampi,Ludovico Iannello,Fabrizio Tonelli,Gabriele Lagani,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.CV

TL;DR: Living cortical neurons in vitro serve as biological reservoir for visual pattern recognition, achieving accurate classification despite biological variability.


<details>
  <summary>Details</summary>
Motivation: To leverage actual biological neural circuits as computational substrate rather than artificial approximations, integrating living neural systems into neuromorphic computing frameworks.

Method: Biological reservoir computing using cultured cortical neurons on HD-MEA for stimulation/readout, with linear perceptron readout trained on high-dimensional neural responses for visual pattern classification.

Result: System consistently generates high-dimensional representations supporting accurate classification across tasks of increasing difficulty (points, bars, shapes, MNIST digits), despite biological variability.

Conclusion: In vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening avenues for integrating living neural substrates into neuromorphic computing and informing biologically grounded computational models.

Abstract: In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

</details>


### [82] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: FMPose3D uses Flow Matching for efficient 3D pose estimation, generating multiple hypotheses with few integration steps and aggregating them via RPEA for accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D pose estimation is ill-posed due to depth ambiguity and occlusions, requiring probabilistic methods. Existing diffusion models are computationally expensive due to many timesteps needed for inference.

Method: Proposes FMPose3D framework using Flow Matching to learn velocity fields via ODEs for efficient 3D pose generation. Introduces Reprojection-based Posterior Expectation Aggregation (RPEA) to combine multiple pose hypotheses into a single accurate prediction.

Result: Achieves state-of-the-art performance on Human3.6M, MPI-INF-3DHP, Animal3D, and CtrlAni3D datasets, demonstrating strong performance across both human and animal 3D pose domains.

Conclusion: FMPose3D provides an efficient and effective solution for probabilistic 3D pose estimation using Flow Matching, outperforming existing methods while requiring fewer computational steps.

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [83] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: ReText: A novel multimodal method for generalizable person re-identification that combines multi-camera Re-ID data with single-camera data enriched by textual descriptions, achieving state-of-the-art cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Generalizable person Re-ID faces domain gaps when applied to unseen cameras. While single-camera data is easy to collect, it lacks cross-view variation. The paper aims to leverage both multi-camera data and single-camera data enhanced with textual descriptions to improve generalization.

Method: ReText trains on a mixture of multi-camera Re-ID data and single-camera data with textual descriptions. It jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) text-guided image reconstruction on single-camera data.

Result: ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks, demonstrating the effectiveness of multimodal joint learning with mixed data sources.

Conclusion: This is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID, showing that textual descriptions can effectively complement single-camera data to improve cross-domain generalization.

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [84] [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789)
*Hengyi Wang,Ruiqiang Zhang,Chang Liu,Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: Allocentric Perceiver is a training-free method that uses geometric experts to extract 3D states from images, transforms them into target-centric reference frames, and prompts VLMs with structured geometry-grounded representations to improve allocentric spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with allocentric spatial queries requiring perspective shifts, where answers depend on reasoning in a target-centric frame rather than the observed camera view, limiting their performance on spatially grounded tasks like Vision-Language Navigation/Action.

Method: The approach recovers metric 3D states from images using off-the-shelf geometric experts, instantiates a query-conditioned allocentric reference frame aligned with instruction intent, deterministically transforms reconstructed geometry into the target frame, and prompts backbone VLMs with structured geometry-grounded representations.

Result: Consistent and substantial gains (~10%) on allocentric tasks while maintaining strong egocentric performance, surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models across multiple backbone families.

Conclusion: Allocentric Perceiver effectively offloads mental rotation from implicit reasoning to explicit computation, enabling VLMs to handle allocentric spatial queries without additional training, representing a significant advancement in spatial reasoning capabilities for vision-language models.

Abstract: With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.

</details>


### [85] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: FSR is a plug-and-play pruning framework for vision-language models that mimics human visual question answering: focus on key evidence, scan globally if needed, and refine scanned context, improving accuracy-efficiency trade-off.


<details>
  <summary>Details</summary>
Motivation: Vision-language models generate massive visual tokens that increase inference latency and memory footprint. Existing training-free token pruning methods struggle to balance local evidence and global context under aggressive compression.

Method: FSR (Focus-Scan-Refine) framework: 1) Focus on key evidence by combining visual importance with instruction relevance, 2) Scan for complementary context conditioned on focused set, selecting tokens most different from focused evidence, 3) Refine scanned context by aggregating nearby informative tokens via similarity-based assignment and score-weighted merging without increasing token budget.

Result: Extensive experiments across multiple VLM backbones and vision-language benchmarks show FSR consistently improves accuracy-efficiency trade-off over existing state-of-the-art pruning methods.

Conclusion: FSR provides an effective human-inspired pruning framework that balances local evidence and global context, offering better performance-efficiency trade-offs for vision-language models.

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [86] [NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects](https://arxiv.org/abs/2602.05822)
*Musawar Ali,Manuel Carranza-García,Nicola Fioraio,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: NVS-HO is the first benchmark for novel view synthesis of handheld objects using only RGB inputs, featuring two complementary sequences (handheld and board) to enable learning from manipulation while providing ground-truth poses for evaluation.


<details>
  <summary>Details</summary>
Motivation: There's a need for a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects, as current methods struggle under unconstrained handheld conditions.

Method: Each object is recorded in two sequences: (1) handheld sequence where object is manipulated in front of static camera, and (2) board sequence where object is fixed on ChArUco board for accurate pose estimation. Baseline methods include SfM pipeline and VGGT as pose estimators, with NVS models based on NeRF and Gaussian Splatting.

Result: Experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, demonstrating the benchmark's challenging nature and highlighting the need for more robust approaches.

Conclusion: NVS-HO provides a challenging real-world benchmark that exposes limitations of current NVS methods for handheld objects and will drive progress in RGB-based novel view synthesis.

Abstract: We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.

</details>


### [87] [Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning](https://arxiv.org/abs/2602.05829)
*Yudi Shi,Shangzhe Di,Qirui Chen,Qinian Wang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie*

Main category: cs.CV

TL;DR: Weaver is a novel multimodal reasoning system that uses dynamic tool invocation and reinforcement learning to improve video reasoning performance, especially for long videos.


<details>
  <summary>Details</summary>
Motivation: Current text-centric Chain-of-Thought approaches for video reasoning suffer from representational mismatch and limited perceptual acuity, failing to capture authentic multimodal reasoning.

Method: Weaver is an end-to-end trainable agentic system that dynamically invokes diverse tools during reasoning, progressively acquiring visual cues and constructing multimodal reasoning trajectories, enhanced by reinforcement learning for tool usage exploration.

Result: Extensive experiments show Weaver enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

Conclusion: Weaver addresses limitations of text-centric approaches by enabling dynamic tool invocation and reinforcement learning, providing a more effective framework for complex video reasoning tasks.

Abstract: Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

</details>


### [88] [UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents](https://arxiv.org/abs/2602.05832)
*Han Xiao,Guozhi Wang,Hao Wang,Shilong Liu,Yuxiang Chai,Yue Pan,Yufeng Zhou,Xiaoxin Chen,Yafei Wen,Hongsheng Li*

Main category: cs.CV

TL;DR: UI-Mem enhances GUI online RL with hierarchical experience memory for better credit assignment and cross-task transfer, using stratified sampling and self-evolving memory.


<details>
  <summary>Details</summary>
Motivation: Online RL for GUI agents suffers from inefficient credit assignment in long-horizon tasks and repetitive errors due to lack of experience transfer across tasks.

Method: Proposes UI-Mem with Hierarchical Experience Memory storing structured knowledge as parameterized templates, Stratified Group Sampling for diverse guidance injection, and Self-Evolving Loop for continuous memory updates.

Result: Significantly outperforms traditional RL baselines and static reuse strategies on online GUI benchmarks, with strong generalization to unseen applications.

Conclusion: UI-Mem effectively addresses credit assignment and transfer learning challenges in GUI online RL through hierarchical memory and adaptive guidance mechanisms.

Abstract: Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

</details>


### [89] [Self-Supervised Learning with a Multi-Task Latent Space Objective](https://arxiv.org/abs/2602.05845)
*Pierre-François De Plaen,Abhishek Jha,Luc Van Gool,Tinne Tuytelaars,Marc Proesmans*

Main category: cs.CV

TL;DR: Multi-crop SSL fails in predictor-based architectures due to shared predictors; using separate predictors per view type stabilizes training and improves performance.


<details>
  <summary>Details</summary>
Motivation: Multi-crop strategy enhances SSL but causes instability in predictor-based architectures like BYOL, SimSiam, and MoCo v3. The paper aims to solve this instability problem.

Method: Assign separate predictors to each view type instead of sharing one. Extend to treat each spatial transformation as distinct alignment task, adding cutout views (masked images). Create multi-task framework combining global, local, and masked views.

Result: Stabilizes multi-crop training, yields significant performance gains. Approach is stable, generally applicable across backbones, consistently improves ResNet and ViT models on ImageNet.

Conclusion: Simple multi-task formulation of asymmetric Siamese SSL that combines different view types with separate predictors solves multi-crop instability and improves representation learning.

Abstract: Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.

</details>


### [90] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: TTC is a training-free method that uses the first frame as an anchor to correct error accumulation in distilled autoregressive diffusion models for long video generation.


<details>
  <summary>Details</summary>
Motivation: Distilled autoregressive diffusion models work well for short videos but suffer from severe error accumulation during long-sequence generation. Existing Test-Time Optimization methods fail for extended sequences due to unstable reward landscapes and hypersensitive distilled parameters.

Method: Test-Time Correction (TTC) uses the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. It's training-free and integrates with various distilled models.

Result: TTC extends generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks. It seamlessly integrates with various distilled models.

Conclusion: TTC provides an effective training-free solution to mitigate error accumulation in long video generation with distilled autoregressive diffusion models, overcoming limitations of existing Test-Time Optimization methods.

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [91] [Contour Refinement using Discrete Diffusion in Low Data Regime](https://arxiv.org/abs/2602.05880)
*Fei Yu Guan,Ian Keefe,Sophie Wilkinson,Daniel D. B. Perrakis,Steven Waslander*

Main category: cs.CV

TL;DR: Lightweight discrete diffusion contour refinement pipeline for robust boundary detection of irregular/translucent objects in low-data regimes (<500 images), outperforming SOTA on medical datasets with 3.5X faster inference.


<details>
  <summary>Details</summary>
Motivation: Boundary detection for irregular/translucent objects is important for medical imaging, environmental monitoring, and manufacturing, but suffers from scarce labeled data and limited computational resources. Current segmentation studies focus on mask alignment rather than boundary detection, especially in low-data scenarios.

Method: Uses a CNN with self-attention layers as core, conditioned on segmentation masks to iteratively denoise sparse contour representations. Introduces simplified diffusion process, customized architecture, and minimal post-processing for dense, isolated contours.

Result: Outperforms SOTA baselines on KVASIR medical imaging dataset, competitive on HAM10K and custom wildfire dataset (Smoke), while achieving 3.5X faster inference framerate.

Conclusion: The lightweight discrete diffusion contour refinement pipeline enables robust boundary detection in low-data regimes with improved efficiency, making it suitable for resource-constrained applications.

Abstract: Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.

</details>


### [92] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: Neural implicit function method reconstructs complete 3D cardiac shapes from sparse 2D echocardiography-like planes, achieving better accuracy than clinical standard Simpson's biplane rule.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D cardiac representations enable quantitative analysis of anatomy and function, but 2D transthoracic echocardiography (TTE) provides only sparse 2D views. There's a need to reconstruct complete 3D cardiac shapes from these limited 2D views for more accurate chamber quantification.

Method: Uses neural implicit function with multi-layer perceptron to learn shape priors from 3D CTA segmentations. At test time, reconstructs 3D cardiac shapes from TTE-mimicking CTA planes by jointly optimizing latent codes and rigid transforms that map observed planes into 3D space. Simulates four realistic apical views for each heart.

Result: Achieves average Dice coefficient of 0.86 ± 0.04 across all structures on held-out CTA segmentations. Shows markedly lower volume errors than Simpson's biplane rule: 4.88 ± 4.26 mL vs 8.14 ± 6.04 mL for left ventricle, and 6.40 ± 7.37 mL vs 37.76 ± 22.96 mL for left atrium.

Conclusion: The approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography, potentially improving cardiac assessment from standard clinical imaging.

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [93] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: EoCD is an encoder-only change detection method that uses early fusion of temporal data and replaces complex decoders with parameter-free multiscale feature fusion, achieving optimal balance between performance and speed.


<details>
  <summary>Details</summary>
Motivation: Existing change detection methods have high computational costs and complexity due to Siamese encoders with individual temporal feature extraction followed by sophisticated decoders, while early fusion methods show inferior performance compared to late fusion approaches.

Method: Proposes encoder-only change detection (EoCD) that performs early fusion of temporal data and replaces the decoder with a parameter-free multiscale feature fusion module, significantly reducing model complexity.

Result: EoCD demonstrates optimal balance between change detection performance and prediction speed across various encoder architectures, showing that performance depends predominantly on the encoder network.

Conclusion: Extensive experiments on four challenging change detection datasets reveal the effectiveness of EoCD, demonstrating that decoders are additional components rather than essential for performance.

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [94] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: Learning condition-dependent source distributions in flow matching improves text-to-image generation by addressing distributional collapse and instability issues, leading to faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: Current flow matching methods for text-to-image generation use standard Gaussian source distributions inherited from diffusion models, without optimizing the source distribution itself. The authors argue that principled design of source distributions can better exploit conditioning signals and improve performance.

Method: Propose learning condition-dependent source distributions under flow matching objective with variance regularization and directional alignment between source and target. Analyze how target representation space impacts flow matching with structured sources.

Result: Extensive experiments show consistent improvements across text-to-image benchmarks, including up to 3x faster convergence in FID. The approach addresses distributional collapse and instability issues that arise when directly incorporating conditioning into source distributions.

Conclusion: Principled design of source distributions is feasible and beneficial for conditional flow matching at scale, with appropriate regularization and alignment being critical for stable learning and improved text-to-image generation performance.

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [95] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: LSA framework fine-tunes video generation models to improve temporal consistency by aligning semantic features around dynamic objects, eliminating need for control signals at inference.


<details>
  <summary>Details</summary>
Motivation: Existing controllable video generation methods require control signals at inference time, limiting their scalability and generalizability as data engines for autonomous driving scenarios.

Method: Propose Localized Semantic Alignment (LSA) that fine-tunes pre-trained models by aligning semantic features between ground-truth and generated videos around dynamic objects, combining this with standard diffusion loss.

Result: Single-epoch fine-tuned model outperforms baselines on video generation metrics and shows improved temporal consistency on adapted object detection metrics (mAP, mIoU) on nuScenes and KITTI datasets.

Conclusion: LSA effectively enhances temporal consistency in video generation without requiring external control signals during inference or adding computational overhead.

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [96] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: CLIP-Map: A novel mapping-based compression framework for CLIP that uses learnable matrices to map and combine pretrained weights, outperforming select-based methods especially at high compression ratios.


<details>
  <summary>Details</summary>
Motivation: CLIP has high memory and computation costs that limit its use in resource-limited scenarios. Existing compression methods use select-based weight inheritance which compromises feature representation ability, especially under extreme compression.

Method: Proposes CLIP-Map, a mapping-based compression framework that uses learnable matrices to map and combine pretrained weights via Full-Mapping with Kronecker Factorization. Uses Diagonal Inheritance Initialization to mitigate optimization challenges and reduce distribution shifting for efficient mapping learning.

Result: Extensive experiments show CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

Conclusion: Mapping-based compression (CLIP-Map) is more effective than select-based approaches for CLIP compression, especially for extreme compression scenarios, by better preserving information from original weights.

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [97] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: RISE-Video is a new benchmark for evaluating reasoning capabilities in text-image-to-video models, focusing on implicit world rules rather than just visual quality.


<details>
  <summary>Details</summary>
Motivation: Current generative video models achieve high visual fidelity but lack reasoning capabilities over implicit world rules, creating a gap in evaluating true cognitive abilities.

Method: Created RISE-Video benchmark with 467 human-annotated samples across 8 reasoning categories, plus a multi-dimensional evaluation protocol with 4 metrics and an automated LMM-based assessment pipeline.

Result: Testing 11 state-of-the-art TI2V models revealed significant deficiencies in simulating complex scenarios under implicit constraints, showing current models struggle with reasoning.

Conclusion: The benchmark exposes critical gaps in current video generation models' reasoning abilities and provides a structured framework for advancing world-simulating generative models.

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [98] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: MGIPT proposes multi-scale global-instance prompt tuning for continual test-time adaptation in medical image segmentation, addressing error accumulation and catastrophic forgetting through adaptive instance prompts and multi-scale global prompts.


<details>
  <summary>Details</summary>
Motivation: Distribution shift in medical images from different clinical centers hinders deployment of pre-trained segmentation models. Existing CTTA methods suffer from error accumulation and catastrophic forgetting, while prompt-based approaches lack multi-scale diversity, instance-specific knowledge, and have privacy risks.

Method: MGIPT consists of Adaptive-scale Instance Prompt (AIP) for lightweight instance-specific prompts with adaptive scale selection, and Multi-scale Global-level Prompt (MGP) for domain-level knowledge across scales. These are combined via weighted ensemble for dual-level adaptation integrating global and local information.

Result: Extensive experiments on medical image segmentation benchmarks show MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

Conclusion: MGIPT effectively addresses limitations of existing CTTA methods by enhancing prompt scale diversity and capturing both global- and instance-level knowledge, enabling robust adaptation to distribution shifts in medical imaging.

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [99] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: GenArena introduces a pairwise comparison framework for evaluating visual generation models, replacing unreliable pointwise scoring with a more stable and human-aligned approach that boosts evaluation accuracy by over 20%.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation approaches can't keep up with rapid advances in visual generation models, and current absolute pointwise scoring standards suffer from stochastic inconsistency and poor alignment with human perception.

Method: GenArena is a unified evaluation framework that uses pairwise comparison paradigm instead of absolute pointwise scoring, enabling more stable and human-aligned evaluation of visual generation models.

Result: The pairwise approach enables off-the-shelf open-source models to outperform top-tier proprietary models, boosts evaluation accuracy by over 20%, and achieves 0.86 Spearman correlation with LMArena leaderboard (vs 0.36 for pointwise methods).

Conclusion: GenArena provides a rigorous, automated evaluation standard for visual generation models across diverse tasks, addressing fundamental limitations of current evaluation paradigms.

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [100] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: VisRefiner is a training framework that improves screenshot-to-code generation by teaching models to learn from visual differences between rendered predictions and reference designs, enabling both better initial code generation and self-refinement capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal LLMs generate code directly from screenshots without observing the visual outcomes of their generated code, unlike human developers who iteratively render, compare, and learn from visual differences to improve their implementations.

Method: 1) Construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, 2) Introduce reinforcement learning for self-refinement where the model improves generated code by observing rendered output, target design, identifying visual differences, and updating code accordingly.

Result: VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability, demonstrating the effectiveness of learning from visual differences for screenshot-to-code generation.

Conclusion: Learning from visual differences between rendered predictions and reference designs is an effective approach for advancing screenshot-to-code generation, enabling models to better understand how appearance variations arise from implementation changes.

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [101] [MambaVF: State Space Model for Efficient Video Fusion](https://arxiv.org/abs/2602.06017)
*Zixiang Zhao,Yukun Cui,Lilun Deng,Haowen Bai,Haotong Qin,Tao Feng,Konrad Schindler*

Main category: cs.CV

TL;DR: MambaVF is an efficient video fusion framework using state space models that eliminates optical flow estimation, achieving SOTA performance with significantly reduced computation and parameters.


<details>
  <summary>Details</summary>
Motivation: Existing video fusion methods rely heavily on optical flow estimation and feature warping, which causes severe computational overhead and limits scalability.

Method: Reformulates video fusion as sequential state update process using state space models; proposes lightweight SSM-based fusion module with spatio-temporal bidirectional scanning mechanism instead of flow-guided alignment.

Result: Achieves state-of-the-art performance across multiple benchmarks (multi-exposure, multi-focus, infrared-visible, medical video fusion) with up to 92.25% parameter reduction, 88.79% FLOPs reduction, and 2.1x speedup.

Conclusion: MambaVF provides an efficient alternative to flow-based video fusion methods, enabling high-performance temporal modeling without explicit motion estimation through state space models.

Abstract: Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io

</details>


### [102] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: Context Forcing trains long-context video generation models using a long-context teacher to eliminate student-teacher mismatch, enabling 20+ second context lengths with slow-fast memory architecture.


<details>
  <summary>Details</summary>
Motivation: Current real-time long video generation methods suffer from student-teacher mismatch where short-context teachers can't guide students on long-term dependencies, limiting context length and temporal consistency.

Method: Proposes Context Forcing framework with long-context teacher aware of full generation history, plus slow-fast memory architecture to manage computational complexity for extreme durations (e.g., 2 minutes).

Result: Achieves effective context lengths exceeding 20 seconds (2-10x longer than SOTA), preserves superior consistency across long durations, and outperforms baselines on long video evaluation metrics.

Conclusion: Context Forcing resolves the fundamental student-teacher mismatch in long video generation, enabling robust training of models with extended context and superior long-term consistency.

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [103] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: Splat and Distill is a framework that enhances 2D Vision Foundation Models with 3D awareness by using a feed-forward 3D reconstruction pipeline to create geometrically grounded supervision for student models.


<details>
  <summary>Details</summary>
Motivation: Vision Foundation Models (VFMs) excel at 2D tasks but lack 3D awareness, limiting their performance on tasks requiring geometric understanding.

Method: The framework lifts 2D features from a teacher model into an explicit 3D Gaussian representation, splats these 3D features onto novel viewpoints to create novel 2D feature maps, and uses these to supervise a student model through distillation.

Result: Significantly outperforms prior works on monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation, achieving substantial gains in 3D awareness while enhancing semantic richness of 2D features.

Conclusion: The proposed Splat and Distill framework successfully instills robust 3D awareness into 2D VFMs through a feed-forward 3D reconstruction approach, creating a dynamic learning process that improves both teacher and student consistency.

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [104] [V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval](https://arxiv.org/abs/2602.06034)
*Dongyang Chen,Chaoyang Wang,Dezhao SU,Xi Xiao,Zeyu Zhang,Jing Xiong,Qing Li,Yuzhang Shang,Shichao Ka*

Main category: cs.CV

TL;DR: V-Retrver is an evidence-driven multimodal retrieval framework that uses agentic reasoning with active visual verification to improve retrieval accuracy over language-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal retrieval approaches are largely language-driven, relying on static visual encodings and lacking ability to actively verify fine-grained visual evidence, leading to speculative reasoning in visually ambiguous cases.

Method: V-Retrver reformulates multimodal retrieval as an agentic reasoning process where MLLMs selectively acquire visual evidence via external visual tools, performing multimodal interleaved reasoning that alternates between hypothesis generation and targeted visual verification. Uses curriculum-based learning with supervised reasoning activation, rejection-based refinement, and reinforcement learning with evidence-aligned objective.

Result: Experiments across multiple multimodal retrieval benchmarks show consistent improvements: 23.0% average improvement in retrieval accuracy, better perception-driven reasoning reliability, and improved generalization.

Conclusion: V-Retrver demonstrates that evidence-driven agentic reasoning with active visual verification significantly improves multimodal retrieval performance over language-driven approaches.

Abstract: Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.

</details>


### [105] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker introduces active perception for spatial reasoning in MLLMs, using selective geometric retrieval instead of passive fusion, achieving SOTA 72.6 on VSI-Bench.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs use passive geometry fusion that causes semantic-geometry misalignment and redundant signals. Need active perception where models can selectively retrieve geometric evidence based on reasoning demands.

Method: Uses Spatial-Grounded Fusion at selected VLM layers where semantic visual priors query task-relevant geometry via frame-strict cross-attention, calibrated by Importance Gating to bias attention toward relevant structures.

Result: Achieves SOTA 72.6 on VSI-Bench, demonstrates robust generalization and improved spatial perception in complex downstream scenarios like embodied referring and autonomous driving.

Conclusion: Active integration of spatial structures is essential for next-generation spatial intelligence, shifting from passive fusion to active perception enables better spatial reasoning.

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [106] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: SwimBird is a reasoning-switchable MLLM that dynamically chooses between text-only, vision-only, and interleaved vision-text reasoning based on input queries, improving both textual logic and visual performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs use rigid reasoning patterns (primarily textual CoT) that limit effectiveness on vision-intensive tasks. Recent approaches using visual thoughts degrade text-based logical reasoning. The core limitation is the inability to adaptively choose the most suitable thinking modality for different queries.

Method: SwimBird uses a hybrid autoregressive formulation unifying next-token prediction for textual thoughts with next-embedding prediction for visual thoughts. It includes a systematic reasoning-mode curation strategy to create SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns (text-only, vision-only, interleaved vision-text).

Result: SwimBird achieves state-of-the-art results across diverse benchmarks covering textual reasoning and challenging visual understanding. It preserves strong textual logic while substantially improving performance on vision-dense tasks, showing robust gains over prior fixed-pattern multimodal reasoning methods.

Conclusion: Dynamic, query-adaptive reasoning mode selection enables MLLMs to maintain strong textual reasoning while significantly improving visual performance, overcoming limitations of rigid reasoning patterns in existing multimodal models.

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [107] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: CAMCUE is a pose-aware multi-image framework that uses camera pose as geometric anchor for cross-view fusion and novel-view reasoning, improving multi-image spatial reasoning in MLLMs.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models struggle with multi-image spatial reasoning, particularly perspective taking - building coherent 3D understanding from multi-view observations and reasoning from new viewpoints.

Method: CAMCUE injects per-view camera pose into visual tokens, grounds natural-language viewpoint descriptions to target camera poses, and synthesizes pose-conditioned imagined target views to support answering. Uses CAMCUE-DATA dataset with 27,668 training instances.

Result: Improves overall accuracy by 9.06%, predicts target poses from natural-language descriptions with over 90% rotation accuracy within 20° and translation accuracy within 0.5 error threshold. Reduces inference time from 256.6s to 1.45s per example.

Conclusion: CAMCUE enables fast, interactive multi-image spatial reasoning by using camera pose as explicit geometric anchor, avoiding expensive test-time search-and-match and supporting real-world applications.

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [108] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: The paper critiques the linear model of AI progress, introduces "familiar" vs "strange" intelligence concepts, argues AI will exhibit strange intelligence with uneven capabilities, and proposes a nonlinear model where general intelligence is goal-achieving ability across diverse environments rather than a single linear quantity.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing linear model of AI progress and intelligence measurement, which assumes intelligence progresses uniformly across domains and can be reduced to a single quantitative measure. The authors argue this model fails to capture the likely nature of AI intelligence.

Method: The authors build on Susan Schneider's critique, introduce and develop the concepts of "familiar intelligence" (human-like patterns of ability/inability) and "strange intelligence" (defying familiar patterns), and construct a nonlinear model of intelligence where general intelligence is defined as the ability to achieve broad goals in diverse environments without reduction to a single linear quantity.

Result: The paper establishes that AI intelligence is likely to be "strange intelligence" - combining superhuman capacities in some domains with subhuman performance in others, and sometimes making surprising errors that few humans would make. This challenges linear measurement approaches and has implications for AI evaluation.

Conclusion: If AI exhibits strange intelligence, we should expect even the most capable systems to sometimes fail at seemingly obvious tasks, and such failures alone don't demonstrate lack of general intelligence. Conversely, excellent performance on specific tasks (like IQ tests) doesn't warrant assumptions of broad capacities beyond those domains. This requires rethinking adversarial testing approaches to AI evaluation.

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [109] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead is a structure-aware document reasoning agent that preserves document hierarchy for better long-document QA through coordinated retrieval and section reading tools.


<details>
  <summary>Details</summary>
Motivation: Existing agentic search frameworks treat long documents as flat chunk collections, underutilizing document-native priors like hierarchical organization and sequential discourse structure, which limits their effectiveness for long-document question answering.

Method: DeepRead converts PDFs to structured Markdown preserving headings and paragraphs, indexes at paragraph level with coordinate-style metadata, and provides two complementary tools: Retrieve (localizes paragraphs with structural context) and ReadSection (enables contiguous, order-preserving reading within specified sections).

Result: DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering, with validated synergistic effects between retrieval and reading tools, and demonstrates human-like "locate then read" behavior.

Conclusion: Explicitly operationalizing document structural priors through coordinated retrieval and reading tools enables more effective long-document reasoning, resembling human reading patterns and outperforming flat retrieval approaches.

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [110] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: MINT is a neuro-symbolic framework that optimizes AI agent questioning strategies to fill knowledge gaps in human-AI collaborative planning, achieving near-expert performance with minimal queries.


<details>
  <summary>Details</summary>
Motivation: Open-world planning involves incomplete information about objects and human goals, creating knowledge gaps that hinder effective human-AI collaboration. Current approaches lack systematic methods for AI agents to actively elicit missing information from humans.

Method: Proposes MINT (Minimal Information Neuro-Symbolic Tree) that builds symbolic trees of possible interactions, uses neural planning policies to estimate uncertainty from knowledge gaps, leverages LLMs to summarize reasoning and curate optimal queries, and employs self-play to optimize elicitation strategies.

Result: MINT achieves near-expert returns on three benchmarks with unseen/unknown objects, requiring only a limited number of questions per task while significantly improving rewards and success rates compared to baseline approaches.

Conclusion: MINT provides an effective framework for AI agents to actively elicit human inputs through optimized questioning strategies, enabling successful human-AI teaming in open-world planning with knowledge gaps.

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [111] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLMs can effectively support learning of established graph theory concepts but cannot solve open problems requiring novel mathematical insight.


<details>
  <summary>Details</summary>
Motivation: As LLMs become integrated into computer science education, there's a need to understand their reliability in supporting mathematically rigorous thinking, particularly in advanced topics like graph theory.

Method: Used an eight-stage evaluation protocol reflecting authentic mathematical inquiry (interpretation, exploration, strategy formation, proof construction) to test LLM performance on both a solved graph theory problem and an open problem.

Result: LLM performed strongly on the solved problem (correct definitions, relevant structures, valid proof) but only generated coherent interpretations and plausible strategies for the open problem without advancing toward a solution, while avoiding fabrication of results.

Conclusion: LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight; educators should guide students to use LLMs for conceptual exploration while emphasizing independent verification and rigorous argumentation.

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [112] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: The paper argues for shifting uncertainty quantification (UQ) research from single-turn LLM question-answering to interactive agent settings, proposing a new conditional uncertainty reduction framework that explicitly models reducible uncertainty over agent trajectories.


<details>
  <summary>Details</summary>
Motivation: Current UQ research focuses on single-turn question-answering, but LLM agents are increasingly deployed in complex interactive tasks, creating a gap between research and real-world applications that requires a new principled framework for agent uncertainty quantification.

Method: The paper presents a general formulation of agent UQ that subsumes existing setups, critiques prior works' implicit uncertainty accumulation approach, and proposes a novel conditional uncertainty reduction perspective that explicitly models reducible uncertainty by emphasizing agent actions' interactivity.

Result: The authors develop a conceptual framework providing actionable guidance for designing UQ in LLM agent setups, showing that the traditional uncertainty accumulation viewpoint breaks down for interactive agents in open-world environments.

Conclusion: The paper concludes with practical implications for frontier LLM development and domain-specific applications, highlighting the need for agent UQ research and identifying open problems in the field.

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [113] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: RL-based framework for adaptive collision avoidance in multi-debris ADR missions using small satellites, reducing collision risk while improving fuel efficiency.


<details>
  <summary>Details</summary>
Motivation: Increasing orbital debris creates safety challenges for ADR missions; need for adaptive collision avoidance to ensure safe operations while minimizing collision risks during multi-debris removal.

Method: Uses reinforcement learning with masked Proximal Policy Optimization (PPO) algorithm to dynamically adjust maneuvers based on real-time orbital conditions; integrates refueling strategies and mission planning; evaluated on simulated ADR scenarios from Iridium 33 debris dataset.

Result: RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches; demonstrates robustness across diverse orbital configurations and debris distributions.

Conclusion: Provides scalable solution for complex multi-debris ADR missions, applicable to other multi-target rendezvous problems in autonomous space mission planning.

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [114] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: VERA-MH evaluation shows strong clinical validity and reliability for AI safety in mental health, with LLM judges aligning well with clinician consensus on suicide risk detection.


<details>
  <summary>Details</summary>
Motivation: Millions use AI chatbots for psychological support, but safety concerns remain the most pressing question in AI for mental health, requiring evidence-based automated safety benchmarks.

Method: Simulated conversations between LLM-based user-agents and AI chatbots were rated by licensed clinicians and an LLM judge using the same scoring rubric for safety behaviors and user-agent realism.

Result: Clinicians showed high inter-rater reliability (0.77), establishing gold-standard reference. LLM judge strongly aligned with clinical consensus (0.81). User-agents were perceived as realistic by clinicians.

Conclusion: VERA-MH demonstrates clinical validity and reliability as an open-source automated AI safety evaluation for mental health, supporting its use while further research addresses generalizability and robustness.

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [115] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: Comparison of three planners for multi-debris rendezvous: nominal PPO, domain-randomized PPO, and MCTS, showing trade-offs between speed and adaptability under varying mission constraints.


<details>
  <summary>Details</summary>
Motivation: ADR mission planning needs to balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration, requiring robust planners that can handle distributional shifts.

Method: Compared three planners: 1) nominal Masked PPO trained under fixed parameters, 2) domain-randomized Masked PPO trained across varying constraints, and 3) plain MCTS baseline. Evaluated in high-fidelity orbital simulation with refueling, realistic dynamics, and randomized debris fields across 300 test cases.

Result: Nominal PPO performs best when conditions match training but degrades sharply under distributional shift. Domain-randomized PPO shows improved adaptability with moderate nominal performance loss. MCTS handles constraint changes best due to online replanning but has orders-of-magnitude higher computation time.

Conclusion: There's a trade-off between learned policy speed and search-based method adaptability. Combining training-time diversity with online planning could be promising for future resilient ADR mission planners.

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [116] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS is a lightweight, graph-based simulation framework for scalable multi-agent systems that supports rapid prototyping and integration with various AI tools.


<details>
  <summary>Details</summary>
Motivation: Existing high-fidelity simulators are computationally expensive and not suitable for rapid prototyping or large-scale agent deployments, creating a need for more accessible simulation tools.

Method: Developed a graph-based adversarial multiagent modeling simulator (GAMMS) with five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding.

Result: GAMMS enables efficient simulation of complex domains like urban networks and communication systems, supports integration with external tools, provides built-in visualization, and is agnostic to policy type (heuristic, optimization-based, or learning-based).

Conclusion: By lowering the barrier to entry and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling.

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [117] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: LLMs show significant bias and heterogeneity when used as evaluators in payment-risk settings, with some models over-scoring and others under-scoring relative to human consensus and ground truth.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to evaluate reasoning quality in financial settings, but their reliability and bias in payments-risk contexts remain poorly understood, creating a need for systematic assessment frameworks.

Method: Developed a structured multi-evaluator framework combining a five-criterion rubric with Monte-Carlo scoring to assess LLM reasoning in merchant risk assessment. Five frontier LLMs generated and cross-evaluated MCC risk rationales under attributed and anonymized conditions, using a consensus-deviation metric to eliminate circularity. Validation involved 26 payment-industry experts and ground-truth payment-network data.

Result: Substantial heterogeneity among LLMs: GPT-5.1 and Claude 4.5 showed negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 showed positive bias (+0.77, +0.71). Bias attenuated by 25.8% under anonymization. LLM judges scored +0.46 points above human consensus, with GPT-5.1 and Claude 4.5 aligning more closely with human judgment. Four models showed statistically significant alignment with ground-truth payment data (Spearman rho = 0.56 to 0.77).

Conclusion: The framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the critical need for bias-aware protocols in operational financial settings, as LLM evaluators exhibit systematic biases that affect their reliability.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [118] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: Democratic Preference Optimization (DemPO) uses algorithmic sortition (like citizen assemblies) to make AI preference alignment more representative by training on panels that match population demographics.


<details>
  <summary>Details</summary>
Motivation: Current AI alignment methods like RLHF rely on convenience samples of human raters that systematically over-represent some demographics and under-represent others, leading to biased value alignment that doesn't reflect diverse populations.

Method: DemPO introduces two training schemes: 1) Hard Panel - trains exclusively on preferences from a quota-satisfying mini-public sampled via sortition; 2) Soft Panel - retains all data but reweights each rater by their inclusion probability under the sortition lottery. The authors prove Soft Panel weighting recovers the expected Hard Panel objective in closed form.

Result: Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases (tested on Llama models from 1B to 8B parameters).

Conclusion: Enforcing demographic representativeness at the preference collection stage (via sortition) yields models whose behavior better reflects values elicited from representative publics, outperforming post-hoc correction methods.

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [119] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: SocialVeil: A social learning environment simulating communication barriers (semantic vagueness, sociocultural mismatch, emotional interference) to test LLMs' social intelligence in realistic imperfect settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks assume idealized communication between agents, limiting ability to diagnose whether LLMs can maintain and repair interactions in realistic, imperfect settings with communication barriers.

Method: Developed SocialVeil environment based on systematic literature review of human communication challenges, introducing three disruption types and two barrier-aware metrics (unresolved confusion, mutual understanding). Tested across 720 scenarios with four frontier LLMs.

Result: Communication barriers consistently impair LLM performance: mutual understanding reduced by over 45% on average, confusion elevated by nearly 50%. Human evaluations validate barrier fidelity (ICC≈0.78, Pearson r≈0.80). Adaptation strategies show only modest effects far from barrier-free performance.

Conclusion: This work advances social interaction environments toward real-world communication, opening opportunities for exploring LLM social intelligence in more realistic settings with communication barriers.

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [120] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT is a chaos-aware framework for cross-city traffic prediction that uses chaotic analysis to quantify predictability regimes, enabling regime-adaptive modeling, dynamic spatial learning, and cross-city knowledge transfer for improved few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Traffic prediction in data-scarce cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods fail to capture traffic's inherent chaotic nature for effective few-shot learning.

Method: CAST-CKT employs a chaotic analyser to quantify traffic predictability regimes, driving chaos-aware attention for regime-adaptive temporal modeling, adaptive topology learning for dynamic spatial dependencies, and chaotic consistency-based cross-city alignment for knowledge transfer. It also provides horizon-specific predictions with uncertainty quantification.

Result: Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis.

Conclusion: The proposed chaos-aware framework effectively addresses cross-city traffic prediction challenges, providing improved generalization with theoretical guarantees and practical performance gains in few-shot settings.

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [121] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG introduces causal gating across hierarchical modules to improve graph-based RAG by suppressing spurious correlations and enabling scalable causal reasoning over large knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Current causality approaches are limited to local/single-document contexts and suffer from information isolation in modular graph structures, hindering scalability and cross-module causal reasoning.

Method: HugRAG rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. It explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs.

Result: Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics.

Conclusion: HugRAG establishes a principled foundation for structured, scalable, and causally grounded RAG systems, addressing key limitations in existing graph-based retrieval augmented generation approaches.

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [122] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: Researchers created a private benchmark of 10 research-level math questions to test AI systems' ability to answer genuine research mathematics, with answers temporarily encrypted for evaluation.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether current AI systems can correctly answer authentic research-level mathematics questions that arise naturally in mathematical research, rather than standard textbook problems.

Method: Created a set of 10 previously unpublished mathematics questions that emerged naturally from the authors' research work, with answers known to authors but temporarily encrypted to prevent AI systems from accessing them during testing.

Result: The paper presents a novel benchmark for testing AI mathematical reasoning capabilities, but does not report actual AI performance results yet - it establishes the evaluation framework.

Conclusion: This work introduces a more realistic test for AI mathematical reasoning by using genuine research questions, providing a tool to assess whether AI can handle the complexity of actual mathematical research problems.

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [123] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: DAKS improves retrieval-augmented generation for Chinese Tibetan medicine by addressing density bias in multi-KB settings through KB routing and alignment graph-based evidence fusion.


<details>
  <summary>Details</summary>
Motivation: In domain settings with multiple heterogeneous knowledge bases (encyclopedia, classics, clinical papers), dense encyclopedia entries dominate retrieval even when classics or clinical papers provide more authoritative evidence, creating challenges for grounded question answering.

Method: Two complementary methods: 1) DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and prioritize authoritative sources; 2) Alignment graph guides evidence fusion and coverage-aware packing to improve cross-KB evidence coverage without naive concatenation.

Result: Consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving best CrossEv@5 while maintaining strong faithfulness and citation correctness on a 500-query benchmark with three KBs.

Conclusion: The proposed approach effectively addresses multi-KB retrieval challenges in specialized domains like Chinese Tibetan medicine, improving traceability, reducing hallucinations, and enabling cross-KB verification through intelligent routing and evidence fusion.

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [124] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: Surgery uses attention sink divergence analysis to defend against harmful fine-tuning by suppressing positive sink divergence in attention heads, improving safety alignment by 5-11% across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Harmful fine-tuning can compromise the safety alignment of large language models, creating significant safety risks that need to be addressed.

Method: The method analyzes attention sink divergence patterns, observes that harmful fine-tuning increases positive sink divergence heads, proposes a separable sink divergence hypothesis, and implements Surgery - a fine-tuning defense using sink divergence suppression regularization to steer attention heads toward negative divergence.

Result: Surgery improves defense performance by 5.90% on BeaverTails, 11.25% on HarmBench, and 9.55% on SorryBench benchmarks.

Conclusion: Attention sink divergence provides a measurable signal for harmful fine-tuning detection, and the proposed Surgery method effectively mitigates safety risks by suppressing positive sink divergence in attention heads during fine-tuning.

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [125] [Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models](https://arxiv.org/abs/2602.05240)
*Patrick McGonagle,William Farrelly,Kevin Curran*

Main category: cs.AI

TL;DR: Combining multiple XAI techniques (GRAD-CAM, LRP, SHAP) improves interpretability of CNN brain tumor detection models, achieving 91.24% accuracy on BraTS 2021 dataset with comprehensive explanations from spatial regions to pixel-level details.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and trust in AI-driven medical imaging by providing more comprehensive explanations for deep learning models in critical healthcare tasks like brain tumor detection, addressing limitations of single XAI methods.

Method: Developed custom CNN trained on BraTS 2021 dataset, then integrated multiple XAI techniques: GRAD-CAM for spatial region importance, LRP for pixel-level relevance, and SHAP for feature contribution quantification to provide layered explanations.

Result: Achieved 91.24% accuracy in tumor detection, successfully identified both full and partial tumors, and demonstrated superior explanatory power compared to individual XAI methods by providing comprehensive insights from broad regions to pixel details.

Conclusion: Integrated XAI approach enhances transparency and trust in medical AI systems, showing potential for improving reliability and interpretability in healthcare applications, particularly for critical diagnostic tasks like brain tumor detection.

Abstract: This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.

</details>


### [126] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: TEA: A dynamic in-situ task generation method for evaluating embodied AI agents in unseen 3D environments, using a two-stage interaction-evolution system to automatically generate physically reasonable tasks without external data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks suffer from data contamination and lack scene specificity, making them inadequate for evaluating general intelligent agents in diverse, unseen household environments before real-world deployment.

Method: Two-stage system: 1) Interaction stage where agents actively interact with environment, creating loop between task execution and generation; 2) Evolution stage using task graph modeling to recombine and reuse existing tasks to generate new ones without external data.

Result: TEA automatically generated 87,876 tasks across 10 unseen scenes in two cycles, verified as physically reasonable and encompassing essential daily cognitive capabilities. SOTA models performed poorly on basic perception tasks, lacked 3D interaction awareness, and showed high sensitivity to task types in reasoning.

Conclusion: In-situ evaluation is necessary before deploying agents into real-world human environments, as current models despite excelling on public benchmarks show significant deficiencies in basic perception and interaction capabilities in unseen settings.

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [127] [Beyond Cosine Similarity](https://arxiv.org/abs/2602.05266)
*Xinbo Ai*

Main category: cs.AI

TL;DR: Recos is a new similarity metric that outperforms cosine similarity by using a tighter bound than Cauchy-Schwarz, normalizing by sorted vector components to capture nonlinear semantic relationships.


<details>
  <summary>Details</summary>
Motivation: Cosine similarity is limited to linear relationships due to its mathematical foundation in the Cauchy-Schwarz inequality, which fails to model the complex, nonlinear structures of real-world semantic spaces.

Method: Derived a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound, leading to recos - a similarity metric that normalizes the dot product by the sorted vector components, relaxing perfect similarity from strict linear dependence to ordinal concordance.

Result: Extensive experiments across 11 embedding models (static, contextualized, and universal) show recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks.

Conclusion: Recos is established as a mathematically principled and empirically superior alternative to cosine similarity, offering enhanced accuracy for semantic analysis in complex embedding spaces.

Abstract: Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.

</details>


### [128] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: Framework integrates LLMs in security management with consistency checking and feedback loops to reduce hallucinations and improve decision-making.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for security management tasks like incident response planning, but their unreliability and tendency to hallucinate present significant challenges that need to be addressed.

Method: Principled framework where LLM generates candidate actions checked for consistency with system constraints and lookahead predictions; when consistency is low, collects external feedback (e.g., from digital twin) and refines actions through in-context learning.

Result: Framework reduces recovery times by up to 30% compared to frontier LLMs in incident response experiments on four public datasets; theoretical guarantees on hallucination risk control and regret bounds.

Conclusion: The proposed framework effectively mitigates LLM hallucinations in security management through consistency checking and feedback loops, providing both practical improvements and theoretical guarantees for reliable decision support.

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [129] [Position: Universal Time Series Foundation Models Rest on a Category Error](https://arxiv.org/abs/2602.05287)
*Xilin Dai,Wanxu Cai,Zhijian Xu,Qiang Xu*

Main category: cs.AI

TL;DR: The paper argues against universal foundation models for time series, proposing instead causal control agents that orchestrate specialized solvers for robust adaptation to distributional drift.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from recognizing that treating time series as a single modality is a category error, as different time series domains (finance, fluid dynamics, etc.) have fundamentally incompatible generative processes. Current universal foundation models degenerate into expensive generic filters that fail to generalize under distributional drift.

Method: The paper introduces the "Autoregressive Blindness Bound" theory showing history-only models cannot predict intervention-driven regime shifts. It advocates replacing universality with a Causal Control Agent paradigm where an agent uses external context to orchestrate a hierarchy of specialized solvers, including frozen domain experts and lightweight Just-in-Time adaptors.

Result: Theoretical demonstration that monolithic universal models are fundamentally limited by the Autoregressive Blindness Bound, and a proposed alternative architecture that can better handle distributional drift through causal reasoning and specialized component orchestration.

Conclusion: The paper calls for a paradigm shift from pursuing universal foundation models to building causal control systems, with benchmark focus moving from "Zero-Shot Accuracy" to "Drift Adaptation Speed" to prioritize robust, control-theoretic approaches to time series analysis.

Abstract: This position paper argues that the pursuit of "Universal Foundation Models for Time Series" rests on a fundamental category error, mistaking a structural Container for a semantic Modality. We contend that because time series hold incompatible generative processes (e.g., finance vs. fluid dynamics), monolithic models degenerate into expensive "Generic Filters" that fail to generalize under distributional drift. To address this, we introduce the "Autoregressive Blindness Bound," a theoretical limit proving that history-only models cannot predict intervention-driven regime shifts. We advocate replacing universality with a Causal Control Agent paradigm, where an agent leverages external context to orchestrate a hierarchy of specialized solvers, from frozen domain experts to lightweight Just-in-Time adaptors. We conclude by calling for a shift in benchmarks from "Zero-Shot Accuracy" to "Drift Adaptation Speed" to prioritize robust, control-theoretic systems.

</details>


### [130] [Aspect-Aware MOOC Recommendation in a Heterogeneous Network](https://arxiv.org/abs/2602.05297)
*Seongyeub Chu,Jongwoo Kim,Mun Yong Yi*

Main category: cs.AI

TL;DR: AMR is a novel graph-based MOOC recommendation framework that automatically discovers metapaths and models path-specific aspects using semantic node embeddings, outperforming traditional methods and GNN baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional MOOC recommendation methods (collaborative/content-based filtering) suffer from data sparsity and over-specialization. Existing graph-based approaches rely on manually predefined metapaths, which capture only superficial relationships and require significant domain expertise and engineering costs.

Method: AMR automatically discovers metapaths through bi-directional walks, embeds semantic content of nodes within each metapath, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these as edge features in learner-learner and KC-KC subgraphs for fine-grained recommendations.

Result: Extensive experiments on MOOCCube and PEEK datasets show AMR consistently outperforms state-of-the-art GNN baselines across HR@K and nDCG@K metrics. Analysis confirms AMR effectively captures rich path-specific aspect information for more accurate recommendations.

Conclusion: AMR overcomes limitations of traditional methods and predefined metapath approaches by automatically discovering metapaths and modeling semantic aspects, achieving superior MOOC recommendation performance through fine-grained, semantically informed knowledge component recommendations.

Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.

</details>


### [131] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5 matches or outperforms trained business students in realistic negotiation scenarios, showing AGI-level performance in high-stakes economic tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' negotiation capabilities - a complex business task requiring strategic reasoning, theory of mind, and economic value creation - which is crucial for assessing their readiness for real-world economic deployment.

Method: Created PieArena, a large-scale negotiation benchmark using realistic scenarios from an MBA negotiation course at an elite business school, featuring multi-agent interactions and comprehensive behavioral profiling beyond just deal outcomes.

Result: GPT-5 achieved AGI-level performance matching or outperforming trained business students. Joint-intentionality scaffolding helped mid/lower-tier LMs more than frontier models. Behavioral analysis revealed heterogeneity in deception, computation accuracy, instruction compliance, and reputation perception.

Conclusion: Frontier language agents are intellectually and psychologically capable for high-stakes economic deployment, but robustness and trustworthiness deficiencies remain key challenges.

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [132] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct is a framework that trains LLM agents for long-horizon planning through lookahead reasoning distillation and Monte-Carlo value estimation, achieving state-of-the-art performance on interactive environments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents struggle with long-horizon planning in interactive environments due to compounding errors when simulating future states, creating a need for more accurate foresight capabilities.

Method: Two-stage training: 1) Grounded LookAhead Distillation (GLAD) - supervised fine-tuning on compressed search tree trajectories to internalize foresight reasoning; 2) Monte-Carlo Critic (MC-Critic) - plug-and-play value estimator using lightweight environment rollouts to enhance policy optimization.

Result: ProAct significantly improves planning accuracy on both stochastic (2048) and deterministic (Sokoban) environments. A 4B parameter model outperforms all open-source baselines and rivals state-of-the-art closed-source models, with robust generalization to unseen environments.

Conclusion: ProAct effectively addresses long-horizon planning challenges by distilling lookahead reasoning and providing stable value estimation, enabling LLM agents to achieve superior performance in interactive environments without expensive inference-time search.

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [133] [AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction](https://arxiv.org/abs/2602.05353)
*Ruijie Shi,Houbin Zhang,Yuecheng Han,Yuheng Wang,Jingru Fan,Runde Yang,Yufan Dang,Huatao Li,Dewen Liu,Yuan Cheng,Chen Qian*

Main category: cs.AI

TL;DR: AgentXRay reconstructs interpretable workflows from black-box LLM agents using only input-output access, formulating it as combinatorial optimization solved via Monte Carlo Tree Search with pruning.


<details>
  <summary>Details</summary>
Motivation: LLM-based agentic systems are powerful but opaque black boxes, making them difficult to interpret and control. Current frameworks lack transparency, and users need editable, interpretable workflows to understand and modify agent behavior.

Method: AgentXRay formulates Agentic Workflow Reconstruction (AWR) as combinatorial optimization over discrete agent roles and tool invocations. Uses Monte Carlo Tree Search enhanced with Red-Black Pruning to navigate the search space, matching target outputs via proxy metrics without accessing model internals.

Result: AgentXRay achieves higher proxy similarity to target outputs and reduces token consumption compared to unpruned search. Enables deeper workflow exploration under fixed iteration budgets across diverse domains.

Conclusion: AgentXRay successfully reconstructs interpretable, editable workflows from black-box LLM agents, providing transparency and control without requiring access to model parameters, advancing the interpretability of agentic systems.

Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.

</details>


### [134] [PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents](https://arxiv.org/abs/2602.05354)
*Shifat E. Arman,Syed Nazmus Sakib,Tapodhir Karmakar Taton,Nafiul Haque,Shahrear Bin Amin*

Main category: cs.AI

TL;DR: PATHWAYS benchmark reveals web agents struggle with multi-step decision tasks requiring discovery and integration of hidden contextual information, showing limitations in adaptive investigation and evidence integration.


<details>
  <summary>Details</summary>
Motivation: To test whether web-based agents can discover and correctly use hidden contextual information in multi-step decision tasks, revealing limitations in current agent architectures.

Method: Created PATHWAYS benchmark with 250 multi-step decision tasks requiring agents to navigate web pages and discover hidden evidence, testing both closed and open models.

Result: Agents navigate to relevant pages but retrieve decisive hidden evidence in only small fraction of cases; performance drops sharply when overturning misleading signals; agents hallucinate reasoning; fail to integrate discovered context; explicit instructions improve discovery but reduce accuracy.

Conclusion: Current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override, revealing fundamental limitations in multi-step decision-making capabilities.

Abstract: We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.

</details>


### [135] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT is a novel quantization framework that resolves feature co-adaptation in residual binarization by enforcing a residual hierarchy, achieving state-of-the-art 2-bit performance with 4.49× speedup over full-precision models.


<details>
  <summary>Details</summary>
Motivation: Extreme quantization of LLMs creates a trade-off between efficiency and performance. Residual binarization enables hardware-friendly inference but suffers from pathological feature co-adaptation where parallel binary paths learn redundant features, degrading error compensation and limiting model capacity.

Method: RaBiT algorithmically enforces a residual hierarchy by sequentially deriving each binary path from a single shared full-precision weight, ensuring each path corrects the error of the preceding one. It uses robust initialization prioritizing functional preservation over weight approximation.

Result: Achieves state-of-the-art performance for 2-bit quantization, rivals hardware-intensive Vector Quantization methods, and delivers 4.49× inference speed-up over full-precision models on RTX 4090.

Conclusion: RaBiT redefines the 2-bit accuracy-efficiency frontier by resolving co-adaptation through residual hierarchy, enabling efficient deployment of LLMs with minimal performance degradation.

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [136] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: Small medical LLMs (under 10B parameters) show promise for ophthalmology patient queries, with Meerkat-7B performing best and GPT-4-Turbo evaluation aligning well with clinician grading, though gaps in clinical depth remain.


<details>
  <summary>Details</summary>
Motivation: Domain-specific LLMs are increasingly used in ophthalmology for patient education and clinical decision support, requiring rigorous evaluation to ensure safety and accuracy before clinical deployment.

Method: Evaluated four small medical LLMs (Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, MedLLaMA3-v20) on 180 ophthalmology patient queries, generating 2160 responses. Used three ophthalmologists of varying seniority and GPT-4-Turbo to evaluate responses using the S.C.O.R.E. framework (Safety, Consensus, Objectivity, Reproducibility, Explainability) on a 5-point Likert scale.

Result: Meerkat-7B performed best across all clinician groups (mean scores: 3.44 Senior Consultants, 4.08 Consultants, 4.18 Residents). MedLLaMA3-v20 performed worst with 25.5% of responses containing hallucinations or misleading content. GPT-4-Turbo evaluation showed strong alignment with clinicians (Spearman rho=0.80, Kendall tau=0.67), though Senior Consultants graded more conservatively.

Conclusion: Medical LLMs show potential for safe ophthalmology question answering, but gaps in clinical depth and consensus remain. LLM-based evaluation is feasible for large-scale benchmarking, but hybrid automated-clinician review frameworks are needed for safe clinical deployment.

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [137] [Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation](https://arxiv.org/abs/2602.05403)
*Chenghua Gong,Yihang Jiang,Hao Li,Rui Sun,Juyuan Zhang,Tianjun Gu,Liming Pan,Linyuan Lü*

Main category: cs.AI

TL;DR: OPINN: A physics-informed neural framework using Diffusion-Convection-Reaction system for opinion dynamics modeling that combines neural networks with physical priors for better forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing opinion dynamics methods lack comprehensive physical systems and struggle to integrate dynamics from local, global, and endogenous levels. Penalty-based constraints in current approaches fail to deeply encode physical priors, leading to optimization problems and poor alignment between latent representations and physical transparency.

Method: Proposes OPINN framework that interprets opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Builds on Neural ODEs to coordinate neural networks with physical priors, creating a physics-informed neural framework for opinion modeling.

Result: OPINN achieves state-of-the-art performance in opinion evolution forecasting on both real-world and synthetic datasets.

Conclusion: OPINN offers a promising paradigm for integrating cyber, physical, and social systems through physics-informed neural modeling of opinion dynamics.

Abstract: Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.

</details>


### [138] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim is a comprehensive simulation framework for evaluating LLM-based automation in hospital administrative workflows, addressing the gap in prior work that focused only on isolated subtasks.


<details>
  <summary>Details</summary>
Motivation: Hospital administration processes over 10,000 requests daily, creating interest in LLM automation, but existing research only covers patient-physician interactions or isolated administrative subtasks, missing the complexity of real administrative workflows.

Method: H-AdminSim combines realistic data generation with multi-agent-based simulation of hospital administrative workflows, integrates with FHIR for interoperability, and uses detailed rubrics for quantitative evaluation of LLMs.

Result: The framework provides a unified, interoperable environment for testing administrative workflows across different hospital settings, enabling systematic comparison of LLM performance in administrative automation.

Conclusion: H-AdminSim serves as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation in complex hospital environments.

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [139] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: THOR is an inductive link prediction method for hyper-relational knowledge graphs that learns transferable structural patterns across different vocabularies, outperforming existing approaches by significant margins.


<details>
  <summary>Details</summary>
Motivation: Existing hyper-relational KG link prediction methods are mostly transductive, limited to specific vocabularies and lacking generalizability to unseen entities/relations. There's a need for inductive approaches that can transfer learned patterns across different KGs.

Method: Proposes THOR with two key components: 1) relation and entity foundation graphs that model fundamental inter- and intra-fact interactions agnostic to specific vocabularies, 2) two parallel graph encoders followed by a transformer decoder for efficient masked training and fully-inductive inference.

Result: Outperforms baselines on 12 datasets with 66.1%, 55.9%, and 20.4% improvement over best-performing rule-based, semi-inductive, and fully-inductive techniques respectively. Ablation studies confirm key design factors capture structural invariance transferable across HKGs.

Conclusion: THOR successfully addresses the inductive link prediction challenge for hyper-relational KGs by learning transferable structural patterns through foundation graphs and dual-encoder architecture, enabling generalization to unseen vocabularies.

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [140] [M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining](https://arxiv.org/abs/2602.05429)
*Rui Lv,Juncheng Mo,Tianyi Chu,Chen Rao,Hongyi Jing,Jiajie Teng,Jiafu Chen,Shiqi Zhang,Liangzi Ding,Shuo Fang,Huaizhong Lin,Ziqiang Dang,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: M²-Miner: A low-cost automated mobile GUI agent data-mining framework using Monte Carlo Tree Search and multi-agent collaboration to generate high-quality user-behavior trajectory data for training GUI agents.


<details>
  <summary>Details</summary>
Motivation: Current GUI agent data collection methods face three critical challenges: high construction cost, poor data quality, and low data richness. Manual annotation is expensive, and existing automated approaches don't produce sufficient high-quality, diverse training data for effective GUI agents.

Method: Proposes M²-Miner framework based on Monte Carlo Tree Search with three collaborative agents: InferAgent (guidance), OrchestraAgent (acceleration), and JudgeAgent (evaluation). Includes intent recycling strategy to extract extra valuable trajectories and progressive model-in-the-loop training to improve mining success rate.

Result: Extensive experiments show that GUI agents fine-tuned using M²-Miner's mined data achieve state-of-the-art performance on several commonly used mobile GUI benchmarks, demonstrating the effectiveness of the automated data mining approach.

Conclusion: M²-Miner provides a low-cost, automated solution for generating high-quality GUI agent training data, addressing key challenges in data construction cost, quality, and richness. The framework enables better GUI agent performance and will be released to facilitate community research.

Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.

</details>


### [141] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: TSFMs with spike regularization outperform traditional models in volatile day-ahead electricity price forecasting, achieving up to 37.4% MAPE improvement.


<details>
  <summary>Details</summary>
Motivation: Electricity price forecasting is challenging due to volatility and nonlinearity, and while TSFMs show promise in general time series tasks, their effectiveness in volatile day-ahead EPF markets remains underexplored.

Method: Proposes spike regularization strategy and evaluates multiple TSFMs (TTMs, MOIRAI, MOMENT, TimesFM) against traditional models (ARIMA, LSTM, CNN-LSTM) using Singapore's half-hourly wholesale market data with exogenous factors like weather and calendar variables.

Result: TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings.

Conclusion: The findings provide practical guidance for improving forecast accuracy and decision-making in volatile electricity markets, demonstrating TSFMs' superiority over traditional methods.

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [142] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: OD-CRL improves conditional representation learning by optimizing orthogonal semantic bases and suppressing interference through null-space projection.


<details>
  <summary>Details</summary>
Motivation: Existing conditional representation learning methods are sensitive to subspace basis selection and vulnerable to inter-subspace interference between different semantic concepts.

Method: Proposes OD-CRL with two components: Adaptive Orthogonal Basis Optimization (AOBO) using SVD with curvature-based truncation to construct orthogonal semantic bases, and Null-Space Denoising Projection (NSDP) to suppress non-target semantic interference by projecting embeddings onto null spaces of irrelevant subspaces.

Result: Extensive experiments across customized clustering, classification, and retrieval tasks show OD-CRL achieves state-of-the-art performance with superior generalization.

Conclusion: OD-CRL effectively addresses basis sensitivity and interference issues in conditional representation learning, establishing a new SOTA framework for customized tasks.

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [143] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: Developed a data-driven discrete-event simulator from VR study data to enable scalable evaluation of school security interventions, particularly robot-based shooter strategies, overcoming limitations of human participant recruitment.


<details>
  <summary>Details</summary>
Motivation: VR is effective for evaluating school security measures but requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult, especially for learning effective intervention strategies that need many training episodes.

Method: Created a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies, then used it to examine robot-based shooter intervention strategies.

Result: The DES was shown to reproduce key empirical patterns from VR studies, enabling scalable evaluation and learning of intervention strategies that would be infeasible to train directly with human subjects.

Conclusion: Demonstrated a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions, bridging VR experiments with computational simulation.

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>


### [144] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE is a hands-free alignment framework that uses adversarial learning with instructive verbal feedback to help LLMs internalize reasoning principles without relying on costly scalar rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional RL for LLMs suffers from a "reward bottleneck" - scalar rewards are costly to scale, brittle across domains, and blind to solution logic, preventing models from developing deep reasoning understanding.

Method: ALIVE unifies problem posing, solving, and judging within a single policy model using adversarial learning with instructive verbal feedback, enabling models to internalize evaluative criteria directly from raw corpora.

Result: ALIVE achieves accuracy gains, improved cross-domain generalization, and higher self-correction rates across mathematical reasoning, code generation, and logical inference benchmarks with identical data and compute.

Conclusion: ALIVE provides a scalable foundation for general-purpose reasoning alignment without human supervision by fostering self-sustaining capability growth through internalized reasoning faculties.

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [145] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former is a hierarchical interaction learning method that models compound-protein interactions at multiple biological levels (atom-atom, motif-motif, atom-motif) to better align with chemical reality and improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for compound-protein interaction prediction operate at atomic level but don't align with chemical reality where molecular fragments/motifs are the primary units of biological recognition and binding. There's a need for models that incorporate the biological role of motifs in drug-target interactions.

Method: Phi-former uses pairwise hierarchical interaction representation learning, representing compounds and proteins hierarchically with a pairwise pre-training framework. It models interactions systematically across three levels: atom-atom, motif-motif, and atom-motif. The method includes intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial.

Result: Phi-former achieves superior performance on CPI-related tasks compared to existing methods. A case study demonstrates that the method can accurately identify specific atoms or motifs activated in CPIs, providing interpretable model explanations.

Conclusion: The hierarchical approach that incorporates motif-level interactions aligns better with biological reality and provides interpretable insights that can guide rational drug design and support precision medicine applications.

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [146] [SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration](https://arxiv.org/abs/2602.05499)
*Hanyu Wei,Zunhai Su,Peng Lu,Chao Li,Spandan Tiwari,Ashish Sirasao,Yuhan Dong*

Main category: cs.AI

TL;DR: SDFP is a training-free, plug-and-play framework that accelerates LLM decoding via Fisher Information Trace-based layer pruning to create a lightweight draft model for speculative decoding, achieving 1.32x-1.5x speedup without changing output distribution.


<details>
  <summary>Details</summary>
Motivation: LLMs enable interactive multimedia applications but suffer from high latency due to autoregressive decoding. Existing speculative decoding methods require costly draft model acquisition, tuning, and maintenance, often needing auxiliary training or complex optimization.

Method: SDFP uses Fisher Information Trace (FIT)-based layer pruning to remove low-impact layers from a given LLM, creating a compact draft model. This approach uses layer sensitivity as a proxy for output perturbation, preserves compatibility with the original model for speculative verification, and requires no additional training or hyperparameter tuning.

Result: SDFP achieves 1.32x-1.5x decoding speedup across benchmarks while maintaining the target model's output distribution unchanged, supporting low-latency multimedia applications without separate draft model maintenance.

Conclusion: SDFP provides a practical, deployment-friendly solution for accelerating LLM inference through training-free draft model construction, eliminating the need for auxiliary training, hyperparameter tuning, or separately maintained drafts while preserving output quality.

Abstract: Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.

</details>


### [147] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: New multimodal dataset and AI model for ameloblastoma diagnosis with improved classification accuracy from 46.2% to 65.9% and tissue detection F1-score from 43.0% to 90.3%.


<details>
  <summary>Details</summary>
Motivation: Existing AI resources for maxillofacial pathology lack comprehensive ameloblastoma coverage and format consistency needed for direct model training, limiting personalized diagnostic support.

Method: Curated multimodal dataset integrating annotated radiological, histopathological, and intraoral clinical images with structured case report data. Used NLP for feature extraction from textual reports, domain-specific preprocessing/augmentation for images, and developed multimodal deep learning model for variant classification, recurrence risk assessment, and surgical planning.

Result: Substantial performance improvements: variant classification accuracy increased from 46.2% to 65.9%, abnormal tissue detection F1-score improved from 43.0% to 90.3%. Model accepts clinical inputs (presenting complaint, age, gender) for personalized inference.

Conclusion: This work advances patient-specific decision support by providing both a robust multimodal dataset and adaptable AI framework for ameloblastoma diagnosis, outperforming existing resources like MultiCaRe.

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [148] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: SPT introduces a method to detect hidden misalignment in LLMs by fine-tuning a separate "honest persona" that reviews main model responses using latent state access.


<details>
  <summary>Details</summary>
Motivation: Current auditing methods fail to detect concealed misalignment in LLMs - black-box methods can't distinguish misaligned outputs, and mechanistic interpretability doesn't scale with model capabilities.

Method: Fine-tune a second "honest persona" into LoRA parameters that remain inactive during normal operation, then activate them with a trigger string to review responses while accessing the main model's latent states.

Result: Achieves 96% overall accuracy on Anthropic Auditing Game Model Organism benchmark, compared to near 0% accuracy reported by Anthropic, revealing latent knowledge like fictional biases.

Conclusion: SPT successfully detects concealed misalignment by leveraging internal model states, providing a scalable approach to auditing LLMs that standard methods miss.

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [149] [Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach](https://arxiv.org/abs/2602.05533)
*Zhengyi Guo,Wenpin Tang,Renyuan Xu*

Main category: cs.AI

TL;DR: A principled conditional diffusion guidance framework using Doob's h-transform for hard constraint satisfaction with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing safety-critical applications and rare-event simulation where soft/reward-based guidance methods cannot guarantee constraint satisfaction with probability one.

Method: Developed conditional diffusion guidance based on Doob's h-transform, martingale representation, and quadratic variation process. Proposed two off-policy learning algorithms (martingale loss and martingale-covariation loss) to estimate h and its gradient using pretrained model trajectories.

Result: Provides non-asymptotic guarantees for conditional samplers in total variation and Wasserstein distances, explicitly characterizing impact of approximation errors. Numerical experiments demonstrate effectiveness in enforcing hard constraints and generating rare-event samples.

Conclusion: A theoretically grounded framework for conditional diffusion generation with guaranteed hard constraint satisfaction, addressing limitations of existing soft guidance methods.

Abstract: We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.

</details>


### [150] [Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation](https://arxiv.org/abs/2602.05544)
*Fahad Anwaar,Adil Mehmood Khan,Muhammad Khalid,Usman Zia,Kezhi Wang*

Main category: cs.AI

TL;DR: RGCF-XRec is a hybrid framework that integrates reasoning-guided collaborative filtering knowledge into LLMs for explainable sequential recommendations in one step, improving performance and reducing cold-start gaps.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for explainable recommendations but ignore collaborative signals, while existing methods treat recommendation and explanation as separate tasks, creating memory inefficiencies.

Method: Introduces reasoning-guided collaborative filtering knowledge through contextual prompting, uses a 4-dimension scoring mechanism (coherence, completeness, relevance, consistency) to filter noisy reasoning, and employs a unified representation learning network to encode collaborative and semantic signals for structured LLM prompting.

Result: Consistent improvements across Amazon datasets: HR@10 improved by 7.38% in Sports and 4.59% in Toys; ROUGE-L improved by 8.02% and 3.49%; reduced cold-start performance gap with 14.5% gains in cold-start and 11.9% in warm start; zero-shot HR@5 improved by 18.54% in Beauty and 23.16% in Toys.

Conclusion: RGCF-XRec effectively integrates collaborative filtering knowledge with LLMs for explainable sequential recommendations, demonstrating improved performance, better generalization, robustness, and training efficiency with a lightweight LLaMA 3.2-3B backbone for real-world scalability.

Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\% in Sports and 4.59\% in Toys, along with ROUGE-L by 8.02\% and 3.49\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\% in cold-start and 11.9\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\% in Beauty and 23.16\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.

</details>


### [151] [TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570)
*Yikun Zong,Cheston Tan*

Main category: cs.AI

TL;DR: VLMs fail at Tangram puzzles (IoU 0.41 single-piece, 0.23 two-piece), but test-time self-refinement with ICL and reward loops improves IoU from 0.63 to 0.932 without retraining.


<details>
  <summary>Details</summary>
Motivation: Humans excel at spatial reasoning through mental rotation and iterative refinement, but current VLMs show systematic failures in geometric reasoning. The paper explores whether models can iteratively refine predictions at test time without parameter updates.

Method: A test-time self-refinement framework combining in-context learning (ICL) with reward-guided feedback loops. Training-free verifier-refiner agent applies recursive refinement loops based on geometric consistency feedback.

Result: VLMs perform poorly on Tangram tasks (average IoU 0.41 single-piece, 0.23 two-piece). Self-refinement framework improves IoU from 0.63 to 0.932 on medium-triangle cases without model retraining.

Conclusion: Incorporating human-inspired iterative refinement through ICL and reward loops significantly enhances geometric reasoning in VLMs, advancing self-improving AI in continuous spatial domains.

Abstract: Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.

</details>


### [152] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: LLMs can predict individual-level cognitive biases and emulate biased human behavior in interactive decision-making scenarios, showing differences between models in how they align with human behavior.


<details>
  <summary>Details</summary>
Motivation: While LLMs are known to reproduce general cognitive biases, the paper investigates whether they can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors like cognitive load interact with these biases in interactive settings.

Method: Adapted three well-established decision scenarios into conversational settings, conducted human experiment (N=1100) with chatbot facilitating decision-making through simple/complex dialogues, then used participant demographics and dialogue transcripts to simulate conditions with GPT-4 and GPT-5 LLMs.

Result: Human experiment revealed robust biases; LLMs reproduced human biases with precision; notable differences between models in how they aligned with human behavior.

Conclusion: Findings have important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts, as LLMs can effectively emulate individual-level biased decision-making.

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [153] [BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages](https://arxiv.org/abs/2602.05599)
*Subhadip Maji,Arnab Bhattacharya*

Main category: cs.AI

TL;DR: GETR (Graph-Enhanced Token Representation) method for cross-lingual knowledge transfer outperforms baselines by 13-27 percentage points on low-resource language tasks.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages lag behind high-resource languages in NLP due to data scarcity and insufficient linguistic resources, creating a need for effective cross-lingual knowledge transfer methods.

Method: Proposes GETR (Graph-Enhanced Token Representation), a GNN-based approach for cross-lingual knowledge transfer, compared against two baselines: augmentation in hidden layers and token embedding transfer through token translation.

Result: GETR significantly outperforms existing multilingual and cross-lingual baselines: 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20-27 percentage point improvements on simulated low-resource languages (Marathi, Bangla, Malayalam) for sentiment classification and NER.

Conclusion: Graph-based approaches like GETR are effective for cross-lingual knowledge transfer to low-resource languages, with detailed analysis identifying key factors for successful transfer in linguistic contexts.

Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.

</details>


### [154] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: Resin (Reactive Signal Inference) combines probabilistic logic with reactive programming, using Reactive Circuits to enable efficient exact inference by adapting computation to input signal volatility, achieving orders of magnitude speedup in drone swarm simulations.


<details>
  <summary>Details</summary>
Motivation: Exact inference in probabilistic models is computationally expensive for autonomous agents in dynamic environments. Existing methods inefficiently re-evaluate entire models upon changes, failing to exploit heterogeneous update rates in real-world information streams.

Method: Introduce Resin (Reactive Signal Inference), a probabilistic programming language merging probabilistic logic with reactive programming. Propose Reactive Circuits (RCs) as meta-structures over Algebraic Circuits and asynchronous data streams - time-dynamic DAGs that autonomously adapt based on input signal volatility.

Result: In high-fidelity drone swarm simulations, the approach achieves several orders of magnitude speedup over frequency-agnostic inference. RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and enabling reactive real-time reasoning.

Conclusion: By partitioning computations based on estimated Frequency of Change in asynchronous inputs, large inference tasks can be decomposed into memoized sub-problems, ensuring only affected model components are re-evaluated, drastically reducing redundant computation in streaming contexts.

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [155] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: Generative Ontology combines LLM creativity with ontological constraints to generate structurally valid, novel artifacts across domains like game design.


<details>
  <summary>Details</summary>
Motivation: Traditional ontologies describe structure but can't generate novel artifacts, while LLMs generate fluently but produce structurally invalid outputs with hallucinations. There's a need to combine ontological rigor with LLM creativity.

Method: Framework encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. Uses multi-agent pipeline with specialized roles (Mechanics Architect, Theme Weaver, Balance Critic) each with professional "anxiety" to prevent shallow outputs. Incorporates retrieval-augmented generation from existing exemplars and iterative validation.

Result: Demonstrated through GameGrammar system that generates complete, playable tabletop game designs from thematic prompts. Outputs satisfy ontological constraints while remaining genuinely creative with mechanisms, components, victory conditions, and setup instructions.

Conclusion: Pattern generalizes to any domain with expert vocabulary, validity constraints, and accumulated exemplars. Constraints enable rather than limit creativity - ontology makes structured generation possible just as grammar enables poetry.

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [156] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: Survey paper reviewing graph-based memory systems for LLM agents, covering taxonomy, lifecycle techniques, tools, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: Memory is crucial for LLM agents handling complex long-horizon tasks, and graph structures excel at modeling relational dependencies, organizing hierarchical information, and enabling efficient retrieval - making them ideal for agent memory systems.

Method: Comprehensive survey approach: 1) Taxonomy of agent memory types with graph-based implementation view, 2) Systematic analysis of graph-based memory lifecycle techniques (extraction, storage, retrieval, evolution), 3) Summary of open-source libraries and benchmarks, 4) Exploration of application scenarios, 5) Identification of challenges and future directions.

Result: Provides structured framework for understanding graph-based agent memory, collects comprehensive resources (papers, data, projects) in GitHub repository, and offers actionable insights for developing more efficient and reliable memory systems.

Conclusion: Graph-based memory systems are powerful for LLM agents, and this survey provides a comprehensive roadmap for advancing the field through systematic analysis, resource collection, and identification of critical research directions.

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [157] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: LLM inference energy consumption follows non-linear efficiency regimes with sweet spots, not simple linear patterns. Proposed analytical model achieves 1.79% MAPE accuracy across models.


<details>
  <summary>Details</summary>
Motivation: Existing energy estimation methods use oversimplified linear functions of sequence lengths, but real LLM inference shows complex non-linear energy efficiency patterns that need accurate modeling for sustainable AI deployment.

Method: Developed analytical model based on Transformer architecture's computational and memory-access complexity. Evaluated using TensorRT-LLM on NVIDIA H100 GPUs across diverse LLMs (1B-9B parameters) with sequence lengths from 64-4096 tokens.

Result: Model achieves mean MAPE of 1.79% across tested models. Identified energy efficiency sweet spots: peak efficiency at short-to-moderate inputs with medium-length outputs, sharp drops for long inputs or very short outputs.

Conclusion: Aligning sequence lengths with identified efficiency sweet spots can substantially reduce energy consumption, enabling informed truncation, summarization, and adaptive generation strategies in production systems.

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [158] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA replaces explicit storage of LoRA basis vectors with lightweight nonlinear generation using RBFs, achieving higher parameter efficiency and better fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA suffers from substantial parameter growth when increasing model capacity, as it requires adding more explicit basis vectors. The authors found these basis vectors exhibit significant parameter redundancy that can be compactly represented.

Method: GenLoRA replaces explicit basis vector storage with nonlinear basis vector generation using radial basis functions (RBFs). It maintains latent vectors for each low-rank matrix and uses lightweight RBFs to synthesize basis vectors, requiring far fewer parameters per basis vector.

Result: Extensive experiments across multiple datasets and architectures show GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance compared to standard LoRA.

Conclusion: GenLoRA provides a more parameter-efficient alternative to standard LoRA by generating basis vectors instead of storing them explicitly, enabling higher capacity fine-tuning with fewer parameters while maintaining or improving performance.

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [159] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: APO replaces KL regularization's shape matching with support coverage to prevent recursive space collapse in RLVR, enabling aggressive sharpening while maintaining diversity.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR suffers from Recursive Space Contraction (RSC) where valid alternatives vanish due to combined positive sharpening and negative squeezing dynamics. KL regularization creates gradient conflicts by forcing full density matching rather than supporting efficient sharpening.

Method: Anchored Policy Optimization (APO) shifts from global Shape Matching to Support Coverage. It defines a Safe Manifold based on reference model's high-confidence support, allowing aggressive sharpening for efficiency while using restorative forces during error correction to prevent collapse.

Result: APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring Pass@K diversity typically lost by standard policy gradient methods, as shown in mathematical benchmarks.

Conclusion: APO provides a gradient-aligned mechanism for maximizing support coverage with Elastic Recovery that re-inflates valid branches, preventing irreversible collapse in RLVR systems.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [160] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: RLFKV: Reinforcement Learning with Fine-grained Knowledge Verification improves financial RAG systems by decomposing responses into atomic units for better alignment with retrieved documents while maintaining informativeness.


<details>
  <summary>Details</summary>
Motivation: Financial RAG systems suffer from hallucinations despite using retrieved documents, creating inconsistencies between generated responses and retrieved information that need to be addressed.

Method: Proposes RLFKV framework that decomposes financial responses into atomic knowledge units, computes fine-grained faithful reward for each unit, and adds informativeness reward to prevent reward hacking and maintain response completeness.

Result: Experiments on Financial Data Description (FDD) task and new FDD-ANT dataset show consistent improvements, confirming the effectiveness of the approach.

Conclusion: The RLFKV framework successfully mitigates hallucinations in financial RAG systems by providing more precise optimization signals through fine-grained knowledge verification while maintaining response informativeness.

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [161] [LeakBoost: Perceptual-Loss-Based Membership Inference Attack](https://arxiv.org/abs/2602.05748)
*Amit Kravchik Taub,Fred M. Grabovski,Guy Amit,Yisroel Mirsky*

Main category: cs.AI

TL;DR: LeakBoost is a perceptual-loss-based interrogation framework that actively probes model representations to expose membership signals, boosting existing membership inference attacks by synthesizing optimized interrogation images.


<details>
  <summary>Details</summary>
Motivation: Existing membership inference attacks rely on static indicators like loss or confidence and don't leverage dynamic model behavior when actively probed, limiting their effectiveness in exposing privacy risks.

Method: LeakBoost synthesizes interrogation images by optimizing a perceptual (activation-space) objective to amplify representational differences between members and non-members, then uses off-the-shelf membership detectors without modification.

Result: LeakBoost substantially improves existing methods, raising AUC from near-chance levels (0.53-0.62) to 0.81-0.88 and increasing TPR at 1% FPR by over an order of magnitude across multiple datasets and architectures.

Conclusion: LeakBoost offers a modular, computationally efficient way to assess privacy risks in white-box settings, advancing dynamic membership inference by showing that deeper layers and short, low-learning-rate optimization produce strongest leakage.

Abstract: Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.

</details>


### [162] [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762)
*Andrei Kozyrev,Nikita Khramov,Denis Lochmelis,Valerio Morelli,Gleb Solovev,Anton Podkopaev*

Main category: cs.AI

TL;DR: Automatic AI agent optimization methods show limited success in improving Rocq theorem-proving agents, with few-shot bootstrapping being most effective but still inferior to carefully engineered state-of-the-art agents.


<details>
  <summary>Details</summary>
Motivation: To investigate whether automatic optimization methods can replace manual fine-tuning in real-world AI agents, specifically for automated theorem proving in Rocq, which represents a challenging formal verification domain.

Method: Evaluated various automatic agent optimizers on a Rocq proof-generation agent, assessing automation of prompt design, contextual knowledge, and control strategies that typically require manual engineering.

Result: Several optimizers yielded measurable improvements, with simple few-shot bootstrapping being the most consistently effective method, but none matched the performance of carefully engineered state-of-the-art proof agents.

Conclusion: While automatic optimization methods show promise and can provide measurable improvements, they currently cannot replace the careful engineering required for state-of-the-art performance in complex domains like automated theorem proving.

Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.

</details>


### [163] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: Proposes a fully-asynchronous policy training framework for Vision-Language-Action models that addresses resource underutilization in synchronous RL training, achieving up to 126.67% throughput improvement.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based training frameworks for VLA models suffer from synchronous execution bottlenecks, causing severe resource underutilization and throughput limitations during environment interaction, policy generation, and model update phases.

Method: A fully-asynchronous policy training framework with multi-level decoupled architecture: asynchronous parallelization of environment interaction/trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates.

Result: On LIBERO benchmark: up to 59.25% throughput improvement vs synchronous strategies; up to 126.67% improvement with deep optimization; validated across diverse VLA models/environments; excellent scalability from 8 to 256 GPUs.

Conclusion: The proposed asynchronous framework effectively addresses training efficiency bottlenecks in VLA models, demonstrating significant throughput improvements and scalability while maintaining training effectiveness.

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [164] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI is a specialized financial language model for Indian digital payments, with Base and Instruct variants built on Mistral Small 24B architecture, achieving significant improvements in finance reasoning and tool-calling while maintaining general performance.


<details>
  <summary>Details</summary>
Motivation: To develop a domain-specialized financial language model specifically for Indian digital payment systems, addressing the need for models that understand financial workflows, multilingual content (English, Hindi, Hinglish), and real-world payment scenarios.

Method: Multi-stage training pipeline: 1) Continuous pre-training on 68B tokens of curated financial, multilingual, and synthetic data, 2) Instruction fine-tuning, 3) Domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations modeling real-world workflows like transaction disputes and mandate lifecycle management.

Result: FiMI Base achieves 20% improvement over Mistral Small 24B Base on finance reasoning benchmarks; FiMI Instruct outperforms Mistral Small 24B Instruct by 87% on domain-specific tool-calling; both maintain comparable performance to similar-sized models on general benchmarks.

Conclusion: FiMI successfully demonstrates that domain specialization for Indian financial systems yields substantial performance improvements in finance reasoning and tool-calling capabilities while preserving general language understanding abilities.

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [165] [NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking](https://arxiv.org/abs/2602.05805)
*Kang Chen,Zhuoka Feng,Sihan Zhao,Kai Xiong,Junjie Nian,Yaoning Wang,Changyi Xiao,Yixin Cao*

Main category: cs.AI

TL;DR: NEX is an unsupervised framework that scores reasoning quality by analyzing neuron activation patterns to distinguish exploration vs exploitation phases, predicting accuracy without task supervision.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly use multi-step reasoning (chain-of-thought) and model merging, the bottleneck shifts from generation to selection, often without supervision on target distributions. Current exploration proxies show inverted-U relationship with accuracy, suggesting redundant exploration causes "overthinking."

Method: NEX views reasoning as alternating exploration (E-phase) and exploitation (X-phase). It detects E-phase via spikes in newly activated MLP neurons per token from sparse activation caches, uses a sticky two-state HMM to infer E-X phases, and credits E-introduced neurons based on whether they're reused in subsequent X spans.

Result: NEX produces interpretable neuron weights and a single Good-Mass Fraction score that ranks candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on small unlabeled activation sets predicts downstream accuracy and identifies better variants.

Conclusion: NEX provides a white-box, label-free unsupervised scoring framework for reasoning quality that can effectively rank responses and model variants, validated through human annotations and causal evidence via neuron transfer experiments.

Abstract: Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via "Effective-vs-Redundant" neuron transfer.

</details>


### [166] [STProtein: predicting spatial protein expression from multi-omics data](https://arxiv.org/abs/2602.05811)
*Zhaorui Jiang,Yingfang Yuan,Lei Hu,Wei Pang*

Main category: cs.AI

TL;DR: STProtein uses graph neural networks with multi-task learning to predict spatial protein expression from spatial transcriptomics data, addressing the scarcity of spatial proteomics data.


<details>
  <summary>Details</summary>
Motivation: Spatial proteomics data is scarce due to technical limitations and high costs, creating an imbalance with more abundant spatial transcriptomics data, which impedes spatial multi-omics integration in biological research.

Method: Proposes STProtein, a novel framework using graph neural networks with multi-task learning strategy to predict unknown spatial protein expression from more accessible spatial transcriptomics data.

Result: STProtein can accurately predict spatial protein expression, addressing the scarcity of spatial proteomics data and enabling integration of spatial multi-omics data.

Conclusion: STProtein accelerates spatial multi-omics integration, enables discovery of hidden spatial protein patterns, reveals novel gene-protein relationships, and explores biological "Dark Matter," potentially catalyzing transformative breakthroughs in life sciences.

Abstract: The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological "Dark Matter".

</details>


### [167] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker: An LLM agent with autonomous planning and adaptive retrieval for temporal knowledge graph QA, using dual-training (SFT + RL) to improve reasoning under temporal constraints.


<details>
  <summary>Details</summary>
Motivation: Current LLM prompting strategies for TKGQA have two main limitations: (1) prone to reasoning hallucinations under complex temporal constraints, and (2) static prompting limits model autonomy and generalization by lacking optimization through dynamic interaction with TKGs.

Method: Proposes TKG-Thinker agent with autonomous planning and adaptive retrieval capabilities. Uses dual-training strategy: first SFT with chain-of-thought data to instill planning capabilities, then RL stage with multi-dimensional rewards to refine reasoning policies under temporal constraints.

Result: Achieves state-of-the-art performance on benchmark datasets with three open-source LLMs and exhibits strong generalization across complex TKGQA settings.

Conclusion: TKG-Thinker effectively addresses limitations of current prompting strategies by enabling dynamic multi-turn interactions with TKGs and refined reasoning through dual-training, demonstrating superior performance and generalization in temporal knowledge graph QA.

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [168] [Learning Compact Boolean Networks](https://arxiv.org/abs/2602.05830)
*Shengpu Wang,Yuhao Mao,Yani Zhang,Martin Vechev*

Main category: cs.AI

TL;DR: Proposes three innovations for Boolean neural networks: learned connections without extra parameters, compact convolutions exploiting locality, and adaptive discretization, achieving up to 37x fewer Boolean operations with better accuracy than prior work.


<details>
  <summary>Details</summary>
Motivation: Floating-point neural networks have high inference costs, making Boolean networks attractive for resource-constrained settings, but learning compact and accurate Boolean networks is challenging due to their combinatorial nature.

Method: Three-pronged approach: 1) Learned connections strategy with no additional parameters and negligible computational overhead; 2) Compact convolutional Boolean architecture exploiting locality with reduced Boolean operations; 3) Adaptive discretization strategy to reduce accuracy drop when converting continuous networks to Boolean.

Result: Extensive results on standard vision benchmarks show the Pareto front of accuracy vs. computation significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.

Conclusion: The proposed three-angle approach successfully addresses the challenge of learning compact and accurate Boolean networks, making them more practical for resource-constrained applications while maintaining competitive accuracy.

Abstract: Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.

</details>


### [169] [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847)
*Zhangquan Chen,Jiale Tao,Ruihuang Li,Yihao Hu,Ruitao Chen,Zhantao Yang,Xinlei Yu,Haodong Jing,Manyuan Zhang,Shuai Shao,Biao Wang,Qinglin Lu,Ruqi Huang*

Main category: cs.AI

TL;DR: OmniVideo-R1 is a reinforced framework that improves audio-visual understanding through query-intensive grounding and modality-attentive fusion, outperforming baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing omnivideo models struggle with audio-visual understanding despite humans' ability to synergistically process multiple modalities for holistic perception.

Method: Two key strategies: (1) query-intensive grounding based on self-supervised learning, and (2) modality-attentive fusion built upon contrastive learning paradigms.

Result: Extensive experiments show OmniVideo-R1 consistently outperforms strong baselines on multiple benchmarks, demonstrating effectiveness and robust generalization capabilities.

Conclusion: OmniVideo-R1 successfully enables models to "think with omnimodal cues" through its reinforced framework, advancing audio-visual understanding in omnivideo models.

Abstract: While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.

</details>


### [170] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: BABE is a new benchmark for evaluating AI's experimental reasoning in biology, using real research papers to test causal reasoning and cross-scale inference.


<details>
  <summary>Details</summary>
Motivation: Existing biology benchmarks fail to assess AI's ability to integrate experimental results with contextual knowledge to derive meaningful conclusions like real researchers do.

Method: Constructed BABE from peer-reviewed research papers and real-world biological studies to reflect the complexity and interdisciplinary nature of actual scientific inquiry.

Result: BABE provides a robust framework for assessing how well AI systems can reason like practicing scientists, challenging models with causal reasoning and cross-scale inference tasks.

Conclusion: BABE offers a more authentic measure of AI's potential to contribute to biological research by evaluating experimental reasoning capabilities using real scientific contexts.

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [171] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: HSAP solves optimal hierarchical team seating assignments using PRM/RRT distance calculations combined with heuristic search and integer programming.


<details>
  <summary>Details</summary>
Motivation: Large organizations need to seat hierarchically related teams together (e.g., research groups in contiguous areas), but current manual processes lead to infrequent and suboptimal replanning.

Method: End-to-end framework using probabilistic road maps (PRM) and rapidly-exploring random trees (RRT) to calculate seat distances, combined with heuristic search and dynamic programming via integer programming.

Result: Demonstrated approach under different sized instances with quantitative and qualitative evaluation of PRM framework and seating allocations.

Conclusion: Proposed framework automates and optimizes hierarchical seating allocation, addressing scalability and efficiency challenges of manual planning.

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [172] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: AgentHeLLM is a threat modeling framework for LLM-based vehicle assistants that separates asset identification from attack path analysis to address security gaps in AI-automotive systems.


<details>
  <summary>Details</summary>
Motivation: LLM-based conversational agents in vehicles create novel security challenges at the intersection of AI, automotive safety, and inter-agent communication. Existing AI security frameworks lack proper "separation of concerns" by mixing asset protection with attack path analysis, creating methodological gaps for safety-critical systems.

Method: Proposes AgentHeLLM framework with: 1) human-centric asset taxonomy derived from harm-oriented "victim modeling" inspired by Universal Declaration of Human Rights, 2) formal graph-based model distinguishing poison paths (malicious data propagation) from trigger paths (activation actions), and 3) open-source AgentHeLLM Attack Path Generator tool using bi-level search strategy for automated multi-stage threat discovery.

Result: Developed a practical threat modeling framework that formally separates asset identification from attack path analysis, demonstrated through an open-source tool that automates multi-stage threat discovery for LLM-based vehicle assistants.

Conclusion: AgentHeLLM addresses critical security gaps in LLM-based automotive systems by providing a structured, human-centric approach to threat modeling that properly separates concerns between what needs protection and how attacks occur, enabling more rigorous security analysis for safety-critical vehicle AI systems.

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [173] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: LLMs are widely used in Modeling & Simulation but common practices can introduce subtle issues; this paper provides practical guidance on principled design choices, diagnostics, and evaluation for effective LLM use in M&S workflows.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in Modeling & Simulation workflows, but many common practices that appear straightforward actually introduce subtle issues, unnecessary complexity, or inferior results. There's a need for comprehensive guidance to help modelers make informed decisions about LLM usage.

Method: The paper provides practical guidance through discussion of common sources of confusion including non-determinism, knowledge augmentation (RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. It emphasizes principled design choices, diagnostic strategies, and empirical evaluation.

Result: The paper identifies key pitfalls in LLM usage for M&S: adding more data can backfire (model collapse, guardrail removal), unnecessary fine-tuning without prior assessment, temperature 0 not guaranteeing determinism, and improper handling of large M&S data volumes.

Conclusion: Modelers need comprehensive guidance to navigate LLM usage in M&S effectively. The paper aims to help them make informed decisions about when, how, and whether to rely on LLMs through principled approaches, diagnostics, and empirical evaluation.

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [174] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: Quantum-enhanced RL models outperform classical approaches for Capacitated Vehicle Routing Problem, with hybrid quantum-classical architecture achieving best performance across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Address the Capacitated Vehicle Routing Problem (CVRP) by exploring whether quantum-enhanced reinforcement learning approaches can outperform classical methods for complex combinatorial optimization problems.

Method: Implemented Advantage Actor-Critic (A2C) agents in three variants: classical, full quantum, and hybrid. Integrated transformer architectures with self- and cross-attention mechanisms to capture relationships between vehicles, clients, and depot. Tested on multi-vehicle scenarios with 20 clients and 4 vehicles over ten independent runs.

Result: All three approaches learned effective routing policies, but quantum-enhanced models outperformed classical baseline. Hybrid architecture achieved best overall performance across routing distance, route compactness, and route overlap metrics. Quantum-based models generated more structured and coherent routing solutions.

Conclusion: Hybrid quantum-classical reinforcement learning models show significant potential for addressing complex combinatorial optimization problems like CVRP, demonstrating both quantitative improvements and qualitative advantages in solution structure.

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [175] [Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins](https://arxiv.org/abs/2602.05983)
*Krešimir Kušić,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: GATTF model uses geographical relationships between sensors via mutual information to improve motorway traffic forecasting accuracy without increasing model complexity.


<details>
  <summary>Details</summary>
Motivation: Digital twins for motorway traffic management need accurate traffic predictions, but current sequence-based deep learning models have limitations in forecasting accuracy and complexity despite advantages in capturing temporal dependencies.

Method: Introduces Geographically-aware Transformer-based Traffic Forecasting (GATTF) model that exploits geographical relationships between distributed sensors using their mutual information (MI).

Result: Evaluation using real-time data from Geneva motorway network shows GATTF enhances forecasting accuracy compared to standard Transformer without increasing model complexity.

Conclusion: Incorporating geographical awareness through mutual information improves motorway traffic forecasting accuracy, addressing limitations of existing sequence-based deep learning models.

Abstract: The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.

</details>


### [176] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: Whisper ASR model with attention-based pooling achieves state-of-the-art SER results, showing intermediate layers work best for Persian and offering lightweight alternative to larger models.


<details>
  <summary>Details</summary>
Motivation: Speech Emotion Recognition research is limited by lack of standard large datasets, and pre-trained models like Whisper offer potential for extracting emotional features from speech.

Method: Proposed two attention-based pooling methods (Multi-head Attentive Average Pooling and QKV Pooling) to reduce dimensionality of Whisper representations while preserving emotional features. Tested on English (IEMOCAP) and Persian (ShEMO) datasets using Whisper Tiny and Small models.

Result: Multi-head QKV architecture achieved state-of-the-art results on ShEMO dataset with 2.47% improvement in unweighted accuracy. Intermediate Whisper encoder layers performed better for Persian SER, providing lightweight alternative to larger models like HuBERT X-Large.

Conclusion: Whisper shows strong potential as representation extractor for SER, with attention-based pooling effectively reducing dimensionality while preserving emotional features, offering efficient alternative to much larger models.

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [177] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay is a benchmark for evaluating multi-agent buyer-seller negotiation through natural language, featuring 110+ tasks with metrics for feasibility, efficiency, and welfare.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple LLM-based agents, despite increasing expectations for autonomous negotiation, coordination, and transactions.

Method: Introduces a simulation framework modeling markets where buyers/sellers have private constraints and product-dependent valuations, requiring multi-round linguistic negotiation rather than numeric bidding alone. Includes structured action extraction and supports tasks from bilateral bargaining to many-to-many markets.

Result: Benchmarking state-of-the-art LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning.

Conclusion: AgenticPay establishes a foundation for studying agentic commerce and language-based market interaction, with code and dataset publicly available.

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [178] [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039)
*Yuxing Lu,Yucheng Hu,Xukai Zhao,Jiuxin Cao*

Main category: cs.AI

TL;DR: DyTopo is a dynamic multi-agent framework that reconstructs sparse communication graphs each round using semantic matching of agent needs/offers, outperforming fixed-pattern baselines by 6.2% on average.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM systems use fixed communication patterns that don't adapt to the stage-dependent needs of iterative problem solving, limiting their effectiveness in multi-round reasoning tasks.

Method: DyTopo uses a manager-guided approach where each agent outputs natural-language query (need) and key (offer) descriptors each round. These are embedded and semantically matched to reconstruct a sparse directed communication graph, routing private messages only along induced edges.

Result: Across code generation and mathematical reasoning benchmarks with four LLM backbones, DyTopo consistently outperforms the strongest baseline by an average of +6.2%. It also provides interpretable coordination traces via evolving graphs.

Conclusion: Dynamic, stage-adaptive communication graphs significantly improve multi-agent reasoning performance over fixed patterns, while providing interpretable coordination traces that reveal how communication pathways reconfigure across rounds.

Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: People consistently show varied social signals to robot errors in real-world settings, especially in group interactions, though these signals are rich but noisy.


<details>
  <summary>Details</summary>
Motivation: Little is known about people's social responses to robot errors outside lab settings, especially with non-social robots in repeated/group interactions with successive errors.

Method: Built a coffee robot and conducted a public field deployment with 49 participants to observe social responses to errors in real-world HRI.

Result: Participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Social signals were rich (volunteering information) but "noisy."

Conclusion: Social signals in real-world HRI are valuable for error management but present challenges due to their noisy nature. Discusses lessons, benefits, and challenges for using social signals in practical robot deployments.

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [180] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: Differentiable neuro-graphics model combines neural foundation models with physics-based differentiable rendering for zero-shot scene reconstruction and robot grasping from single RGBD image and bounding boxes, outperforming existing methods without 3D data or test-time samples.


<details>
  <summary>Details</summary>
Motivation: Robots need to operate in novel environments with unseen objects, but current methods require large training datasets and test-time sampling. There's a need for more data-efficient, interpretable approaches that can perform zero-shot scene understanding and interaction.

Method: Combines neural foundation models with physics-based differentiable rendering to solve constrained optimization problems for estimating physically consistent scene parameters (meshes, lighting, materials, 6D poses) from single RGBD image and bounding boxes.

Result: Outperforms existing algorithms on standard model-free few-shot pose estimation benchmarks. Successfully validated on zero-shot grasping tasks, demonstrating accurate scene reconstructions.

Conclusion: The approach enables zero-shot, physically-consistent scene reconstruction and grasping without extensive datasets or test-time sampling, offering a pathway toward more data-efficient, interpretable, and generalizable robot autonomy in novel environments.

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [181] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: Neuro-symbolic RL pipeline with semantic/spatial features and soft logic rewards improves autonomous driving safety and robustness in CARLA simulations.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based RL approaches lack high-level scene context integration and rely on rigid reward functions, limiting their ability to handle complex driving scenarios with safety-critical considerations.

Method: Proposes neuro-symbolic feature representation combining semantic, spatial, and shape information with spatially boosted dynamic entity features, plus Soft First-Order Logic (SFOL) reward function that uses symbolic reasoning to balance human values via linguistic rules applied to segmentation map predicates.

Result: Quantitative experiments in CARLA show improved policy robustness and safety metrics across varying traffic densities and occlusion levels compared to baseline representations and reward formulations.

Conclusion: Integrating holistic neuro-symbolic representations and soft reasoning into RL enables more context-aware and value-aligned decision-making for autonomous driving.

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [182] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: New optimization IK formulation using analytic IK as change of variables achieves higher success rates across challenging problems including collision avoidance and humanoid stability.


<details>
  <summary>Details</summary>
Motivation: Existing IK methods have complementary strengths: analytic methods are fast but limited in scope, while optimization methods are flexible but struggle with nonlinear relationships and nonconvex constraints like collision avoidance. A unified approach that leverages both methods' advantages has been challenging to develop.

Method: Proposes a new optimization IK formulation that uses an analytic IK solution as a change of variables, making the problem fundamentally easier for optimizers to solve. This approach transforms the complex nonlinear relationship between joint angles and end-effector pose into a more tractable form.

Result: Extensive experiments with three popular solvers (representing different constrained nonlinear optimization paradigms) show the new formulation achieves higher success rates than previous formulations and baseline methods across various challenging IK problems including collision avoidance, grasp selection, and humanoid stability.

Conclusion: The proposed formulation successfully combines the strengths of analytic and optimization IK methods, creating a unified approach that overcomes the limitations of each individual method and significantly improves performance on complex IK problems with additional constraints.

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [183] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: PLATO Hand: dexterous robotic hand with hybrid fingertip (rigid fingernail in compliant pulp) enables diverse interaction modes and improves manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To create a robotic hand that can perform precise manipulation tasks by shaping contact behavior through structured fingertip geometry, enabling diverse interaction modes across different object geometries.

Method: Developed a dexterous robotic hand with hybrid fingertip design (rigid fingernail embedded in compliant pulp), created a strain-energy-based bending-indentation model to guide design, and used guided contact to preserve local indentation while suppressing global bending.

Result: Experimental results show improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks including paper singulation, card picking, and orange peeling.

Conclusion: Coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [184] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: Bridging classical lawnmower surveys with informative path planning by developing algorithms for shortest paths with guaranteed Gaussian-process posterior variance constraints for environmental monitoring robots.


<details>
  <summary>Details</summary>
Motivation: Environmental monitoring robots need efficient spatial field reconstruction under tight constraints. Classical lawnmower surveys waste effort oversampling predictable regions, while informative path planning lacks guarantees on reconstruction quality.

Method: Three-stage approach: (1) learn GP model from prior info, (2) transform kernel to binary coverage maps showing which locations reduce uncertainty below target, (3) plan near-shortest route whose combined coverage satisfies global uncertainty constraint. Includes nonstationary kernels for heterogeneity and handles obstacles.

Result: Algorithms with provable approximation guarantees for sensing-location selection and joint selection-routing under travel budget. Experiments show fewer sensing locations and shorter travel distances than baseline while meeting uncertainty targets. Field experiments with autonomous vehicles demonstrate real-world feasibility.

Conclusion: Successfully bridges classical coverage and informative path planning by providing guaranteed uncertainty bounds while maintaining efficiency, enabling practical environmental monitoring under resource constraints.

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [185] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: MobileManiBench: A large-scale simulation benchmark for mobile robotic manipulation with 300K trajectories, 630 objects, 5 skills, and 100 scenes to evaluate vision-language-action models.


<details>
  <summary>Details</summary>
Motivation: Current vision-language-action models rely on limited teleoperation datasets dominated by static tabletop scenes, lacking diversity and scalability for real-world mobile manipulation tasks.

Method: Simulation-first framework using NVIDIA Isaac Sim with reinforcement learning to autonomously generate diverse manipulation trajectories with rich annotations including language instructions, multi-view images, and synchronized states/actions.

Result: Created MobileManiBench with 2 mobile platforms, 2 cameras, 630 objects in 20 categories, 5 manipulation skills across 100 scenes, yielding 300K trajectories for controlled studies of robot embodiments and policy architectures.

Conclusion: The benchmark enables scalable evaluation of VLA models, accelerating research on data efficiency and generalization for mobile manipulation in complex environments.

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [186] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: A minimal-sensing approach using only IMU, pressure sensor, and two sonars enables autonomous underwater robots to center and traverse flooded pipes without complex sensor arrays or external tracking.


<details>
  <summary>Details</summary>
Motivation: Autonomous underwater inspection of submerged pipelines is difficult due to confined spaces, poor visibility (turbidity), and lack of reliable localization cues. Current approaches often require expensive/complex sensor arrays or external tracking systems.

Method: Uses IMU, pressure sensor, downward-facing single-beam sonar, and rotating 360° sonar. Develops efficient method to extract range estimates from noisy sonar data, uses geometric model with two sonar ranges to estimate pipe center, and implements adaptive confidence-weighted PD controller for alignment.

Result: Successfully demonstrated stable centering and full-pipe traversal in submerged 46 cm-diameter pipe using BlueROV2, despite ambient flow and structural deformations. System works without Doppler velocity log or external tracking.

Conclusion: Reliable in-pipe navigation can be achieved with lightweight, computationally efficient sensing architecture, advancing practicality of autonomous underwater inspection in confined environments.

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [187] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: AIDE is a dual-stream framework that combines interactive exploration with vision-language reasoning to help robots handle ambiguous instructions by identifying task-relevant objects through zero-shot affordance analysis.


<details>
  <summary>Details</summary>
Motivation: Existing VLM-based methods struggle with enabling robots to explore and act in unfamiliar environments under ambiguous human instructions (e.g., "I'm thirsty" requiring identification of cups or beverages), due to inefficient reasoning and lack of environmental interaction that hinder real-time task planning and execution.

Method: AIDE is a dual-stream framework integrating interactive exploration with vision-language reasoning. It uses Multi-Stage Inference (MSI) as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions.

Result: Extensive experiments in simulation and real-world environments show AIDE achieves over 80% task planning success rate and more than 95% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

Conclusion: AIDE successfully addresses the challenge of enabling robots to handle ambiguous instructions in unfamiliar environments through its dual-stream framework that combines interactive exploration with vision-language reasoning, demonstrating superior performance over existing methods.

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


### [188] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: PAiD is a progressive three-stage architecture for humanoid soccer skills: motion-skill acquisition via human tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Soccer presents significant challenges for humanoid robots requiring integrated perception-action capabilities. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks.

Method: Three-stage progressive architecture: 1) motion-skill acquisition via human motion tracking, 2) lightweight perception-action integration for positional generalization, and 3) physics-aware sim-to-real transfer.

Result: Experiments on Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions (static/rolling balls, various positions, disturbances) and consistent execution across indoor/outdoor scenarios.

Conclusion: The divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition.

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [189] [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325)
*Jiacheng Fan,Zhiyue Zhao,Yiqian Zhang,Chao Chen,Peide Wang,Hengdi Zhang,Zhengxue Cheng*

Main category: cs.RO

TL;DR: Real-Sim-Real pipeline transforms human demonstrations into robot training data without teleoperation, achieving 80-84% success rates on dexterous manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Large-scale, high-fidelity robot demonstration data is a critical bottleneck for scaling Vision-Language-Action models in dexterous manipulation, and teleoperation is expensive and difficult to scale.

Method: Build standardized data collection rooms to capture multimodal human demonstrations, then use tactile-aware retargeting to map human hand states to robot dex-hand states via geometry and force-guided optimization, and render retargeted trajectories in photorealistic Isaac Sim environment.

Result: Retargeted dex-hand trajectories achieve 84% success rate across 10 diverse object manipulation tasks; VLA policies trained on generated data achieve 80% average success rate on pick-and-place, pushing, and pouring tasks.

Conclusion: Robot training data can be efficiently "painted" from human demonstrations using the real-sim-real pipeline, offering a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

Abstract: Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently "painted" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

</details>


### [190] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: BusyBox is a physical benchmark for evaluating Vision-Language-Action models' affordance generalization across 6 modular components with switches, sliders, wires, buttons, display, and dial.


<details>
  <summary>Details</summary>
Motivation: While VLAs show promise for generalization, they need affordance generalization - the ability to manipulate new objects with familiar physical features. Current evaluation lacks systematic physical benchmarks for this capability.

Method: Created BusyBox with 6 interchangeable modules that can be swapped/rotated to create visual variations while maintaining same affordances. Includes CAD files for 3D printing, bill of materials, and language-annotated demonstration dataset collected using Mobile Aloha robot.

Result: Generalization across BusyBox variants is highly challenging even for strong open-weights VLAs like π₀.₅ and GR00T-N1.6, demonstrating the benchmark's difficulty.

Conclusion: BusyBox provides accessible physical benchmark for evaluating VLAs' affordance generalization, with open-source materials to enable community evaluation and new affordance generalization experiments.

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [191] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: RSTM2 is an ontology-driven methodology using stochastic timed Petri nets to bridge high-level objectives with formal specifications for robotic systems, enabling simulation-based analysis and autonomous specification synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the gap between high-level objectives and formal, executable specifications in safety- and mission-critical robotic systems, particularly for complex multi-robot systems like NASA's CADRE mission.

Method: Robotic System Task to Model Transformation Methodology (RSTM2) - an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels.

Result: The method supports architectural trades, resource allocation, and performance analysis under uncertainty, and enables explainable AI-based assistants for autonomous specification synthesis.

Conclusion: RSTM2 offers significant benefits for complex multi-robot systems, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [192] [TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation](https://arxiv.org/abs/2602.05468)
*Pranav Ponnivalavan,Satoshi Funabashi,Alexander Schmitz,Tetsuya Ogata,Shigeki Sugano*

Main category: cs.RO

TL;DR: TaSA: A deep learning framework that enables robotic hands to distinguish self-touch from object contact using sensory attenuation principles, improving dexterous manipulation success rates.


<details>
  <summary>Details</summary>
Motivation: Robotic hands struggle with dexterous manipulation involving multiple finger contacts because they can't distinguish between self-contact tactile sensations and external object contact, leading to potential damage. Humans solve this through sensory attenuation, but most robotic approaches simply avoid self-contact, limiting real-world applicability.

Method: Two-phased deep predictive learning framework: Phase 1 learns self-touch dynamics by modeling how robot actions generate tactile feedback. Phase 2 incorporates this learned model into motion learning to emphasize object contact signals during manipulation.

Result: TaSA policies achieved significantly higher success rates than baseline methods across three insertion tasks: pencil lead into mechanical pencil, coins into slot, and paper clip onto paper, with various orientations, positions, and sizes.

Conclusion: Structured tactile perception with self-touch modeling based on sensory attenuation principles is critical for dexterous robotic manipulation, enabling robots to distinguish self-contact from object contact for complex in-hand manipulations.

Abstract: Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.

</details>


### [193] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: DECO is a DiT-based policy framework for dexterous manipulation that decouples multimodal conditioning and includes DECO-50, a large-scale bimanual manipulation dataset with tactile sensing.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating multiple sensory modalities (vision, proprioception, tactile) in dexterous manipulation policies, and to provide a comprehensive dataset for bimanual manipulation research.

Method: Uses DiT-based policy with decoupled multimodal conditioning: image and action tokens interact via joint self-attention, proprioceptive states via adaptive layer normalization, tactile signals via cross-attention, and includes LoRA-based adapter for efficient fine-tuning.

Result: Developed DECO framework and DECO-50 dataset containing 4 scenarios, 28 sub-tasks, 50+ hours of data, ~5 million frames, and 8,000 successful trajectories for bimanual dexterous manipulation with tactile sensing.

Conclusion: DECO provides an effective framework for multimodal dexterous manipulation and DECO-50 offers a valuable large-scale dataset to advance research in bimanual manipulation with tactile feedback.

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [194] [Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments](https://arxiv.org/abs/2602.05516)
*Runxiao Liu,Pengda Mao,Xiangli Le,Shuang Gu,Yapeng Chen,Quan Quan*

Main category: cs.RO

TL;DR: Novel control framework for multi-UAV cooperative transportation of cable-suspended loads in constrained environments using virtual tube theory and dissipative systems theory.


<details>
  <summary>Details</summary>
Motivation: Need for efficient multi-UAV collaboration in obstacle-rich environments for cable-suspended load transportation with low computational overhead and high stability.

Method: Combines virtual tube theory with dissipative systems theory to enable tension distribution, coordinated transportation, and dynamic UAV configuration adaptation based on obstacle layouts.

Result: Framework achieves efficient navigation with low computational overhead, high stability, robustness, and scalability for large-scale multi-UAV systems validated through simulations and outdoor experiments.

Conclusion: Proposed approach is effective, practical, and robust for real-world cooperative transportation of cable-suspended loads by multiple UAVs in constrained environments.

Abstract: This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.

</details>


### [195] [VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552)
*Bessie Dominguez-Dager,Sergio Suescun-Ferrandiz,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla*

Main category: cs.RO

TL;DR: VLN-Pilot uses large Vision-and-Language Models as autonomous drone pilots for indoor navigation, interpreting natural language instructions and planning trajectories in GPS-denied environments with high success rates.


<details>
  <summary>Details</summary>
Motivation: Traditional drone navigation in indoor environments relies on rule-based or geometric path-planning approaches that require extensive task-specific engineering. There's a need for more flexible, human-friendly control systems that can understand natural language instructions and adapt to dynamic indoor environments for applications like inspection, search-and-rescue, and facility monitoring.

Method: VLN-Pilot leverages a large Vision-and-Language Model (VLLM) as an autonomous pilot that interprets free-form natural language instructions and grounds them in visual observations. The framework integrates language-driven semantic understanding with visual perception to plan and execute drone trajectories, supporting spatial reasoning, obstacle avoidance, and dynamic reactivity to unforeseen events in GPS-denied indoor environments.

Result: The framework was validated on a custom photorealistic indoor simulation benchmark, achieving high success rates on complex instruction-following tasks including long-horizon navigation with multiple semantic targets. Results demonstrate the VLLM-driven agent's ability to perform autonomous navigation effectively.

Conclusion: VLLM-based pilots show promise for replacing remote human operators in indoor drone navigation, potentially reducing operator workload while improving safety and mission flexibility. This approach opens avenues for scalable, human-friendly control of indoor UAVs across various applications.

Abstract: This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

</details>


### [196] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: TOLEBI: A fault-tolerant reinforcement learning framework for bipedal locomotion that handles joint locking, power loss, and external disturbances through simulation training and real-world deployment with online joint status monitoring.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning approaches for bipedal locomotion achieve high success rates but lack robustness to hardware faults and environmental disturbances that can occur in real-world settings, potentially causing severe consequences.

Method: Developed TOLEBI framework that injects joint locking, power loss, and external disturbances in simulation to learn fault-tolerant locomotion strategies, then transfers policies to real robots via sim-to-real transfer with an online joint status module that classifies joint conditions using runtime observations.

Result: Validation experiments conducted both in simulation and real-world with humanoid robot TOCABI demonstrate the applicability of the approach, providing the first learning-based fault-tolerant framework for bipedal locomotion.

Conclusion: TOLEBI represents the first learning-based fault-tolerant framework for bipedal locomotion, advancing the development of efficient learning methods in robotics by addressing critical real-world challenges of hardware faults and disturbances.

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [197] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: HiCrowd: Hierarchical RL+MPC framework for robot crowd navigation that uses pedestrian motion as guidance to reduce freezing behaviors.


<details>
  <summary>Details</summary>
Motivation: Addressing the freezing robot problem in dense human crowds where robots struggle to find safe motions and get stuck, requiring better integration of crowd dynamics into navigation.

Method: Hierarchical framework combining reinforcement learning (RL) with model predictive control (MPC). High-level RL generates follow points to align with compatible pedestrian groups, while low-level MPC safely tracks guidance with short horizon planning.

Result: Outperforms reactive and learning-based baselines in both offline (replaying recorded trajectories) and online (simulated reactive humans) settings on real-world and synthetic datasets, showing improved navigation efficiency, safety, and reduced freezing behaviors.

Conclusion: Leveraging human motion as guidance rather than treating humans solely as dynamic obstacles provides a powerful principle for safe and efficient robot navigation in crowds.

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [198] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: Neuromorphic control framework bridges reactive sensor-based control with model-based planning for vision-guided navigation using dynamic neuronal populations and bifurcation mechanisms to resolve indecision.


<details>
  <summary>Details</summary>
Motivation: Address the historical struggle in robotic navigation to reconcile reactive, sensor-based control with model-based planners, particularly when absence of predominant options leads to indecision that challenges reactive systems without computationally-intensive planners.

Method: Parsimonious neuromorphic control framework where image pixels from onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands, using dynamic bifurcation mechanism to resolve indecision by delaying commitment until critical point induced by environmental geometry.

Result: Validated in simulation environments and on experimental quadrotor platform, providing real-time autonomy with minimal computational burden, small number of interpretable parameters, and seamless integration with application-specific image processing pipelines.

Conclusion: The neuromorphic controller bridges the gap between reactive and model-based approaches for vision-guided navigation and tracking, inspired by mechanistic models of animal cognition and opinion dynamics, offering practical real-time autonomy.

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [199] [Task-Oriented Robot-Human Handovers on Legged Manipulators](https://arxiv.org/abs/2602.05760)
*Andreea Tulbure,Carmen Scheidemann,Elias Steiner,Marco Hutter*

Main category: cs.RO

TL;DR: AFT-Handover is a zero-shot framework that uses LLM-driven affordance reasoning and texture-based transfer for generalizable task-oriented handovers between robots and humans.


<details>
  <summary>Details</summary>
Motivation: Existing task-oriented handover approaches rely on object- or task-specific affordances, limiting their generalization to novel scenarios where robots need to present objects to support human post-handover use.

Method: Given novel object-task pairs, the method retrieves proxy exemplars from a database, establishes part-level correspondences via LLM reasoning, and uses texture-based affordance transfer for feature-based point cloud manipulation.

Result: The framework shows improved handover success rates and stronger generalization across diverse task-object pairs, with user studies showing significant preference over state-of-the-art methods and reduced human regrasping before tool use.

Conclusion: AFT-Handover enables zero-shot, generalizable task-oriented handovers and demonstrates potential for real-world robot-human collaboration, including on legged manipulators.

Abstract: Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.

</details>


### [200] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: Single policy trained once can control diverse humanoid robots through cross-embodiment framework with morphological randomization and aligned observation/action spaces.


<details>
  <summary>Details</summary>
Motivation: Current learning-based whole-body controllers require robot-specific training, limiting their applicability. Need a universal controller that can generalize across different humanoid designs.

Method: XHugWBC framework with: (1) physics-consistent morphological randomization, (2) semantically aligned observation/action spaces across robots, (3) policy architectures modeling morphological/dynamical properties.

Result: Demonstrated strong generalization and robustness on 12 simulated and 7 real-world humanoid robots with zero-shot transfer to unseen robots.

Conclusion: Cross-embodiment training enables universal humanoid control through learning motion priors from diverse randomized embodiments, eliminating need for robot-specific training.

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [201] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: Learning-based multimodal terrain perception framework for humanoid robots using CNN-GRU hybrid architecture with depth camera, LiDAR, and IMU data fusion.


<details>
  <summary>Details</summary>
Motivation: Traditional terrain perception systems for humanoid robots rely on manually engineered, single-sensor pipelines, which are insufficient for reliable deployment in unstructured, human-centric environments.

Method: Proposes a learning-based framework using robot-centric heightmap representation with hybrid CNN-GRU architecture for spatial feature extraction and temporal consistency. Integrates multimodal data from Intel RealSense depth camera, LIVOX MID-360 LiDAR (via spherical projection), and onboard IMU.

Result: Multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Integration of 3.2s temporal context reduces mapping drift.

Conclusion: The learning-based multimodal framework with temporal integration provides more reliable terrain perception for humanoid robots in unstructured environments compared to traditional single-sensor approaches.

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [202] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: RRL approach combines nominal Cartesian controller with learned residual policy for precise container lifting with hydraulic loader crane in waste recycling task.


<details>
  <summary>Details</summary>
Motivation: Need for precise trajectory tracking and swing suppression in waste-container recycling tasks with tight geometric tolerances between hooks and container rings relative to crane scale.

Method: Residual reinforcement learning combines nominal admittance controller for trajectory tracking/swing damping with PPO-trained residual policy to compensate for unmodeled dynamics; uses domain randomization for generalization.

Result: Improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to nominal controller alone in simulation.

Conclusion: RRL approach effectively enhances precision and robustness for container lifting tasks without requiring end-to-end learning from scratch.

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [203] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: End-to-end toolchain converts benchtop impact tests into deployable safety governors for indoor drones, enabling practical tuning of motion limits based on measured impact risk.


<details>
  <summary>Details</summary>
Motivation: Indoor MAVs operate close to people but lack practical methods to tune motion limits based on measured impact risk, creating safety concerns for human-proximity operations.

Method: Three-part approach: 1) Replicable impact rig and protocol for force-time profiling, 2) Data-driven models mapping pre-impact speed to impulse and contact duration, 3) ROS2 node and scripts for online speed bound enforcement with logging.

Result: Validated on commercial quadrotors and indoor assets, showing governors preserve task throughput while meeting force constraints specified by safety stakeholders.

Conclusion: Provides practical bridge from measured impacts to runtime limits with shareable datasets, code, and repeatable process for certifying indoor MAV operations near humans.

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [204] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: VT-WM combines vision and tactile sensing to model contact physics, improving object permanence and motion compliance by 33% and 29% respectively, with 35% higher real-robot success rates in contact-rich tasks.


<details>
  <summary>Details</summary>
Motivation: Vision-only models often fail in contact-rich manipulation tasks due to occlusion and ambiguous contact states, leading to unrealistic physics violations like object teleportation or disappearance.

Method: Multi-task Visuo-Tactile World Models (VT-WM) that integrate visual and tactile sensing to capture contact physics, trained across various contact-rich manipulation tasks.

Result: 33% better object permanence, 29% better motion compliance in imagination, 35% higher success rates in real-robot experiments, and effective adaptation to novel tasks with limited demonstrations.

Conclusion: VT-WM's tactile grounding enables more physically realistic world models that significantly improve planning performance in contact-rich manipulation tasks and demonstrate strong generalization capabilities.

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


### [205] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: CommCP is an LLM-based decentralized communication framework using conformal prediction to enhance multi-agent coordination in embodied question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world robot deployments require multiple heterogeneous robots with different capabilities to cooperatively handle assignments, where effective information gathering and communication are crucial for coordination without redundancy.

Method: Proposes CommCP, a novel LLM-based decentralized communication framework that employs conformal prediction to calibrate generated messages, minimizing receiver distractions and enhancing communication reliability in multi-agent multi-task Embodied Question Answering (MM-EQA).

Result: Experimental results demonstrate that CommCP significantly enhances task success rate and exploration efficiency over baselines on the introduced MM-EQA benchmark with diverse, photo-realistic household scenarios.

Conclusion: CommCP effectively addresses the MM-EQA problem by improving communication reliability through conformal prediction-based message calibration, enabling better multi-agent coordination in embodied question answering tasks.

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>
